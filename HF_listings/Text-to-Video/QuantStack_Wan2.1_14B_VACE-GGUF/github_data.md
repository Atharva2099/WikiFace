# GitHub Data for QuantStack_Wan2.1_14B_VACE-GGUF

**Task Category:** Text-to-Video

## Repository 1: ggerganov/llama.cpp

# GitHub Repository Data

**Repository:** [ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)

## Basic Information

- **Description:** LLM inference in C/C++
- **Created:** 2023-03-10T18:58:00+00:00
- **Last Updated:** 2025-06-22T02:18:06+00:00
- **Last Pushed:** 2025-06-21T17:53:36+00:00
- **Default Branch:** master
- **Size:** 132319 KB

## Statistics

- **Stars:** 82,045
- **Forks:** 12,154
- **Watchers:** 82,045
- **Open Issues:** 773
- **Total Issues:** 0
- **Pull Requests:** 6,536

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/ggml-org/llama.cpp/blob/master/LICENSE)

## Languages

- **C++:** 7,918,004 bytes
- **C:** 1,950,997 bytes
- **Python:** 1,181,230 bytes
- **Cuda:** 891,619 bytes
- **Objective-C:** 316,989 bytes
- **Metal:** 293,879 bytes
- **Shell:** 248,538 bytes
- **HTML:** 165,429 bytes
- **CMake:** 163,201 bytes
- **TypeScript:** 139,910 bytes
- **JavaScript:** 100,120 bytes
- **Jinja:** 83,756 bytes
- **CSS:** 77,045 bytes
- **Makefile:** 51,739 bytes
- **Nix:** 24,416 bytes
- **Dockerfile:** 17,561 bytes
- **SCSS:** 1,387 bytes
- **Batchfile:** 802 bytes

## Topics

- `llama`
- `ggml`

## Top Contributors

1. **ggerganov** - 1301 contributions
2. **slaren** - 335 contributions
3. **ngxson** - 246 contributions
4. **JohannesGaessler** - 242 contributions
5. **danbev** - 122 contributions
6. **cebtenzzre** - 101 contributions
7. **ikawrakow** - 100 contributions
8. **jeffbolznv** - 91 contributions
9. **CISC** - 79 contributions
10. **ochafik** - 73 contributions

## File Structure (Sample of 10 files)

Total files: 1,546

- `.clang-format` (blob)
- `.clang-tidy` (blob)
- `.devops` (tree)
- `.devops/cloud-v-pipeline` (blob)
- `.devops/cpu.Dockerfile` (blob)
- `.devops/cuda.Dockerfile` (blob)
- `.devops/intel.Dockerfile` (blob)
- `.devops/llama-cli-cann.Dockerfile` (blob)
- `.devops/llama-cpp-cuda.srpm.spec` (blob)
- `.devops/llama-cpp.srpm.spec` (blob)

## Recent Issues

- 游릭 **#14320** Conv2D: Add CPU version (open)
- 游릭 **#14319** common : use std::string_view now that we target c++17 (open)
- 游릭 **#14318** Feature Request: Add support for moonshotai/Kimi-VL-A3B-Instruct (open)
- 游릭 **#14317** ggml-cpu: enable IBM NNPA Vector Intrinsics (open)
- 游릭 **#14316** ggml: adds CONV_2D op and direct GEMM Vulkan implementation (open)

## Recent Pull Requests

- 游릭 **#14320** Conv2D: Add CPU version (open)
- 游릭 **#14319** common : use std::string_view now that we target c++17 (open)
- 游릭 **#14317** ggml-cpu: enable IBM NNPA Vector Intrinsics (open)
- 游릭 **#14316** ggml: adds CONV_2D op and direct GEMM Vulkan implementation (open)
- 游댮 **#14314** gguf-py : fix Qwen3-Embedding eos token (closed)

## Recent Commits

- **aa0ef5c5** gguf-py : fix Qwen3-Embedding eos token (#14314) - Sigbj칮rn Skj칝ret (2025-06-21T16:12:05+00:00)
- **bb16041c** Add support for VK_EXT_debug_utils to add labels to Vulkan objects. (#13792) - Markus Tavenrath (2025-06-21T06:17:12+00:00)
- **58cba76a** gguf-py : fix TemplateProcessing pair when bos/eos is missing (#14312) - Sigbj칮rn Skj칝ret (2025-06-21T05:33:21+00:00)
- **67ae5312** metal : fix thread-safety (#14300) - Georgi Gerganov (2025-06-21T05:04:18+00:00)
- **692e3cdd** memory : rename interface to llama_memory_context_i (#14296) - Georgi Gerganov (2025-06-21T05:03:46+00:00)
- **b23fa0b3** convert : fix Llama 4 conversion (#14311) - Daniel Han (2025-06-21T04:32:01+00:00)
- **06cbedfc** sync : ggml - Georgi Gerganov (2025-06-20T17:50:24+00:00)
- **b7147673** Add `ggml_roll` (ggml/1274) - Acly (2025-06-18T11:34:50+00:00)
- **d860dd99** docs : fix the link to llama.h (#14293) - David Chiu (2025-06-20T17:43:35+00:00)
- **c959f462** CUDA: add conv_2d_transpose (#14287) - Aman Gupta (2025-06-20T14:48:24+00:00)

## External Links Found in README

- https://github.com/pythops/tenere
- https://ai.facebook.com/blog/large-language-model-llama-meta-ai/
- https://huggingface.co/collections/speakleash/bielik-11b-v23-66ee813238d9b526a072408a
- https://huggingface.co/collections/liuhaotian/llava-15-653aac15d994e992e2677a7e
- https://huggingface.co/databricks/dbrx-instruct
- https://huggingface.co/models?search=xverse
- https://huggingface.co/models?search=Smaug
- https://github.com/ggml-org/llama.cpp/pull/13012
- http://localhost:8080/v1/chat/completions
- https://bair.berkeley.edu/blog/2023/04/03/koala/
- https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct
- https://huggingface.co/gpt2
- https://huggingface.co/THUDM/glm-edge-4b-chat
- https://github.com/ggml-org/ggml/blob/master/docs/gguf.md
- https://github.com/LostRuins/koboldcpp
- https://github.com/ggml-org/llama.cpp/releases/download/b5046/llama-b5046-xcframework.zip",
- https://huggingface.co/vikhyatk/moondream2
- https://github.com/ggml-org/llama.cpp/pull/3187
- https://github.com/ggml-org/llama.cpp/discussions/10123
- https://huggingface.co/collections/inclusionAI/ling-67c51c85b34a7ea0aba94c32

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 612354784,
  "name": "llama.cpp",
  "full_name": "ggml-org/llama.cpp",
  "description": "LLM inference in C/C++",
  "html_url": "https://github.com/ggml-org/llama.cpp",
  "clone_url": "https://github.com/ggml-org/llama.cpp.git",
  "ssh_url": "git@github.com:ggml-org/llama.cpp.git",
  "homepage": "",
  "topics": [
    "llama",
    "ggml"
  ],
  "default_branch": "master",
  "created_at": "2023-03-10T18:58:00+00:00",
  "updated_at": "2025-06-22T02:18:06+00:00",
  "pushed_at": "2025-06-21T17:53:36+00:00",
  "size_kb": 132319,
  "watchers_count": 82045,
  "stargazers_count": 82045,
  "forks_count": 12154,
  "open_issues_count": 773,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/ggml-org/llama.cpp/blob/master/LICENSE"
  },
  "languages": {
    "C++": 7918004,
    "C": 1950997,
    "Python": 1181230,
    "Cuda": 891619,
    "Objective-C": 316989,
    "Metal": 293879,
    "Shell": 248538,
    "HTML": 165429,
    "CMake": 163201,
    "TypeScript": 139910,
    "JavaScript": 100120,
    "Jinja": 83756,
    "CSS": 77045,
    "Makefile": 51739,
    "Nix": 24416,
    "Dockerfile": 17561,
    "SCSS": 1387,
    "Batchfile": 802
  },
  "top_contributors": [
    {
      "login": "ggerganov",
      "contributions": 1301
    },
    {
      "login": "slaren",
      "contributions": 335
    },
    {
      "login": "ngxson",
      "contributions": 246
    },
    {
      "login": "JohannesGaessler",
      "contributions": 242
    },
    {
      "login": "danbev",
      "contributions": 122
    },
    {
      "login": "cebtenzzre",
      "contributions": 101
    },
    {
      "login": "ikawrakow",
      "contributions": 100
    },
    {
      "login": "jeffbolznv",
      "contributions": 91
    },
    {
      "login": "CISC",
      "contributions": 79
    },
    {
      "login": "ochafik",
      "contributions": 73
    },
    {
      "login": "phymbert",
      "contributions": 58
    },
    {
      "login": "0cc4m",
      "contributions": 46
    },
    {
      "login": "compilade",
      "contributions": 43
    },
    {
      "login": "KerfuffleV2",
      "contributions": 42
    },
    {
      "login": "rgerganov",
      "contributions": 38
    },
    {
      "login": "NeoZhangJianyu",
      "contributions": 33
    },
    {
      "login": "qnixsynapse",
      "contributions": 33
    },
    {
      "login": "sw",
      "contributions": 32
    },
    {
      "login": "mofosyne",
      "contributions": 31
    },
    {
      "login": "netrunnereve",
      "contributions": 30
    }
  ],
  "file_tree_count": 1546,
  "file_tree_sample": [
    {
      "path": ".clang-format",
      "type": "blob"
    },
    {
      "path": ".clang-tidy",
      "type": "blob"
    },
    {
      "path": ".devops",
      "type": "tree"
    },
    {
      "path": ".devops/cloud-v-pipeline",
      "type": "blob"
    },
    {
      "path": ".devops/cpu.Dockerfile",
      "type": "blob"
    },
    {
      "path": ".devops/cuda.Dockerfile",
      "type": "blob"
    },
    {
      "path": ".devops/intel.Dockerfile",
      "type": "blob"
    },
    {
      "path": ".devops/llama-cli-cann.Dockerfile",
      "type": "blob"
    },
    {
      "path": ".devops/llama-cpp-cuda.srpm.spec",
      "type": "blob"
    },
    {
      "path": ".devops/llama-cpp.srpm.spec",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 6536,
  "recent_issues": [
    {
      "number": 14320,
      "title": "Conv2D: Add CPU version",
      "state": "open"
    },
    {
      "number": 14319,
      "title": "common : use std::string_view now that we target c++17",
      "state": "open"
    },
    {
      "number": 14318,
      "title": "Feature Request: Add support for moonshotai/Kimi-VL-A3B-Instruct",
      "state": "open"
    },
    {
      "number": 14317,
      "title": "ggml-cpu: enable IBM NNPA Vector Intrinsics",
      "state": "open"
    },
    {
      "number": 14316,
      "title": "ggml: adds CONV_2D op and direct GEMM Vulkan implementation",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 14320,
      "title": "Conv2D: Add CPU version",
      "state": "open"
    },
    {
      "number": 14319,
      "title": "common : use std::string_view now that we target c++17",
      "state": "open"
    },
    {
      "number": 14317,
      "title": "ggml-cpu: enable IBM NNPA Vector Intrinsics",
      "state": "open"
    },
    {
      "number": 14316,
      "title": "ggml: adds CONV_2D op and direct GEMM Vulkan implementation",
      "state": "open"
    },
    {
      "number": 14314,
      "title": "gguf-py : fix Qwen3-Embedding eos token",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "aa0ef5c578eef4c2adc7be1282f21bab5f3e8d26",
      "author": "Sigbj\u00f8rn Skj\u00e6ret",
      "date": "2025-06-21T16:12:05+00:00",
      "message": "gguf-py : fix Qwen3-Embedding eos token (#14314)"
    },
    {
      "sha": "bb16041caef45cd4348cd6f84906b5dfec7a1f6a",
      "author": "Markus Tavenrath",
      "date": "2025-06-21T06:17:12+00:00",
      "message": "Add support for VK_EXT_debug_utils to add labels to Vulkan objects. (#13792)"
    },
    {
      "sha": "58cba76a9aab728717509d62b67c13afd8dc227a",
      "author": "Sigbj\u00f8rn Skj\u00e6ret",
      "date": "2025-06-21T05:33:21+00:00",
      "message": "gguf-py : fix TemplateProcessing pair when bos/eos is missing (#14312)"
    },
    {
      "sha": "67ae5312e255ae97852a4a216e2245580bfafd72",
      "author": "Georgi Gerganov",
      "date": "2025-06-21T05:04:18+00:00",
      "message": "metal : fix thread-safety (#14300)"
    },
    {
      "sha": "692e3cdd0a069ab56411b64506a67537d767683e",
      "author": "Georgi Gerganov",
      "date": "2025-06-21T05:03:46+00:00",
      "message": "memory : rename interface to llama_memory_context_i (#14296)"
    },
    {
      "sha": "b23fa0b3f40165ca3aae8ad4ee756e72f9a130dd",
      "author": "Daniel Han",
      "date": "2025-06-21T04:32:01+00:00",
      "message": "convert : fix Llama 4 conversion (#14311)"
    },
    {
      "sha": "06cbedfca1587473df9b537f1dd4d6bfa2e3de13",
      "author": "Georgi Gerganov",
      "date": "2025-06-20T17:50:24+00:00",
      "message": "sync : ggml"
    },
    {
      "sha": "b7147673f26c7ceb926a43c4734002bba291bcb8",
      "author": "Acly",
      "date": "2025-06-18T11:34:50+00:00",
      "message": "Add `ggml_roll` (ggml/1274)"
    },
    {
      "sha": "d860dd99a4178f58d1d1fa64eebc2aabc95392a7",
      "author": "David Chiu",
      "date": "2025-06-20T17:43:35+00:00",
      "message": "docs : fix the link to llama.h (#14293)"
    },
    {
      "sha": "c959f462a0b4d42eaf930ffd72df0e435c97d5d5",
      "author": "Aman Gupta",
      "date": "2025-06-20T14:48:24+00:00",
      "message": "CUDA: add conv_2d_transpose (#14287)"
    },
    {
      "sha": "22015b2092e291022ea3cfedc6aeb8f2643807da",
      "author": "Sigbj\u00f8rn Skj\u00e6ret",
      "date": "2025-06-20T14:37:44+00:00",
      "message": "lint : remove trailing whitepace (#14304)"
    },
    {
      "sha": "dd6e6d0b6a4bbe3ebfc931d1eb14db2f2b1d70af",
      "author": "Ruikai Peng",
      "date": "2025-06-20T14:13:06+00:00",
      "message": "vocab : prevent tokenizer overflow (#14301)"
    },
    {
      "sha": "8308f98c7fb778e54bf75538f5234d8bd20915e9",
      "author": "Nicol\u00f2 Scipione",
      "date": "2025-06-20T13:07:21+00:00",
      "message": "sycl: add usage of enqueue_functions extension (#14244)"
    },
    {
      "sha": "6369be07359d03723f38c0a4a014ff0f698a0738",
      "author": "Christian Kastner",
      "date": "2025-06-20T12:17:32+00:00",
      "message": "Implement GGML_CPU_ALL_VARIANTS for PowerPC (#14286)"
    },
    {
      "sha": "88fc854b4bd2e3caf10e705e6afcbbca136f0a3c",
      "author": "Sigbj\u00f8rn Skj\u00e6ret",
      "date": "2025-06-20T12:04:09+00:00",
      "message": "llama : improve sep token handling (#14272)"
    },
    {
      "sha": "e28c1b93fd7d3f8faf9551d962e8a65fe2122e38",
      "author": "Diego Devesa",
      "date": "2025-06-20T11:57:36+00:00",
      "message": "cuda : synchronize graph capture and cublas handle destruction (#14288)"
    },
    {
      "sha": "d27b3ca1758dfb1718e333d497ef4b68ad109bc2",
      "author": "Georgi Gerganov",
      "date": "2025-06-20T08:19:15+00:00",
      "message": "ggml : fix repack work size for mul_mat_id (#14292)"
    },
    {
      "sha": "9230dbe2c757e2d5071329095727d0fa9d4b85c4",
      "author": "Charles Xu",
      "date": "2025-06-20T07:51:01+00:00",
      "message": "ggml: Update KleidiAI to v1.9.0 (#14277)"
    },
    {
      "sha": "812939a9e90f99d1bd5bb1bc6b99d12600671d50",
      "author": "Georgi Gerganov",
      "date": "2025-06-20T07:50:27+00:00",
      "message": "model : more uniform output id handling (#14275)"
    },
    {
      "sha": "4c9fdfbe1580a66fd7d77c77418ce2c606a29fdd",
      "author": "Georgi Gerganov",
      "date": "2025-06-20T07:14:14+00:00",
      "message": "ubatch : new splitting logic (#14217)"
    }
  ],
  "readme_text": "# llama.cpp\n\n![llama](https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png)\n\n[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n[![Release](https://img.shields.io/github/v/release/ggml-org/llama.cpp)](https://github.com/ggml-org/llama.cpp/releases)\n[![Server](https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml/badge.svg)](https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml)\n\n[Roadmap](https://github.com/users/ggerganov/projects/7) / [Manifesto](https://github.com/ggml-org/llama.cpp/discussions/205) / [ggml](https://github.com/ggml-org/ggml)\n\nInference of Meta's [LLaMA](https://arxiv.org/abs/2302.13971) model (and others) in pure C/C++\n\n## Recent API changes\n\n- [Changelog for `libllama` API](https://github.com/ggml-org/llama.cpp/issues/9289)\n- [Changelog for `llama-server` REST API](https://github.com/ggml-org/llama.cpp/issues/9291)\n\n## Hot topics\n\n- \ud83d\udd25 Multimodal support arrived in `llama-server`: [#12898](https://github.com/ggml-org/llama.cpp/pull/12898) | [documentation](./docs/multimodal.md)\n- A new binary `llama-mtmd-cli` is introduced to replace `llava-cli`, `minicpmv-cli`, `gemma3-cli` ([#13012](https://github.com/ggml-org/llama.cpp/pull/13012)) and `qwen2vl-cli` ([#13141](https://github.com/ggml-org/llama.cpp/pull/13141)), `libllava` will be deprecated\n- VS Code extension for FIM completions: https://github.com/ggml-org/llama.vscode\n- Universal [tool call support](./docs/function-calling.md) in `llama-server` https://github.com/ggml-org/llama.cpp/pull/9639\n- Vim/Neovim plugin for FIM completions: https://github.com/ggml-org/llama.vim\n- Introducing GGUF-my-LoRA https://github.com/ggml-org/llama.cpp/discussions/10123\n- Hugging Face Inference Endpoints now support GGUF out of the box! https://github.com/ggml-org/llama.cpp/discussions/9669\n- Hugging Face GGUF editor: [discussion](https://github.com/ggml-org/llama.cpp/discussions/9268) | [tool](https://huggingface.co/spaces/CISCai/gguf-editor)\n\n----\n\n## Quick start\n\nGetting started with llama.cpp is straightforward. Here are several ways to install it on your machine:\n\n- Install `llama.cpp` using [brew, nix or winget](docs/install.md)\n- Run with Docker - see our [Docker documentation](docs/docker.md)\n- Download pre-built binaries from the [releases page](https://github.com/ggml-org/llama.cpp/releases)\n- Build from source by cloning this repository - check out [our build guide](docs/build.md)\n\nOnce installed, you'll need a model to work with. Head to the [Obtaining and quantizing models](#obtaining-and-quantizing-models) section to learn more.\n\nExample command:\n\n```sh\n# Use a local model file\nllama-cli -m my_model.gguf\n\n# Or download and run a model directly from Hugging Face\nllama-cli -hf ggml-org/gemma-3-1b-it-GGUF\n\n# Launch OpenAI-compatible API server\nllama-server -hf ggml-org/gemma-3-1b-it-GGUF\n```\n\n## Description\n\nThe main goal of `llama.cpp` is to enable LLM inference with minimal setup and state-of-the-art performance on a wide\nrange of hardware - locally and in the cloud.\n\n- Plain C/C++ implementation without any dependencies\n- Apple silicon is a first-class citizen - optimized via ARM NEON, Accelerate and Metal frameworks\n- AVX, AVX2, AVX512 and AMX support for x86 architectures\n- 1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit integer quantization for faster inference and reduced memory use\n- Custom CUDA kernels for running LLMs on NVIDIA GPUs (support for AMD GPUs via HIP and Moore Threads GPUs via MUSA)\n- Vulkan and SYCL backend support\n- CPU+GPU hybrid inference to partially accelerate models larger than the total VRAM capacity\n\nThe `llama.cpp` project is the main playground for developing new features for the [ggml](https://github.com/ggml-org/ggml) library.\n\n<details>\n<summary>Models</summary>\n\nTypically finetunes of the base models below are supported as well.\n\nInstructions for adding support for new models: [HOWTO-add-model.md](docs/development/HOWTO-add-model.md)\n\n#### Text-only\n\n- [X] LLaMA \ud83e\udd99\n- [x] LLaMA 2 \ud83e\udd99\ud83e\udd99\n- [x] LLaMA 3 \ud83e\udd99\ud83e\udd99\ud83e\udd99\n- [X] [Mistral 7B](https://huggingface.co/mistralai/Mistral-7B-v0.1)\n- [x] [Mixtral MoE](https://huggingface.co/models?search=mistral-ai/Mixtral)\n- [x] [DBRX](https://huggingface.co/databricks/dbrx-instruct)\n- [X] [Falcon](https://huggingface.co/models?search=tiiuae/falcon)\n- [X] [Chinese LLaMA / Alpaca](https://github.com/ymcui/Chinese-LLaMA-Alpaca) and [Chinese LLaMA-2 / Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2)\n- [X] [Vigogne (French)](https://github.com/bofenghuang/vigogne)\n- [X] [BERT](https://github.com/ggml-org/llama.cpp/pull/5423)\n- [X] [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/)\n- [X] [Baichuan 1 & 2](https://huggingface.co/models?search=baichuan-inc/Baichuan) + [derivations](https://huggingface.co/hiyouga/baichuan-7b-sft)\n- [X] [Aquila 1 & 2](https://huggingface.co/models?search=BAAI/Aquila)\n- [X] [Starcoder models](https://github.com/ggml-org/llama.cpp/pull/3187)\n- [X] [Refact](https://huggingface.co/smallcloudai/Refact-1_6B-fim)\n- [X] [MPT](https://github.com/ggml-org/llama.cpp/pull/3417)\n- [X] [Bloom](https://github.com/ggml-org/llama.cpp/pull/3553)\n- [x] [Yi models](https://huggingface.co/models?search=01-ai/Yi)\n- [X] [StableLM models](https://huggingface.co/stabilityai)\n- [x] [Deepseek models](https://huggingface.co/models?search=deepseek-ai/deepseek)\n- [x] [Qwen models](https://huggingface.co/models?search=Qwen/Qwen)\n- [x] [PLaMo-13B](https://github.com/ggml-org/llama.cpp/pull/3557)\n- [x] [Phi models](https://huggingface.co/models?search=microsoft/phi)\n- [x] [PhiMoE](https://github.com/ggml-org/llama.cpp/pull/11003)\n- [x] [GPT-2](https://huggingface.co/gpt2)\n- [x] [Orion 14B](https://github.com/ggml-org/llama.cpp/pull/5118)\n- [x] [InternLM2](https://huggingface.co/models?search=internlm2)\n- [x] [CodeShell](https://github.com/WisdomShell/codeshell)\n- [x] [Gemma](https://ai.google.dev/gemma)\n- [x] [Mamba](https://github.com/state-spaces/mamba)\n- [x] [Grok-1](https://huggingface.co/keyfan/grok-1-hf)\n- [x] [Xverse](https://huggingface.co/models?search=xverse)\n- [x] [Command-R models](https://huggingface.co/models?search=CohereForAI/c4ai-command-r)\n- [x] [SEA-LION](https://huggingface.co/models?search=sea-lion)\n- [x] [GritLM-7B](https://huggingface.co/GritLM/GritLM-7B) + [GritLM-8x7B](https://huggingface.co/GritLM/GritLM-8x7B)\n- [x] [OLMo](https://allenai.org/olmo)\n- [x] [OLMo 2](https://allenai.org/olmo)\n- [x] [OLMoE](https://huggingface.co/allenai/OLMoE-1B-7B-0924)\n- [x] [Granite models](https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330)\n- [x] [GPT-NeoX](https://github.com/EleutherAI/gpt-neox) + [Pythia](https://github.com/EleutherAI/pythia)\n- [x] [Snowflake-Arctic MoE](https://huggingface.co/collections/Snowflake/arctic-66290090abe542894a5ac520)\n- [x] [Smaug](https://huggingface.co/models?search=Smaug)\n- [x] [Poro 34B](https://huggingface.co/LumiOpen/Poro-34B)\n- [x] [Bitnet b1.58 models](https://huggingface.co/1bitLLM)\n- [x] [Flan T5](https://huggingface.co/models?search=flan-t5)\n- [x] [Open Elm models](https://huggingface.co/collections/apple/openelm-instruct-models-6619ad295d7ae9f868b759ca)\n- [x] [ChatGLM3-6b](https://huggingface.co/THUDM/chatglm3-6b) + [ChatGLM4-9b](https://huggingface.co/THUDM/glm-4-9b) + [GLMEdge-1.5b](https://huggingface.co/THUDM/glm-edge-1.5b-chat) + [GLMEdge-4b](https://huggingface.co/THUDM/glm-edge-4b-chat)\n- [x] [GLM-4-0414](https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e)\n- [x] [SmolLM](https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966)\n- [x] [EXAONE-3.0-7.8B-Instruct](https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct)\n- [x] [FalconMamba Models](https://huggingface.co/collections/tiiuae/falconmamba-7b-66b9a580324dd1598b0f6d4a)\n- [x] [Jais](https://huggingface.co/inceptionai/jais-13b-chat)\n- [x] [Bielik-11B-v2.3](https://huggingface.co/collections/speakleash/bielik-11b-v23-66ee813238d9b526a072408a)\n- [x] [RWKV-6](https://github.com/BlinkDL/RWKV-LM)\n- [x] [QRWKV-6](https://huggingface.co/recursal/QRWKV6-32B-Instruct-Preview-v0.1)\n- [x] [GigaChat-20B-A3B](https://huggingface.co/ai-sage/GigaChat-20B-A3B-instruct)\n- [X] [Trillion-7B-preview](https://huggingface.co/trillionlabs/Trillion-7B-preview)\n- [x] [Ling models](https://huggingface.co/collections/inclusionAI/ling-67c51c85b34a7ea0aba94c32)\n\n#### Multimodal\n\n- [x] [LLaVA 1.5 models](https://huggingface.co/collections/liuhaotian/llava-15-653aac15d994e992e2677a7e), [LLaVA 1.6 models](https://huggingface.co/collections/liuhaotian/llava-16-65b9e40155f60fd046a5ccf2)\n- [x] [BakLLaVA](https://huggingface.co/models?search=SkunkworksAI/Bakllava)\n- [x] [Obsidian](https://huggingface.co/NousResearch/Obsidian-3B-V0.5)\n- [x] [ShareGPT4V](https://huggingface.co/models?search=Lin-Chen/ShareGPT4V)\n- [x] [MobileVLM 1.7B/3B models](https://huggingface.co/models?search=mobileVLM)\n- [x] [Yi-VL](https://huggingface.co/models?search=Yi-VL)\n- [x] [Mini CPM](https://huggingface.co/models?search=MiniCPM)\n- [x] [Moondream](https://huggingface.co/vikhyatk/moondream2)\n- [x] [Bunny](https://github.com/BAAI-DCAI/Bunny)\n- [x] [GLM-EDGE](https://huggingface.co/models?search=glm-edge)\n- [x] [Qwen2-VL](https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d)\n\n</details>\n\n<details>\n<summary>Bindings</summary>\n\n- Python: [ddh0/easy-llama](https://github.com/ddh0/easy-llama)\n- Python: [abetlen/llama-cpp-python](https://github.com/abetlen/llama-cpp-python)\n- Go: [go-skynet/go-llama.cpp](https://github.com/go-skynet/go-llama.cpp)\n- Node.js: [withcatai/node-llama-cpp](https://github.com/withcatai/node-llama-cpp)\n- JS/TS (llama.cpp server client): [lgrammel/modelfusion](https://modelfusion.dev/integration/model-provider/llamacpp)\n- JS/TS (Programmable Prompt Engine CLI): [offline-ai/cli](https://github.com/offline-ai/cli)\n- JavaScript/Wasm (works in browser): [tangledgroup/llama-cpp-wasm](https://github.com/tangledgroup/llama-cpp-wasm)\n- Typescript/Wasm (nicer API, available on npm): [ngxson/wllama](https://github.com/ngxson/wllama)\n- Ruby: [yoshoku/llama_cpp.rb](https://github.com/yoshoku/llama_cpp.rb)\n- Rust (more features): [edgenai/llama_cpp-rs](https://github.com/edgenai/llama_cpp-rs)\n- Rust (nicer API): [mdrokz/rust-llama.cpp](https://github.com/mdrokz/rust-llama.cpp)\n- Rust (more direct bindings): [utilityai/llama-cpp-rs](https://github.com/utilityai/llama-cpp-rs)\n- Rust (automated build from crates.io): [ShelbyJenkins/llm_client](https://github.com/ShelbyJenkins/llm_client)\n- C#/.NET: [SciSharp/LLamaSharp](https://github.com/SciSharp/LLamaSharp)\n- C#/VB.NET (more features - community license): [LM-Kit.NET](https://docs.lm-kit.com/lm-kit-net/index.html)\n- Scala 3: [donderom/llm4s](https://github.com/donderom/llm4s)\n- Clojure: [phronmophobic/llama.clj](https://github.com/phronmophobic/llama.clj)\n- React Native: [mybigday/llama.rn](https://github.com/mybigday/llama.rn)\n- Java: [kherud/java-llama.cpp](https://github.com/kherud/java-llama.cpp)\n- Zig: [deins/llama.cpp.zig](https://github.com/Deins/llama.cpp.zig)\n- Flutter/Dart: [netdur/llama_cpp_dart](https://github.com/netdur/llama_cpp_dart)\n- Flutter: [xuegao-tzx/Fllama](https://github.com/xuegao-tzx/Fllama)\n- PHP (API bindings and features built on top of llama.cpp): [distantmagic/resonance](https://github.com/distantmagic/resonance) [(more info)](https://github.com/ggml-org/llama.cpp/pull/6326)\n- Guile Scheme: [guile_llama_cpp](https://savannah.nongnu.org/projects/guile-llama-cpp)\n- Swift [srgtuszy/llama-cpp-swift](https://github.com/srgtuszy/llama-cpp-swift)\n- Swift [ShenghaiWang/SwiftLlama](https://github.com/ShenghaiWang/SwiftLlama)\n- Delphi [Embarcadero/llama-cpp-delphi](https://github.com/Embarcadero/llama-cpp-delphi)\n\n</details>\n\n<details>\n<summary>UIs</summary>\n\n*(to have a project listed here, it should clearly state that it depends on `llama.cpp`)*\n\n- [AI Sublime Text plugin](https://github.com/yaroslavyaroslav/OpenAI-sublime-text) (MIT)\n- [cztomsik/ava](https://github.com/cztomsik/ava) (MIT)\n- [Dot](https://github.com/alexpinel/Dot) (GPL)\n- [eva](https://github.com/ylsdamxssjxxdd/eva) (MIT)\n- [iohub/collama](https://github.com/iohub/coLLaMA) (Apache-2.0)\n- [janhq/jan](https://github.com/janhq/jan) (AGPL)\n- [johnbean393/Sidekick](https://github.com/johnbean393/Sidekick) (MIT)\n- [KanTV](https://github.com/zhouwg/kantv?tab=readme-ov-file) (Apache-2.0)\n- [KodiBot](https://github.com/firatkiral/kodibot) (GPL)\n- [llama.vim](https://github.com/ggml-org/llama.vim) (MIT)\n- [LARS](https://github.com/abgulati/LARS) (AGPL)\n- [Llama Assistant](https://github.com/vietanhdev/llama-assistant) (GPL)\n- [LLMFarm](https://github.com/guinmoon/LLMFarm?tab=readme-ov-file) (MIT)\n- [LLMUnity](https://github.com/undreamai/LLMUnity) (MIT)\n- [LMStudio](https://lmstudio.ai/) (proprietary)\n- [LocalAI](https://github.com/mudler/LocalAI) (MIT)\n- [LostRuins/koboldcpp](https://github.com/LostRuins/koboldcpp) (AGPL)\n- [MindMac](https://mindmac.app) (proprietary)\n- [MindWorkAI/AI-Studio](https://github.com/MindWorkAI/AI-Studio) (FSL-1.1-MIT)\n- [Mobile-Artificial-Intelligence/maid](https://github.com/Mobile-Artificial-Intelligence/maid) (MIT)\n- [Mozilla-Ocho/llamafile](https://github.com/Mozilla-Ocho/llamafile) (Apache-2.0)\n- [nat/openplayground](https://github.com/nat/openplayground) (MIT)\n- [nomic-ai/gpt4all](https://github.com/nomic-ai/gpt4all) (MIT)\n- [ollama/ollama](https://github.com/ollama/ollama) (MIT)\n- [oobabooga/text-generation-webui](https://github.com/oobabooga/text-generation-webui) (AGPL)\n- [PocketPal AI](https://github.com/a-ghorbani/pocketpal-ai) (MIT)\n- [psugihara/FreeChat](https://github.com/psugihara/FreeChat) (MIT)\n- [ptsochantaris/emeltal](https://github.com/ptsochantaris/emeltal) (MIT)\n- [pythops/tenere](https://github.com/pythops/tenere) (AGPL)\n- [ramalama](https://github.com/containers/ramalama) (MIT)\n- [semperai/amica](https://github.com/semperai/amica) (MIT)\n- [withcatai/catai](https://github.com/withcatai/catai) (MIT)\n- [Autopen](https://github.com/blackhole89/autopen) (GPL)\n\n</details>\n\n<details>\n<summary>Tools</summary>\n\n- [akx/ggify](https://github.com/akx/ggify) \u2013 download PyTorch models from HuggingFace Hub and convert them to GGML\n- [akx/ollama-dl](https://github.com/akx/ollama-dl) \u2013 download models from the Ollama library to be used directly with llama.cpp\n- [crashr/gppm](https://github.com/crashr/gppm) \u2013 launch llama.cpp instances utilizing NVIDIA Tesla P40 or P100 GPUs with reduced idle power consumption\n- [gpustack/gguf-parser](https://github.com/gpustack/gguf-parser-go/tree/main/cmd/gguf-parser) - review/check the GGUF file and estimate the memory usage\n- [Styled Lines](https://marketplace.unity.com/packages/tools/generative-ai/styled-lines-llama-cpp-model-292902) (proprietary licensed, async wrapper of inference part for game development in Unity3d with pre-built Mobile and Web platform wrappers and a model example)\n\n</details>\n\n<details>\n<summary>Infrastructure</summary>\n\n- [Paddler](https://github.com/distantmagic/paddler) - Stateful load balancer custom-tailored for llama.cpp\n- [GPUStack](https://github.com/gpustack/gpustack) - Manage GPU clusters for running LLMs\n- [llama_cpp_canister](https://github.com/onicai/llama_cpp_canister) - llama.cpp as a smart contract on the Internet Computer, using WebAssembly\n- [llama-swap](https://github.com/mostlygeek/llama-swap) - transparent proxy that adds automatic model switching with llama-server\n- [Kalavai](https://github.com/kalavai-net/kalavai-client) - Crowdsource end to end LLM deployment at any scale\n- [llmaz](https://github.com/InftyAI/llmaz) - \u2638\ufe0f Easy, advanced inference platform for large language models on Kubernetes.\n</details>\n\n<details>\n<summary>Games</summary>\n\n- [Lucy's Labyrinth](https://github.com/MorganRO8/Lucys_Labyrinth) - A simple maze game where agents controlled by an AI model will try to trick you.\n\n</details>\n\n\n## Supported backends\n\n| Backend | Target devices |\n| --- | --- |\n| [Metal](docs/build.md#metal-build) | Apple Silicon |\n| [BLAS](docs/build.md#blas-build) | All |\n| [BLIS](docs/backend/BLIS.md) | All |\n| [SYCL](docs/backend/SYCL.md) | Intel and Nvidia GPU |\n| [MUSA](docs/build.md#musa) | Moore Threads GPU |\n| [CUDA](docs/build.md#cuda) | Nvidia GPU |\n| [HIP](docs/build.md#hip) | AMD GPU |\n| [Vulkan](docs/build.md#vulkan) | GPU |\n| [CANN](docs/build.md#cann) | Ascend NPU |\n| [OpenCL](docs/backend/OPENCL.md) | Adreno GPU |\n| [RPC](https://github.com/ggml-org/llama.cpp/tree/master/tools/rpc) | All |\n\n## Obtaining and quantizing models\n\nThe [Hugging Face](https://huggingface.co) platform hosts a [number of LLMs](https://huggingface.co/models?library=gguf&sort=trending) compatible with `llama.cpp`:\n\n- [Trending](https://huggingface.co/models?library=gguf&sort=trending)\n- [LLaMA](https://huggingface.co/models?sort=trending&search=llama+gguf)\n\nYou can either manually download the GGUF file or directly use any `llama.cpp`-compatible models from [Hugging Face](https://huggingface.co/) or other model hosting sites, such as [ModelScope](https://modelscope.cn/), by using this CLI argument: `-hf <user>/<model>[:quant]`. For example:\n\n```sh\nllama-cli -hf ggml-org/gemma-3-1b-it-GGUF\n```\n\nBy default, the CLI would download from Hugging Face, you can switch to other options with the environment variable `MODEL_ENDPOINT`. For example, you may opt to downloading model checkpoints from ModelScope or other model sharing communities by setting the environment variable, e.g. `MODEL_ENDPOINT=https://www.modelscope.cn/`.\n\nAfter downloading a model, use the CLI tools to run it locally - see below.\n\n`llama.cpp` requires the model to be stored in the [GGUF](https://github.com/ggml-org/ggml/blob/master/docs/gguf.md) file format. Models in other data formats can be converted to GGUF using the `convert_*.py` Python scripts in this repo.\n\nThe Hugging Face platform provides a variety of online tools for converting, quantizing and hosting models with `llama.cpp`:\n\n- Use the [GGUF-my-repo space](https://huggingface.co/spaces/ggml-org/gguf-my-repo) to convert to GGUF format and quantize model weights to smaller sizes\n- Use the [GGUF-my-LoRA space](https://huggingface.co/spaces/ggml-org/gguf-my-lora) to convert LoRA adapters to GGUF format (more info: https://github.com/ggml-org/llama.cpp/discussions/10123)\n- Use the [GGUF-editor space](https://huggingface.co/spaces/CISCai/gguf-editor) to edit GGUF meta data in the browser (more info: https://github.com/ggml-org/llama.cpp/discussions/9268)\n- Use the [Inference Endpoints](https://ui.endpoints.huggingface.co/) to directly host `llama.cpp` in the cloud (more info: https://github.com/ggml-org/llama.cpp/discussions/9669)\n\nTo learn more about model quantization, [read this documentation](tools/quantize/README.md)\n\n## [`llama-cli`](tools/main)\n\n#### A CLI tool for accessing and experimenting with most of `llama.cpp`'s functionality.\n\n- <details open>\n    <summary>Run in conversation mode</summary>\n\n    Models with a built-in chat template will automatically activate conversation mode. If this doesn't occur, you can manually enable it by adding `-cnv` and specifying a suitable chat template with `--chat-template NAME`\n\n    ```bash\n    llama-cli -m model.gguf\n\n    # > hi, who are you?\n    # Hi there! I'm your helpful assistant! I'm an AI-powered chatbot designed to assist and provide information to users like you. I'm here to help answer your questions, provide guidance, and offer support on a wide range of topics. I'm a friendly and knowledgeable AI, and I'm always happy to help with anything you need. What's on your mind, and how can I assist you today?\n    #\n    # > what is 1+1?\n    # Easy peasy! The answer to 1+1 is... 2!\n    ```\n\n    </details>\n\n- <details>\n    <summary>Run in conversation mode with custom chat template</summary>\n\n    ```bash\n    # use the \"chatml\" template (use -h to see the list of supported templates)\n    llama-cli -m model.gguf -cnv --chat-template chatml\n\n    # use a custom template\n    llama-cli -m model.gguf -cnv --in-prefix 'User: ' --reverse-prompt 'User:'\n    ```\n\n    </details>\n\n- <details>\n    <summary>Run simple text completion</summary>\n\n    To disable conversation mode explicitly, use `-no-cnv`\n\n    ```bash\n    llama-cli -m model.gguf -p \"I believe the meaning of life is\" -n 128 -no-cnv\n\n    # I believe the meaning of life is to find your own truth and to live in accordance with it. For me, this means being true to myself and following my passions, even if they don't align with societal expectations. I think that's what I love about yoga \u2013 it's not just a physical practice, but a spiritual one too. It's about connecting with yourself, listening to your inner voice, and honoring your own unique journey.\n    ```\n\n    </details>\n\n- <details>\n    <summary>Constrain the output with a custom grammar</summary>\n\n    ```bash\n    llama-cli -m model.gguf -n 256 --grammar-file grammars/json.gbnf -p 'Request: schedule a call at 8pm; Command:'\n\n    # {\"appointmentTime\": \"8pm\", \"appointmentDetails\": \"schedule a a call\"}\n    ```\n\n    The [grammars/](grammars/) folder contains a handful of sample grammars. To write your own, check out the [GBNF Guide](grammars/README.md).\n\n    For authoring more complex JSON grammars, check out https://grammar.intrinsiclabs.ai/\n\n    </details>\n\n\n## [`llama-server`](tools/server)\n\n#### A lightweight, [OpenAI API](https://github.com/openai/openai-openapi) compatible, HTTP server for serving LLMs.\n\n- <details open>\n    <summary>Start a local HTTP server with default configuration on port 8080</summary>\n\n    ```bash\n    llama-server -m model.gguf --port 8080\n\n    # Basic web UI can be accessed via browser: http://localhost:8080\n    # Chat completion endpoint: http://localhost:8080/v1/chat/completions\n    ```\n\n    </details>\n\n- <details>\n    <summary>Support multiple-users and parallel decoding</summary>\n\n    ```bash\n    # up to 4 concurrent requests, each with 4096 max context\n    llama-server -m model.gguf -c 16384 -np 4\n    ```\n\n    </details>\n\n- <details>\n    <summary>Enable speculative decoding</summary>\n\n    ```bash\n    # the draft.gguf model should be a small variant of the target model.gguf\n    llama-server -m model.gguf -md draft.gguf\n    ```\n\n    </details>\n\n- <details>\n    <summary>Serve an embedding model</summary>\n\n    ```bash\n    # use the /embedding endpoint\n    llama-server -m model.gguf --embedding --pooling cls -ub 8192\n    ```\n\n    </details>\n\n- <details>\n    <summary>Serve a reranking model</summary>\n\n    ```bash\n    # use the /reranking endpoint\n    llama-server -m model.gguf --reranking\n    ```\n\n    </details>\n\n- <details>\n    <summary>Constrain all outputs with a grammar</summary>\n\n    ```bash\n    # custom grammar\n    llama-server -m model.gguf --grammar-file grammar.gbnf\n\n    # JSON\n    llama-server -m model.gguf --grammar-file grammars/json.gbnf\n    ```\n\n    </details>\n\n\n## [`llama-perplexity`](tools/perplexity)\n\n#### A tool for measuring the perplexity [^1][^2] (and other quality metrics) of a model over a given text.\n\n- <details open>\n    <summary>Measure the perplexity over a text file</summary>\n\n    ```bash\n    llama-perplexity -m model.gguf -f file.txt\n\n    # [1]15.2701,[2]5.4007,[3]5.3073,[4]6.2965,[5]5.8940,[6]5.6096,[7]5.7942,[8]4.9297, ...\n    # Final estimate: PPL = 5.4007 +/- 0.67339\n    ```\n\n    </details>\n\n- <details>\n    <summary>Measure KL divergence</summary>\n\n    ```bash\n    # TODO\n    ```\n\n    </details>\n\n[^1]: [tools/perplexity/README.md](./tools/perplexity/README.md)\n[^2]: [https://huggingface.co/docs/transformers/perplexity](https://huggingface.co/docs/transformers/perplexity)\n\n## [`llama-bench`](tools/llama-bench)\n\n#### Benchmark the performance of the inference for various parameters.\n\n- <details open>\n    <summary>Run default benchmark</summary>\n\n    ```bash\n    llama-bench -m model.gguf\n\n    # Output:\n    # | model               |       size |     params | backend    | threads |          test |                  t/s |\n    # | ------------------- | ---------: | ---------: | ---------- | ------: | ------------: | -------------------: |\n    # | qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         pp512 |      5765.41 \u00b1 20.55 |\n    # | qwen2 1.5B Q4_0     | 885.97 MiB |     1.54 B | Metal,BLAS |      16 |         tg128 |        197.71 \u00b1 0.81 |\n    #\n    # build: 3e0ba0e60 (4229)\n    ```\n\n    </details>\n\n## [`llama-run`](tools/run)\n\n#### A comprehensive example for running `llama.cpp` models. Useful for inferencing. Used with RamaLama [^3].\n\n- <details>\n    <summary>Run a model with a specific prompt (by default it's pulled from Ollama registry)</summary>\n\n    ```bash\n    llama-run granite-code\n    ```\n\n    </details>\n\n[^3]: [RamaLama](https://github.com/containers/ramalama)\n\n## [`llama-simple`](examples/simple)\n\n#### A minimal example for implementing apps with `llama.cpp`. Useful for developers.\n\n- <details>\n    <summary>Basic text completion</summary>\n\n    ```bash\n    llama-simple -m model.gguf\n\n    # Hello my name is Kaitlyn and I am a 16 year old girl. I am a junior in high school and I am currently taking a class called \"The Art of\n    ```\n\n    </details>\n\n\n## Contributing\n\n- Contributors can open PRs\n- Collaborators can push to branches in the `llama.cpp` repo and merge PRs into the `master` branch\n- Collaborators will be invited based on contributions\n- Any help with managing issues, PRs and projects is very appreciated!\n- See [good first issues](https://github.com/ggml-org/llama.cpp/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) for tasks suitable for first contributions\n- Read the [CONTRIBUTING.md](CONTRIBUTING.md) for more information\n- Make sure to read this: [Inference at the edge](https://github.com/ggml-org/llama.cpp/discussions/205)\n- A bit of backstory for those who are interested: [Changelog podcast](https://changelog.com/podcast/532)\n\n## Other documentation\n\n- [main (cli)](tools/main/README.md)\n- [server](tools/server/README.md)\n- [GBNF grammars](grammars/README.md)\n\n#### Development documentation\n\n- [How to build](docs/build.md)\n- [Running on Docker](docs/docker.md)\n- [Build on Android](docs/android.md)\n- [Performance troubleshooting](docs/development/token_generation_performance_tips.md)\n- [GGML tips & tricks](https://github.com/ggml-org/llama.cpp/wiki/GGML-Tips-&-Tricks)\n\n#### Seminal papers and background on the models\n\nIf your issue is with model generation quality, then please at least scan the following links and papers to understand the limitations of LLaMA models. This is especially important when choosing an appropriate model size and appreciating both the significant and subtle differences between LLaMA models and ChatGPT:\n- LLaMA:\n    - [Introducing LLaMA: A foundational, 65-billion-parameter large language model](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)\n    - [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/abs/2302.13971)\n- GPT-3\n    - [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)\n- GPT-3.5 / InstructGPT / ChatGPT:\n    - [Aligning language models to follow instructions](https://openai.com/research/instruction-following)\n    - [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)\n\n## XCFramework\nThe XCFramework is a precompiled version of the library for iOS, visionOS, tvOS,\nand macOS. It can be used in Swift projects without the need to compile the\nlibrary from source. For example:\n```swift\n// swift-tools-version: 5.10\n// The swift-tools-version declares the minimum version of Swift required to build this package.\n\nimport PackageDescription\n\nlet package = Package(\n    name: \"MyLlamaPackage\",\n    targets: [\n        .executableTarget(\n            name: \"MyLlamaPackage\",\n            dependencies: [\n                \"LlamaFramework\"\n            ]),\n        .binaryTarget(\n            name: \"LlamaFramework\",\n            url: \"https://github.com/ggml-org/llama.cpp/releases/download/b5046/llama-b5046-xcframework.zip\",\n            checksum: \"c19be78b5f00d8d29a25da41042cb7afa094cbf6280a225abe614b03b20029ab\"\n        )\n    ]\n)\n```\nThe above example is using an intermediate build `b5046` of the library. This can be modified\nto use a different version by changing the URL and checksum.\n\n## Completions\nCommand-line completion is available for some environments.\n\n#### Bash Completion\n```bash\n$ build/bin/llama-cli --completion-bash > ~/.llama-completion.bash\n$ source ~/.llama-completion.bash\n```\nOptionally this can be added to your `.bashrc` or `.bash_profile` to load it\nautomatically. For example:\n```console\n$ echo \"source ~/.llama-completion.bash\" >> ~/.bashrc\n```\n\n## Dependencies\n\n- [yhirose/cpp-httplib](https://github.com/yhirose/cpp-httplib) - Single-header HTTP server, used by `llama-server` - MIT license\n- [stb-image](https://github.com/nothings/stb) - Single-header image format decoder, used by multimodal subsystem - Public domain\n- [nlohmann/json](https://github.com/nlohmann/json) - Single-header JSON library, used by various tools/examples - MIT License\n- [minja](https://github.com/google/minja) - Minimal Jinja parser in C++, used by various tools/examples - MIT License\n- [linenoise.cpp](./tools/run/linenoise.cpp/linenoise.cpp) - C++ library that provides readline-like line editing capabilities, used by `llama-run` - BSD 2-Clause License\n- [curl](https://curl.se/) - Client-side URL transfer library, used by various tools/examples - [CURL License](https://curl.se/docs/copyright.html)\n- [miniaudio.h](https://github.com/mackron/miniaudio) - Single-header audio format decoder, used by multimodal subsystem - Public domain\n",
  "external_links_in_readme": [
    "https://github.com/pythops/tenere",
    "https://ai.facebook.com/blog/large-language-model-llama-meta-ai/",
    "https://huggingface.co/collections/speakleash/bielik-11b-v23-66ee813238d9b526a072408a",
    "https://huggingface.co/collections/liuhaotian/llava-15-653aac15d994e992e2677a7e",
    "https://huggingface.co/databricks/dbrx-instruct",
    "https://huggingface.co/models?search=xverse",
    "https://huggingface.co/models?search=Smaug",
    "https://github.com/ggml-org/llama.cpp/pull/13012",
    "http://localhost:8080/v1/chat/completions",
    "https://bair.berkeley.edu/blog/2023/04/03/koala/",
    "https://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct",
    "https://huggingface.co/gpt2",
    "https://huggingface.co/THUDM/glm-edge-4b-chat",
    "https://github.com/ggml-org/ggml/blob/master/docs/gguf.md",
    "https://github.com/LostRuins/koboldcpp",
    "https://github.com/ggml-org/llama.cpp/releases/download/b5046/llama-b5046-xcframework.zip\",",
    "https://huggingface.co/vikhyatk/moondream2",
    "https://github.com/ggml-org/llama.cpp/pull/3187",
    "https://github.com/ggml-org/llama.cpp/discussions/10123",
    "https://huggingface.co/collections/inclusionAI/ling-67c51c85b34a7ea0aba94c32",
    "https://github.com/firatkiral/kodibot",
    "https://github.com/edgenai/llama_cpp-rs",
    "https://github.com/nomic-ai/gpt4all",
    "https://github.com/tangledgroup/llama-cpp-wasm",
    "https://github.com/withcatai/catai",
    "https://opensource.org/licenses/MIT",
    "https://github.com/ggml-org/llama.cpp/pull/12898",
    "https://github.com/google/minja",
    "https://github.com/a-ghorbani/pocketpal-ai",
    "https://github.com/akx/ollama-dl",
    "https://github.com/ggml-org/llama.vscode",
    "https://huggingface.co/models?search=mistral-ai/Mixtral",
    "https://github.com/ggml-org/llama.cpp/issues/9291",
    "https://github.com/ggml-org/llama.cpp/pull/3557",
    "https://arxiv.org/abs/2005.14165",
    "https://huggingface.co/models?search=glm-edge",
    "https://github.com/WisdomShell/codeshell",
    "https://huggingface.co/collections/ibm-granite/granite-code-models-6624c5cec322e4c148c8b330",
    "https://github.com/srgtuszy/llama-cpp-swift",
    "https://github.com/Deins/llama.cpp.zig",
    "https://github.com/ggml-org/llama.cpp/pull/11003",
    "https://github.com/iohub/coLLaMA",
    "https://huggingface.co/spaces/ggml-org/gguf-my-lora",
    "https://github.com/onicai/llama_cpp_canister",
    "https://github.com/psugihara/FreeChat",
    "https://github.com/semperai/amica",
    "https://github.com/nat/openplayground",
    "https://huggingface.co/LumiOpen/Poro-34B",
    "https://modelscope.cn/",
    "https://github.com/BAAI-DCAI/Bunny",
    "https://github.com/ymcui/Chinese-LLaMA-Alpaca",
    "https://huggingface.co/models?library=gguf&sort=trending",
    "https://huggingface.co/spaces/ggml-org/gguf-my-repo",
    "https://arxiv.org/abs/2203.02155",
    "https://huggingface.co/models?search=01-ai/Yi",
    "https://modelfusion.dev/integration/model-provider/llamacpp",
    "https://huggingface.co/NousResearch/Obsidian-3B-V0.5",
    "https://github.com/ggml-org/llama.cpp/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22",
    "https://huggingface.co/models?search=CohereForAI/c4ai-command-r",
    "https://github.com/oobabooga/text-generation-webui",
    "https://huggingface.co",
    "https://huggingface.co/models?search=deepseek-ai/deepseek",
    "https://savannah.nongnu.org/projects/guile-llama-cpp",
    "https://huggingface.co/docs/transformers/perplexity](https://huggingface.co/docs/transformers/perplexity",
    "https://github.com/ShenghaiWang/SwiftLlama",
    "https://github.com/utilityai/llama-cpp-rs",
    "https://github.com/netdur/llama_cpp_dart",
    "https://github.com/InftyAI/llmaz",
    "https://huggingface.co/models?search=mobileVLM",
    "https://huggingface.co/stabilityai",
    "https://github.com/EleutherAI/pythia",
    "https://img.shields.io/github/v/release/ggml-org/llama.cpp",
    "https://github.com/ddh0/easy-llama",
    "https://github.com/withcatai/node-llama-cpp",
    "https://github.com/ggml-org/llama.cpp/pull/13141",
    "https://huggingface.co/smallcloudai/Refact-1_6B-fim",
    "https://github.com/distantmagic/paddler",
    "https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966",
    "https://github.com/bofenghuang/vigogne",
    "https://huggingface.co/inceptionai/jais-13b-chat",
    "https://github.com/akx/ggify",
    "https://github.com/ggml-org/llama.cpp/pull/9639",
    "https://github.com/janhq/jan",
    "https://github.com/undreamai/LLMUnity",
    "https://github.com/xuegao-tzx/Fllama",
    "https://github.com/SciSharp/LLamaSharp",
    "https://github.com/MorganRO8/Lucys_Labyrinth",
    "https://github.com/ptsochantaris/emeltal",
    "https://github.com/EleutherAI/gpt-neox",
    "https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml/badge.svg",
    "https://github.com/yhirose/cpp-httplib",
    "https://github.com/ggml-org/llama.cpp/pull/5118",
    "https://huggingface.co/spaces/CISCai/gguf-editor",
    "https://github.com/abetlen/llama-cpp-python",
    "https://curl.se/docs/copyright.html",
    "https://huggingface.co/models?search=internlm2",
    "https://ai.google.dev/gemma",
    "https://huggingface.co/ai-sage/GigaChat-20B-A3B-instruct",
    "https://huggingface.co/models?search=Qwen/Qwen",
    "https://github.com/vietanhdev/llama-assistant",
    "https://marketplace.unity.com/packages/tools/generative-ai/styled-lines-llama-cpp-model-292902",
    "https://github.com/ggml-org/llama.cpp/wiki/GGML-Tips-&-Tricks",
    "https://github.com/BlinkDL/RWKV-LM",
    "https://github.com/openai/openai-openapi",
    "https://docs.lm-kit.com/lm-kit-net/index.html",
    "https://github.com/Mobile-Artificial-Intelligence/maid",
    "https://github.com/nothings/stb",
    "https://huggingface.co/models?search=BAAI/Aquila",
    "https://github.com/ggml-org/llama.cpp/issues/9289",
    "https://huggingface.co/trillionlabs/Trillion-7B-preview",
    "https://changelog.com/podcast/532",
    "http://localhost:8080",
    "https://openai.com/research/instruction-following",
    "https://huggingface.co/mistralai/Mistral-7B-v0.1",
    "https://huggingface.co/recursal/QRWKV6-32B-Instruct-Preview-v0.1",
    "https://github.com/ggml-org/llama.cpp/discussions/205",
    "https://huggingface.co/THUDM/glm-edge-1.5b-chat",
    "https://github.com/yoshoku/llama_cpp.rb",
    "https://github.com/users/ggerganov/projects/7",
    "https://user-images.githubusercontent.com/1991296/230134379-7181e485-c521-4d23-a0d6-f7b3b61ba524.png",
    "https://github.com/crashr/gppm",
    "https://huggingface.co/GritLM/GritLM-7B",
    "https://github.com/donderom/llm4s",
    "https://github.com/ymcui/Chinese-LLaMA-Alpaca-2",
    "https://huggingface.co/1bitLLM",
    "https://huggingface.co/models?search=MiniCPM",
    "https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d",
    "https://github.com/gpustack/gpustack",
    "https://github.com/distantmagic/resonance",
    "https://www.modelscope.cn/`.",
    "https://github.com/ggml-org/llama.cpp/actions/workflows/server.yml",
    "https://github.com/kalavai-net/kalavai-client",
    "https://github.com/Embarcadero/llama-cpp-delphi",
    "https://github.com/cztomsik/ava",
    "https://github.com/ggml-org/llama.vim",
    "https://huggingface.co/models?search=sea-lion",
    "https://arxiv.org/abs/2302.13971",
    "https://huggingface.co/hiyouga/baichuan-7b-sft",
    "https://github.com/ggml-org/llama.cpp/releases",
    "https://github.com/alexpinel/Dot",
    "https://huggingface.co/collections/apple/openelm-instruct-models-6619ad295d7ae9f868b759ca",
    "https://github.com/guinmoon/LLMFarm?tab=readme-ov-file",
    "https://huggingface.co/models?search=microsoft/phi",
    "https://github.com/ggml-org/llama.cpp/pull/3553",
    "https://github.com/phronmophobic/llama.clj",
    "https://huggingface.co/THUDM/chatglm3-6b",
    "https://huggingface.co/models?search=baichuan-inc/Baichuan",
    "https://huggingface.co/GritLM/GritLM-8x7B",
    "https://github.com/offline-ai/cli",
    "https://huggingface.co/",
    "https://github.com/ggml-org/llama.cpp/discussions/9669",
    "https://huggingface.co/collections/liuhaotian/llava-16-65b9e40155f60fd046a5ccf2",
    "https://github.com/zhouwg/kantv?tab=readme-ov-file",
    "https://github.com/yaroslavyaroslav/OpenAI-sublime-text",
    "https://lmstudio.ai/",
    "https://github.com/Mozilla-Ocho/llamafile",
    "https://github.com/nlohmann/json",
    "https://github.com/johnbean393/Sidekick",
    "https://ui.endpoints.huggingface.co/",
    "https://github.com/kherud/java-llama.cpp",
    "https://allenai.org/olmo",
    "https://huggingface.co/collections/tiiuae/falconmamba-7b-66b9a580324dd1598b0f6d4a",
    "https://huggingface.co/models?search=SkunkworksAI/Bakllava",
    "https://huggingface.co/models?search=flan-t5",
    "https://huggingface.co/models?search=tiiuae/falcon",
    "https://github.com/abgulati/LARS",
    "https://github.com/ggml-org/ggml",
    "https://grammar.intrinsiclabs.ai/",
    "https://curl.se/",
    "https://github.com/MindWorkAI/AI-Studio",
    "https://github.com/ollama/ollama",
    "https://github.com/containers/ramalama",
    "https://github.com/ggml-org/llama.cpp/pull/6326",
    "https://huggingface.co/allenai/OLMoE-1B-7B-0924",
    "https://github.com/ggml-org/llama.cpp/pull/5423",
    "https://github.com/ggml-org/llama.cpp/tree/master/tools/rpc",
    "https://github.com/mdrokz/rust-llama.cpp",
    "https://github.com/ggml-org/llama.cpp/pull/3417",
    "https://github.com/ylsdamxssjxxdd/eva",
    "https://github.com/gpustack/gguf-parser-go/tree/main/cmd/gguf-parser",
    "https://huggingface.co/models?sort=trending&search=llama+gguf",
    "https://huggingface.co/models?search=Lin-Chen/ShareGPT4V",
    "https://img.shields.io/badge/license-MIT-blue.svg",
    "https://github.com/ShelbyJenkins/llm_client",
    "https://github.com/blackhole89/autopen",
    "https://huggingface.co/keyfan/grok-1-hf",
    "https://huggingface.co/collections/THUDM/glm-4-0414-67f3cbcb34dd9d252707cb2e",
    "https://huggingface.co/collections/Snowflake/arctic-66290090abe542894a5ac520",
    "https://github.com/state-spaces/mamba",
    "https://github.com/ggml-org/llama.cpp/discussions/9268",
    "https://mindmac.app",
    "https://github.com/mybigday/llama.rn",
    "https://huggingface.co/models?search=Yi-VL",
    "https://github.com/mackron/miniaudio",
    "https://github.com/ngxson/wllama",
    "https://github.com/go-skynet/go-llama.cpp",
    "https://huggingface.co/THUDM/glm-4-9b",
    "https://github.com/mudler/LocalAI",
    "https://github.com/mostlygeek/llama-swap"
  ]
}
```

</details>


---

## Repository 2: city96/ComfyUI-GGUF

# GitHub Repository Data

**Repository:** [city96/ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF)

## Basic Information

- **Description:** GGUF Quantization support for native ComfyUI models
- **Created:** 2024-08-15T03:26:26+00:00
- **Last Updated:** 2025-06-21T20:19:28+00:00
- **Last Pushed:** 2025-06-14T23:07:45+00:00
- **Default Branch:** main
- **Size:** 157 KB

## Statistics

- **Stars:** 2,088
- **Forks:** 139
- **Watchers:** 2,088
- **Open Issues:** 124
- **Total Issues:** 0
- **Pull Requests:** 25

## License

- **Type:** Apache License 2.0
- **SPDX ID:** Apache-2.0
- **URL:** [License](https://github.com/city96/ComfyUI-GGUF/blob/main/LICENSE)

## Languages

- **Python:** 64,049 bytes

## Top Contributors

1. **city96** - 140 contributions
2. **blepping** - 20 contributions
3. **RandomGitUser321** - 1 contributions
4. **StrongerXi** - 1 contributions
5. **YarvixPA** - 1 contributions
6. **ddh0** - 1 contributions

## File Structure (Sample of 10 files)

Total files: 17

- `.gitignore` (blob)
- `LICENSE` (blob)
- `README.md` (blob)
- `__init__.py` (blob)
- `dequant.py` (blob)
- `loader.py` (blob)
- `nodes.py` (blob)
- `ops.py` (blob)
- `pyproject.toml` (blob)
- `requirements.txt` (blob)

## Recent Issues

- 游릭 **#291** Custom Wan2.1 VACE model failing to convert (open)
- 游릭 **#290** Request wan2.1-magref GGUF support (open)
- 游릭 **#289** Does not work on windows? (open)
- 游릭 **#288** Updated quantizing tool guide to compile with Visual Studio 2022 (open)
- 游댮 **#287** Comfyui-GGUF (closed)

## Recent Pull Requests

- 游릭 **#288** Updated quantizing tool guide to compile with Visual Studio 2022 (open)
- 游릭 **#286** [Efficiency improvement] GGUF conversion - file caching. (open)
- 游릭 **#274** Added gguf conversion gradio gui (open)
- 游댮 **#266** LLaMA OOM hotfix (closed)
- 游댮 **#263** Dynamic CLIP input types (closed)

## Recent Commits

- **b3ec875a** Cosmos convert logic - City (2025-06-14T22:09:07+00:00)
- **a2b75978** Create pyproject.toml - City (2025-05-08T23:08:35+00:00)
- **6570efec** Better warning for incompatible models - City (2025-05-07T03:13:08+00:00)
- **03842fab** Remove old workaround - City (2025-05-07T02:42:20+00:00)
- **3d673c5c** Wan FLF2V convert logic - City (2025-04-26T19:16:43+00:00)
- **54a4854e** Allow embedding shape mismatch - City (2025-04-22T13:50:05+00:00)
- **66c50234** Merge pull request #266 from city96/llama_oom_hotfix - City (2025-04-21T21:30:02+00:00)
- **220f78ec** Force recalculation of model size on clone - City (2025-04-21T13:22:11+00:00)
- **9d282bcd** LLaMA OOM hotfix - City (2025-04-21T02:25:30+00:00)
- **a355f8b5** Skip unlink logic if load/offload dev match - City (2025-04-18T21:53:12+00:00)

## External Links Found in README

- https://github.com/ggerganov/llama.cpp
- https://huggingface.co/city96/FLUX.1-dev-gguf
- https://huggingface.co/city96/FLUX.1-schnell-gguf
- https://github.com/city96/ComfyUI-GGUF/issues/107
- https://github.com/city96/ComfyUI-GGUF
- https://huggingface.co/city96/stable-diffusion-3.5-large-gguf
- https://huggingface.co/city96/t5-v1_1-xxl-encoder-gguf
- https://github.com/city96/ComfyUI-GGUF/tree/main/tools
- https://github.com/user-attachments/assets/70d16d97-c522-4ef4-9435-633f128644c8
- https://huggingface.co/city96/stable-diffusion-3.5-large-turbo-gguf

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 842768870,
  "name": "ComfyUI-GGUF",
  "full_name": "city96/ComfyUI-GGUF",
  "description": "GGUF Quantization support for native ComfyUI models",
  "html_url": "https://github.com/city96/ComfyUI-GGUF",
  "clone_url": "https://github.com/city96/ComfyUI-GGUF.git",
  "ssh_url": "git@github.com:city96/ComfyUI-GGUF.git",
  "homepage": null,
  "topics": [],
  "default_branch": "main",
  "created_at": "2024-08-15T03:26:26+00:00",
  "updated_at": "2025-06-21T20:19:28+00:00",
  "pushed_at": "2025-06-14T23:07:45+00:00",
  "size_kb": 157,
  "watchers_count": 2088,
  "stargazers_count": 2088,
  "forks_count": 139,
  "open_issues_count": 124,
  "license": {
    "key": "apache-2.0",
    "name": "Apache License 2.0",
    "spdx_id": "Apache-2.0",
    "url": "https://github.com/city96/ComfyUI-GGUF/blob/main/LICENSE"
  },
  "languages": {
    "Python": 64049
  },
  "top_contributors": [
    {
      "login": "city96",
      "contributions": 140
    },
    {
      "login": "blepping",
      "contributions": 20
    },
    {
      "login": "RandomGitUser321",
      "contributions": 1
    },
    {
      "login": "StrongerXi",
      "contributions": 1
    },
    {
      "login": "YarvixPA",
      "contributions": 1
    },
    {
      "login": "ddh0",
      "contributions": 1
    }
  ],
  "file_tree_count": 17,
  "file_tree_sample": [
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "__init__.py",
      "type": "blob"
    },
    {
      "path": "dequant.py",
      "type": "blob"
    },
    {
      "path": "loader.py",
      "type": "blob"
    },
    {
      "path": "nodes.py",
      "type": "blob"
    },
    {
      "path": "ops.py",
      "type": "blob"
    },
    {
      "path": "pyproject.toml",
      "type": "blob"
    },
    {
      "path": "requirements.txt",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 25,
  "recent_issues": [
    {
      "number": 291,
      "title": "Custom Wan2.1 VACE model failing to convert",
      "state": "open"
    },
    {
      "number": 290,
      "title": "Request wan2.1-magref GGUF support",
      "state": "open"
    },
    {
      "number": 289,
      "title": "Does not work on windows?",
      "state": "open"
    },
    {
      "number": 288,
      "title": "Updated quantizing tool guide to compile with Visual Studio 2022",
      "state": "open"
    },
    {
      "number": 287,
      "title": "Comfyui-GGUF",
      "state": "closed"
    }
  ],
  "recent_pulls": [
    {
      "number": 288,
      "title": "Updated quantizing tool guide to compile with Visual Studio 2022",
      "state": "open"
    },
    {
      "number": 286,
      "title": "[Efficiency improvement] GGUF conversion - file caching.",
      "state": "open"
    },
    {
      "number": 274,
      "title": "Added gguf conversion gradio gui",
      "state": "open"
    },
    {
      "number": 266,
      "title": "LLaMA OOM hotfix",
      "state": "closed"
    },
    {
      "number": 263,
      "title": "Dynamic CLIP input types",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "b3ec875a68d94b758914fd48d30571d953bb7a54",
      "author": "City",
      "date": "2025-06-14T22:09:07+00:00",
      "message": "Cosmos convert logic"
    },
    {
      "sha": "a2b75978fd50c0227a58316619b79d525b88e570",
      "author": "City",
      "date": "2025-05-08T23:08:35+00:00",
      "message": "Create pyproject.toml"
    },
    {
      "sha": "6570efec6992015085f11b84e42d32f6cc71e8b7",
      "author": "City",
      "date": "2025-05-07T03:13:08+00:00",
      "message": "Better warning for incompatible models"
    },
    {
      "sha": "03842fab0a78fd142b020e42ec76d871d947bc32",
      "author": "City",
      "date": "2025-05-07T02:42:20+00:00",
      "message": "Remove old workaround"
    },
    {
      "sha": "3d673c5c098ecaa6e6027f834659ba8de534ca32",
      "author": "City",
      "date": "2025-04-26T19:16:43+00:00",
      "message": "Wan FLF2V convert logic"
    },
    {
      "sha": "54a4854e0c006cf61494d29644ed5f4a20ad02c3",
      "author": "City",
      "date": "2025-04-22T13:50:05+00:00",
      "message": "Allow embedding shape mismatch"
    },
    {
      "sha": "66c50234a45b949072f8c063147211037add9030",
      "author": "City",
      "date": "2025-04-21T21:30:02+00:00",
      "message": "Merge pull request #266 from city96/llama_oom_hotfix"
    },
    {
      "sha": "220f78ec7038751701598a16a2d75365efa47db1",
      "author": "City",
      "date": "2025-04-21T13:22:11+00:00",
      "message": "Force recalculation of model size on clone"
    },
    {
      "sha": "9d282bcdfd3299e2bc9f0f55a4ac535cbf340935",
      "author": "City",
      "date": "2025-04-21T02:25:30+00:00",
      "message": "LLaMA OOM hotfix"
    },
    {
      "sha": "a355f8b5eab3f83408eb95546f09cec72e028f04",
      "author": "City",
      "date": "2025-04-18T21:53:12+00:00",
      "message": "Skip unlink logic if load/offload dev match"
    },
    {
      "sha": "3564e95b26d0d9c86df88bf0d3d55ca20750f265",
      "author": "City",
      "date": "2025-04-18T21:48:53+00:00",
      "message": "Dynamic CLIP input types (#263)"
    },
    {
      "sha": "e29f526c6826f06166d1ad81910b60d28b19cf8b",
      "author": "City",
      "date": "2025-04-16T14:07:51+00:00",
      "message": "Disable mixing scaled FP8 + GGUF"
    },
    {
      "sha": "47bec6147569a138dd30ad3e14f190a36a3be456",
      "author": "City",
      "date": "2025-04-16T04:10:28+00:00",
      "message": "HiDream text encoder support"
    },
    {
      "sha": "3605bfbead40f4d56622281519ef8f04ba7b33fb",
      "author": "City",
      "date": "2025-04-16T00:22:31+00:00",
      "message": "Keep ffn down for HiDream"
    },
    {
      "sha": "2ef5a66a9fbecc44f1928ea112a2505c9b16e38d",
      "author": "City",
      "date": "2025-04-15T23:43:15+00:00",
      "message": "HiDream support"
    },
    {
      "sha": "794fc285363a7250102bbd8a687fe21439d570b1",
      "author": "City",
      "date": "2025-04-13T21:01:15+00:00",
      "message": "Clean up logging"
    },
    {
      "sha": "6de4bdba30f142955ebf6f210533000ef094bf0e",
      "author": "City",
      "date": "2025-04-10T06:11:58+00:00",
      "message": "Merge pull request #216 from city96/convert_refactor_new"
    },
    {
      "sha": "ca97e050b3ad621dd3b2a5f05249f31164cf3377",
      "author": "City",
      "date": "2025-04-10T06:04:17+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "de33c984c19eb0efc72dc2a4f5ac23887e1c28a6",
      "author": "City",
      "date": "2025-04-10T05:12:14+00:00",
      "message": "Merge branch 'main' into convert_refactor_new"
    },
    {
      "sha": "624687939f1d361b59770f0a0b5bf75b7d5892e9",
      "author": "City",
      "date": "2025-04-10T05:12:10+00:00",
      "message": "Refactor 5D patch and save loop"
    }
  ],
  "readme_text": "# ComfyUI-GGUF\nGGUF Quantization support for native ComfyUI models\n\nThis is currently very much WIP. These custom nodes provide support for model files stored in the GGUF format popularized by [llama.cpp](https://github.com/ggerganov/llama.cpp).\n\nWhile quantization wasn't feasible for regular UNET models (conv2d), transformer/DiT models such as flux seem less affected by quantization. This allows running it in much lower bits per weight variable bitrate quants on low-end GPUs. For further VRAM savings, a node to load a quantized version of the T5 text encoder is also included.\n\n![Comfy_Flux1_dev_Q4_0_GGUF_1024](https://github.com/user-attachments/assets/70d16d97-c522-4ef4-9435-633f128644c8)\n\nNote: The \"Force/Set CLIP Device\" is **NOT** part of this node pack. Do not install it if you only have one GPU. Do not set it to cuda:0 then complain about OOM errors if you do not undestand what it is for. There is not need to copy the workflow above, just use your own workflow and replace the stock \"Load Diffusion Model\" with the \"Unet Loader (GGUF)\" node.\n\n## Installation\n\n> [!IMPORTANT]  \n> Make sure your ComfyUI is on a recent-enough version to support custom ops when loading the UNET-only.\n\nTo install the custom node normally, git clone this repository into your custom nodes folder (`ComfyUI/custom_nodes`) and install the only dependency for inference (`pip install --upgrade gguf`)\n\n```\ngit clone https://github.com/city96/ComfyUI-GGUF\n```\n\nTo install the custom node on a standalone ComfyUI release, open a CMD inside the \"ComfyUI_windows_portable\" folder (where your `run_nvidia_gpu.bat` file is) and use the following commands:\n\n```\ngit clone https://github.com/city96/ComfyUI-GGUF ComfyUI/custom_nodes/ComfyUI-GGUF\n.\\python_embeded\\python.exe -s -m pip install -r .\\ComfyUI\\custom_nodes\\ComfyUI-GGUF\\requirements.txt\n```\n\nOn MacOS sequoia, torch 2.4.1 seems to be required, as 2.6.X nightly versions cause a \"M1 buffer is not large enough\" error. See [this issue](https://github.com/city96/ComfyUI-GGUF/issues/107) for more information/workarounds.\n\n## Usage\n\nSimply use the GGUF Unet loader found under the `bootleg` category. Place the .gguf model files in your `ComfyUI/models/unet` folder.\n\nLoRA loading is experimental but it should work with just the built-in LoRA loader node(s).\n\nPre-quantized models:\n\n- [flux1-dev GGUF](https://huggingface.co/city96/FLUX.1-dev-gguf)\n- [flux1-schnell GGUF](https://huggingface.co/city96/FLUX.1-schnell-gguf)\n- [stable-diffusion-3.5-large GGUF](https://huggingface.co/city96/stable-diffusion-3.5-large-gguf)\n- [stable-diffusion-3.5-large-turbo GGUF](https://huggingface.co/city96/stable-diffusion-3.5-large-turbo-gguf)\n\nInitial support for quantizing T5 has also been added recently, these can be used using the various `*CLIPLoader (gguf)` nodes which can be used inplace of the regular ones. For the CLIP model, use whatever model you were using before for CLIP. The loader can handle both types of files - `gguf` and regular `safetensors`/`bin`.\n\n- [t5_v1.1-xxl GGUF](https://huggingface.co/city96/t5-v1_1-xxl-encoder-gguf)\n\nSee the instructions in the [tools](https://github.com/city96/ComfyUI-GGUF/tree/main/tools) folder for how to create your own quants.\n",
  "external_links_in_readme": [
    "https://github.com/ggerganov/llama.cpp",
    "https://huggingface.co/city96/FLUX.1-dev-gguf",
    "https://huggingface.co/city96/FLUX.1-schnell-gguf",
    "https://github.com/city96/ComfyUI-GGUF/issues/107",
    "https://github.com/city96/ComfyUI-GGUF",
    "https://huggingface.co/city96/stable-diffusion-3.5-large-gguf",
    "https://huggingface.co/city96/t5-v1_1-xxl-encoder-gguf",
    "https://github.com/city96/ComfyUI-GGUF/tree/main/tools",
    "https://github.com/user-attachments/assets/70d16d97-c522-4ef4-9435-633f128644c8",
    "https://huggingface.co/city96/stable-diffusion-3.5-large-turbo-gguf"
  ]
}
```

</details>


---

