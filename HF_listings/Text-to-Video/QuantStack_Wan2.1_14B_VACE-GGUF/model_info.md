# QuantStack/Wan2.1_14B_VACE-GGUF

## Model Information

- **Model ID**: QuantStack/Wan2.1_14B_VACE-GGUF
- **Author**: QuantStack
- **Last Updated**: 2025-06-11 13:59:15+00:00
- **Repository**: https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF

## Tags

gguf, video, video-generation, text-to-video, base_model:Wan-AI/Wan2.1-VACE-14B, base_model:quantized:Wan-AI/Wan2.1-VACE-14B, license:apache-2.0, region:us

## File Tree

- **.gitattributes** (None bytes)
  - Download: [https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/.gitattributes](https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/.gitattributes)
- **README.md** (None bytes)
  - Download: [https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/README.md](https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/README.md)
- **Wan2.1_14B_VACE-BF16.gguf** (None bytes)
  - Download: [https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-BF16.gguf](https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-BF16.gguf)
- **Wan2.1_14B_VACE-F16.gguf** (None bytes)
  - Download: [https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-F16.gguf](https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-F16.gguf)
- **Wan2.1_14B_VACE-Q3_K_S.gguf** (None bytes)
  - Download: [https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q3_K_S.gguf](https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q3_K_S.gguf)
- **Wan2.1_14B_VACE-Q4_0.gguf** (None bytes)
  - Download: [https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q4_0.gguf](https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q4_0.gguf)
- **Wan2.1_14B_VACE-Q4_1.gguf** (None bytes)
  - Download: [https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q4_1.gguf](https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q4_1.gguf)
- **Wan2.1_14B_VACE-Q4_K_M.gguf** (None bytes)
  - Download: [https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q4_K_M.gguf](https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q4_K_M.gguf)
- **Wan2.1_14B_VACE-Q4_K_S.gguf** (None bytes)
  - Download: [https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q4_K_S.gguf](https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q4_K_S.gguf)
- **Wan2.1_14B_VACE-Q5_0.gguf** (None bytes)
  - Download: [https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q5_0.gguf](https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q5_0.gguf)
- **Wan2.1_14B_VACE-Q5_1.gguf** (None bytes)
  - Download: [https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q5_1.gguf](https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q5_1.gguf)
- **Wan2.1_14B_VACE-Q5_K_M.gguf** (None bytes)
  - Download: [https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q5_K_M.gguf](https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q5_K_M.gguf)
- **Wan2.1_14B_VACE-Q5_K_S.gguf** (None bytes)
  - Download: [https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q5_K_S.gguf](https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q5_K_S.gguf)
- **Wan2.1_14B_VACE-Q6_K.gguf** (None bytes)
  - Download: [https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q6_K.gguf](https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q6_K.gguf)
- **Wan2.1_14B_VACE-Q8_0.gguf** (None bytes)
  - Download: [https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q8_0.gguf](https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/Wan2.1_14B_VACE-Q8_0.gguf)
- **vace_v2v_example_workflow.json** (None bytes)
  - Download: [https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/vace_v2v_example_workflow.json](https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF/resolve/main/vace_v2v_example_workflow.json)


## External Links

- [docs.comfy.org](https://docs.comfy.org/tutorials/video/wan/vace)
- [github.com](https://github.com/city96/ComfyUI-GGUF)
- [github.com](https://github.com/ggerganov/llama.cpp/blob/master/examples/perplexity/README.md#llama-3-8b-scoreboard)
- [huggingface.co](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)
- [huggingface.co](https://huggingface.co/QuantStack/Wan2.1-VACE-14B-GGUF/blob/main/vace_v2v_example_workflow.json)
- [huggingface.co](https://huggingface.co/Wan-AI/Wan2.1-VACE-14B)
- [huggingface.co](https://huggingface.co/city96)


## README.md

```markdown
---
license: apache-2.0
library_name: gguf
base_model:
- Wan-AI/Wan2.1-VACE-14B
tags:
- video
- video-generation
pipeline_tag: text-to-video
---

[**Example workflow**](https://huggingface.co/QuantStack/Wan2.1-VACE-14B-GGUF/blob/main/vace_v2v_example_workflow.json) - based on the [Comfyui example workflow](https://docs.comfy.org/tutorials/video/wan/vace)

This is a direct GGUF conversion of [Wan-AI/Wan2.1-VACE-14B](https://huggingface.co/Wan-AI/Wan2.1-VACE-14B)

All quants are created from the FP32 base file, though I only uploaded the Q8_0 and less, if you want the F16 or BF16 one I would upload it per request. 

The model files can be used with the [ComfyUI-GGUF](https://github.com/city96/ComfyUI-GGUF) custom node.

Place model files in `ComfyUI/models/unet` - see the GitHub readme for further install instructions.

The VAE can be downloaded from [here](https://huggingface.co/Comfy-Org/Wan_2.1_ComfyUI_repackaged/resolve/main/split_files/vae/wan_2.1_vae.safetensors?download=true)

Please refer to [this chart](https://github.com/ggerganov/llama.cpp/blob/master/examples/perplexity/README.md#llama-3-8b-scoreboard) for a basic overview of quantization types.

For conversion I used the conversion scripts from [city96](https://huggingface.co/city96)
```


---

*Generated on 2025-06-21 15:07:26*
*Source: https://huggingface.co/QuantStack/Wan2.1_14B_VACE-GGUF*
