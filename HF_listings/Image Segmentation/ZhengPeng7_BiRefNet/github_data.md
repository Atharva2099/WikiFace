# GitHub Data for ZhengPeng7_BiRefNet

**Task Category:** Image Segmentation

## Repository 1: ZhengPeng7/BiRefNet.git

# GitHub Data Extraction Error

Error: 404 {"message": "Not Found", "documentation_url": "https://docs.github.com/rest/repos/repos#get-a-repository", "status": "404"}

Repository: https://github.com/ZhengPeng7/BiRefNet.git

---

## Repository 2: ZhengPeng7/BiRefNet

# GitHub Repository Data

**Repository:** [ZhengPeng7/BiRefNet](https://github.com/ZhengPeng7/BiRefNet)

## Basic Information

- **Description:** [CAAI AIR'24] Bilateral Reference for High-Resolution Dichotomous Image Segmentation
- **Created:** 2022-08-17T09:13:39+00:00
- **Last Updated:** 2025-06-21T22:32:48+00:00
- **Last Pushed:** 2025-06-11T07:38:12+00:00
- **Default Branch:** main
- **Size:** 5575 KB

## Statistics

- **Stars:** 2,374
- **Forks:** 174
- **Watchers:** 2,374
- **Open Issues:** 9
- **Total Issues:** 0
- **Pull Requests:** 15

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/ZhengPeng7/BiRefNet/blob/main/LICENSE)

## Languages

- **Python:** 178,915 bytes
- **Jupyter Notebook:** 137,250 bytes
- **Shell:** 3,938 bytes

## Topics

- `dichotomous-image-segmentation`
- `salient-object-detection`
- `camouflaged-object-detection`
- `background-removal`
- `birefnet`
- `image-segmentation`
- `high-resolution-image-segmentation`

## Top Contributors

1. **ZhengPeng7** - 336 contributions
2. **PierreMarieCurie** - 2 contributions
3. **DengPingFan** - 1 contributions
4. **Alive1024** - 1 contributions
5. **helinyu** - 1 contributions

## File Structure (Sample of 10 files)

Total files: 40

- `.gitignore` (blob)
- `LICENSE` (blob)
- `README.md` (blob)
- `config.py` (blob)
- `dataset.py` (blob)
- `eval_existingOnes.py` (blob)
- `evaluation` (tree)
- `evaluation/metrics.py` (blob)
- `gen_best_ep.py` (blob)
- `image_proc.py` (blob)

## Recent Issues

- ðŸŸ¢ **#228** Support Prompting with Vague Segmentation Masks for Refinement (open)
- ðŸ”´ **#227** Prompt outputs failed validation: ImagePreprocessor: - Required input is missing: remove_bg_fn (closed)
- ðŸŸ¢ **#226** The refine_foreground function takes a long time to execute; how can it be optimized? (open)
- ðŸ”´ **#225** How to convert Matting model to FP16 (closed)
- ðŸ”´ **#223** Fine-tune with Matting data. (closed)

## Recent Pull Requests

- ðŸ”´ **#219** Initiate multi class segmentation using BiRefNet (closed)
- ðŸ”´ **#201** Matteo birefnet version (closed)
- ðŸ”´ **#196** Handle non-RGB Pillow images (closed)
- ðŸ”´ **#194** Restructure Repository as a Python Module to Fix Import Conflicts (closed)
- ðŸŸ¢ **#167** Export dynamic batch size ONNX using ONNX's DeformConv (open)

## Recent Commits

- **3180f1f7** Fix a mistake in logging. - Peng Zheng (2025-06-11T07:38:11+00:00)
- **76c1df05** Show the exception reasons. - root (2025-06-09T10:46:08+00:00)
- **81c95f53** Accelerate refine_background a little bit via manually setting dtype to np.float32. Fix a bug in using timeit in a notebook. - root (2025-06-05T10:17:14+00:00)
- **5085740f** Fix a typo in print. - Peng Zheng (2025-06-03T09:50:47+00:00)
- **56981f73** Remove an `1 or ..` in an if from inference.py. - Peng Zheng (2025-05-30T08:25:19+00:00)
- **08849779** Explicitly set bash as the interpreter for all shell scripts. - root (2025-05-22T10:06:53+00:00)
- **e699d6cc** Make the whole project bf16 adaptable and remove the force half precision conversion in inference notebooks. - root (2025-05-21T09:20:48+00:00)
- **8d7514dc** Add the links to the video tutorials on fine-tuning and the updates of BiRefNet_dynamic in the README.md. - Peng Zheng (2025-05-15T03:50:12+00:00)
- **392944c9** Turn off the find_unused_parameters in DDP args of accelerator in the train.py. - Peng Zheng (2025-05-15T03:01:28+00:00)
- **c489f94b** Specify the resume_weights_path in one var in train.sh. - Peng Zheng (2025-05-15T02:43:16+00:00)

## External Links Found in README

- https://github.com/ZhengPeng7/BiRefNet/assets/25921713/1a32860c-0893-49dd-b557-c2e35a83c160>
- https://github.com/lldacing/ComfyUI_BiRefNet_ll
- https://github.com/lldacing/ComfyUI_BiRefNet_ll/raw/main/doc/video.gif"
- https://scholar.google.com/citations?user=0uPb8MMAAAAJ'
- https://cy.ncss.cn/information/2c93f4c691983c5b0194264b1880207b
- https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-te3
- https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-blue
- https://arxiv.org/pdf/2401.03407
- https://drive.google.com/file/d/1FWvKDWTnK9RsiywfCsIxsnQzqv-dlO5u/view'><img
- https://drive.google.com/thumbnail?id=1nvVIFt_Ezs-crPSQxUDqkUBz598fTe63&sz=w1620"
- https://drive.google.com/thumbnail?id=16EuyqKFJOqwMmagvfnbC9hUurL9pYLLB&sz=w1620"
- https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/salient-object-detection-on-duts-te
- https://fal.ai/
- https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue
- https://drive.google.com/file/d/1tM5M72k7a8aKF-dYy-QXaqvfEhbFaWkC/view
- https://drive.google.com/file/d/1Pu1mv3ORobJatIuUoEuZaWDl2ylP3Gw7/view
- https://drive.google.com/thumbnail?id=1TYZF8pVZc2V0V6g3ik4iAr9iKvJ8BNrf&sz=w1620"
- https://www.alibaba.com
- https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-vd
- https://drive.google.com/thumbnail?id=1DLt6CFXdT1QSWDj_6jRkyZINXZ4vmyRp&sz=w1620"

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 525717745,
  "name": "BiRefNet",
  "full_name": "ZhengPeng7/BiRefNet",
  "description": "[CAAI AIR'24] Bilateral Reference for High-Resolution Dichotomous Image Segmentation",
  "html_url": "https://github.com/ZhengPeng7/BiRefNet",
  "clone_url": "https://github.com/ZhengPeng7/BiRefNet.git",
  "ssh_url": "git@github.com:ZhengPeng7/BiRefNet.git",
  "homepage": "https://www.birefnet.top",
  "topics": [
    "dichotomous-image-segmentation",
    "salient-object-detection",
    "camouflaged-object-detection",
    "background-removal",
    "birefnet",
    "image-segmentation",
    "high-resolution-image-segmentation"
  ],
  "default_branch": "main",
  "created_at": "2022-08-17T09:13:39+00:00",
  "updated_at": "2025-06-21T22:32:48+00:00",
  "pushed_at": "2025-06-11T07:38:12+00:00",
  "size_kb": 5575,
  "watchers_count": 2374,
  "stargazers_count": 2374,
  "forks_count": 174,
  "open_issues_count": 9,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/ZhengPeng7/BiRefNet/blob/main/LICENSE"
  },
  "languages": {
    "Python": 178915,
    "Jupyter Notebook": 137250,
    "Shell": 3938
  },
  "top_contributors": [
    {
      "login": "ZhengPeng7",
      "contributions": 336
    },
    {
      "login": "PierreMarieCurie",
      "contributions": 2
    },
    {
      "login": "DengPingFan",
      "contributions": 1
    },
    {
      "login": "Alive1024",
      "contributions": 1
    },
    {
      "login": "helinyu",
      "contributions": 1
    }
  ],
  "file_tree_count": 40,
  "file_tree_sample": [
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "config.py",
      "type": "blob"
    },
    {
      "path": "dataset.py",
      "type": "blob"
    },
    {
      "path": "eval_existingOnes.py",
      "type": "blob"
    },
    {
      "path": "evaluation",
      "type": "tree"
    },
    {
      "path": "evaluation/metrics.py",
      "type": "blob"
    },
    {
      "path": "gen_best_ep.py",
      "type": "blob"
    },
    {
      "path": "image_proc.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 15,
  "recent_issues": [
    {
      "number": 228,
      "title": "Support Prompting with Vague Segmentation Masks for Refinement",
      "state": "open"
    },
    {
      "number": 227,
      "title": "Prompt outputs failed validation: ImagePreprocessor: - Required input is missing: remove_bg_fn",
      "state": "closed"
    },
    {
      "number": 226,
      "title": "The refine_foreground function takes a long time to execute; how can it be optimized?",
      "state": "open"
    },
    {
      "number": 225,
      "title": "How to convert Matting model to FP16",
      "state": "closed"
    },
    {
      "number": 223,
      "title": "Fine-tune with Matting data.",
      "state": "closed"
    }
  ],
  "recent_pulls": [
    {
      "number": 219,
      "title": "Initiate multi class segmentation using BiRefNet",
      "state": "closed"
    },
    {
      "number": 201,
      "title": "Matteo birefnet version",
      "state": "closed"
    },
    {
      "number": 196,
      "title": "Handle non-RGB Pillow images",
      "state": "closed"
    },
    {
      "number": 194,
      "title": "Restructure Repository as a Python Module to Fix Import Conflicts",
      "state": "closed"
    },
    {
      "number": 167,
      "title": "Export dynamic batch size ONNX using ONNX's DeformConv",
      "state": "open"
    }
  ],
  "recent_commits": [
    {
      "sha": "3180f1f7855ba1d2d27b9f231a631274b773d04a",
      "author": "Peng Zheng",
      "date": "2025-06-11T07:38:11+00:00",
      "message": "Fix a mistake in logging."
    },
    {
      "sha": "76c1df05b67bef8e9af8f062b7ec2041c54d59f4",
      "author": "root",
      "date": "2025-06-09T10:46:08+00:00",
      "message": "Show the exception reasons."
    },
    {
      "sha": "81c95f5390ec7e2535628972bcec3d2de88fa7ed",
      "author": "root",
      "date": "2025-06-05T10:17:14+00:00",
      "message": "Accelerate refine_background a little bit via manually setting dtype to np.float32. Fix a bug in using timeit in a notebook."
    },
    {
      "sha": "5085740f57cc2d6e495d81533cab6adc9342bf27",
      "author": "Peng Zheng",
      "date": "2025-06-03T09:50:47+00:00",
      "message": "Fix a typo in print."
    },
    {
      "sha": "56981f73c9aa8cf3f7a9aa85ed702499f09441a8",
      "author": "Peng Zheng",
      "date": "2025-05-30T08:25:19+00:00",
      "message": "Remove an `1 or ..` in an if from inference.py."
    },
    {
      "sha": "088497793af002e40e89d7a721ca6aa60c935d60",
      "author": "root",
      "date": "2025-05-22T10:06:53+00:00",
      "message": "Explicitly set bash as the interpreter for all shell scripts."
    },
    {
      "sha": "e699d6cc8dbafc3e828327346da2bb2a31aa453d",
      "author": "root",
      "date": "2025-05-21T09:20:48+00:00",
      "message": "Make the whole project bf16 adaptable and remove the force half precision conversion in inference notebooks."
    },
    {
      "sha": "8d7514dc3abd647bfedb2bf55ef315973e50aeee",
      "author": "Peng Zheng",
      "date": "2025-05-15T03:50:12+00:00",
      "message": "Add the links to the video tutorials on fine-tuning and the updates of BiRefNet_dynamic in the README.md."
    },
    {
      "sha": "392944c927794a612df88be1389a77dbfd88eed5",
      "author": "Peng Zheng",
      "date": "2025-05-15T03:01:28+00:00",
      "message": "Turn off the find_unused_parameters in DDP args of accelerator in the train.py."
    },
    {
      "sha": "c489f94baf1471c04fcf2f4a8df75f07084048f2",
      "author": "Peng Zheng",
      "date": "2025-05-15T02:43:16+00:00",
      "message": "Specify the resume_weights_path in one var in train.sh."
    },
    {
      "sha": "b85432b5e949a5159dd319eb80f6cafcda04589a",
      "author": "Peng Zheng",
      "date": "2025-05-13T14:53:46+00:00",
      "message": "Release the restriction of torch version in the requirements.txt."
    },
    {
      "sha": "ee1a77212be401bc5f2980b4f6bb0b48cdfde1dc",
      "author": "root",
      "date": "2025-05-13T14:50:04+00:00",
      "message": "Set the size in config.py as the default size used in inference."
    },
    {
      "sha": "c44a8e54bcca26886f09d531cbe72b39e7fc9efb",
      "author": "root",
      "date": "2025-05-13T07:38:37+00:00",
      "message": "Fix a bug in the using of pix_loss in training."
    },
    {
      "sha": "f2366eebe53135f91dcee3c0b72dc097348252de",
      "author": "root",
      "date": "2025-05-13T07:37:25+00:00",
      "message": "Fix a bug in the using of pix_loss in training."
    },
    {
      "sha": "c8266bba91253104b393222942ca9801186e2683",
      "author": "ZhengPeng7",
      "date": "2025-05-08T07:38:41+00:00",
      "message": "Upgrade the loss info callbacks."
    },
    {
      "sha": "dc48f78046db366b0b742eb3abb79d3ea17fa11a",
      "author": "ZhengPeng7",
      "date": "2025-05-08T07:01:28+00:00",
      "message": "Move model saving into only the main process."
    },
    {
      "sha": "38fc1bd6ec579245f74d230464967263886f10df",
      "author": "ZhengPeng7",
      "date": "2025-05-08T07:01:11+00:00",
      "message": "Move model saving into only the main process."
    },
    {
      "sha": "0e1574c4f878d6b21e4a9029c3791948735054d3",
      "author": "root",
      "date": "2025-04-23T03:22:05+00:00",
      "message": "Update the bg color to standard green screen color."
    },
    {
      "sha": "259da71a41ca310e44e5ef37820c698419451170",
      "author": "root",
      "date": "2025-04-23T03:20:16+00:00",
      "message": "Upgrade the tutorial notebook for inference with a clearer visual comparison."
    },
    {
      "sha": "2bc9dd4a1ebfb42ad19730eff729201f0f335ef4",
      "author": "root",
      "date": "2025-04-18T12:59:19+00:00",
      "message": "Fix bugs with using CNN backbones."
    }
  ],
  "readme_text": "<h1 align=\"center\">Bilateral Reference for High-Resolution Dichotomous Image Segmentation</h1>\n\n<div align='center'>\n    <a href='https://scholar.google.com/citations?user=TZRzWOsAAAAJ' target='_blank'><strong>Peng Zheng</strong></a><sup> 1,4,5,6</sup>,&thinsp;\n    <a href='https://scholar.google.com/citations?user=0uPb8MMAAAAJ' target='_blank'><strong>Dehong Gao</strong></a><sup> 2</sup>,&thinsp;\n    <a href='https://scholar.google.com/citations?user=kakwJ5QAAAAJ' target='_blank'><strong>Deng-Ping Fan</strong></a><sup> 1*</sup>,&thinsp;\n    <a href='https://scholar.google.com/citations?user=9cMQrVsAAAAJ' target='_blank'><strong>Li Liu</strong></a><sup> 3</sup>,&thinsp;\n    <a href='https://scholar.google.com/citations?user=qQP6WXIAAAAJ' target='_blank'><strong>Jorma Laaksonen</strong></a><sup> 4</sup>,&thinsp;\n    <a href='https://scholar.google.com/citations?user=pw_0Z_UAAAAJ' target='_blank'><strong>Wanli Ouyang</strong></a><sup> 5</sup>,&thinsp;\n    <a href='https://scholar.google.com/citations?user=stFCYOAAAAAJ' target='_blank'><strong>Nicu Sebe</strong></a><sup> 6</sup>\n</div>\n\n<div align='center'>\n    <sup>1 </sup>Nankai University&ensp;  <sup>2 </sup>Northwestern Polytechnical University&ensp;  <sup>3 </sup>National University of Defense Technology&ensp; \n    <br />\n    <sup>4 </sup>Aalto University&ensp;  <sup>5 </sup>Shanghai AI Laboratory&ensp;  <sup>6 </sup>University of Trento&ensp; \n</div>\n\n<div align=\"center\" style=\"display: flex; justify-content: center; flex-wrap: wrap;\">\n  <a href='https://www.sciopen.com/article/pdf/10.26599/AIR.2024.9150038.pdf'><img src='https://img.shields.io/badge/Journal-Paper-red'></a>&ensp; \n  <a href='https://arxiv.org/pdf/2401.03407'><img src='https://img.shields.io/badge/arXiv-Paper-red'></a>&ensp; \n  <a href='https://drive.google.com/file/d/1FWvKDWTnK9RsiywfCsIxsnQzqv-dlO5u/view'><img src='https://img.shields.io/badge/\u4e2d\u6587\u7248-Paper-red'></a>&ensp; \n  <a href='https://www.birefnet.top'><img src='https://img.shields.io/badge/Page-Project-red'></a>&ensp; \n  <a href='https://drive.google.com/drive/folders/1s2Xe0cjq-2ctnJBR24563yMSCOu4CcxM'><img src='https://img.shields.io/badge/GDrive-Stuff-green'></a>&ensp; \n  <a href='LICENSE'><img src='https://img.shields.io/badge/License-MIT-yellow'></a>&ensp; \n  <a href='https://huggingface.co/spaces/ZhengPeng7/BiRefNet_demo'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20HF-Space-blue'></a>&ensp; \n  <a href='https://huggingface.co/ZhengPeng7/BiRefNet'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20HF-Model-blue'></a>&ensp; \n</div>\n\n<div align=\"center\" style=\"display: flex; justify-content: center; flex-wrap: wrap;\">\n  <a href='https://colab.research.google.com/drive/14Dqg7oeBkFEtchaHLNpig2BcdkZEogba'><img src='https://img.shields.io/badge/Multiple_Images_Inference-F9AB00?style=for-the-badge&logo=googlecolab&color=525252'></a>&ensp; \n  <a href='https://colab.research.google.com/drive/1MaEiBfJ4xIaZZn0DqKrhydHB8X97hNXl'><img src='https://img.shields.io/badge/Inference_&_Evaluation-F9AB00?style=for-the-badge&logo=googlecolab&color=525252'></a>&ensp; \n  <a href='https://colab.research.google.com/drive/1B6aKZ3ekcvKMkSBn0N5mCASLUYMp0whK'><img src='https://img.shields.io/badge/Box_Guided_Segmentation-F9AB00?style=for-the-badge&logo=googlecolab&color=525252'></a>&ensp; \n</div>\n\n\n|            *DIS-Sample_1*        |             *DIS-Sample_2*        |\n| :------------------------------: | :-------------------------------: |\n| <img src=\"https://drive.google.com/thumbnail?id=1ItXaA26iYnE8XQ_GgNLy71MOWePoS2-g&sz=w400\" /> |  <img src=\"https://drive.google.com/thumbnail?id=1Z-esCujQF_uEa_YJjkibc3NUrW4aR_d4&sz=w400\" /> |\n\nThis repo is the official implementation of \"[**Bilateral Reference for High-Resolution Dichotomous Image Segmentation**](https://arxiv.org/pdf/2401.03407)\" (___CAAI AIR 2024___).\n\n> [!note]\n> **We need more GPU resources** to push forward the performance of BiRefNet, especially on *video* tasks and more *efficient* model designs on higher-resolution images. If you are happy to cooperate, please contact me at zhengpeng0108@gmail.com.\nvideo of \n\n## News :newspaper:\n* **`May 15, 2025`:**  We released a video of the tutorial (screen recording) on BiRefNet fine-tuning on both my [YouTube](https://youtu.be/FwGT_0V9E-k) and [Bilibili](https://www.bilibili.com/video/BV1dxEkzgE3J) channels.\n* **`Mar 31, 2025`:**  We released the [BiRefNet_dynamic](https://huggingface.co/ZhengPeng7/BiRefNet_dynamic) for general use, which was trained on images in a dynamic resolution range from `256x256` to `2304x2304` and shows great and **robust** performance on **any resolution** images! Thanks again to [Freepik](https://www.freepik.com) their kind GPU support.\n* **`Feb 12, 2025`:**  We released the [BiRefNet_HR-matting](https://huggingface.co/ZhengPeng7/BiRefNet_HR-matting) for general matting use, which was trained on images in `2048x2048` and shows great matting performance on higher resolution images! Thanks again to [Freepik](https://www.freepik.com) their kind GPU support.\n* **`Feb 12, 2025`:**  We released the [BiRefNet_HR-matting](https://huggingface.co/ZhengPeng7/BiRefNet_HR-matting) for general matting use, which was trained on images in `2048x2048` and shows great matting performance on higher resolution images! Thanks again to [Freepik](https://www.freepik.com) their kind GPU support.\n* **`Feb 1, 2025`:**  We released the [BiRefNet_HR](https://huggingface.co/ZhengPeng7/BiRefNet_HR) for general use, which was trained on images in `2048x2048` and shows great performance on higher resolution images! Thanks to [Freepik](https://www.freepik.com) for offering H200x4 GPU for this huge training (~3 weeks).\n* **`Jan 6, 2025`:**  Validate the success of FP16 inference with ~0 decrease of performance and better efficiency: the standard BiRefNet can run in `17 FPS` with `resolution==1024x1024` with `3.45GB GPU memory` on a single `RTX 4090`. Check more details in the model efficiency part below in [model zoo section](https://github.com/ZhengPeng7/BiRefNet?tab=readme-ov-file#model-zoo).\n* **`Dec 5, 2024`:**  Fix the bug of using `torch.compile` in latest PyTorch versions (2.5.1) and the slow iteration in FP16 training with accelerate (set as default).\n* **`Nov 28, 2024`:**  Congrats to students @Nankai University employed BiRefNet to build their project and won the [provincial gold medal](https://drive.google.com/file/d/1WDgcHzzmbPtj3O4tlZyT3HLfNKLBPkje/view?usp=drive_link) and [national bronze medal](https://cy.ncss.cn/information/2c93f4c691983c5b0194264b1880207b) on the [China International College Students\u2019 Innovation Competition 2024](https://cy.ncss.cn/en).\n* **`Oct 26, 2024`:**  We added the [guideline of conducting fine-tuning on custom data](https://github.com/ZhengPeng7/BiRefNet?tab=readme-ov-file#pen-fine-tuning-on-custom-data) with existing weights.\n* **`Oct 6, 2024`:**  We uploaded the [BiRefNet-matting](https://huggingface.co/ZhengPeng7/BiRefNet-matting) model for general trimap-free matting use.\n* **`Sep 24, 2024`:**  We uploaded the [BiRefNet_lite-2K](https://huggingface.co/ZhengPeng7/BiRefNet_lite-2K) model, which takes inputs in a much higher resolution (2560x1440). We also added the [notebook](https://github.com/ZhengPeng7/BiRefNet/blob/main/tutorials/BiRefNet_inference_video.ipynb) for inference on videos.\n* **`Sep 7, 2024`:**  Thanks to [Freepik](https://www.freepik.com) for supporting me with GPUs for more extensive experiments, especially on BiRefNet for 2K inference!\n* **`Aug 30, 2024`:** We uploaded notebooks in `tutorials` to run the inference and ONNX conversion locally.\n* **`Aug 23, 2024`:** Our BiRefNet is now officially released [online](https://www.sciopen.com/article/10.26599/AIR.2024.9150038) on CAAI AIR journal. And thanks to the [press release](https://www.eurekalert.org/news-releases/1055380).\n* **`Aug 19, 2024`:** We uploaded the ONNX model files of all weights in the [GitHub release](https://github.com/ZhengPeng7/BiRefNet/releases/tag/v1) and [GDrive folder](https://drive.google.com/drive/u/0/folders/1kZM55bwsRdS__bdnsXpkmH6QPyza-9-N). Check out the **ONNX conversion** part in [model zoo](https://github.com/ZhengPeng7/BiRefNet?tab=readme-ov-file#model-zoo) for more details.\n* **`Jul 30, 2024`:** Thanks to @not-lain for his kind efforts in adding BiRefNet to the official huggingface.js [repo](https://github.com/huggingface/huggingface.js/blob/3a8651fbc6508920475564a692bf0e5b601d9343/packages/tasks/src/model-libraries-snippets.ts#L763).\n* **`Jul 28, 2024`:** We released the [Colab demo for box-guided segmentation](https://colab.research.google.com/drive/1B6aKZ3ekcvKMkSBn0N5mCASLUYMp0whK).\n* **`Jul 15, 2024`:** We deployed our BiRefNet on [Hugging Face Models](https://huggingface.co/ZhengPeng7/BiRefNet) for users to easily load it in one line code.\n* **`Jun 21, 2024`:** We released and uploaded the Chinese version of our original paper to my [GDrive](https://drive.google.com/file/d/1aBnJ_R9lbnC2dm8dqD0-pzP2Cu-U1Xpt/view).\n* **`May 28, 2024`:** We hold a [model zoo](https://github.com/ZhengPeng7/BiRefNet?tab=readme-ov-file#model-zoo) with well-trained weights of our BiRefNet in different sizes and for different tasks, including general use, matting segmentation, DIS, HRSOD, COD, etc.\n* **`May 7, 2024`:**  We also released the [Colab demo for multiple images inference](https://colab.research.google.com/drive/14Dqg7oeBkFEtchaHLNpig2BcdkZEogba). Many thanks to @rishabh063 for his support on it.\n* **`Apr 9, 2024`:**  Thanks to [Features and Labels Inc.](https://fal.ai/) for deploying a cool online BiRefNet [inference API](https://fal.ai/models/fal-ai/birefnet/playground) and providing me with strong GPU resources for 4 months on more extensive experiments!\n* **`Mar 7, 2024`:**  We released BiRefNet codes, the well-trained weights for all tasks in the original papers, and all related stuff in my [GDrive folder](https://drive.google.com/drive/folders/1s2Xe0cjq-2ctnJBR24563yMSCOu4CcxM). Meanwhile, we also deployed our BiRefNet on [Hugging Face Spaces](https://huggingface.co/spaces/ZhengPeng7/BiRefNet_demo) for easier online use and released the [Colab demo for inference and evaluation](https://colab.research.google.com/drive/1MaEiBfJ4xIaZZn0DqKrhydHB8X97hNXl).\n* **`Jan 7, 2024`:**  We released our paper on [arXiv](https://arxiv.org/pdf/2401.03407).\n\n\n## :rocket: Load BiRefNet in _ONE LINE_ by HuggingFace, check more: [![BiRefNet](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-blue)](https://huggingface.co/ZhengPeng7/birefnet)\n```python\nfrom transformers import AutoModelForImageSegmentation\nbirefnet = AutoModelForImageSegmentation.from_pretrained('zhengpeng7/BiRefNet', trust_remote_code=True)\n```\n## :flight_arrival: Inference Partner:\nYou can access the **inference API** service of BiRefNet on [FAL](https://fal.ai) or click the `Deploy` button on our [HF model page](https://huggingface.co/ZhengPeng7/BiRefNet) to set up your own deployment.\n+ https://fal.ai/models/fal-ai/birefnet/v2\n+ https://ui.endpoints.huggingface.co/new?repository=ZhengPeng7/BiRefNet\n\nOur BiRefNet has achieved SOTA on many similar HR tasks:\n\n**DIS**: [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-te1)](https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-te1?p=bilateral-reference-for-high-resolution) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-te2)](https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-te2?p=bilateral-reference-for-high-resolution) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-te3)](https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-te3?p=bilateral-reference-for-high-resolution) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-te4)](https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-te4?p=bilateral-reference-for-high-resolution) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-vd)](https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-vd?p=bilateral-reference-for-high-resolution)\n\n<details><summary>Figure of Comparison on DIS Papers with Codes (by the time of this work):</summary>\n<img src=\"https://drive.google.com/thumbnail?id=1DLt6CFXdT1QSWDj_6jRkyZINXZ4vmyRp&sz=w1620\" />\n<img src=\"https://drive.google.com/thumbnail?id=1gn5GyKFlJbMIkre1JyEdHDSYcrFmcLD0&sz=w1620\" />\n<img src=\"https://drive.google.com/thumbnail?id=16CVYYOtafEeZhHqv0am2Daku1n_exMP6&sz=w1620\" />\n<img src=\"https://drive.google.com/thumbnail?id=10K45xwPXmaTG4Ex-29ss9payA9yBnyLn&sz=w1620\" />\n<img src=\"https://drive.google.com/thumbnail?id=16EuyqKFJOqwMmagvfnbC9hUurL9pYLLB&sz=w1620\" />\n</details>\n<br />\n\n**COD**:[![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/camouflaged-object-segmentation-on-cod)](https://paperswithcode.com/sota/camouflaged-object-segmentation-on-cod?p=bilateral-reference-for-high-resolution) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/camouflaged-object-segmentation-on-nc4k)](https://paperswithcode.com/sota/camouflaged-object-segmentation-on-nc4k?p=bilateral-reference-for-high-resolution) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/camouflaged-object-segmentation-on-camo)](https://paperswithcode.com/sota/camouflaged-object-segmentation-on-camo?p=bilateral-reference-for-high-resolution) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/camouflaged-object-segmentation-on-chameleon)](https://paperswithcode.com/sota/camouflaged-object-segmentation-on-chameleon?p=bilateral-reference-for-high-resolution)\n\n<details><summary>Figure of Comparison on COD Papers with Codes (by the time of this work):</summary>\n<img src=\"https://drive.google.com/thumbnail?id=1DLt6CFXdT1QSWDj_6jRkyZINXZ4vmyRp&sz=w1620\" />\n<img src=\"https://drive.google.com/thumbnail?id=1gn5GyKFlJbMIkre1JyEdHDSYcrFmcLD0&sz=w1620\" />\n<img src=\"https://drive.google.com/thumbnail?id=16CVYYOtafEeZhHqv0am2Daku1n_exMP6&sz=w1620\" />\n</details>\n<br />\n\n**HRSOD**: [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/rgb-salient-object-detection-on-davis-s)](https://paperswithcode.com/sota/rgb-salient-object-detection-on-davis-s?p=bilateral-reference-for-high-resolution) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/rgb-salient-object-detection-on-hrsod)](https://paperswithcode.com/sota/rgb-salient-object-detection-on-hrsod?p=bilateral-reference-for-high-resolution) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/rgb-salient-object-detection-on-uhrsd)](https://paperswithcode.com/sota/rgb-salient-object-detection-on-uhrsd?p=bilateral-reference-for-high-resolution) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/salient-object-detection-on-duts-te)](https://paperswithcode.com/sota/salient-object-detection-on-duts-te?p=bilateral-reference-for-high-resolution) [![PWC](https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/salient-object-detection-on-dut-omron)](https://paperswithcode.com/sota/salient-object-detection-on-dut-omron?p=bilateral-reference-for-high-resolution)\n\n<details><summary>Figure of Comparison on HRSOD Papers with Codes (by the time of this work):</summary>\n<img src=\"https://drive.google.com/thumbnail?id=1hNfQtlTAHT4-AVbk_47852zyRp1NOFLs&sz=w1620\" />\n<img src=\"https://drive.google.com/thumbnail?id=1bcVldUAxYkMI3OMTyaP_jNuOugDfYj-d&sz=w1620\" />\n<img src=\"https://drive.google.com/thumbnail?id=1p1zgyVz27cGEqQMtOKzm_6zoYK3Sw_Zk&sz=w1620\" />\n<img src=\"https://drive.google.com/thumbnail?id=1TubAvcoEbH_mHu3I-AxflnB71nkf35jJ&sz=w1620\" />\n<img src=\"https://drive.google.com/thumbnail?id=1A3V9HjVtcMQdnGPwuy-DBVhwKuo0q2lT&sz=w1620\" />\n</details>\n<br />\n\n#### Try our online demos for inference:\n\n+ **Inference and evaluation** of your given weights: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1MaEiBfJ4xIaZZn0DqKrhydHB8X97hNXl)\n+ **Online Inference with GUI** with adjustable resolutions: [![Hugging Face Spaces](https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue)](https://huggingface.co/spaces/ZhengPeng7/BiRefNet_demo)  \n+ Online **Multiple Images Inference** on Colab: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/14Dqg7oeBkFEtchaHLNpig2BcdkZEogba)\n\n<img src=\"https://drive.google.com/thumbnail?id=12XmDhKtO1o2fEvBu4OE4ULVB2BK0ecWi&sz=w1620\" />\n\n\n\n## Model Zoo\n\n> For more general use of our BiRefNet, I extended the original academic one to more general ones for better real-life application.\n>\n> Datasets and datasets are suggested to be downloaded from official pages. But you can also download the packaged ones: [DIS](https://drive.google.com/drive/folders/1hZW6tAGPJwo9mPS7qGGGdpxuvuXiyoMJ), [HRSOD](https://drive.google.com/drive/folders/18_hAE3QM4cwAzEAKXuSNtKjmgFXTQXZN), [COD](https://drive.google.com/drive/folders/1EyHmKWsXfaCR9O0BiZEc3roZbRcs4ECO), [Backbones](https://drive.google.com/drive/folders/1cmce_emsS8A5ha5XT2c_CZiJzlLM81ms).\n>\n> Find performances (almost all metrics) of all models in the `exp-TASK_SETTINGS` folders in [[**stuff**](https://drive.google.com/drive/folders/1s2Xe0cjq-2ctnJBR24563yMSCOu4CcxM)].\n\n\n\n<details><summary>Models in the original paper, for <b>comparison on benchmarks</b>:</summary>\n\n| Task  |        Training Sets        |   Backbone    |                           Download                           |\n| :---: | :-------------------------: | :-----------: | :----------------------------------------------------------: |\n|  DIS  |          DIS5K-TR           | swin_v1_large | [google-drive](https://drive.google.com/file/d/1J90LucvDQaS3R_-9E7QUh1mgJ8eQvccb/view) |\n|  COD  |     COD10K-TR, CAMO-TR      | swin_v1_large | [google-drive](https://drive.google.com/file/d/1tM5M72k7a8aKF-dYy-QXaqvfEhbFaWkC/view) |\n| HRSOD |           DUTS-TR           | swin_v1_large | [google-drive](https://drive.google.com/file/d/1f7L0Pb1Y3RkOMbqLCW_zO31dik9AiUFa/view) |\n| HRSOD |      DUTS-TR, HRSOD-TR      | swin_v1_large | [google-drive](https://drive.google.com/file/d/1WJooyTkhoDLllaqwbpur_9Hle0XTHEs_/view) |\n| HRSOD |      DUTS-TR, UHRSD-TR      | swin_v1_large | [google-drive](https://drive.google.com/file/d/1Pu1mv3ORobJatIuUoEuZaWDl2ylP3Gw7/view) |\n| HRSOD |     HRSOD-TR, UHRSD-TR      | swin_v1_large | [google-drive](https://drive.google.com/file/d/1xEh7fsgWGaS5c3IffMswasv0_u-aVM9E/view) |\n| HRSOD | DUTS-TR, HRSOD-TR, UHRSD-TR | swin_v1_large | [google-drive](https://drive.google.com/file/d/13FaxyyOwyCddfZn2vZo1xG1KNZ3cZ-6B/view) |\n\n</details>\n\n\n\n<details><summary>Models trained with customed data (general, matting), for <b>general use in practical application</b>:</summary>\n\n|           Task            |                        Training Sets                         |   Backbone    | Test Set  | Metric (S, wF[, HCE]) |                           Download                           |\n| :-----------------------: | :----------------------------------------------------------: | :-----------: | :-------: | :-------------------: | :----------------------------------------------------------: |\n|      **general use (2048x2048)**      | AIM-500, DIS-TR, DIS-TEs, HIM2K, PPM-100, TE-HRS10K, TE-Human-2k, TE-P3M-500-P, TR-AM-2k, TR-HRSOD, TR-UHRSD, Distinctions-646_BG-20k, Human-2k_BG-20k, TE-AM-2k, TE-HRSOD, TE-P3M-500-NP, TE-UHRSD, TR-HRS10K, TR-P3M-10k, [TR-humans](https://huggingface.co/datasets/schirrmacher/humans) | swin_v1_large |  DIS-VD   |  0.927, 0.894, 881   | [google-drive](https://drive.google.com/file/d/1lWVyimvxQ3bmDlaHeO1IOUbFZqUj9DYg/view) |\n|      **general use**      | DIS5K-TR, DIS-TEs, DUTS-TR_TE, HRSOD-TR_TE, UHRSD-TR_TE, HRS10K-TR_TE, TR-P3M-10k, TE-P3M-500-NP, TE-P3M-500-P, [TR-humans](https://huggingface.co/datasets/schirrmacher/humans) | swin_v1_large |  DIS-VD   |  0.911, 0.875, 1069   | [google-drive](https://drive.google.com/file/d/1_IfUnu8Fpfn-nerB89FzdNXQ7zk6FKxc/view) |\n|      **general use**      | DIS5K-TR, DIS-TEs, DUTS-TR_TE, HRSOD-TR_TE, UHRSD-TR_TE, HRS10K-TR_TE, TR-P3M-10k, TE-P3M-500-NP, TE-P3M-500-P, [TR-humans](https://huggingface.co/datasets/schirrmacher/humans) | swin_v1_tiny |  DIS-VD   |  0.882, 0.830, 1175   | [google-drive](https://drive.google.com/file/d/1fzInDWiE2n65tmjaHDSZpqhL0VME6-Yl/view) |\n|      **general use**      |                      DIS5K-TR, DIS-TEs                       | swin_v1_large |  DIS-VD   |  0.907, 0.865, 1059   | [google-drive](https://drive.google.com/file/d/1P6NJzG3Jf1sl7js2q1CPC3yqvBn_O8UJ/view) |\n| **general matting** | P3M-10k (except TE-P3M-500-NP), [TR-humans](https://huggingface.co/datasets/schirrmacher/humans), AM-2k, AIM-500, Human-2k (synthesized with BG-20k), Distinctions-646 (synthesized with BG-20k), HIM2K, PPM-100 | swin_v1_large | TE-P3M-500-NP | 0.979, 0.988 | [google-drive](https://drive.google.com/file/d/1Nlcg58d5bvE-Tbbm8su_eMQba10hdcwQ/view) |\n| **portrait matting** |                           [P3M-10k](https://github.com/JizhiziLi/P3M), [humans](https://huggingface.co/datasets/schirrmacher/humans)                            | swin_v1_large | P3M-500-P |     0.983, 0.989      | [google-drive](https://drive.google.com/file/d/1uUeXjEUoD2XF_6YjD_fsct-TJp7TFiqh) |\n\n</details>\n\n\n\n<details><summary>Segmentation with box <b>guidance</b>:</summary>\n\n+ Given box guidance: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1B6aKZ3ekcvKMkSBn0N5mCASLUYMp0whK)\n\n</details>\n\n\n\n<details><summary>Model <b>efficiency</b>:</summary>\n\n> Screenshot from the original paper. All tests here are conducted on a single A100 GPU.\n\n<img src=\"https://drive.google.com/thumbnail?id=1mTfSD_qt-rFO1t8DRQcyIa5cgWLf1w2-&sz=h300\" />  <img src=\"https://drive.google.com/thumbnail?id=1F_OURIWILVe4u1rSz-aqt6ur__bAef25&sz=h300\" />\n\n> The devices used in the below table differ from those in the original paper (the standard). So, it's only for reference.\n\n| Runtime | *FP32* | *FP16* |\n| :-----: | :----: | :----: |\n|  A100   | 86.8ms | 69.4ms |\n|  4090   | 95.8ms | 57.7ms |\n|  V100   | 384ms  | 152ms  |\n\n| GPU Memory | *FP32* | *FP16* |\n| :--------: | :----: | :----: |\n| Inference  | 4.76GB | 3.45GB |\n| Training (\\#GPU=1, batch\\_size=2, compile=False+PyTorch=2.5.1) | 36.3GB | 30.4GB |\n| Training (\\#GPU=1, batch\\_size=2, compile=True+PyTorch=2.5.1) | 35.9GB | **22.5GB (4090), 23.5GB (A100)** |\n\n</details>\n\n\n\n<details><summary><b>ONNX</b> conversion:</summary>\n\n> We converted from `.pth` weights files to `.onnx` files.  \n> We referred a lot to the [Kazuhito00/BiRefNet-ONNX-Sample](https://github.com/Kazuhito00/BiRefNet-ONNX-Sample), many thanks to @Kazuhito00.\n\n+ Check our [Colab demo for ONNX conversion](https://colab.research.google.com/drive/1z6OruR52LOvDDpnp516F-N4EyPGrp5om) or the [notebook file for local running](https://drive.google.com/file/d/1cgL2qyvOO5q3ySfhytypX46swdQwZLrJ), where you can do the conversion/inference by yourself and find all relevant info.\n+ As tested, BiRefNets with SwinL (default backbone) cost `~90%` more time (the inference costs `~165ms` on an A100 GPU) using ONNX files. Meanwhile, BiRefNets with SwinT (lightweight) cost `~75%` more time (the inference costs `~93.8ms` on an A100 GPU) using ONNX files. Input resolution is `1024x1024` as default.\n+ The results of the original pth files and the converted onnx files are slightly different, which is acceptable.\n+ Pay attention to the compatibility among `onnxruntime-gpu, CUDA, and CUDNN` (we use `torch==2.0.1, cuda=11.8` here).\n\n</details>\n\n\n\n## Third-Party Creations\n>We found there've been some 3rd party applications based on our BiRefNet. Many thanks for their contribution to the community!  \nChoose the one you like to try with clicks instead of codes:  \n\n1. **Applications**:\n\n   + Thanks [**tin2tin/2D_Asset_Generator**](https://github.com/tin2tin/2D_Asset_Generator): this project combined BiRefNet and FLUX as a **Blender add-on** for \"AI generating 2D cutout assets for ex. previz\".\n\n     https://github.com/user-attachments/assets/6cce7ca7-7817-4406-b6c4-6d4e8c414ed4\n\n   + Thanks [**camenduru/text-behind-tost**](https://github.com/camenduru/text-behind-tost): this project employed BiRefNet to extract foreground subjects and **add texts between the subjects and background**, which looks amazing especially for videos. Check their [tweets](https://x.com/camenduru/status/1856290408294220010) for more examples.\n\n     <p align=\"center\"><img src=\"https://github.com/user-attachments/assets/9969dd10-38a8-4cf2-a6c7-5b11f074b9b4\" height=\"300\"/></p>\n\n   + Thanks [**briaai/RMBG-2.0**](https://huggingface.co/briaai/RMBG-2.0): this project trained BiRefNet with their **high-quality private data**, which brings improvement on the DIS task. Note that their weights are for only **non-commercial use** and are **not aware of transparency** due to training in the DIS task setting, which focuses only on predicting binary masks.\n\n     <p align=\"center\"><img src=\"https://huggingface.co/briaai/RMBG-2.0/media/main/t4.png\" height=\"300\"/></p>\n\n   + Thanks [**lldacing/ComfyUI_BiRefNet_ll**](https://github.com/lldacing/ComfyUI_BiRefNet_ll): this project further upgrade the **ComfyUI node** for BiRefNet with both our **latest weights** and **the legacy ones**.\n\n     <p align=\"center\"><img src=\"https://github.com/lldacing/ComfyUI_BiRefNet_ll/raw/main/doc/video.gif\" height=\"300\"/></p>\n\n   + Thanks [**MoonHugo/ComfyUI-BiRefNet-Hugo**](https://github.com/MoonHugo/ComfyUI-BiRefNet-Hugo): this project further upgrade the **ComfyUI node** for BiRefNet with our **latest weights**.\n\n     <p align=\"center\"><img src=\"https://github.com/MoonHugo/ComfyUI-BiRefNet-Hugo/raw/main/assets/demo4.gif\" height=\"300\"/></p>\n\n   + Thanks [**lbq779660843/BiRefNet-Tensorrt**](https://github.com/lbq779660843/BiRefNet-Tensorrt) and [**yuanyang1991/birefnet_tensorrt**](https://github.com/yuanyang1991/birefnet_tensorrt): they both provided the project to convert BiRefNet to **TensorRT**, which is faster and better for deployment. Their repos offer solid local establishment (Win and Linux) and [colab demo](https://colab.research.google.com/drive/1r8GkFPyMMO0OkMX6ih5FjZnUCQrl2SHV?usp=sharing), respectively. And @yuanyang1991 kindly offered the comparison among the inference efficiency of naive PyTorch, ONNX, and TensorRT on an RTX 4080S:\n\n| Methods | [Pytorch](https://drive.google.com/file/d/1_IfUnu8Fpfn-nerB89FzdNXQ7zk6FKxc/view) | [ONNX](https://drive.google.com/drive/u/0/folders/1kZM55bwsRdS__bdnsXpkmH6QPyza-9-N) | TensorRT |\n|:------------------------------------------------------------------------------------:|:--------------:|:--------------:|:--------------:|\n|  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;First Inference Time&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;  |     0.71s      |     5.32s      |     **0.17s**      |\n\n| Methods | [Pytorch](https://drive.google.com/file/d/1_IfUnu8Fpfn-nerB89FzdNXQ7zk6FKxc/view) | [ONNX](https://drive.google.com/drive/u/0/folders/1kZM55bwsRdS__bdnsXpkmH6QPyza-9-N) | TensorRT |\n|:------------------------------------------------------------------------------------:|:--------------:|:--------------:|:--------------:|\n|  Avg Inf Time (excluding 1st)   |     0.15s      |     4.43s      |     **0.11s**      |\n\n   + Thanks [**dimitribarbot/sd-webui-birefnet**](https://github.com/dimitribarbot/sd-webui-birefnet): this project allows to add a BiRefNet section to the original **Stable Diffusion WebUI**'s Extras tab.\n     <p align=\"center\"><img src=\"https://drive.google.com/thumbnail?id=159bLXI71FWh4ZsHTvc-wApSN9ytVRmua&sz=w1620\" /></p>\n\n   + Thanks [**fal.ai/birefnet**](https://fal.ai/models/birefnet): this project on `fal.ai` encapsulates BiRefNet **online** with more useful options in **UI** and **API** to call the model.\n     <p align=\"center\"><img src=\"https://drive.google.com/thumbnail?id=1rNk81YV_Pzb2GykrzfGvX6T7KBXR0wrA&sz=w1620\" /></p>\n\n   + Thanks [**ZHO-ZHO-ZHO/ComfyUI-BiRefNet-ZHO**](https://github.com/ZHO-ZHO-ZHO/ComfyUI-BiRefNet-ZHO): this project further improves the **UI** for BiRefNet in ComfyUI, especially for **video data**.\n     <p align=\"center\"><img src=\"https://drive.google.com/thumbnail?id=1GOqEreyS7ENzTPN0RqxEjaA76RpMlkYM&sz=w1620\" /></p>\n     \n     <https://github.com/ZhengPeng7/BiRefNet/assets/25921713/3a1c7ab2-9847-4dac-8935-43a2d3cd2671>\n\n   + Thanks [**viperyl/ComfyUI-BiRefNet**](https://github.com/viperyl/ComfyUI-BiRefNet): this project packs BiRefNet as **ComfyUI nodes**, and makes this SOTA model easier use for everyone.\n     <p align=\"center\"><img src=\"https://drive.google.com/thumbnail?id=1KfxCQUUa2y9T-aysEaeVVjCUt3Z0zSkL&sz=w1620\" /></p>\n\n   + Thanks [**Rishabh**](https://github.com/rishabh063) for offering a demo for the [easier multiple images inference on colab](https://colab.research.google.com/drive/14Dqg7oeBkFEtchaHLNpig2BcdkZEogba).\n\n2. **More Visual Comparisons**\n   + Thanks [**twitter.com/ZHOZHO672070**](https://twitter.com/ZHOZHO672070) for the comparison with more background-removal methods in images:\n\n     <img src=\"https://drive.google.com/thumbnail?id=1nvVIFt_Ezs-crPSQxUDqkUBz598fTe63&sz=w1620\" />\n\n   + Thanks [**twitter.com/toyxyz3**](https://twitter.com/toyxyz3) for the comparison with more background-removal methods in videos:\n\n    <https://github.com/ZhengPeng7/BiRefNet/assets/25921713/40136198-01cc-4106-81f9-81c985f02e31>\n\n    <https://github.com/ZhengPeng7/BiRefNet/assets/25921713/1a32860c-0893-49dd-b557-c2e35a83c160>\n\n\n## Usage\n\n#### Environment Setup\n\n```shell\n# PyTorch==2.5.1+CUDA12.4 (or 2.0.1+CUDA11.8) is used for faster training (~40%) with compilation.\nconda create -n birefnet python=3.10 -y && conda activate birefnet\npip install -r requirements.txt\n```\n\n#### Dataset Preparation\n\nDownload combined training / test sets I have organized well from: [DIS](https://drive.google.com/drive/folders/1hZW6tAGPJwo9mPS7qGGGdpxuvuXiyoMJ)--[COD](https://drive.google.com/drive/folders/1EyHmKWsXfaCR9O0BiZEc3roZbRcs4ECO)--[HRSOD](https://drive.google.com/drive/folders/18_hAE3QM4cwAzEAKXuSNtKjmgFXTQXZN) or the single official ones in the `single_ones` folder, or their official pages. You can also find the same ones on my **BaiduDisk**: [DIS](https://pan.baidu.com/s/1O_pQIGAE4DKqL93xOxHpxw?pwd=PSWD)--[COD](https://pan.baidu.com/s/1RnxAzaHSTGBC1N6r_RfeqQ?pwd=PSWD)--[HRSOD](https://pan.baidu.com/s/1_Del53_0lBuG0DKJJAk4UA?pwd=PSWD).\n\n#### Weights Preparation\n\nDownload backbone weights from [my google-drive folder](https://drive.google.com/drive/folders/1s2Xe0cjq-2ctnJBR24563yMSCOu4CcxM) or their official pages.\n\n## Run\n\n```shell\n# Train & Test & Evaluation\n./train_test.sh RUN_NAME GPU_NUMBERS_FOR_TRAINING GPU_NUMBERS_FOR_TEST\n# Example: ./train_test.sh tmp-proj 0,1,2,3,4,5,6,7 0\n\n# See train.sh / test.sh for only training / test-evaluation.\n# After the evaluation, run `gen_best_ep.py` to select the best ckpt from a specific metric (you choose it from Sm, wFm, HCE (DIS only)).\n```\n\n### :pen: Fine-tuning on Custom Data\n\n> A video of the tutorial on BiRefNet fine-tuning has been released on my YouTube channel \u2b07\ufe0f\n\n[![BiRefNet Fine-tuning Tutorial](https://img.youtube.com/vi/FwGT_0V9E-k/0.jpg)](https://youtu.be/FwGT_0V9E-k)\n\n> Suppose you have some custom data, fine-tuning on it tends to bring improvement.\n\n1. **Pre-requisites**: you have put your datasets in the path `${data_root_dir}/TASK_NAME/DATASET_NAME`. For example, `${data_root_dir}/DIS5K/DIS-TR` and `${data_root_dir}/General/TR-HRSOD`, where `im` and `gt` are both in each dataset folder.\n2. **Change an existing task to your custom one**: replace all `'General'` (with single quotes) in the whole project with `your custom task name` as the screenshot of vscode given below shows:<img src=\"https://drive.google.com/thumbnail?id=1J6gzTmrVnQsmtt3hi6ch3ZrH7Op9PKSB&sz=w400\" />\n3. **Adapt settings**:\n   + `sys_home_dir`: path to the root folder, which contains codes / datasets / weights / ... -- project folder / data folder / backbone weights folder are `${sys_home_dir}/codes/dis/BiRefNet / ${sys_home_dir}/datasets/dis/General / ${sys_home_dir}/weights/cv/swin_xxx`, respectively.\n   + `testsets`: your validation set.\n   + `training_set`: your training set.\n   + `lambdas_pix_last`: adapt the weights of different losses if you want, especially for the difference between segmentation (classification task) and matting (regression task).\n4. **Use existing weights**: if you want to use some existing weights to fine-tune that model, please refer to the `resume` argument in `train.py`. Attention: the epoch of training continues from the epochs the weights file name indicates (e.g., `244` in `BiRefNet-general-epoch_244.pth`), instead of `1`. So, if you want to fine-tune `50` more epochs, please specify the epochs as `294`. `\\#Epochs, \\#last epochs for validation, and validation step` are set in `train.sh`.\n5. Good luck to your training :) If you still have questions, feel free to leave issues (recommended way) or contact me.\n\n\n\n\n## Well-trained weights:\n\nDownload the `BiRefNet-{TASK}-{EPOCH}.pth` from [[**stuff**](https://drive.google.com/drive/folders/1s2Xe0cjq-2ctnJBR24563yMSCOu4CcxM)] and [the release page](https://github.com/ZhengPeng7/BiRefNet/releases) of this repo. Info of the corresponding (predicted\\_maps/performance/training\\_log) weights can be also found in folders like `exp-BiRefNet-{TASK_SETTINGS}` in the same directory.\n\nYou can also download the weights from the release of this repo.\n\nThe results might be a bit different from those in the original paper, you can see them in the `eval_results-BiRefNet-{TASK_SETTINGS}` folder in each `exp-xx`, we will update them in the following days. Due to the very high cost I used (A100-80G x 8), which many people cannot afford (including myself....), I re-trained BiRefNet on a single A100-40G only and achieved the performance on the same level (even better). It means you can directly train the model on a single GPU with 36.5G+ memory. BTW, 5.5G GPU memory is needed for inference in 1024x1024. (I personally paid a lot for renting an A100-40G to re-train BiRefNet on the three tasks... T_T. Hope it can help you.)\n\nBut if you have more and more powerful GPUs, you can set GPU IDs and increase the batch size in `config.py` to accelerate the training. We have made all these kinds of things adaptive in scripts to seamlessly switch between single-card training and multi-card training. Enjoy it :)\n\n## Some of my messages:\n\nThis project was originally built for DIS only. But after the updates one by one, I made it larger and larger with many functions embedded together. Finally, you can **use it for any binary image segmentation tasks**, such as DIS/COD/SOD, medical image segmentation, anomaly segmentation, etc. You can eaily open/close below things (usually in `config.py`):\n+ Multi-GPU training: open/close with one variable.\n+ Backbone choices: Swin_v1, PVT_v2, ConvNets, ...\n+ Weighted losses: BCE, IoU, SSIM, MAE, Reg, ...\n+ Training tricks: multi-scale supervision, freezing backbone, multi-scale input...\n+ Data collator: loading all in memory, smooth combination of different datasets for combined training and test.\n+ ...\nI really hope you enjoy this project and use it in more works to achieve new SOTAs.\n\n\n### Quantitative Results\n\n<p align=\"center\"><img src=\"https://drive.google.com/thumbnail?id=1Ymkh8WN16XMTBOS8dmPTg5eAf-NIl2m5&sz=w1620\" /></p>\n\n<p align=\"center\"><img src=\"https://drive.google.com/thumbnail?id=1W0mi0ZiYbqsaGuohNXU8Gh7Zj4M3neFg&sz=w1620\" /></p>\n\n\n\n### Qualitative Results\n\n<p align=\"center\"><img src=\"https://drive.google.com/thumbnail?id=1TYZF8pVZc2V0V6g3ik4iAr9iKvJ8BNrf&sz=w1620\" /></p>\n\n<p align=\"center\"><img src=\"https://drive.google.com/thumbnail?id=1ZGHC32CAdT9cwRloPzOCKWCrVQZvUAlJ&sz=w1620\" /></p>\n\n\n## Acknowledgement:\n\nMany of my thanks to the companies / institutes below.\n+ [FAL](https://fal.ai).\n+ [Freepik](https://www.freepik.com).\n+ [Redmond.ai](https://redmond.ai).\n+ [Alibaba-ICBU](https://www.alibaba.com).\n\n\n### Citation\n\n```\n@article{zheng2024birefnet,\n  title={Bilateral Reference for High-Resolution Dichotomous Image Segmentation},\n  author={Zheng, Peng and Gao, Dehong and Fan, Deng-Ping and Liu, Li and Laaksonen, Jorma and Ouyang, Wanli and Sebe, Nicu},\n  journal={CAAI Artificial Intelligence Research},\n  volume = {3},\n  pages = {9150038},\n  year={2024}\n}\n```\n\n\n\n## Contact\n\nAny questions, discussions, or even complaints, feel free to leave issues here (recommended) or send me e-mails (zhengpeng0108@gmail.com) or book a meeting with me: [calendly.com/zhengpeng0108/30min](https://calendly.com/zhengpeng0108/30min). You can also join the Discord Group (https://discord.gg/d9NN5sgFrq) if you want to talk a lot publicly.\n\n",
  "external_links_in_readme": [
    "https://github.com/ZhengPeng7/BiRefNet/assets/25921713/1a32860c-0893-49dd-b557-c2e35a83c160>",
    "https://github.com/lldacing/ComfyUI_BiRefNet_ll",
    "https://github.com/lldacing/ComfyUI_BiRefNet_ll/raw/main/doc/video.gif\"",
    "https://scholar.google.com/citations?user=0uPb8MMAAAAJ'",
    "https://cy.ncss.cn/information/2c93f4c691983c5b0194264b1880207b",
    "https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-te3",
    "https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Models-blue",
    "https://arxiv.org/pdf/2401.03407",
    "https://drive.google.com/file/d/1FWvKDWTnK9RsiywfCsIxsnQzqv-dlO5u/view'><img",
    "https://drive.google.com/thumbnail?id=1nvVIFt_Ezs-crPSQxUDqkUBz598fTe63&sz=w1620\"",
    "https://drive.google.com/thumbnail?id=16EuyqKFJOqwMmagvfnbC9hUurL9pYLLB&sz=w1620\"",
    "https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/salient-object-detection-on-duts-te",
    "https://fal.ai/",
    "https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Spaces-blue",
    "https://drive.google.com/file/d/1tM5M72k7a8aKF-dYy-QXaqvfEhbFaWkC/view",
    "https://drive.google.com/file/d/1Pu1mv3ORobJatIuUoEuZaWDl2ylP3Gw7/view",
    "https://drive.google.com/thumbnail?id=1TYZF8pVZc2V0V6g3ik4iAr9iKvJ8BNrf&sz=w1620\"",
    "https://www.alibaba.com",
    "https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-vd",
    "https://drive.google.com/thumbnail?id=1DLt6CFXdT1QSWDj_6jRkyZINXZ4vmyRp&sz=w1620\"",
    "https://drive.google.com/drive/folders/1EyHmKWsXfaCR9O0BiZEc3roZbRcs4ECO",
    "https://www.eurekalert.org/news-releases/1055380",
    "https://drive.google.com/drive/folders/1cmce_emsS8A5ha5XT2c_CZiJzlLM81ms",
    "https://paperswithcode.com/sota/salient-object-detection-on-duts-te?p=bilateral-reference-for-high-resolution",
    "https://www.sciopen.com/article/pdf/10.26599/AIR.2024.9150038.pdf'><img",
    "https://www.birefnet.top'><img",
    "https://drive.google.com/thumbnail?id=159bLXI71FWh4ZsHTvc-wApSN9ytVRmua&sz=w1620\"",
    "https://pan.baidu.com/s/1_Del53_0lBuG0DKJJAk4UA?pwd=PSWD",
    "https://colab.research.google.com/assets/colab-badge.svg",
    "https://drive.google.com/thumbnail?id=10K45xwPXmaTG4Ex-29ss9payA9yBnyLn&sz=w1620\"",
    "https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/camouflaged-object-segmentation-on-cod",
    "https://drive.google.com/thumbnail?id=1J6gzTmrVnQsmtt3hi6ch3ZrH7Op9PKSB&sz=w400\"",
    "https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-te2?p=bilateral-reference-for-high-resolution",
    "https://www.bilibili.com/video/BV1dxEkzgE3J",
    "https://img.shields.io/badge/%F0%9F%A4%97%20HF-Model-blue'></a>&ensp;",
    "https://img.shields.io/badge/License-MIT-yellow'></a>&ensp;",
    "https://www.sciopen.com/article/10.26599/AIR.2024.9150038",
    "https://github.com/ZhengPeng7/BiRefNet?tab=readme-ov-file#model-zoo",
    "https://pan.baidu.com/s/1O_pQIGAE4DKqL93xOxHpxw?pwd=PSWD",
    "https://drive.google.com/thumbnail?id=1hNfQtlTAHT4-AVbk_47852zyRp1NOFLs&sz=w1620\"",
    "https://drive.google.com/thumbnail?id=1bcVldUAxYkMI3OMTyaP_jNuOugDfYj-d&sz=w1620\"",
    "https://github.com/ZHO-ZHO-ZHO/ComfyUI-BiRefNet-ZHO",
    "https://ui.endpoints.huggingface.co/new?repository=ZhengPeng7/BiRefNet",
    "https://img.youtube.com/vi/FwGT_0V9E-k/0.jpg",
    "https://drive.google.com/thumbnail?id=1Z-esCujQF_uEa_YJjkibc3NUrW4aR_d4&sz=w400\"",
    "https://github.com/rishabh063",
    "https://drive.google.com/thumbnail?id=1Ymkh8WN16XMTBOS8dmPTg5eAf-NIl2m5&sz=w1620\"",
    "https://drive.google.com/thumbnail?id=1GOqEreyS7ENzTPN0RqxEjaA76RpMlkYM&sz=w1620\"",
    "https://drive.google.com/thumbnail?id=12XmDhKtO1o2fEvBu4OE4ULVB2BK0ecWi&sz=w1620\"",
    "https://drive.google.com/thumbnail?id=1rNk81YV_Pzb2GykrzfGvX6T7KBXR0wrA&sz=w1620\"",
    "https://drive.google.com/thumbnail?id=1ZGHC32CAdT9cwRloPzOCKWCrVQZvUAlJ&sz=w1620\"",
    "https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/camouflaged-object-segmentation-on-nc4k",
    "https://scholar.google.com/citations?user=qQP6WXIAAAAJ'",
    "https://www.freepik.com",
    "https://huggingface.co/ZhengPeng7/BiRefNet_HR-matting",
    "https://paperswithcode.com/sota/camouflaged-object-segmentation-on-nc4k?p=bilateral-reference-for-high-resolution",
    "https://github.com/ZhengPeng7/BiRefNet/blob/main/tutorials/BiRefNet_inference_video.ipynb",
    "https://drive.google.com/thumbnail?id=1W0mi0ZiYbqsaGuohNXU8Gh7Zj4M3neFg&sz=w1620\"",
    "https://drive.google.com/drive/folders/1hZW6tAGPJwo9mPS7qGGGdpxuvuXiyoMJ",
    "https://fal.ai/models/fal-ai/birefnet/playground",
    "https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/rgb-salient-object-detection-on-uhrsd",
    "https://redmond.ai",
    "https://drive.google.com/thumbnail?id=1TubAvcoEbH_mHu3I-AxflnB71nkf35jJ&sz=w1620\"",
    "https://img.shields.io/badge/\u4e2d\u6587\u7248-Paper-red'></a>&ensp;",
    "https://huggingface.co/ZhengPeng7/BiRefNet_lite-2K",
    "https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-te1?p=bilateral-reference-for-high-resolution",
    "https://github.com/dimitribarbot/sd-webui-birefnet",
    "https://scholar.google.com/citations?user=9cMQrVsAAAAJ'",
    "https://scholar.google.com/citations?user=TZRzWOsAAAAJ'",
    "https://drive.google.com/file/d/1aBnJ_R9lbnC2dm8dqD0-pzP2Cu-U1Xpt/view",
    "https://drive.google.com/thumbnail?id=16CVYYOtafEeZhHqv0am2Daku1n_exMP6&sz=w1620\"",
    "https://drive.google.com/file/d/1P6NJzG3Jf1sl7js2q1CPC3yqvBn_O8UJ/view",
    "https://huggingface.co/ZhengPeng7/BiRefNet",
    "https://huggingface.co/ZhengPeng7/birefnet",
    "https://paperswithcode.com/sota/camouflaged-object-segmentation-on-cod?p=bilateral-reference-for-high-resolution",
    "https://github.com/MoonHugo/ComfyUI-BiRefNet-Hugo/raw/main/assets/demo4.gif\"",
    "https://github.com/user-attachments/assets/9969dd10-38a8-4cf2-a6c7-5b11f074b9b4\"",
    "https://twitter.com/ZHOZHO672070",
    "https://huggingface.co/spaces/ZhengPeng7/BiRefNet_demo'><img",
    "https://calendly.com/zhengpeng0108/30min",
    "https://arxiv.org/pdf/2401.03407'><img",
    "https://drive.google.com/file/d/1uUeXjEUoD2XF_6YjD_fsct-TJp7TFiqh",
    "https://drive.google.com/thumbnail?id=1A3V9HjVtcMQdnGPwuy-DBVhwKuo0q2lT&sz=w1620\"",
    "https://huggingface.co/ZhengPeng7/BiRefNet_dynamic",
    "https://drive.google.com/thumbnail?id=1F_OURIWILVe4u1rSz-aqt6ur__bAef25&sz=h300\"",
    "https://colab.research.google.com/drive/1z6OruR52LOvDDpnp516F-N4EyPGrp5om",
    "https://scholar.google.com/citations?user=stFCYOAAAAAJ'",
    "https://github.com/ZhengPeng7/BiRefNet/assets/25921713/3a1c7ab2-9847-4dac-8935-43a2d3cd2671>",
    "https://img.shields.io/badge/GDrive-Stuff-green'></a>&ensp;",
    "https://drive.google.com/file/d/1WDgcHzzmbPtj3O4tlZyT3HLfNKLBPkje/view?usp=drive_link",
    "https://drive.google.com/thumbnail?id=1mTfSD_qt-rFO1t8DRQcyIa5cgWLf1w2-&sz=h300\"",
    "https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-te4",
    "https://huggingface.co/briaai/RMBG-2.0",
    "https://github.com/ZhengPeng7/BiRefNet/assets/25921713/40136198-01cc-4106-81f9-81c985f02e31>",
    "https://colab.research.google.com/drive/1B6aKZ3ekcvKMkSBn0N5mCASLUYMp0whK'><img",
    "https://x.com/camenduru/status/1856290408294220010",
    "https://colab.research.google.com/drive/1MaEiBfJ4xIaZZn0DqKrhydHB8X97hNXl'><img",
    "https://paperswithcode.com/sota/camouflaged-object-segmentation-on-chameleon?p=bilateral-reference-for-high-resolution",
    "https://drive.google.com/drive/folders/18_hAE3QM4cwAzEAKXuSNtKjmgFXTQXZN",
    "https://drive.google.com/thumbnail?id=1KfxCQUUa2y9T-aysEaeVVjCUt3Z0zSkL&sz=w1620\"",
    "https://scholar.google.com/citations?user=kakwJ5QAAAAJ'",
    "https://img.shields.io/badge/Journal-Paper-red'></a>&ensp;",
    "https://github.com/user-attachments/assets/6cce7ca7-7817-4406-b6c4-6d4e8c414ed4",
    "https://discord.gg/d9NN5sgFrq",
    "https://colab.research.google.com/drive/14Dqg7oeBkFEtchaHLNpig2BcdkZEogba'><img",
    "https://drive.google.com/file/d/1xEh7fsgWGaS5c3IffMswasv0_u-aVM9E/view",
    "https://huggingface.co/ZhengPeng7/BiRefNet-matting",
    "https://drive.google.com/file/d/1_IfUnu8Fpfn-nerB89FzdNXQ7zk6FKxc/view",
    "https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-te4?p=bilateral-reference-for-high-resolution",
    "https://github.com/JizhiziLi/P3M",
    "https://colab.research.google.com/drive/1MaEiBfJ4xIaZZn0DqKrhydHB8X97hNXl",
    "https://paperswithcode.com/sota/salient-object-detection-on-dut-omron?p=bilateral-reference-for-high-resolution",
    "https://img.shields.io/badge/arXiv-Paper-red'></a>&ensp;",
    "https://img.shields.io/badge/Page-Project-red'></a>&ensp;",
    "https://img.shields.io/badge/Box_Guided_Segmentation-F9AB00?style=for-the-badge&logo=googlecolab&color=525252'></a>&ensp;",
    "https://drive.google.com/file/d/1f7L0Pb1Y3RkOMbqLCW_zO31dik9AiUFa/view",
    "https://drive.google.com/thumbnail?id=1ItXaA26iYnE8XQ_GgNLy71MOWePoS2-g&sz=w400\"",
    "https://paperswithcode.com/sota/rgb-salient-object-detection-on-uhrsd?p=bilateral-reference-for-high-resolution",
    "https://huggingface.co/datasets/schirrmacher/humans",
    "https://twitter.com/toyxyz3",
    "https://drive.google.com/file/d/1fzInDWiE2n65tmjaHDSZpqhL0VME6-Yl/view",
    "https://drive.google.com/file/d/1J90LucvDQaS3R_-9E7QUh1mgJ8eQvccb/view",
    "https://github.com/huggingface/huggingface.js/blob/3a8651fbc6508920475564a692bf0e5b601d9343/packages/tasks/src/model-libraries-snippets.ts#L763",
    "https://drive.google.com/file/d/1cgL2qyvOO5q3ySfhytypX46swdQwZLrJ",
    "https://github.com/MoonHugo/ComfyUI-BiRefNet-Hugo",
    "https://github.com/Kazuhito00/BiRefNet-ONNX-Sample",
    "https://paperswithcode.com/sota/rgb-salient-object-detection-on-hrsod?p=bilateral-reference-for-high-resolution",
    "https://github.com/ZhengPeng7/BiRefNet/releases/tag/v1",
    "https://github.com/tin2tin/2D_Asset_Generator",
    "https://drive.google.com/drive/u/0/folders/1kZM55bwsRdS__bdnsXpkmH6QPyza-9-N",
    "https://paperswithcode.com/sota/rgb-salient-object-detection-on-davis-s?p=bilateral-reference-for-high-resolution",
    "https://img.shields.io/badge/Inference_&_Evaluation-F9AB00?style=for-the-badge&logo=googlecolab&color=525252'></a>&ensp;",
    "https://github.com/lbq779660843/BiRefNet-Tensorrt",
    "https://drive.google.com/file/d/13FaxyyOwyCddfZn2vZo1xG1KNZ3cZ-6B/view",
    "https://huggingface.co/ZhengPeng7/BiRefNet'><img",
    "https://img.shields.io/badge/Multiple_Images_Inference-F9AB00?style=for-the-badge&logo=googlecolab&color=525252'></a>&ensp;",
    "https://huggingface.co/ZhengPeng7/BiRefNet_HR",
    "https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/salient-object-detection-on-dut-omron",
    "https://drive.google.com/drive/folders/1s2Xe0cjq-2ctnJBR24563yMSCOu4CcxM",
    "https://scholar.google.com/citations?user=pw_0Z_UAAAAJ'",
    "https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/camouflaged-object-segmentation-on-camo",
    "https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-te2",
    "https://drive.google.com/thumbnail?id=1p1zgyVz27cGEqQMtOKzm_6zoYK3Sw_Zk&sz=w1620\"",
    "https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/rgb-salient-object-detection-on-hrsod",
    "https://img.shields.io/badge/%F0%9F%A4%97%20HF-Space-blue'></a>&ensp;",
    "https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-vd?p=bilateral-reference-for-high-resolution",
    "https://paperswithcode.com/sota/dichotomous-image-segmentation-on-dis-te3?p=bilateral-reference-for-high-resolution",
    "https://github.com/ZhengPeng7/BiRefNet/releases",
    "https://github.com/yuanyang1991/birefnet_tensorrt",
    "https://paperswithcode.com/sota/camouflaged-object-segmentation-on-camo?p=bilateral-reference-for-high-resolution",
    "https://drive.google.com/file/d/1lWVyimvxQ3bmDlaHeO1IOUbFZqUj9DYg/view",
    "https://huggingface.co/briaai/RMBG-2.0/media/main/t4.png\"",
    "https://colab.research.google.com/drive/14Dqg7oeBkFEtchaHLNpig2BcdkZEogba",
    "https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/camouflaged-object-segmentation-on-chameleon",
    "https://github.com/viperyl/ComfyUI-BiRefNet",
    "https://cy.ncss.cn/en",
    "https://colab.research.google.com/drive/1B6aKZ3ekcvKMkSBn0N5mCASLUYMp0whK",
    "https://huggingface.co/spaces/ZhengPeng7/BiRefNet_demo",
    "https://drive.google.com/file/d/1WJooyTkhoDLllaqwbpur_9Hle0XTHEs_/view",
    "https://github.com/camenduru/text-behind-tost",
    "https://fal.ai/models/birefnet",
    "https://colab.research.google.com/drive/1r8GkFPyMMO0OkMX6ih5FjZnUCQrl2SHV?usp=sharing",
    "https://youtu.be/FwGT_0V9E-k",
    "https://drive.google.com/thumbnail?id=1gn5GyKFlJbMIkre1JyEdHDSYcrFmcLD0&sz=w1620\"",
    "https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/rgb-salient-object-detection-on-davis-s",
    "https://img.shields.io/endpoint.svg?url=https://paperswithcode.com/badge/bilateral-reference-for-high-resolution/dichotomous-image-segmentation-on-dis-te1",
    "https://drive.google.com/drive/folders/1s2Xe0cjq-2ctnJBR24563yMSCOu4CcxM'><img",
    "https://github.com/ZhengPeng7/BiRefNet?tab=readme-ov-file#pen-fine-tuning-on-custom-data",
    "https://pan.baidu.com/s/1RnxAzaHSTGBC1N6r_RfeqQ?pwd=PSWD",
    "https://fal.ai/models/fal-ai/birefnet/v2",
    "https://drive.google.com/file/d/1Nlcg58d5bvE-Tbbm8su_eMQba10hdcwQ/view",
    "https://fal.ai"
  ]
}
```

</details>


---

## Repository 3: ZhengPeng7/BiRefNet](https:

# GitHub Data Extraction Error

Error: 404 {"message": "Not Found", "documentation_url": "https://docs.github.com/rest/repos/repos#get-a-repository", "status": "404"}

Repository: https://github.com/ZhengPeng7/BiRefNet](https://github.com/ZhengPeng7/BiRefNet

---

