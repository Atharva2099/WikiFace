# GitHub Data for microsoft_deberta-large-mnli

**Task Category:** Text Classification

## Repository 1: microsoft/DeBERTa

# GitHub Repository Data

**Repository:** [microsoft/DeBERTa](https://github.com/microsoft/DeBERTa)

## Basic Information

- **Description:** The implementation of DeBERTa
- **Created:** 2020-06-08T15:57:14+00:00
- **Last Updated:** 2025-06-20T08:29:46+00:00
- **Last Pushed:** 2023-09-29T11:14:26+00:00
- **Default Branch:** master
- **Size:** 243 KB

## Statistics

- **Stars:** 2,103
- **Forks:** 232
- **Watchers:** 2,103
- **Open Issues:** 75
- **Total Issues:** 0
- **Pull Requests:** 32

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/microsoft/DeBERTa/blob/master/LICENSE)

## Languages

- **Python:** 349,890 bytes
- **Shell:** 39,201 bytes
- **Dockerfile:** 2,097 bytes

## Topics

- `bert`
- `deeplearning`
- `representation-learning`
- `roberta`
- `language-model`
- `natural-language-understanding`
- `self-attention`
- `transformer-encoder`

## Top Contributors

1. **BigBird01** - 36 contributions
2. **microsoftopensource** - 4 contributions
3. **chenweizhu** - 2 contributions
4. **alisafaya** - 1 contributions
5. **anukaal** - 1 contributions
6. **tirkarthi** - 1 contributions
7. **luw315** - 1 contributions
8. **saksham-singhal** - 1 contributions
9. **shenfe** - 1 contributions
10. **namisan** - 1 contributions

## File Structure (Sample of 10 files)

Total files: 139

- `.gitignore` (blob)
- `CODE_OF_CONDUCT.md` (blob)
- `DeBERTa` (tree)
- `DeBERTa/.gitignore` (blob)
- `DeBERTa/__init__.py` (blob)
- `DeBERTa/apps` (tree)
- `DeBERTa/apps/__init__.py` (blob)
- `DeBERTa/apps/_utils.py` (blob)
- `DeBERTa/apps/models` (tree)
- `DeBERTa/apps/models/__init__.py` (blob)

## Recent Issues

- 游댮 **#159** Converting from Tiktoken failed (closed)
- 游릭 **#158** decoding selection 2 (open)
- 游댮 **#157** Runner Wrapper to call run.py (closed)
- 游릭 **#156** Update requirements.txt - sklearn to scikit-learn (open)
- 游릭 **#155** How to pre training mDeBERTa model? (open)

## Recent Pull Requests

- 游댮 **#157** Runner Wrapper to call run.py (closed)
- 游릭 **#156** Update requirements.txt - sklearn to scikit-learn (open)
- 游릭 **#144** sklearn changed to scikit-learn to avoid pip install failure (open)
- 游릭 **#143** Fix typo (open)
- 游릭 **#142** fix: Corrected code instructions in README.md (open)

## Recent Commits

- **4d7fe0bd** Update README.md - Pengcheng He (2023-03-25T10:22:59+00:00)
- **000d76f0** Update README.md - Pengcheng He (2023-03-25T10:19:10+00:00)
- **fdea415c** Update README.md - Weizhu Chen (2023-03-20T04:02:25+00:00)
- **af986696** Update README.md - Weizhu Chen (2023-03-20T03:58:54+00:00)
- **cf84c436** Update README.md - Pengcheng He (2023-03-19T06:16:22+00:00)
- **c794b711** 1. Add code for DeBERTaV3 pre-training; 2. Fix error in torch 1.11; 3. Add code for ONNX export - Pengcheng He (2023-03-19T06:08:57+00:00)
- **2c5b6b2a** Fix: a few typos as I read through the README.md - cpcdoy (2022-11-30T09:21:05+00:00)
- **0eb1283e** Import ABC from collections.abc for Python 3.10 compatibility - Karthikeyan Singaravelan (2022-03-22T13:39:39+00:00)
- **c558ad99** Fix compatibility with latest pytorch - Pengcheng He (2022-01-20T16:38:47+00:00)
- **be55f762** Add fine-tuning script for alpha nli - Pengcheng He (2022-01-19T21:10:21+00:00)

## External Links Found in README

- https://huggingface.co/models?other=deberta-v3
- https://huggingface.co/microsoft/deberta-base-mnli
- https://huggingface.co/models?search=microsoft%2Fdeberta
- https://openreview.net/forum?id=sE7-XhLxHA
- https://hub.docker.com/r/bagai/deberta
- https://github.com/huggingface/transformers
- https://huggingface.co/microsoft/deberta-large
- https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/
- https://huggingface.co/microsoft/deberta-xlarge-mnli
- https://huggingface.co/microsoft/deberta-v3-large
- https://github.com/google/sentencepiece
- https://huggingface.co/microsoft/deberta-base
- https://arxiv.org/abs/2006.03654
- https://huggingface.co/microsoft/deberta-v3-small
- https://openreview.net/forum?id=XPZIaotutsD}
- https://huggingface.co/microsoft/deberta-v2-xlarge
- https://deberta.readthedocs.io/en/latest/
- https://docs.docker.com/engine/install/ubuntu/
- https://huggingface.co/microsoft/deberta-v2-xxlarge-mnli
- https://huggingface.co/microsoft/deberta-xxlarge-v2

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 270730507,
  "name": "DeBERTa",
  "full_name": "microsoft/DeBERTa",
  "description": "The implementation of DeBERTa",
  "html_url": "https://github.com/microsoft/DeBERTa",
  "clone_url": "https://github.com/microsoft/DeBERTa.git",
  "ssh_url": "git@github.com:microsoft/DeBERTa.git",
  "homepage": null,
  "topics": [
    "bert",
    "deeplearning",
    "representation-learning",
    "roberta",
    "language-model",
    "natural-language-understanding",
    "self-attention",
    "transformer-encoder"
  ],
  "default_branch": "master",
  "created_at": "2020-06-08T15:57:14+00:00",
  "updated_at": "2025-06-20T08:29:46+00:00",
  "pushed_at": "2023-09-29T11:14:26+00:00",
  "size_kb": 243,
  "watchers_count": 2103,
  "stargazers_count": 2103,
  "forks_count": 232,
  "open_issues_count": 75,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/microsoft/DeBERTa/blob/master/LICENSE"
  },
  "languages": {
    "Python": 349890,
    "Shell": 39201,
    "Dockerfile": 2097
  },
  "top_contributors": [
    {
      "login": "BigBird01",
      "contributions": 36
    },
    {
      "login": "microsoftopensource",
      "contributions": 4
    },
    {
      "login": "chenweizhu",
      "contributions": 2
    },
    {
      "login": "alisafaya",
      "contributions": 1
    },
    {
      "login": "anukaal",
      "contributions": 1
    },
    {
      "login": "tirkarthi",
      "contributions": 1
    },
    {
      "login": "luw315",
      "contributions": 1
    },
    {
      "login": "saksham-singhal",
      "contributions": 1
    },
    {
      "login": "shenfe",
      "contributions": 1
    },
    {
      "login": "namisan",
      "contributions": 1
    },
    {
      "login": "cpcdoy",
      "contributions": 1
    },
    {
      "login": "fatcat-z",
      "contributions": 1
    },
    {
      "login": "microsoft-github-operations[bot]",
      "contributions": 1
    },
    {
      "login": "vilhub",
      "contributions": 1
    }
  ],
  "file_tree_count": 139,
  "file_tree_sample": [
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "CODE_OF_CONDUCT.md",
      "type": "blob"
    },
    {
      "path": "DeBERTa",
      "type": "tree"
    },
    {
      "path": "DeBERTa/.gitignore",
      "type": "blob"
    },
    {
      "path": "DeBERTa/__init__.py",
      "type": "blob"
    },
    {
      "path": "DeBERTa/apps",
      "type": "tree"
    },
    {
      "path": "DeBERTa/apps/__init__.py",
      "type": "blob"
    },
    {
      "path": "DeBERTa/apps/_utils.py",
      "type": "blob"
    },
    {
      "path": "DeBERTa/apps/models",
      "type": "tree"
    },
    {
      "path": "DeBERTa/apps/models/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 32,
  "recent_issues": [
    {
      "number": 159,
      "title": "Converting from Tiktoken failed",
      "state": "closed"
    },
    {
      "number": 158,
      "title": "decoding selection 2",
      "state": "open"
    },
    {
      "number": 157,
      "title": "Runner Wrapper to call run.py",
      "state": "closed"
    },
    {
      "number": 156,
      "title": "Update requirements.txt - sklearn to scikit-learn",
      "state": "open"
    },
    {
      "number": 155,
      "title": "How to pre training mDeBERTa model?",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 157,
      "title": "Runner Wrapper to call run.py",
      "state": "closed"
    },
    {
      "number": 156,
      "title": "Update requirements.txt - sklearn to scikit-learn",
      "state": "open"
    },
    {
      "number": 144,
      "title": "sklearn changed to scikit-learn to avoid pip install failure",
      "state": "open"
    },
    {
      "number": 143,
      "title": "Fix typo",
      "state": "open"
    },
    {
      "number": 142,
      "title": "fix: Corrected code instructions in README.md",
      "state": "open"
    }
  ],
  "recent_commits": [
    {
      "sha": "4d7fe0bd4fb3c7d4f4005a7cafabde9800372098",
      "author": "Pengcheng He",
      "date": "2023-03-25T10:22:59+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "000d76f0aa860f0fd134a6d013d6315f94c42efe",
      "author": "Pengcheng He",
      "date": "2023-03-25T10:19:10+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "fdea415c1403ba10a2f76fc062603b5cb6360b2a",
      "author": "Weizhu Chen",
      "date": "2023-03-20T04:02:25+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "af9866966f37529f84bae6570b2392f1d748a4ef",
      "author": "Weizhu Chen",
      "date": "2023-03-20T03:58:54+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "cf84c436b9db9b87270f1a121ed4aba154ada10d",
      "author": "Pengcheng He",
      "date": "2023-03-19T06:16:22+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "c794b711ed10633251dd049babefb7c2a5435565",
      "author": "Pengcheng He",
      "date": "2023-03-19T06:08:57+00:00",
      "message": "1. Add code for DeBERTaV3 pre-training; 2. Fix error in torch 1.11; 3. Add code for ONNX export"
    },
    {
      "sha": "2c5b6b2a3aeacda399c55666055568f42dac5e16",
      "author": "cpcdoy",
      "date": "2022-11-30T09:21:05+00:00",
      "message": "Fix: a few typos as I read through the README.md"
    },
    {
      "sha": "0eb1283efc51251af8a10b6bb289d3701f4be5e9",
      "author": "Karthikeyan Singaravelan",
      "date": "2022-03-22T13:39:39+00:00",
      "message": "Import ABC from collections.abc for Python 3.10 compatibility"
    },
    {
      "sha": "c558ad99373dac695128c9ec45f39869aafd374e",
      "author": "Pengcheng He",
      "date": "2022-01-20T16:38:47+00:00",
      "message": "Fix compatibility with latest pytorch"
    },
    {
      "sha": "be55f762d693e4b3a9eb07e083ab3bdde7705252",
      "author": "Pengcheng He",
      "date": "2022-01-19T21:10:21+00:00",
      "message": "Add fine-tuning script for alpha nli"
    },
    {
      "sha": "b9a6555101825413ca6ab6593813ca05ed65bbda",
      "author": "Pengcheng He",
      "date": "2021-12-08T21:27:58+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "a3f1d7280bbbc5dd715ea85b8269cddfd567696d",
      "author": "Pengcheng He",
      "date": "2021-12-08T21:03:24+00:00",
      "message": "Add script for XSmall model fine-tuning. Optimize the code for better accuracy."
    },
    {
      "sha": "c24bbab816fc0d6e84f4ef4e5e8bb236b9204744",
      "author": "Pengcheng He",
      "date": "2021-12-03T18:58:11+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "994f643ec20db09e33235496dfd0144643479dff",
      "author": "Pengcheng He",
      "date": "2021-11-20T01:21:06+00:00",
      "message": "1. Add document for DeBERTa pre-training"
    },
    {
      "sha": "e59f09fbd4bb72d447dd0057047cfbaf1d565659",
      "author": "Anurag Kumar",
      "date": "2021-09-11T16:11:17+00:00",
      "message": "Update setup.py"
    },
    {
      "sha": "f21b7a9048bbfc54cc240796b51445ff133b5557",
      "author": "vilhub",
      "date": "2021-11-23T09:07:42+00:00",
      "message": "Fix broken link for mDeBERTa-V3-Base in README"
    },
    {
      "sha": "87580930689ec9f75ef8dbebba367953ed3dfe63",
      "author": "Pengcheng He",
      "date": "2021-11-19T21:35:46+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "cc5cefbf1dccbf8b32636667811afe172480dea5",
      "author": "Pengcheng He",
      "date": "2021-11-19T02:22:43+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "c8efdecffbd2d57a6f53742c54b20bbf52b53ad0",
      "author": "Pengcheng He",
      "date": "2021-11-16T15:39:23+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "63b2dcc58e2c1acece7f5bb8d493b973896b3855",
      "author": "Pengcheng He",
      "date": "2021-11-16T10:16:40+00:00",
      "message": "Update README.md"
    }
  ],
  "readme_text": "# DeBERTa: Decoding-enhanced BERT with Disentangled Attention\n\nThis repository is the official implementation of [ **DeBERTa**: **D**ecoding-**e**nhanced **BERT** with Disentangled **A**ttention ](https://arxiv.org/abs/2006.03654) and [DeBERTa V3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing](https://arxiv.org/abs/2111.09543)\n\n## News\n### 03/18/2023\n- [DeBERTaV3](https://openreview.net/forum?id=sE7-XhLxHA) paper is accepted by ICLR 2023.\n- The code for DeBERTaV3 pre-training and continous training is added. Please check [Language Model](experiments/language_model) for details.\n\n### 12/8/2021\n- [DeBERTa-V3-XSmall](https://huggingface.co/microsoft/deberta-v3-xsmall) is added. With only **22M** backbone parameters which is only 1/4 of RoBERTa-Base and XLNet-Base, DeBERTa-V3-XSmall significantly outperforms the later on MNLI and SQuAD v2.0 tasks (i.e. 1.2% on MNLI-m, 1.5% EM score on SQuAD v2.0). This further demonstrates the efficiency of DeBERTaV3 models.\n\n### 11/16/2021\n- The models of our new work [DeBERTa V3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing](https://arxiv.org/abs/2111.09543) are publicly available at [huggingface model hub](https://huggingface.co/models?other=deberta-v3) now. The new models are based on DeBERTa-V2 models by replacing MLM with ELECTRA-style objective plus gradient-disentangled embedding sharing which further improves the model efficiency.\n- Scripts for DeBERTa V3 model fine-tuning are added\n- Code of RTD task head is added\n- [Document](experiments/language_model) for language model pre-training is added\n\n### 3/31/2021\n- Masked language model task is added\n- SuperGLUE tasks is added\n- SiFT code is added\n\n### 2/03/2021\nDeBERTa v2 code and the **900M, 1.5B** [model](https://huggingface.co/models?search=microsoft%2Fdeberta) are here now. This includes the 1.5B model used for our SuperGLUE single-model submission and achieving 89.9, versus human baseline 89.8. You can find more details about this submission in our [blog](https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/)\n\n#### What's new in v2\n- **Vocabulary** In v2 we use a new vocabulary of size 128K built from the training data. Instead of GPT2 tokenizer, we use [sentencepiece](https://github.com/google/sentencepiece) tokenizer.\n- **nGiE(nGram Induced Input Encoding)** In v2 we use an additional convolution layer aside with the first transformer layer to better learn the local dependency of input tokens. We will add more ablation studies on this feature.\n- **Sharing position projection matrix with content projection matrix in attention layer** Based on our previous experiment, we found this can save parameters without affecting performance.\n- **Apply bucket to encode relative positions** In v2 we use log bucket to encode relative positions similar to T5. \n- **900M model & 1.5B model** In v2 we scale our model size to 900M and 1.5B which significantly improves the performance of downstream tasks.\n\n### 12/29/2020\nWith DeBERTa 1.5B model, we surpass T5 11B model and human performance on SuperGLUE leaderboard. Code and model will be released soon. Please check out our paper for more details.\n\n### 06/13/2020\nWe released the pre-trained models, source code, and fine-tuning scripts to reproduce some of the experimental results in the paper. You can follow similar scripts to apply DeBERTa to your own experiments or applications. Pre-training scripts will be released in the next step. \n\n\n## Introduction to DeBERTa \nDeBERTa (Decoding-enhanced BERT with disentangled attention) improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions. Second, an enhanced mask decoder is used to replace the output softmax layer to predict the masked tokens for model pretraining. We show that these two techniques significantly improve the efficiency of model pre-training and performance of downstream tasks.\n\n# Pre-trained Models\n\nOur pre-trained models are packaged into zipped files. You can download them from our [releases](https://huggingface.co/models?search=microsoft%2Fdeberta), or download an individual model via the links below:\n\n|Model        | Vocabulary(K)|Backbone Parameters(M)| Hidden Size | Layers| Note|\n|-------------|------|---------|-----|-----|---------|\n|**[V2-XXLarge](https://huggingface.co/microsoft/deberta-v2-xxlarge)<sup>1</sup>**|128|1320|1536| 48|128K new SPM vocab |\n|[V2-XLarge](https://huggingface.co/microsoft/deberta-v2-xlarge)|128|710|1536| 24| 128K new SPM vocab|\n|[XLarge](https://huggingface.co/microsoft/deberta-xlarge)|50|700|1024|48| Same vocab as RoBERTa|\n|[Large](https://huggingface.co/microsoft/deberta-large)|50|350|1024|24|Same vocab as RoBERTa|\n|[Base](https://huggingface.co/microsoft/deberta-base)|50|100|768|12|Same vocab as RoBERTa|\n|[V2-XXLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xxlarge-mnli)|128|1320|1536| 48|Fine-turned with MNLI |\n|[V2-XLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xlarge-mnli)|128|710|1536| 24|Fine-turned with MNLI |\n|[XLarge-MNLI](https://huggingface.co/microsoft/deberta-xlarge-mnli)|50|700|1024|48|Fine-turned with MNLI|\n|[Large-MNLI](https://huggingface.co/microsoft/deberta-large-mnli)|50|350|1024|24|Fine-turned with MNLI|\n|[Base-MNLI](https://huggingface.co/microsoft/deberta-base-mnli)|50|86|768|12|Fine-turned with MNLI|\n|[DeBERTa-V3-Large](https://huggingface.co/microsoft/deberta-v3-large)<sup>2</sup>|128|304|1024| 24| 128K new SPM vocab|\n|[DeBERTa-V3-Base](https://huggingface.co/microsoft/deberta-v3-base)<sup>2</sup>|128|86|768| 12| 128K new SPM vocab|\n|[DeBERTa-V3-Small](https://huggingface.co/microsoft/deberta-v3-small)<sup>2</sup>|128|44|768| 6| 128K new SPM vocab|\n|[DeBERTa-V3-XSmall](https://huggingface.co/microsoft/deberta-v3-xsmall)<sup>2</sup>|128|22|384| 12| 128K new SPM vocab|\n|[mDeBERTa-V3-Base](https://huggingface.co/microsoft/mdeberta-v3-base)<sup>2</sup>|250|86|768| 12| 250K new SPM vocab, multi-lingual model with 102 languages|\n\n## Note \n- 1 This is the model(89.9) that surpassed **T5 11B(89.3) and human performance(89.8)** on **SuperGLUE** for the first time. 128K new SPM vocab. \n- 2 These V3 DeBERTa models are deberta models pre-trained with ELECTRA-style objective plus gradient-disentangled embedding sharing which significantly improves the model efficiency.\n\n\n# Try the model\n\nRead our [documentation](https://deberta.readthedocs.io/en/latest/)\n\n## Requirements\n- Linux system, e.g. Ubuntu 18.04LTS\n- CUDA 10.0\n- pytorch 1.3.0\n- python 3.6\n- bash shell 4.0\n- curl\n- docker (optional)\n- nvidia-docker2 (optional)\n\nThere are several ways to try our code,\n### Use docker\n\nDocker is the recommended way to run the code as we already built every dependency into our docker [bagai/deberta](https://hub.docker.com/r/bagai/deberta) and you can follow the [docker official site](https://docs.docker.com/engine/install/ubuntu/) to install docker on your machine.\n\nTo run with docker, make sure your system fulfills the requirements in the above list. Here are the steps to try the GLUE experiments: Pull the code, run `./run_docker.sh` \n, and then you can run the bash commands under `/DeBERTa/experiments/glue/`\n\n\n### Use pip\nPull the code and run `pip3 install -r requirements.txt` in the root directory of the code, then enter `experiments/glue/` folder of the code and try the bash commands under that folder for glue experiments.\n\n### Install as a pip package\n`pip install deberta`\n\n#### Use DeBERTa in existing code\n``` Python\n\n# To apply DeBERTa to your existing code, you need to make two changes to your code,\n# 1. change your model to consume DeBERTa as the encoder\nfrom DeBERTa import deberta\nimport torch\nclass MyModel(torch.nn.Module):\n  def __init__(self):\n    super().__init__()\n    # Your existing model code\n    self.deberta = deberta.DeBERTa(pre_trained='base') # Or 'large' 'base-mnli' 'large-mnli' 'xlarge' 'xlarge-mnli' 'xlarge-v2' 'xxlarge-v2'\n    # Your existing model code\n    # do inilization as before\n    # \n    self.deberta.apply_state() # Apply the pre-trained model of DeBERTa at the end of the constructor\n    #\n  def forward(self, input_ids):\n    # The inputs to DeBERTa forward are\n    # `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length] with the word token indices in the vocabulary\n    # `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token types indices selected in [0, 1]. \n    #    Type 0 corresponds to a `sentence A` and type 1 corresponds to a `sentence B` token (see BERT paper for more details).\n    # `attention_mask`: an optional parameter for input mask or attention mask. \n    #   - If it's an input mask, then it will be torch.LongTensor of shape [batch_size, sequence_length] with indices selected in [0, 1]. \n    #      It's a mask to be used if the input sequence length is smaller than the max input sequence length in the current batch. \n    #      It's the mask that we typically use for attention when a batch has varying length sentences.\n    #   - If it's an attention mask then if will be torch.LongTensor of shape [batch_size, sequence_length, sequence_length]. \n    #      In this case, it's a mask indicating which tokens in the sequence should be attended by other tokens in the sequence. \n    # `output_all_encoded_layers`: whether to output results of all encoder layers, default, True\n    encoding = deberta.bert(input_ids)[-1]\n\n# 2. Change your tokenizer with the tokenizer built-in DeBERta\nfrom DeBERTa import deberta\nvocab_path, vocab_type = deberta.load_vocab(pretrained_id='base')\ntokenizer = deberta.tokenizers[vocab_type](vocab_path)\n# We apply the same schema of special tokens as BERT, e.g. [CLS], [SEP], [MASK]\nmax_seq_len = 512\ntokens = tokenizer.tokenize('Examples input text of DeBERTa')\n# Truncate long sequence\ntokens = tokens[:max_seq_len -2]\n# Add special tokens to the `tokens`\ntokens = ['[CLS]'] + tokens + ['[SEP]']\ninput_ids = tokenizer.convert_tokens_to_ids(tokens)\ninput_mask = [1]*len(input_ids)\n# padding\npaddings = max_seq_len-len(input_ids)\ninput_ids = input_ids + [0]*paddings\ninput_mask = input_mask + [0]*paddings\nfeatures = {\n'input_ids': torch.tensor(input_ids, dtype=torch.int),\n'input_mask': torch.tensor(input_mask, dtype=torch.int)\n}\n\n```\n\n#### Run DeBERTa experiments from command line\nFor glue tasks, \n1. Get the data\n``` bash\ncache_dir=/tmp/DeBERTa/\ncd experiments/glue\n./download_data.sh  $cache_dir/glue_tasks\n```\n\n2. Run task\n\n``` bash\ntask=STS-B \nOUTPUT=/tmp/DeBERTa/exps/$task\nexport OMP_NUM_THREADS=1\npython3 -m DeBERTa.apps.run --task_name $task --do_train  \\\n  --data_dir $cache_dir/glue_tasks/$task \\\n  --eval_batch_size 128 \\\n  --predict_batch_size 128 \\\n  --output_dir $OUTPUT \\\n  --scale_steps 250 \\\n  --loss_scale 16384 \\\n  --accumulative_update 1 \\  \n  --num_train_epochs 6 \\\n  --warmup 100 \\\n  --learning_rate 2e-5 \\\n  --train_batch_size 32 \\\n  --max_seq_len 128\n```\n\n## Notes\n- 1. By default we will cache the pre-trained model and tokenizer at `$HOME/.~DeBERTa`, you may need to clean it if the downloading failed unexpectedly.\n- 2. You can also try our models with [HF Transformers](https://github.com/huggingface/transformers). But when you try XXLarge model you need to specify --sharded_ddp argument. Please check our [XXLarge model card](https://huggingface.co/microsoft/deberta-xxlarge-v2) for more details.\n\n## Experiments\nOur fine-tuning experiments are carried on half a DGX-2 node with 8x32 V100 GPU cards, the results may vary due to different GPU models, drivers, CUDA SDK versions, using FP16 or FP32, and random seeds. \nWe report our numbers based on multiple runs with different random seeds here. Here are the results from the Large model:\n\n|Task\t |Command\t|Results\t|Running Time(8x32G V100 GPUs)|\n|--------|---------------|---------------|-------------------------|\n|**MNLI xxlarge v2**|\t`experiments/glue/mnli.sh xxlarge-v2`|\t**91.7/91.9** +/-0.1|\t4h|\n|MNLI xlarge v2|\t`experiments/glue/mnli.sh xlarge-v2`|\t91.7/91.6 +/-0.1|\t2.5h|\n|MNLI xlarge|\t`experiments/glue/mnli.sh xlarge`|\t91.5/91.2 +/-0.1|\t2.5h|\n|MNLI large|\t`experiments/glue/mnli.sh large`|\t91.3/91.1 +/-0.1|\t2.5h|\n|QQP large|\t`experiments/glue/qqp.sh large`|\t92.3 +/-0.1|\t\t6h|\n|QNLI large|\t`experiments/glue/qnli.sh large`|\t95.3 +/-0.2|\t\t2h|\n|MRPC large|\t`experiments/glue/mrpc.sh large`|\t91.9 +/-0.5|\t\t0.5h|\n|RTE large|\t`experiments/glue/rte.sh large`|\t86.6 +/-1.0|\t\t0.5h|\n|SST-2 large|\t`experiments/glue/sst2.sh large`|\t96.7 +/-0.3|\t\t1h|\n|STS-b large|\t`experiments/glue/Stsb.sh large`|\t92.5 +/-0.3|\t\t0.5h|\n|CoLA large|\t`experiments/glue/cola.sh`|\t70.5 +/-1.0|\t\t0.5h|\n\nAnd here are the results from the Base model\n\n|Task\t |Command\t|Results\t|Running Time(8x32G V100 GPUs)|\n|--------|---------------|---------------|-------------------------|\n|MNLI base|\t`experiments/glue/mnli.sh base`|\t88.8/88.5 +/-0.2|\t1.5h|\n\n\n### Fine-tuning on NLU tasks\n\nWe present the dev results on SQuAD 1.1/2.0 and several GLUE benchmark tasks.\n\n| Model                     | SQuAD 1.1 | SQuAD 2.0 | MNLI-m/mm   | SST-2 | QNLI | CoLA | RTE    | MRPC  | QQP   |STS-B |\n|---------------------------|-----------|-----------|-------------|-------|------|------|--------|-------|-------|------|\n|                           | F1/EM     | F1/EM     | Acc         | Acc   | Acc  | MCC  | Acc    |Acc/F1 |Acc/F1 |P/S   |\n| BERT-Large                | 90.9/84.1 | 81.8/79.0 | 86.6/-      | 93.2  | 92.3 | 60.6 | 70.4   | 88.0/-       | 91.3/- |90.0/- |\n| RoBERTa-Large             | 94.6/88.9 | 89.4/86.5 | 90.2/-      | 96.4  | 93.9 | 68.0 | 86.6   | 90.9/-       | 92.2/- |92.4/- |\n| XLNet-Large               | 95.1/89.7 | 90.6/87.9 | 90.8/-      | 97.0  | 94.9 | 69.0 | 85.9   | 90.8/-       | 92.3/- |92.5/- |\n| [DeBERTa-Large](https://huggingface.co/microsoft/deberta-large)<sup>1</sup> | 95.5/90.1 | 90.7/88.0 | 91.3/91.1| 96.5|95.3| 69.5| 91.0| 92.6/94.6| 92.3/- |92.8/92.5 |\n| [DeBERTa-XLarge](https://huggingface.co/microsoft/deberta-xlarge)<sup>1</sup> | -/-  | -/-  | 91.5/91.2| 97.0 | - | -    | 93.1   | 92.1/94.3    | -    |92.9/92.7|\n| [DeBERTa-V2-XLarge](https://huggingface.co/microsoft/deberta-v2-xlarge)<sup>1</sup>|95.8/90.8| 91.4/88.9|91.7/91.6| **97.5**| 95.8|71.1|**93.9**|92.0/94.2|92.3/89.8|92.9/92.9|\n|**[DeBERTa-V2-XXLarge](https://huggingface.co/microsoft/deberta-v2-xxlarge)<sup>1,2</sup>**|**96.1/91.4**|**92.2/89.7**|**91.7/91.9**|97.2|**96.0**|72.0| 93.5| **93.1/94.9**|**92.7/90.3** |**93.2/93.1** |\n|**[DeBERTa-V3-Large](https://huggingface.co/microsoft/deberta-v3-large)**|-/-|91.5/89.0|**91.8/91.9**|96.9|**96.0**|**75.3**| 92.7| 92.2/-|**93.0/-** |93.0/- |\n|[DeBERTa-V3-Base](https://huggingface.co/microsoft/deberta-v3-base)|-/-|88.4/85.4|90.6/90.7|-|-|-| -| -|- |- |\n|[DeBERTa-V3-Small](https://huggingface.co/microsoft/deberta-v3-small)|-/-|82.9/80.4|88.3/87.7|-|-|-| -| -|- |- |\n|[DeBERTa-V3-XSmall](https://huggingface.co/microsoft/deberta-v3-xsmall)|-/-|84.8/82.0|88.1/88.3|-|-|-| -| -|- |- |\n\n### Fine-tuning on XNLI\n\nWe present the dev results on XNLI with zero-shot crosslingual transfer setting, i.e. training with english data only, test on other languages.\n\n| Model        |avg | en |  fr| es  | de  | el  | bg  | ru  |tr   |ar   |vi   | th  | zh | hi  | sw  | ur  | \n|--------------| ----|----|----|---- |--   |--   |--   | --  |--   |--   |--   | --  | -- | --  | --  | --  |\n| XLM-R-base   |76.2 |85.8|79.7|80.7 |78.7 |77.5 |79.6 |78.1 |74.2 |73.8 |76.5 |74.6 |76.7| 72.4| 66.5| 68.3|\n| [mDeBERTa-V3-Base](https://huggingface.co/microsoft/mdeberta-v3-base)|**79.8**+/-0.2|**88.2**|**82.6**|**84.4** |**82.7** |**82.3** |**82.4** |**80.8** |**79.5** |**78.5** |**78.1** |**76.4** |**79.5**| **75.9**| **73.9**| **72.4**|\n\n--------\n#### Notes.\n - <sup>1</sup> Following RoBERTa, for RTE, MRPC, STS-B, we fine-tune the tasks based on [DeBERTa-Large-MNLI](https://huggingface.co/microsoft/deberta-large-mnli), [DeBERTa-XLarge-MNLI](https://huggingface.co/microsoft/deberta-xlarge-mnli), [DeBERTa-V2-XLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xlarge-mnli), [DeBERTa-V2-XXLarge-MNLI](https://huggingface.co/microsoft/deberta-v2-xxlarge-mnli). The results of SST-2/QQP/QNLI/SQuADv2 will also be slightly improved when starting from MNLI fine-tuned models, however, we only report the numbers fine-tuned from pretrained base models for those 4 tasks.\n\n### Pre-training with MLM and RTD objectives\n\nTo pre-train DeBERTa with MLM and RTD objectives, please check [`experiments/language_models`](experiments/language_model)\n\n \n## Contacts\n\nPengcheng He(penhe@microsoft.com), Xiaodong Liu(xiaodl@microsoft.com), Jianfeng Gao(jfgao@microsoft.com), Weizhu Chen(wzchen@microsoft.com)\n\n# Citation\n``` latex\n@misc{he2021debertav3,\n      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, \n      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},\n      year={2021},\n      eprint={2111.09543},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n``` latex\n@inproceedings{\nhe2021deberta,\ntitle={DEBERTA: DECODING-ENHANCED BERT WITH DISENTANGLED ATTENTION},\nauthor={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},\nbooktitle={International Conference on Learning Representations},\nyear={2021},\nurl={https://openreview.net/forum?id=XPZIaotutsD}\n}\n```\n",
  "external_links_in_readme": [
    "https://huggingface.co/models?other=deberta-v3",
    "https://huggingface.co/microsoft/deberta-base-mnli",
    "https://huggingface.co/models?search=microsoft%2Fdeberta",
    "https://openreview.net/forum?id=sE7-XhLxHA",
    "https://hub.docker.com/r/bagai/deberta",
    "https://github.com/huggingface/transformers",
    "https://huggingface.co/microsoft/deberta-large",
    "https://www.microsoft.com/en-us/research/blog/microsoft-deberta-surpasses-human-performance-on-the-superglue-benchmark/",
    "https://huggingface.co/microsoft/deberta-xlarge-mnli",
    "https://huggingface.co/microsoft/deberta-v3-large",
    "https://github.com/google/sentencepiece",
    "https://huggingface.co/microsoft/deberta-base",
    "https://arxiv.org/abs/2006.03654",
    "https://huggingface.co/microsoft/deberta-v3-small",
    "https://openreview.net/forum?id=XPZIaotutsD}",
    "https://huggingface.co/microsoft/deberta-v2-xlarge",
    "https://deberta.readthedocs.io/en/latest/",
    "https://docs.docker.com/engine/install/ubuntu/",
    "https://huggingface.co/microsoft/deberta-v2-xxlarge-mnli",
    "https://huggingface.co/microsoft/deberta-xxlarge-v2",
    "https://huggingface.co/microsoft/deberta-v2-xxlarge",
    "https://huggingface.co/microsoft/deberta-v2-xlarge-mnli",
    "https://huggingface.co/microsoft/deberta-v3-base",
    "https://huggingface.co/microsoft/deberta-v3-xsmall",
    "https://huggingface.co/microsoft/mdeberta-v3-base",
    "https://huggingface.co/microsoft/deberta-xlarge",
    "https://arxiv.org/abs/2111.09543",
    "https://huggingface.co/microsoft/deberta-large-mnli"
  ]
}
```

</details>


---

