# GitHub Data for cardiffnlp_twitter-roberta-base-sentiment-latest

**Task Category:** Text Classification

## Repository 1: cardiffnlp/tweetnlp

# GitHub Repository Data

**Repository:** [cardiffnlp/tweetnlp](https://github.com/cardiffnlp/tweetnlp)

## Basic Information

- **Description:** TweetNLP for all the NLP enthusiasts working on Twitter! The Python library tweetnlp provides a collection of useful tools to analyze/understand tweets such as sentiment analysis, emoji prediction, and named entity recognition, powered by state-of-the-art language models specialised on Twitter.
- **Created:** 2022-05-11T21:06:32+00:00
- **Last Updated:** 2025-06-16T08:48:22+00:00
- **Last Pushed:** 2025-04-02T11:13:40+00:00
- **Default Branch:** main
- **Size:** 509 KB

## Statistics

- **Stars:** 350
- **Forks:** 31
- **Watchers:** 350
- **Open Issues:** 6
- **Total Issues:** 0
- **Pull Requests:** 8

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/cardiffnlp/tweetnlp/blob/main/LICENSE)

## Languages

- **Python:** 118,880 bytes

## Topics

- `computational-social-science`
- `language-model`
- `natural-language-processing`
- `python`
- `sentiment-analysis`
- `sns`
- `twitter`

## Top Contributors

1. **asahi417** - 145 contributions
2. **pedrada88** - 14 contributions
3. **ashyam95** - 1 contributions
4. **rmotsar** - 1 contributions
5. **antypasD** - 1 contributions

## File Structure (Sample of 10 files)

Total files: 49

- `.gitignore` (blob)
- `FINETUNING_RESULT.md` (blob)
- `LICENSE` (blob)
- `README.md` (blob)
- `assets` (tree)
- `assets/cardiff_nlp_logo.png` (blob)
- `assets/cardiff_nlp_logo_name.png` (blob)
- `setup.cfg` (blob)
- `setup.py` (blob)
- `tests` (tree)

## Recent Issues

- ðŸŸ¢ **#32** Tweetnlp and case sensitivity (open)
- ðŸŸ¢ **#31** In what languages does your model multilingual sentiment model support? (open)
- ðŸ”´ **#30** Colab error (closed)
- ðŸ”´ **#28** How does TweetNLP compare to Vader? (closed)
- ðŸŸ¢ **#27** Ignore case in regex in preprocessor (open)

## Recent Pull Requests

- ðŸ”´ **#26** Bump transformers version (closed)
- ðŸ”´ **#25** fix? setup.py (closed)
- ðŸ”´ **#16** Fix finetuning results table (closed)
- ðŸ”´ **#11** Add training (closed)
- ðŸ”´ **#10** Add training (closed)

## Recent Commits

- **eb99c8f1** Update trainer.py - Asahi Ushio (2025-04-02T11:13:40+00:00)
- **8f55e089** Update setup.py - Asahi Ushio (2025-04-02T11:11:40+00:00)
- **ae984389** Update setup.py - Asahi Ushio (2025-04-02T11:02:53+00:00)
- **68b08c83** Fix bold - Jose Camacho-Collados (2024-07-19T15:40:08+00:00)
- **f2da27fe** Add HF collection - Jose Camacho-Collados (2024-07-19T15:39:34+00:00)
- **1b3a6a1f** Update README.md - Asahi Ushio (2023-11-13T20:03:33+00:00)
- **9c245d56** Update hate speech README - Jose Camacho-Collados (2023-07-30T02:12:07+00:00)
- **1eb0c696** added workshop notebooks - antypasd (2023-06-25T16:21:18+00:00)
- **4543d7c1** Merge pull request #16 from rmotsar/main - Asahi Ushio (2023-06-20T18:46:15+00:00)
- **0b468df2** fix finetuning results table - Roman Motsar (2023-06-20T06:09:13+00:00)

## External Links Found in README

- https://img.shields.io/badge/License-MIT-brightgreen.svg
- https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp#scrollTo=WeREiLEjBlrj
- https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp#scrollTo=reZDePaBmYhA&line=4&uniqifier=1
- https://arxiv.org/abs/2210.03992
- https://badge.fury.io/py/tweetnlp.svg
- https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp#scrollTo=COOoZHVAFCIG&line=1&uniqifier=1
- https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion
- https://huggingface.co/tner/roberta-large-tweetner7-all
- https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-topic-single
- https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp#scrollTo=2plrPTqk7OHp
- https://arxiv.org/pdf/2010.12421.pdf
- https://huggingface.co/lmqg/t5-base-tweetqa-qag
- https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-topic-multi
- https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment
- https://huggingface.co/datasets/tweet_eval
- https://docs.ray.io/en/latest/tune/index.html
- https://arxiv.org/abs/2209.09824
- https://aclanthology.org/2022.emnlp-demos.5/
- https://github.com/cardiffnlp/tweetnlp/tree/add_training#model-fine-tuning
- https://huggingface.co/datasets/lmqg/qag_tweetqa

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 491272191,
  "name": "tweetnlp",
  "full_name": "cardiffnlp/tweetnlp",
  "description": "TweetNLP for all the NLP enthusiasts working on Twitter! The Python library tweetnlp provides a collection of useful tools to analyze/understand tweets such as sentiment analysis, emoji prediction, and named entity recognition, powered by state-of-the-art language models specialised on Twitter.",
  "html_url": "https://github.com/cardiffnlp/tweetnlp",
  "clone_url": "https://github.com/cardiffnlp/tweetnlp.git",
  "ssh_url": "git@github.com:cardiffnlp/tweetnlp.git",
  "homepage": "https://tweetnlp.org/",
  "topics": [
    "computational-social-science",
    "language-model",
    "natural-language-processing",
    "python",
    "sentiment-analysis",
    "sns",
    "twitter"
  ],
  "default_branch": "main",
  "created_at": "2022-05-11T21:06:32+00:00",
  "updated_at": "2025-06-16T08:48:22+00:00",
  "pushed_at": "2025-04-02T11:13:40+00:00",
  "size_kb": 509,
  "watchers_count": 350,
  "stargazers_count": 350,
  "forks_count": 31,
  "open_issues_count": 6,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/cardiffnlp/tweetnlp/blob/main/LICENSE"
  },
  "languages": {
    "Python": 118880
  },
  "top_contributors": [
    {
      "login": "asahi417",
      "contributions": 145
    },
    {
      "login": "pedrada88",
      "contributions": 14
    },
    {
      "login": "ashyam95",
      "contributions": 1
    },
    {
      "login": "rmotsar",
      "contributions": 1
    },
    {
      "login": "antypasD",
      "contributions": 1
    }
  ],
  "file_tree_count": 49,
  "file_tree_sample": [
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FINETUNING_RESULT.md",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "assets",
      "type": "tree"
    },
    {
      "path": "assets/cardiff_nlp_logo.png",
      "type": "blob"
    },
    {
      "path": "assets/cardiff_nlp_logo_name.png",
      "type": "blob"
    },
    {
      "path": "setup.cfg",
      "type": "blob"
    },
    {
      "path": "setup.py",
      "type": "blob"
    },
    {
      "path": "tests",
      "type": "tree"
    }
  ],
  "issues_count": 0,
  "pulls_count": 8,
  "recent_issues": [
    {
      "number": 32,
      "title": "Tweetnlp and case sensitivity",
      "state": "open"
    },
    {
      "number": 31,
      "title": "In what languages does your model multilingual sentiment model support?",
      "state": "open"
    },
    {
      "number": 30,
      "title": "Colab error",
      "state": "closed"
    },
    {
      "number": 28,
      "title": "How does TweetNLP compare to Vader?",
      "state": "closed"
    },
    {
      "number": 27,
      "title": "Ignore case in regex in preprocessor",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 26,
      "title": "Bump transformers version",
      "state": "closed"
    },
    {
      "number": 25,
      "title": "fix? setup.py",
      "state": "closed"
    },
    {
      "number": 16,
      "title": "Fix finetuning results table",
      "state": "closed"
    },
    {
      "number": 11,
      "title": "Add training",
      "state": "closed"
    },
    {
      "number": 10,
      "title": "Add training",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "eb99c8f18565b2177c17d1daf37ad0a3eda90218",
      "author": "Asahi Ushio",
      "date": "2025-04-02T11:13:40+00:00",
      "message": "Update trainer.py"
    },
    {
      "sha": "8f55e089654c6be36467d4990c0670e1303539fb",
      "author": "Asahi Ushio",
      "date": "2025-04-02T11:11:40+00:00",
      "message": "Update setup.py"
    },
    {
      "sha": "ae984389e30ca5def1536c64e88ffec8a0ecc392",
      "author": "Asahi Ushio",
      "date": "2025-04-02T11:02:53+00:00",
      "message": "Update setup.py"
    },
    {
      "sha": "68b08c83ed42df116248dc116e489de7d64e614e",
      "author": "Jose Camacho-Collados",
      "date": "2024-07-19T15:40:08+00:00",
      "message": "Fix bold"
    },
    {
      "sha": "f2da27feeb389ee47356a8cf2dc28cba61ded447",
      "author": "Jose Camacho-Collados",
      "date": "2024-07-19T15:39:34+00:00",
      "message": "Add HF collection"
    },
    {
      "sha": "1b3a6a1f6d5f4edae6fc8898653dfaad6432750c",
      "author": "Asahi Ushio",
      "date": "2023-11-13T20:03:33+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "9c245d561c339a3553b4c64752c495265ddc37c9",
      "author": "Jose Camacho-Collados",
      "date": "2023-07-30T02:12:07+00:00",
      "message": "Update hate speech README"
    },
    {
      "sha": "1eb0c696a2cde7eb6e22b05b789749b2aa6d6e9e",
      "author": "antypasd",
      "date": "2023-06-25T16:21:18+00:00",
      "message": "added workshop notebooks"
    },
    {
      "sha": "4543d7c19226ebc480d70c09818d0f306d9ea2c0",
      "author": "Asahi Ushio",
      "date": "2023-06-20T18:46:15+00:00",
      "message": "Merge pull request #16 from rmotsar/main"
    },
    {
      "sha": "0b468df259f86d87ee6af8d4c5f60301e395951b",
      "author": "Roman Motsar",
      "date": "2023-06-20T06:09:13+00:00",
      "message": "fix finetuning results table"
    },
    {
      "sha": "134a99c116e6574e6ddca1551664891327c61cd1",
      "author": "asahi417",
      "date": "2023-06-14T20:16:13+00:00",
      "message": "enable MPS for Apple Sillicon chips https://github.com/cardiffnlp/tweetnlp/issues/15"
    },
    {
      "sha": "189b68d32eef988cd6e17d0b2af86bee87648772",
      "author": "Jose Camacho-Collados",
      "date": "2023-05-28T05:17:38+00:00",
      "message": "Update news"
    },
    {
      "sha": "9003d460f276935a07ad20a8a9145913f2889a49",
      "author": "Jose Camacho-Collados",
      "date": "2023-05-28T05:14:13+00:00",
      "message": "Update hate and emotion descriptions, fix reference"
    },
    {
      "sha": "38b0ddb0d630ead20f3ae1fd9c1ad7d82bc69445",
      "author": "asahi417",
      "date": "2023-04-18T17:07:23+00:00",
      "message": "update emotion model"
    },
    {
      "sha": "2a215a81e9d081b5412bad2f22b20ec26a919093",
      "author": "asahi417",
      "date": "2023-04-18T16:55:16+00:00",
      "message": "update hate model"
    },
    {
      "sha": "870647abf228094699a111bb60e341e294887482",
      "author": "asahi417",
      "date": "2023-01-28T20:38:45+00:00",
      "message": "fix readme"
    },
    {
      "sha": "ed2f05cc88d3d3750b6fc815f77c1f7df40daa94",
      "author": "Asahi Ushio",
      "date": "2023-01-17T13:41:35+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "6ebbc0ef3d3e5daf086fb9b4aed310ea9aefd073",
      "author": "Jose Camacho-Collados",
      "date": "2022-12-15T14:59:41+00:00",
      "message": "Update README"
    },
    {
      "sha": "ceeae8485b2e995570f1580cf9bc75476df7ae54",
      "author": "Jose Camacho-Collados",
      "date": "2022-12-09T06:09:05+00:00",
      "message": "Fix output error QA"
    },
    {
      "sha": "d8168a782f263d37de829e37ccf233d81da0a624",
      "author": "asahi417",
      "date": "2022-12-04T19:42:27+00:00",
      "message": "add Question Answering and Question Generation"
    }
  ],
  "readme_text": "[![license](https://img.shields.io/badge/License-MIT-brightgreen.svg)](https://github.com/asahi417/tweetnlp/blob/master/LICENSE)\n[![PyPI version](https://badge.fury.io/py/tweetnlp.svg)](https://badge.fury.io/py/tweetnlp)\n[![PyPI pyversions](https://img.shields.io/pypi/pyversions/tweetnlp.svg)](https://pypi.python.org/pypi/tweetnlp/)\n[![PyPI status](https://img.shields.io/pypi/status/tweetnlp.svg)](https://pypi.python.org/pypi/tweetnlp/)\n\n# TweetNLP\nTweetNLP for all the NLP enthusiasts working on Twitter and social media! \nThe python library `tweetnlp` provides a collection of useful tools to analyze/understand tweets such as sentiment analysis,\nemoji prediction, and named-entity recognition, powered by state-of-the-art language modeling specialized on social media.\n\n***News (December 2022):*** We presented a TweetNLP demo paper (\"TweetNLP: Cutting-Edge Natural Language Processing for Social Media\"), at EMNLP 2022. The final version can be found [here](https://aclanthology.org/2022.emnlp-demos.5/).\n\n***TweetNLP Hugging Face page*** All the main TweetNLP models can be found [here on Hugging Face](https://huggingface.co/collections/cardiffnlp/tweetnlp-65e6f9ff5a0c4550ef7f1c70).\n\n\nResources:\n- Quick Tour with Colab Notebook: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp?usp=sharing)\n- Play with the TweetNLP Online Demo: [link](https://tweetnlp.org/demo/)\n- EMNLP 2022 paper: [link](https://arxiv.org/abs/2206.14774)\n- 2nd Cardiff NLP Summer Workshop Tutorial: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1whic817jQzdHl4wKI8ZzeLGtmT9Gmv1m?usp=sharing)\n- 2nd Cardiff NLP Summer Workshop Tutorial (solutions): [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1WVqt54dkZIBInzZTXZADNj7vCTnMXimx?usp=sharing)\n\n\n\nTable of Contents:\n1. [***Load Model & Dataset***](https://github.com/cardiffnlp/tweetnlp/tree/add_training#model--dataset)\n2. [***Fine-tune Model***](https://github.com/cardiffnlp/tweetnlp/tree/add_training#model-fine-tuning)\n\n## Get Started\n\nInstall TweetNLP via pip on your console. \n```shell\npip install tweetnlp\n```\n## Model & Dataset\n\nIn this section, you will learn how to get the models and datasets with `tweetnlp`.\nThe models follow [huggingface model](https://huggingface.co/) and the datasets are in the format of [huggingface datasets](https://huggingface.co/docs/datasets/load_hub).\nEasy introductions of huggingface models and datasets should be found at [huggingface webpage](https://huggingface.co/), so\nplease check them if you are new to huggingface.\n\n### Tweet Classification\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp#scrollTo=KAZYjeskBqL4)\n\nThe classification module consists of six different tasks (Topic Classification, Sentiment Analysis, Irony Detection, Hate Speech Detection, Offensive Language Detection, Emoji Prediction, and Emotion Analysis).\nIn each example, the model is instantiated by `tweetnlp.load_model(\"task-name\")`, and run the prediction by passing a text or a list of texts as argument to the corresponding function.\n\n- ***Topic Classification***: The aim of this task is, given a tweet to assign topics related to its content. The task is formed as a supervised multi-label classification problem where each tweet is assigned one or more topics from a total of 19 available topics. The topics were carefully curated based on Twitter trends with the aim to be broad and general and consist of classes such as: arts and culture, music, or sports. Our internally-annotated dataset contains over 10K manually-labeled tweets (check the paper [here](https://arxiv.org/abs/2209.09824), or the [huggingface dataset page](https://huggingface.co/datasets/cardiffnlp/tweet_topic_single)).\n\n```python\nimport tweetnlp\n\n# MULTI-LABEL MODEL \nmodel = tweetnlp.load_model('topic_classification')  # Or `model = tweetnlp.TopicClassification()`\nmodel.topic(\"Jacob Collier is a Grammy-awarded English artist from London.\")  # Or `model.predict`\n>>> {'label': ['celebrity_&_pop_culture', 'music']}\n# Note: the probability of the multi-label model is the output of sigmoid function on binary prediction whether each topic is positive or negative.\nmodel.topic(\"Jacob Collier is a Grammy-awarded English artist from London.\", return_probability=True)\n>>> {'label': ['celebrity_&_pop_culture', 'music'],\n 'probability': {'arts_&_culture': 0.037371691316366196,\n  'business_&_entrepreneurs': 0.010188567452132702,\n  'celebrity_&_pop_culture': 0.92448890209198,\n  'diaries_&_daily_life': 0.03425711765885353,\n  'family': 0.00796138122677803,\n  'fashion_&_style': 0.020642118528485298,\n  'film_tv_&_video': 0.08062587678432465,\n  'fitness_&_health': 0.006343095097690821,\n  'food_&_dining': 0.0042883665300905704,\n  'gaming': 0.004327300935983658,\n  'learning_&_educational': 0.010652057826519012,\n  'music': 0.8291937112808228,\n  'news_&_social_concern': 0.24688217043876648,\n  'other_hobbies': 0.020671198144555092,\n  'relationships': 0.020371075719594955,\n  'science_&_technology': 0.0170074962079525,\n  'sports': 0.014291072264313698,\n  'travel_&_adventure': 0.010423899628221989,\n  'youth_&_student_life': 0.008605164475739002}}\n\n# SINGLE-LABEL MODEL\nmodel = tweetnlp.load_model('topic_classification', multi_label=False)  # Or `model = tweetnlp.TopicClassification(multi_label=False)`\nmodel.topic(\"Jacob Collier is a Grammy-awarded English artist from London.\")\n>>> {'label': 'pop_culture'}\n# NOTE: the probability of the sinlge-label model the softmax over the label.\nmodel.topic(\"Jacob Collier is a Grammy-awarded English artist from London.\", return_probability=True)\n>>> {'label': 'pop_culture',\n 'probability': {'arts_&_culture': 9.20625461731106e-05,\n  'business_&_entrepreneurs': 6.916998972883448e-05,\n  'pop_culture': 0.9995898604393005,\n  'daily_life': 0.00011083036952186376,\n  'sports_&_gaming': 8.668467489769682e-05,\n  'science_&_technology': 5.152115045348182e-05}}\n\n# GET DATASET\ndataset_multi_label, label2id_multi_label = tweetnlp.load_dataset('topic_classification')\ndataset_single_label, label2id_single_label = tweetnlp.load_dataset('topic_classification', multi_label=False)\n```\n\n\n- ***Sentiment Analysis***: The sentiment analysis task integrated in TweetNLP is a simplified version where the goal is to predict the sentiment of a tweet with one of the three following labels: positive, neutral or negative. The base dataset for English is the unified TweetEval version of the Semeval-2017 dataset from the task on Sentiment Analysis in Twitter (check the paper [here](https://arxiv.org/pdf/2010.12421.pdf)).\n\n```python\nimport tweetnlp\n\n# ENGLISH MODEL\nmodel = tweetnlp.load_model('sentiment')  # Or `model = tweetnlp.Sentiment()` \nmodel.sentiment(\"Yes, including Medicare and social security saving\ud83d\udc4d\")  # Or `model.predict`\n>>> {'label': 'positive'}\nmodel.sentiment(\"Yes, including Medicare and social security saving\ud83d\udc4d\", return_probability=True)\n>>> {'label': 'positive', 'probability': {'negative': 0.004584966693073511, 'neutral': 0.19360853731632233, 'positive': 0.8018065094947815}}\n\n# MULTILINGUAL MODEL\nmodel = tweetnlp.load_model('sentiment', multilingual=True)  # Or `model = tweetnlp.Sentiment(multilingual=True)` \nmodel.sentiment(\"\u5929\u6c17\u304c\u826f\u3044\u3068\u3084\u3063\u3071\u308a\u6c17\u6301\u3061\u826f\u3044\u306a\u3042\u2728\")\n>>> {'label': 'positive'}\nmodel.sentiment(\"\u5929\u6c17\u304c\u826f\u3044\u3068\u3084\u3063\u3071\u308a\u6c17\u6301\u3061\u826f\u3044\u306a\u3042\u2728\", return_probability=True)\n>>> {'label': 'positive', 'probability': {'negative': 0.028369612991809845, 'neutral': 0.08128828555345535, 'positive': 0.8903420567512512}}\n\n# GET DATASET (ENGLISH)\ndataset, label2id = tweetnlp.load_dataset('sentiment')\n# GET DATASET (MULTILINGUAL)\nfor l in ['all', 'arabic', 'english', 'french', 'german', 'hindi', 'italian', 'portuguese', 'spanish']:\n    dataset_multilingual, label2id_multilingual = tweetnlp.load_dataset('sentiment', multilingual=True, task_language=l)\n```\n\n- ***Irony Detection***: This is a binary classification task where given a tweet, the goal is to detect whether it is ironic or not. It is based on the Irony Detection dataset from the SemEval 2018 task (check the paper [here](https://arxiv.org/pdf/2010.12421.pdf)).\n\n```python\nimport tweetnlp\n\n# MODEL\nmodel = tweetnlp.load_model('irony')  # Or `model = tweetnlp.Irony()` \nmodel.irony('If you wanna look like a badass, have drama on social media')  # Or `model.predict`\n>>> {'label': 'irony'}\nmodel.irony('If you wanna look like a badass, have drama on social media', return_probability=True)\n>>> {'label': 'irony', 'probability': {'non_irony': 0.08390884101390839, 'irony': 0.9160911440849304}} \n\n# GET DATASET\ndataset, label2id = tweetnlp.load_dataset('irony')\n```\n\n- ***Hate Speech Detection***: The hate speech detection task consists of detecting whether a tweet is hateful towards a target community. The underlying model is based on a suite of unified hate speech detection datasets (see [reference paper](https://aclanthology.org/2023.woah-1.25/)).\n\n```python\nimport tweetnlp\n\n# MODEL\nmodel = tweetnlp.load_model('hate')  # Or `model = tweetnlp.Hate()` \nmodel.hate('Whoever just unfollowed me you a bitch')  # Or `model.predict`\n>>> {'label': 'not-hate'}\nmodel.hate('Whoever just unfollowed me you a bitch', return_probability=True)\n>>> {'label': 'non-hate', 'probability': {'non-hate': 0.7263831496238708, 'hate': 0.27361682057380676}}\n\n# GET DATASET\ndataset, label2id = tweetnlp.load_dataset('hate')\n```\n\n- ***Offensive Language Identification***: This task consists in identifying whether some form of offensive language is present in a tweet. For our benchmark we rely on the SemEval2019 OffensEval dataset (check the paper [here](https://arxiv.org/pdf/2010.12421.pdf)).\n\n```python\nimport tweetnlp\n\n# MODEL\nmodel = tweetnlp.load_model('offensive')  # Or `model = tweetnlp.Offensive()` \nmodel.offensive(\"All two of them taste like ass.\")  # Or `model.predict`\n>>> {'label': 'offensive'}\nmodel.offensive(\"All two of them taste like ass.\", return_probability=True)\n>>> {'label': 'offensive', 'probability': {'non-offensive': 0.16420328617095947, 'offensive': 0.8357967734336853}}\n\n# GET DATASET\ndataset, label2id = tweetnlp.load_dataset('offensive')\n```\n\n- ***Emoji Prediction***: The goal of emoji prediction is to predict the final emoji on a given tweet. The dataset used to fine-tune our models is the TweetEval adaptation from the SemEval 2018 task on Emoji Prediction (check the paper [here](https://arxiv.org/pdf/2010.12421.pdf)), including 20 emoji as labels (\u2764, \ud83d\ude0d, \ud83d\ude02, \ud83d\udc95, \ud83d\udd25, \ud83d\ude0a, \ud83d\ude0e, \u2728, \ud83d\udc99, \ud83d\ude18, \ud83d\udcf7, \ud83c\uddfa\ud83c\uddf8, \u2600, \ud83d\udc9c, \ud83d\ude09, \ud83d\udcaf, \ud83d\ude01, \ud83c\udf84, \ud83d\udcf8, \ud83d\ude1c).\t\n\n```python\nimport tweetnlp\n\n# MODEL\nmodel = tweetnlp.load_model('emoji')  # Or `model = tweetnlp.Emoji()` \nmodel.emoji('Beautiful sunset last night from the pontoon @TupperLakeNY')  # Or `model.predict`\n>>> {'label': '\ud83d\ude0a'}\nmodel.emoji('Beautiful sunset last night from the pontoon @TupperLakeNY', return_probability=True)\n>>> {'label': '\ud83d\udcf7',\n 'probability': {'\u2764': 0.13197319209575653,\n  '\ud83d\ude0d': 0.11246423423290253,\n  '\ud83d\ude02': 0.008415069431066513,\n  '\ud83d\udc95': 0.04842926934361458,\n  '\ud83d\udd25': 0.014528146013617516,\n  '\ud83d\ude0a': 0.1509675830602646,\n  '\ud83d\ude0e': 0.08625403046607971,\n  '\u2728': 0.01616635173559189,\n  '\ud83d\udc99': 0.07396604865789413,\n  '\ud83d\ude18': 0.03033279813826084,\n  '\ud83d\udcf7': 0.16525287926197052,\n  '\ud83c\uddfa\ud83c\uddf8': 0.020336611196398735,\n  '\u2600': 0.00799981877207756,\n  '\ud83d\udc9c': 0.016111424192786217,\n  '\ud83d\ude09': 0.012984540313482285,\n  '\ud83d\udcaf': 0.012557178735733032,\n  '\ud83d\ude01': 0.031386848539114,\n  '\ud83c\udf84': 0.006829539313912392,\n  '\ud83d\udcf8': 0.04188741743564606,\n  '\ud83d\ude1c': 0.011156936176121235}}\n\n# GET DATASET\ndataset, label2id = tweetnlp.load_dataset('emoji')\n```\n\n- ***Emotion Recognition***: Given a tweet, this task consists of associating it with its most appropriate emotion. As a reference dataset we use the SemEval 2018 task on Affect in Tweets (check the paper [here](https://arxiv.org/pdf/2010.12421.pdf)). The latest multi-label model includes eleven emotion types.\n\n```python\nimport tweetnlp\n\n# MULTI-LABEL MODEL \nmodel = tweetnlp.load_model('emotion')  # Or `model = tweetnlp.Emotion()` \nmodel.emotion('I love swimming for the same reason I love meditating...the feeling of weightlessness.')  # Or `model.predict`\n>>> {'label': 'joy'}\n# Note: the probability of the multi-label model is the output of sigmoid function on binary prediction whether each topic is positive or negative.\nmodel.emotion('I love swimming for the same reason I love meditating...the feeling of weightlessness.', return_probability=True)\n>>> {'label': 'joy',\n 'probability': {'anger': 0.00025800734874792397,\n  'anticipation': 0.0005329723935574293,\n  'disgust': 0.00026112011983059347,\n  'fear': 0.00027552215033210814,\n  'joy': 0.7721399068832397,\n  'love': 0.1806265264749527,\n  'optimism': 0.04208092764019966,\n  'pessimism': 0.00025325192837044597,\n  'sadness': 0.0006160663324408233,\n  'surprise': 0.0005619609728455544,\n  'trust': 0.002393839880824089}}\n\n# SINGLE-LABEL MODEL\nmodel = tweetnlp.load_model('emotion')  # Or `model = tweetnlp.Emotion()` \nmodel.emotion('I love swimming for the same reason I love meditating...the feeling of weightlessness.')  # Or `model.predict`\n>>> {'label': 'joy'}\n# NOTE: the probability of the sinlge-label model the softmax over the label.\nmodel.emotion('I love swimming for the same reason I love meditating...the feeling of weightlessness.', return_probability=True)\n>>> {'label': 'optimism', 'probability': {'joy': 0.01367587223649025, 'optimism': 0.7345258593559265, 'anger': 0.1770714670419693, 'sadness': 0.07472680509090424}}\n\n# GET DATASET\ndataset, label2id = tweetnlp.load_dataset('emotion')\n```\n\nWARNING: The single-label and multi-label emotion model have diiferent label set (single-label has four classes of 'joy'/'optimism'/'anger'/'sadness', \nwhile multi-label has eleven classes of 'joy'/'optimism'/'anger'/'sadness'/'love'/'trust'/'fear'/'surprise'/'anticipation'/'disgust'/'pessimism').\n\n### Named Entity Recognition\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp#scrollTo=WeREiLEjBlrj)\n\nThis module consists of a named-entity recognition (NER) model specifically trained for tweets. The model is instantiated by `tweetnlp.load_model(\"ner\")`, and runs the prediction by giving a text or a list of texts as argument to the `ner` function (check the paper [here](https://arxiv.org/abs/2210.03797), or the [huggingface dataset page](https://huggingface.co/datasets/tner/tweetner7)). \n\n```python3\nimport tweetnlp\n\n# MODEL\nmodel = tweetnlp.load_model('ner')  # Or `model = tweetnlp.NER()` \nmodel.ner('Jacob Collier is a Grammy-awarded English artist from London.')  # Or `model.predict`\n>>> [{'type': 'person', 'entity': 'Jacob Collier'}, {'type': 'event', 'entity': ' Grammy'}, {'type': 'location', 'entity': ' London'}]\n# Note: the probability for the predicted entity is the mean of the probabilities over the sub-tokens representing the entity. \nmodel.ner('Jacob Collier is a Grammy-awarded English artist from London.', return_probability=True)  # Or `model.predict`\n>>> [\n  {'type': 'person', 'entity': 'Jacob Collier', 'probability': 0.9905318220456442},\n  {'type': 'event', 'entity': ' Grammy', 'probability': 0.19164378941059113},\n  {'type': 'location', 'entity': ' London', 'probability': 0.9607000350952148}\n]\n\n# GET DATASET\ndataset, label2id = tweetnlp.load_dataset('ner')\n```\n\n### Question Answering\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp#scrollTo=reZDePaBmYhA&line=4&uniqifier=1)\n\nThis module consists of a question answering model specifically trained for tweets.\nThe model is instantiated by `tweetnlp.load_model(\"question_answering\")`, \nand runs the prediction by giving a question or a list of questions along with a context or a list of contexts\nas argument to the `question_answering` function (check the paper [here](https://arxiv.org/abs/2210.03992), or the [huggingface dataset page](https://huggingface.co/datasets/lmqg/qg_tweetqa)). \n\n```python3\nimport tweetnlp\n\n# MODEL\nmodel = tweetnlp.load_model('question_answering')  # Or `model = tweetnlp.QuestionAnswering()` \nmodel.question_answering(\n  question='who created the post as we know it today?',\n  context=\"'So much of The Post is Ben,' Mrs. Graham said in 1994, three years after Bradlee retired as editor. 'He created it as we know it today.'\u2014 Ed O'Keefe (@edatpost) October 21, 2014\"\n)  # Or `model.predict`\n>>> {'generated_text': 'ben'}\n\n# GET DATASET\ndataset = tweetnlp.load_dataset('question_answering')\n```\n\n### Question Answer Generation\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp#scrollTo=uqd7sBHhnwym&line=6&uniqifier=1)\n\nThis module consists of a question & answer pair generation specifically trained for tweets.\nThe model is instantiated by `tweetnlp.load_model(\"question_answer_generation\")`, \nand runs the prediction by giving a context or a list of contexts\nas argument to the `question_answer_generation` function (check the paper [here](https://arxiv.org/abs/2210.03992), or the [huggingface dataset page](https://huggingface.co/datasets/lmqg/qag_tweetqa)). \n\n```python3\nimport tweetnlp\n\n# MODEL\nmodel = tweetnlp.load_model('question_answer_generation')  # Or `model = tweetnlp.QuestionAnswerGeneration()` \nmodel.question_answer_generation(\n  text=\"'So much of The Post is Ben,' Mrs. Graham said in 1994, three years after Bradlee retired as editor. 'He created it as we know it today.'\u2014 Ed O'Keefe (@edatpost) October 21, 2014\"\n)  # Or `model.predict`\n>>> [\n    {'question': 'who created the post?', 'answer': 'ben'},\n    {'question': 'what did ben do in 1994?', 'answer': 'he retired as editor'}\n]\n\n# GET DATASET\ndataset = tweetnlp.load_dataset('question_answer_generation')\n```\n\n### Language Modeling\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp#scrollTo=COOoZHVAFCIG&line=1&uniqifier=1)\n\nThe masked language model predicts the masked token in the given sentence. This is instantiated by `tweetnlp.load_model('language_model')`, and runs the prediction by giving a text or a list of texts as argument to the `mask_prediction` function. Please make sure that each text has a `<mask>` token, since that is eventually the following by the objective of the model to predict.\n\n```python\nimport tweetnlp\nmodel = tweetnlp.load_model('language_model')  # Or `model = tweetnlp.LanguageModel()` \nmodel.mask_prediction(\"How many more <mask> until opening day? \ud83d\ude29\", best_n=2)  # Or `model.predict`\n>>> {'best_tokens': ['days', 'hours'],\n 'best_scores': [5.498564104033932e-11, 4.906026140893971e-10],\n 'best_sentences': ['How many more days until opening day? \ud83d\ude29',\n  'How many more hours until opening day? \ud83d\ude29']}\n```\n\n### Tweet Embedding\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp#scrollTo=MUT31bNQYTNz)\n\nThe tweet embedding model produces a fixed length embedding for a tweet. The embedding represents the semantics by meaning of the tweet, and this can be used for semantic search of tweets by using the similarity between the embeddings. Model is instantiated by `tweet_nlp.load_model('sentence_embedding')`, and run the prediction by passing a text or a list of texts as argument to the `embedding` function.\n\n- ***Get Embedding***\n\n```python\nimport tweetnlp\nmodel = tweetnlp.load_model('sentence_embedding')  # Or `model = tweetnlp.SentenceEmbedding()` \n\n# Get sentence embedding\ntweet = \"I will never understand the decision making of the people of Alabama. Their new Senator is a definite downgrade. You have served with honor.  Well done.\"\nvectors = model.embedding(tweet)\nvectors.shape\n>>> (768,)\n\n# Get sentence embedding (multiple inputs)\ntweet_corpus = [\n    \"Free, fair elections are the lifeblood of our democracy. Charges of unfairness are serious. But calling an election unfair does not make it so. Charges require specific allegations and then proof. We have neither here.\",\n    \"Trump appointed judge Stephanos Bibas \",\n    \"If your members can go to Puerto Rico they can get their asses back in the classroom. @CTULocal1\",\n    \"@PolitiBunny @CTULocal1 Political leverage, science said schools could reopen, teachers and unions protested to keep'em closed and made demands for higher wages and benefits, they're usin Covid as a crutch at the expense of life and education.\",\n    \"Congratulations to all the exporters on achieving record exports in Dec 2020 with a growth of 18 % over the previous year. Well done &amp; keep up this trend. A major pillar of our govt's economic policy is export enhancement &amp; we will provide full support to promote export culture.\",\n    \"@ImranKhanPTI Pakistan seems a worst country in term of exporting facilities. I am a small business man and if I have to export a t-shirt having worth of $5 to USA or Europe. Postal cost will be around $30. How can we grow as an exporting country if this situation prevails. Think about it. #PM\",\n    \"The thing that doesn\u2019t sit right with me about \u201cnothing good happened in 2020\u201d is that it ignores the largest protest movement in our history. The beautiful, powerful Black Lives Matter uprising reached every corner of the country and should be central to our look back at 2020.\",\n    \"@JoshuaPotash I kinda said that in the 2020 look back for @washingtonpost\",\n    \"Is this a confirmation from Q that Lin is leaking declassified intelligence to the public? I believe so. If @realDonaldTrump didn\u2019t approve of what @LLinWood is doing he would have let us know a lonnnnnng time ago. I\u2019ve always wondered why Lin\u2019s Twitter handle started with \u201cLLin\u201d https://t.co/0G7zClOmi2\",\n    \"@ice_qued @realDonaldTrump @LLinWood Yeah 100%\",\n    \"Tomorrow is my last day as Senator from Alabama.  I believe our opportunities are boundless when we find common ground. As we swear in a new Congress &amp; a new President, demand from them that they do just that &amp; build a stronger, more just society.  It\u2019s been an honor to serve you.\" \n    \"The mask cult can\u2019t ever admit masks don\u2019t work because their ideology is based on feeling like a \u201cgood person\u201d  Wearing a mask makes them a \u201cgood person\u201d &amp; anyone who disagrees w/them isn\u2019t  They can\u2019t tolerate any idea that makes them feel like their self-importance is unearned\",\n    \"@ianmSC Beyond that, they put such huge confidence in masks so early with no strong evidence that they have any meaningful benefit, they don\u2019t want to backtrack or admit they were wrong. They put the cart before the horse, now desperate to find any results that match their hypothesis.\",\n]\nvectors = model.embedding(tweet_corpus, batch_size=4)\nvectors.shape\n>>> (12, 768)\n```\n\n- ***Similarity Search***\n\n```python\nsims = []\nfor n, i in enumerate(tweet_corpus):\n  _sim = model.similarity(tweet, i)\n  sims.append([n, _sim])\nprint(f'anchor tweet: {tweet}\\n')\nfor m, (n, s) in enumerate(sorted(sims, key=lambda x: x[1], reverse=True)[:3]):\n  print(f' - top {m}: {tweet_corpus[n]}\\n - similaty: {s}\\n')\n\n>>> anchor tweet: I will never understand the decision making of the people of Alabama. Their new Senator is a definite downgrade. You have served with honor.  Well done.\n\n - top 0: Tomorrow is my last day as Senator from Alabama.  I believe our opportunities are boundless when we find common ground. As we swear in a new Congress &amp; a new President, demand from them that they do just that &amp; build a stronger, more just society.  It\u2019s been an honor to serve you.The mask cult can\u2019t ever admit masks don\u2019t work because their ideology is based on feeling like a \u201cgood person\u201d  Wearing a mask makes them a \u201cgood person\u201d &amp; anyone who disagrees w/them isn\u2019t  They can\u2019t tolerate any idea that makes them feel like their self-importance is unearned\n - similaty: 0.7480925982953287\n\n - top 1: Trump appointed judge Stephanos Bibas \n - similaty: 0.6289173306344258\n\n - top 2: Free, fair elections are the lifeblood of our democracy. Charges of unfairness are serious. But calling an election unfair does not make it so. Charges require specific allegations and then proof. We have neither here.\n - similaty: 0.6017154109745276\n```\n\n### Resources & Custom Model Loading \n\nHere is a table of the default model used in each task. \n\n| Task                              | Model                                                                                                                                                   | Dataset |\n|-----------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------|\n|Topic Classification (single-label)| [cardiffnlp/twitter-roberta-base-dec2021-tweet-topic-single-all](https://huggingface.co/cardiffnlp/twitter-roberta-base-dec2021-tweet-topic-single-all) | [cardiffnlp/tweet_topic_single](https://huggingface.co/datasets/cardiffnlp/tweet_topic_single) |\n|Topic Classification (multi-label) | [cardiffnlp/twitter-roberta-base-dec2021-tweet-topic-multi-all](https://huggingface.co/cardiffnlp/twitter-roberta-base-dec2021-tweet-topic-multi-all)   | [cardiffnlp/tweet_topic_multi](https://huggingface.co/datasets/cardiffnlp/tweet_topic_multi) |\n|Sentiment Analysis (Multilingual)  | [cardiffnlp/twitter-xlm-roberta-base-sentiment](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment)                                   | [cardiffnlp/tweet_sentiment_multilingual](https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual) |\n|Sentiment Analysis                 | [cardiffnlp/twitter-roberta-base-sentiment-latest](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest)                             | [tweet_eval](https://huggingface.co/datasets/tweet_eval) |\n|Irony Detection                    | [cardiffnlp/twitter-roberta-base-irony](https://huggingface.co/cardiffnlp/twitter-roberta-base-irony)                                                   | [tweet_eval](https://huggingface.co/datasets/tweet_eval) |\n|Hate Detection                     | [cardiffnlp/twitter-roberta-base-hate-latest](https://huggingface.co/cardiffnlp/twitter-roberta-base-hate-latest)                                       | [tweet_eval](https://huggingface.co/datasets/tweet_eval) |\n|Offensive Detection                | [cardiffnlp/twitter-roberta-base-offensive](https://huggingface.co/cardiffnlp/twitter-roberta-base-offensive)                                           | [tweet_eval](https://huggingface.co/datasets/tweet_eval) |\n|Emoji Prediction                   | [cardiffnlp/twitter-roberta-base-emoji](https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji)                                                   | [tweet_eval](https://huggingface.co/datasets/tweet_eval) |\n|Emotion Analysis (single-label)    | [cardiffnlp/twitter-roberta-base-emotion](https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion)                                               | [tweet_eval](https://huggingface.co/datasets/tweet_eval) |\n|Emotion Analysis (multi-label)     | [cardiffnlp/twitter-roberta-base-emotion-multilabel-latest](https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion-multilabel-latest)           | TBA |\n|Named Entity Recognition           | [tner/roberta-large-tweetner7-all](https://huggingface.co/tner/roberta-large-tweetner7-all)                                                             | [tner/tweetner7](https://huggingface.co/datasets/tner/tweetner7) |\n|Question Answering                 | [lmqg/t5-small-tweetqa-qa](https://huggingface.co/lmqg/t5-small-tweetqa-qa)                                                                             | [lmqg/qg_tweetqa](https://huggingface.co/datasets/lmqg/qg_tweetqa) |\n|Question Answer Generation         | [lmqg/t5-base-tweetqa-qag](https://huggingface.co/lmqg/t5-base-tweetqa-qag)                                                                             | [lmqg/qag_tweetqa](https://huggingface.co/datasets/lmqg/qag_tweetqa) |\n|Language Modeling                  | [cardiffnlp/twitter-roberta-base-2021-124m](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m)                                           | TBA |\n|Tweet Embedding                    | [cambridgeltl/tweet-roberta-base-embeddings-v1](https://huggingface.co/cambridgeltl/tweet-roberta-base-embeddings-v1)                                   | TBA |\n\n\nTo use an other model from local/huggingface modelhub, one can simply provide the model path/alias to the `load_model` function.\nBelow is an example to load a model for NER.\n\n```python\nimport tweetnlp\ntweetnlp.load_model('ner', model_name='tner/twitter-roberta-base-2019-90m-tweetner7-continuous')\n```\n\n## Model Fine-tuning\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp#scrollTo=2plrPTqk7OHp)\n\nTweetNLP provides an easy interface to fine-tune language models on the datasets supported by HuggingFace for model hosting/fine-tuning with [RAY TUNE](https://docs.ray.io/en/latest/tune/index.html) for parameter search.\n\n- Supported Tasks: `sentiment`, `offensive`, `irony`, `hate`, `emotion`, `topic_classification`\n\nThe results of experiments with `tweetnlp`'s trainer can be found in the following table. Results are competitive and can be used as baselines for each task.\nSee [the leaderboard page](https://github.com/cardiffnlp/tweetnlp/blob/main/FINETUNING_RESULT.md) to know more about the results.\n\n| task      | language_model                                                                                                |   eval_f1 |   eval_f1_macro |   eval_accuracy | link                                                                                                                              |\n|:----------|:--------------------------------------------------------------------------------------------------------------|----------:|----------------:|----------------:|:----------------------------------------------------------------------------------------------------------------------------------|\n| emoji     | [cardiffnlp/twitter-roberta-base-2021-124m](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m) |      0.46 |            0.35 |            0.46 | [cardiffnlp/twitter-roberta-base-2021-124m-emoji](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-emoji)         |\n| emotion   | [cardiffnlp/twitter-roberta-base-2021-124m](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m) |      0.83 |            0.79 |            0.83 | [cardiffnlp/twitter-roberta-base-2021-124m-emotion](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-emotion)     |\n| hate      | [cardiffnlp/twitter-roberta-base-2021-124m](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m) |      0.56 |            0.53 |            0.56 | [cardiffnlp/twitter-roberta-base-2021-124m-hate](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-hate)           |\n| irony     | [cardiffnlp/twitter-roberta-base-2021-124m](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m) |      0.79 |            0.78 |            0.79 | [cardiffnlp/twitter-roberta-base-2021-124m-irony](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-irony)         |\n| offensive | [cardiffnlp/twitter-roberta-base-2021-124m](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m) |      0.86 |            0.82 |            0.86 | [cardiffnlp/twitter-roberta-base-2021-124m-offensive](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-offensive) |\n| sentiment | [cardiffnlp/twitter-roberta-base-2021-124m](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m) |      0.71 |            0.72 |            0.71 | [cardiffnlp/twitter-roberta-base-2021-124m-sentiment](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-sentiment) |\n| topic_classification (single) | [cardiffnlp/twitter-roberta-base-2021-124m](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m) |      0.9  |  0.8  |            0.9  | [cardiffnlp/twitter-roberta-base-2021-124m-topic-single](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-topic-single)                 |                                                                                                                               \n| topic_classification (multi)  | [cardiffnlp/twitter-roberta-base-2021-124m](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m) |      0.75 |            0.56 |            0.54 | [cardiffnlp/twitter-roberta-base-2021-124m-topic-multi](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-topic-multi)                   |\n| sentiment (multilingual)      | [cardiffnlp/twitter-xlm-roberta-base](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base)             |      0.69 |  0.69 |            0.69 | [cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual](https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual)         |                                                                                                                               \n\n\n### Example \nThe following example will reproduce our irony model [cardiffnlp/twitter-roberta-base-2021-124m-irony](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-irony).\n\n```python\nimport logging\nimport tweetnlp\n\nlogging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s', level=logging.INFO, datefmt='%Y-%m-%d %H:%M:%S')\n\n# load dataset\ndataset, label_to_id = tweetnlp.load_dataset(\"irony\")\n# load trainer class\ntrainer_class = tweetnlp.load_trainer(\"irony\")\n# setup trainer\ntrainer = trainer_class(\n    language_model='cardiffnlp/twitter-roberta-base-2021-124m',  # language model to fine-tune\n    dataset=dataset,\n    label_to_id=label_to_id,\n    max_length=128,\n    split_test='test',\n    split_train='train',\n    split_validation='validation',\n    output_dir='model_ckpt/irony' \n)\n# start model fine-tuning with parameter optimization\ntrainer.train(\n  eval_step=50,  # each `eval_step`, models are validated on the validation set \n  n_trials=10,  # number of trial at parameter optimization\n  search_range_lr=[1e-6, 1e-4],  # define the search space for learning rate (min and max value)\n  search_range_epoch=[1, 6],  # define the search space for epoch (min and max value)\n  search_list_batch=[4, 8, 16, 32, 64]  # define the search space for batch size (list of integer to test) \n)\n# evaluate model on the test set\ntrainer.evaluate()\n>>> {\n  \"eval_loss\": 1.3228046894073486,\n  \"eval_f1\": 0.7959183673469388,\n  \"eval_f1_macro\": 0.791350632069195,\n  \"eval_accuracy\": 0.7959183673469388,\n  \"eval_runtime\": 2.2267,\n  \"eval_samples_per_second\": 352.084,\n  \"eval_steps_per_second\": 44.01\n}\n# save model locally (saved at `{output_dir}/best_model` as default)\ntrainer.save_model()\n# run prediction\ntrainer.predict('If you wanna look like a badass, have drama on social media')\n>>> {'label': 'irony'}\n# push your model on huggingface hub\ntrainer.push_to_hub(hf_organization='cardiffnlp', model_alias='twitter-roberta-base-2021-124m-irony')\n```\nThe saved checkpoint can be loaded as a custom model as below.\n```python\nimport tweetnlp\nmodel = tweetnlp.load_model('irony', model_name=\"model_ckpt/irony/best_model\")\n```\nIf `split_validation` is not given, trainer will do a single run with default parameters without parameter search.\n\n## Reference Paper\n\nFor more details, please read the accompanying [TweetNLP's reference paper](https://arxiv.org/pdf/2206.14774.pdf). If you use TweetNLP in your research, please use the following `bib` entry to cite the reference paper:\n\n```\n@inproceedings{camacho-collados-etal-2022-tweetnlp,\n    title={{T}weet{NLP}: {C}utting-{E}dge {N}atural {L}anguage {P}rocessing for {S}ocial {M}edia},\n    author={Camacho-Collados, Jose and Rezaee, Kiamehr and Riahi, Talayeh and Ushio, Asahi and Loureiro, Daniel and Antypas, Dimosthenis and Boisson, Joanne and Espinosa-Anke, Luis and Liu, Fangyu and Mart{\\'\\i}nez-C{\\'a}mara, Eugenio and others},\n    booktitle = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = nov,\n    year = \"2022\",\n    address = \"Abu Dhabi, U.A.E.\",\n    publisher = \"Association for Computational Linguistics\",\n}\n```\n",
  "external_links_in_readme": [
    "https://img.shields.io/badge/License-MIT-brightgreen.svg",
    "https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp#scrollTo=WeREiLEjBlrj",
    "https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp#scrollTo=reZDePaBmYhA&line=4&uniqifier=1",
    "https://arxiv.org/abs/2210.03992",
    "https://badge.fury.io/py/tweetnlp.svg",
    "https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp#scrollTo=COOoZHVAFCIG&line=1&uniqifier=1",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion",
    "https://huggingface.co/tner/roberta-large-tweetner7-all",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-topic-single",
    "https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp#scrollTo=2plrPTqk7OHp",
    "https://arxiv.org/pdf/2010.12421.pdf",
    "https://huggingface.co/lmqg/t5-base-tweetqa-qag",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-topic-multi",
    "https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment",
    "https://huggingface.co/datasets/tweet_eval",
    "https://docs.ray.io/en/latest/tune/index.html",
    "https://arxiv.org/abs/2209.09824",
    "https://aclanthology.org/2022.emnlp-demos.5/",
    "https://github.com/cardiffnlp/tweetnlp/tree/add_training#model-fine-tuning",
    "https://huggingface.co/datasets/lmqg/qag_tweetqa",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-dec2021-tweet-topic-multi-all",
    "https://huggingface.co/datasets/cardiffnlp/tweet_sentiment_multilingual",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-hate",
    "https://colab.research.google.com/drive/1whic817jQzdHl4wKI8ZzeLGtmT9Gmv1m?usp=sharing",
    "https://huggingface.co/cambridgeltl/tweet-roberta-base-embeddings-v1",
    "https://huggingface.co/docs/datasets/load_hub",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-offensive",
    "https://huggingface.co/datasets/lmqg/qg_tweetqa",
    "https://tweetnlp.org/demo/",
    "https://github.com/cardiffnlp/tweetnlp/tree/add_training#model--dataset",
    "https://pypi.python.org/pypi/tweetnlp/",
    "https://colab.research.google.com/drive/1WVqt54dkZIBInzZTXZADNj7vCTnMXimx?usp=sharing",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-irony",
    "https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp?usp=sharing",
    "https://arxiv.org/abs/2206.14774",
    "https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp#scrollTo=uqd7sBHhnwym&line=6&uniqifier=1",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion-multilabel-latest",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment-latest",
    "https://colab.research.google.com/assets/colab-badge.svg",
    "https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base-sentiment-multilingual",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-offensive",
    "https://huggingface.co/datasets/cardiffnlp/tweet_topic_multi",
    "https://aclanthology.org/2023.woah-1.25/",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-irony",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-emoji",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-dec2021-tweet-topic-single-all",
    "https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp#scrollTo=KAZYjeskBqL4",
    "https://huggingface.co/cardiffnlp/twitter-xlm-roberta-base",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-emotion",
    "https://img.shields.io/pypi/pyversions/tweetnlp.svg",
    "https://img.shields.io/pypi/status/tweetnlp.svg",
    "https://colab.research.google.com/drive/104MtF9MXkDFimlJLr4SFBX0HjidLTfvp#scrollTo=MUT31bNQYTNz",
    "https://github.com/cardiffnlp/tweetnlp/blob/main/FINETUNING_RESULT.md",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-hate-latest",
    "https://huggingface.co/collections/cardiffnlp/tweetnlp-65e6f9ff5a0c4550ef7f1c70",
    "https://t.co/0G7zClOmi2\",",
    "https://github.com/asahi417/tweetnlp/blob/master/LICENSE",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m-sentiment",
    "https://huggingface.co/datasets/tner/tweetner7",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m",
    "https://arxiv.org/pdf/2206.14774.pdf",
    "https://huggingface.co/",
    "https://arxiv.org/abs/2210.03797",
    "https://badge.fury.io/py/tweetnlp",
    "https://huggingface.co/lmqg/t5-small-tweetqa-qa",
    "https://huggingface.co/datasets/cardiffnlp/tweet_topic_single"
  ]
}
```

</details>


---

## Repository 2: cardiffnlp/timelms

# GitHub Repository Data

**Repository:** [cardiffnlp/timelms](https://github.com/cardiffnlp/timelms)

## Basic Information

- **Description:** TimeLMs: Diachronic Language Models from Twitter
- **Created:** 2022-01-21T16:54:27+00:00
- **Last Updated:** 2025-05-11T14:48:25+00:00
- **Last Pushed:** 2024-03-05T11:58:11+00:00
- **Default Branch:** main
- **Size:** 411944 KB

## Statistics

- **Stars:** 107
- **Forks:** 8
- **Watchers:** 107
- **Open Issues:** 0
- **Total Issues:** 1
- **Pull Requests:** 0

## License

- **Type:** No license specified

## Languages

- **Jupyter Notebook:** 108,154 bytes
- **Python:** 69,033 bytes

## Top Contributors

1. **danlou** - 58 contributions
2. **pedrada88** - 5 contributions

## File Structure (Sample of 10 files)

Total files: 51

- `.gitignore` (blob)
- `README.md` (blob)
- `data` (tree)
- `data/ids` (tree)
- `data/ids/test` (tree)
- `data/ids/test/test_ids.2020-Q1.txt.zip` (blob)
- `data/ids/test/test_ids.2020-Q2.txt.zip` (blob)
- `data/ids/test/test_ids.2020-Q3.txt.zip` (blob)
- `data/ids/test/test_ids.2020-Q4.txt.zip` (blob)
- `data/ids/test/test_ids.2021-Q1.txt.zip` (blob)

## Recent Issues

- ðŸ”´ **#1** Pipeline for TweetEval evaluation (closed)

## Recent Commits

- **967ebbf6** Remove duplicate model - Jose Camacho-Collados (2024-03-05T11:58:11+00:00)
- **5164d435** update default base model, add entry for large model - Dan Lou (2023-03-17T14:29:55+00:00)
- **650dee66** link 2022 base and large models - Dan Lou (2023-03-17T14:27:14+00:00)
- **5dff1047** add verified users (full legacy) - Dan Lou (2023-02-14T15:36:19+00:00)
- **cdba8979** Merge branch 'main' of https://github.com/cardiffnlp/timelms into main - danlou (2022-11-07T16:26:04+00:00)
- **2003ee1f** include script to train sentiment models - danlou (2022-11-07T16:25:19+00:00)
- **3475cc05** Update models.json - Dan Lou (2022-10-14T15:37:02+00:00)
- **0ca001df** Update README.md - Dan Lou (2022-10-14T15:33:24+00:00)
- **62cbed5b** Update models.json - Dan Lou (2022-10-11T12:42:58+00:00)
- **5f89f4f9** Update README.md - Dan Lou (2022-10-10T17:03:02+00:00)

## External Links Found in README

- https://github.com/awslabs/mlm-scoring/blob/master/LICENSE
- https://huggingface.co/cardiffnlp/twitter-roberta-base-sep2021
- https://huggingface.co/cardiffnlp/twitter-roberta-base-jun2022-15M-incr
- https://arxiv.org/pdf/2202.03829.pdf
- https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v091122.txt
- https://github.com/awslabs/mlm-scoring](https://github.com/awslabs/mlm-scoring
- https://huggingface.co/cardiffnlp/twitter-roberta-base-sep2020
- https://aclanthology.org/2022.acl-demo.25",
- https://arxiv.org/pdf/2202.03829.pdf](https://arxiv.org/pdf/2202.03829.pdf
- https://huggingface.co/cardiffnlp/twitter-roberta-base-jun2022
- https://huggingface.co/cardiffnlp/twitter-roberta-base-jun2020
- https://huggingface.co/cardiffnlp/twitter-roberta-base-sep2022
- https://huggingface.co/cardiffnlp/twitter-roberta-base-2019-90m
- https://colab.research.google.com/assets/colab-badge.svg
- https://huggingface.co/cardiffnlp/twitter-roberta-base-mar2022-15M-incr
- https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v050422.txt
- https://huggingface.co/cardiffnlp/twitter-roberta-base-dec2020
- https://github.com/cardiffnlp/tweetnlp
- https://huggingface.co/cardiffnlp/twitter-roberta-base-jun2021
- https://huggingface.co/cardiffnlp/twitter-roberta-base-2022-154m

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 450570464,
  "name": "timelms",
  "full_name": "cardiffnlp/timelms",
  "description": "TimeLMs: Diachronic Language Models from Twitter",
  "html_url": "https://github.com/cardiffnlp/timelms",
  "clone_url": "https://github.com/cardiffnlp/timelms.git",
  "ssh_url": "git@github.com:cardiffnlp/timelms.git",
  "homepage": "",
  "topics": [],
  "default_branch": "main",
  "created_at": "2022-01-21T16:54:27+00:00",
  "updated_at": "2025-05-11T14:48:25+00:00",
  "pushed_at": "2024-03-05T11:58:11+00:00",
  "size_kb": 411944,
  "watchers_count": 107,
  "stargazers_count": 107,
  "forks_count": 8,
  "open_issues_count": 0,
  "license": null,
  "languages": {
    "Jupyter Notebook": 108154,
    "Python": 69033
  },
  "top_contributors": [
    {
      "login": "danlou",
      "contributions": 58
    },
    {
      "login": "pedrada88",
      "contributions": 5
    }
  ],
  "file_tree_count": 51,
  "file_tree_sample": [
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "data",
      "type": "tree"
    },
    {
      "path": "data/ids",
      "type": "tree"
    },
    {
      "path": "data/ids/test",
      "type": "tree"
    },
    {
      "path": "data/ids/test/test_ids.2020-Q1.txt.zip",
      "type": "blob"
    },
    {
      "path": "data/ids/test/test_ids.2020-Q2.txt.zip",
      "type": "blob"
    },
    {
      "path": "data/ids/test/test_ids.2020-Q3.txt.zip",
      "type": "blob"
    },
    {
      "path": "data/ids/test/test_ids.2020-Q4.txt.zip",
      "type": "blob"
    },
    {
      "path": "data/ids/test/test_ids.2021-Q1.txt.zip",
      "type": "blob"
    }
  ],
  "issues_count": 1,
  "pulls_count": 0,
  "recent_issues": [
    {
      "number": 1,
      "title": "Pipeline for TweetEval evaluation",
      "state": "closed"
    }
  ],
  "recent_pulls": [],
  "recent_commits": [
    {
      "sha": "967ebbf66917343b75940104e1e3918a7e78e0c1",
      "author": "Jose Camacho-Collados",
      "date": "2024-03-05T11:58:11+00:00",
      "message": "Remove duplicate model"
    },
    {
      "sha": "5164d4356edc5abc29b710be916fc05f9610dd2a",
      "author": "Dan Lou",
      "date": "2023-03-17T14:29:55+00:00",
      "message": "update default base model, add entry for large model"
    },
    {
      "sha": "650dee66872837e96ce8b259d30693e93fa313a3",
      "author": "Dan Lou",
      "date": "2023-03-17T14:27:14+00:00",
      "message": "link 2022 base and large models"
    },
    {
      "sha": "5dff1047b79f21b4bfa37ebaf37a9cbf574841aa",
      "author": "Dan Lou",
      "date": "2023-02-14T15:36:19+00:00",
      "message": "add verified users (full legacy)"
    },
    {
      "sha": "cdba8979e66d0918be6a7f8384153810ec1646d5",
      "author": "danlou",
      "date": "2022-11-07T16:26:04+00:00",
      "message": "Merge branch 'main' of https://github.com/cardiffnlp/timelms into main"
    },
    {
      "sha": "2003ee1f420e7a54eea3a95d131e9bf462f05e63",
      "author": "danlou",
      "date": "2022-11-07T16:25:19+00:00",
      "message": "include script to train sentiment models"
    },
    {
      "sha": "3475cc054a4bc20a03a3b373a8f83ff075917ab5",
      "author": "Dan Lou",
      "date": "2022-10-14T15:37:02+00:00",
      "message": "Update models.json"
    },
    {
      "sha": "0ca001df28209f9c7fa35db7591976ead11a9cd0",
      "author": "Dan Lou",
      "date": "2022-10-14T15:33:24+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "62cbed5bd2260a9484f42f2d9c1b2fd671a639f3",
      "author": "Dan Lou",
      "date": "2022-10-11T12:42:58+00:00",
      "message": "Update models.json"
    },
    {
      "sha": "5f89f4f9a3e412c2070eb1cf662534158564854e",
      "author": "Dan Lou",
      "date": "2022-10-10T17:03:02+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "a0de18a74cbbf18d7402dbd737e433a1572d9795",
      "author": "danlou",
      "date": "2022-08-04T17:30:24+00:00",
      "message": "argparse for preprocessing script, with option to keep specified ids"
    },
    {
      "sha": "e9c53f194ce03098b70c53945fa9b5204457ca3c",
      "author": "danlou",
      "date": "2022-08-04T17:28:40+00:00",
      "message": "Merge branch 'main' of https://github.com/cardiffnlp/timelms into main"
    },
    {
      "sha": "53ecc44ae2324218c90bc6e75f2281166b914ba4",
      "author": "danlou",
      "date": "2022-08-04T17:28:05+00:00",
      "message": "argparse for preprocessing script, with option to keep specified ids"
    },
    {
      "sha": "a4e1958b09c9fc58367e1ebbe9d9afea215e799a",
      "author": "Dan Lou",
      "date": "2022-07-27T17:05:37+00:00",
      "message": "Add colab badge"
    },
    {
      "sha": "f053b90019cdfa2b8c95397a22550a34a3d403b9",
      "author": "Jose Camacho-Collados",
      "date": "2022-07-27T14:10:45+00:00",
      "message": "Update README and add link"
    },
    {
      "sha": "072b4dd85251ba1ce3d42e9fd9bf5893cf9b6fb8",
      "author": "danlou",
      "date": "2022-07-20T15:38:16+00:00",
      "message": "include train and test tweet ids for jun2022 model"
    },
    {
      "sha": "d39b2111aa6b90998c02fd23b47fde91658544a5",
      "author": "danlou",
      "date": "2022-07-19T17:14:48+00:00",
      "message": "Typo"
    },
    {
      "sha": "819d4d9f20fde0fc41460af12fb9ea50e84e8260",
      "author": "danlou",
      "date": "2022-07-19T17:13:30+00:00",
      "message": "Merge branch 'main' of https://github.com/cardiffnlp/timelms into main"
    },
    {
      "sha": "3192d5b7b44c8c4009e2ced61173d9d3c80c592a",
      "author": "danlou",
      "date": "2022-07-19T17:12:52+00:00",
      "message": "Adding jun2022"
    },
    {
      "sha": "e274573d33fa8148adb1f0c760d1383d1c10a2da",
      "author": "Jose Camacho-Collados",
      "date": "2022-06-19T19:19:33+00:00",
      "message": "Update reference paper"
    }
  ],
  "readme_text": "# TimeLMs: Diachronic Language Models from Twitter\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1pmHi5GU-5VHtXhbK_QV2A-7mDGwddjp8?usp=sharing)\n\nTimeLMs allows for easy access to models continuously trained on social media over regular intervals for researching language model degradation, as well as cultural shifts affecting language usage on social media.\n\nPaper (ACL-2022 Demo): [https://arxiv.org/pdf/2202.03829.pdf](https://arxiv.org/pdf/2202.03829.pdf)\n\nBelow we provide instructions for getting started with TimeLMs and a few usage examples. For a more detailed guide, please see our [notebook demo](demo.ipynb). The demo is also available as a [Google Colab notebook](https://colab.research.google.com/drive/1pmHi5GU-5VHtXhbK_QV2A-7mDGwddjp8?usp=sharing).\n\nTimeLMs has also been integrated into the [TweetNLP library](https://github.com/cardiffnlp/tweetnlp).\n\n\n# Released Models\n\nList of models released in the scope of TimeLMs (available through the Hugging Face hub):\n\n| Model Name | # Tweets | Last Date | Link | Verified Users Set |\n| ----------- | ----------- | ----------- | ----------- | ----------- |\n| twitter-roberta-base-2019-90m | 90.26M | 2019-12-31 | [Hub Link](https://huggingface.co/cardiffnlp/twitter-roberta-base-2019-90m) | [v310821](https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v310821.txt) |\n| twitter-roberta-base-mar2020 | 94.46M | 2020-03-31 | [Hub Link](https://huggingface.co/cardiffnlp/twitter-roberta-base-mar2020) | [v310821](https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v310821.txt) |\n| twitter-roberta-base-jun2020 | 98.66M | 2020-06-30 | [Hub Link](https://huggingface.co/cardiffnlp/twitter-roberta-base-jun2020) | [v310821](https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v310821.txt) |\n| twitter-roberta-base-sep2020 | 102.86M | 2020-09-30 | [Hub Link](https://huggingface.co/cardiffnlp/twitter-roberta-base-sep2020) | [v310821](https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v310821.txt) |\n| twitter-roberta-base-dec2020 | 107.06M | 2020-12-31 | [Hub Link](https://huggingface.co/cardiffnlp/twitter-roberta-base-dec2020) | [v310821](https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v310821.txt) |\n| twitter-roberta-base-mar2021 | 111.26M | 2021-03-31 | [Hub Link](https://huggingface.co/cardiffnlp/twitter-roberta-base-mar2021) | [v310821](https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v310821.txt) |\n| twitter-roberta-base-jun2021 | 115.46M | 2021-06-30 | [Hub Link](https://huggingface.co/cardiffnlp/twitter-roberta-base-jun2021) | [v310821](https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v310821.txt) |\n| twitter-roberta-base-sep2021 | 119.66M | 2021-09-30 | [Hub Link](https://huggingface.co/cardiffnlp/twitter-roberta-base-sep2021) | [v310821](https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v310821.txt) |\n| twitter-roberta-base-dec2021 | 123.86M | 2021-12-31 | [Hub Link](https://huggingface.co/cardiffnlp/twitter-roberta-base-dec2021) | [v310821](https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v310821.txt) |\n| twitter-roberta-base-2021-124m | 123.86M | 2021-12-31 | [Hub Link](https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m) | [v310821](https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v310821.txt) |\n| twitter-roberta-base-mar2022 | 128.06M | 2022-03-31 | [Hub Link](https://huggingface.co/cardiffnlp/twitter-roberta-base-mar2022) | [v050422](https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v050422.txt) |\n| twitter-roberta-base-jun2022 | 132.26M | 2022-06-30 | [Hub Link](https://huggingface.co/cardiffnlp/twitter-roberta-base-jun2022) | [v050422](https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v050422.txt) |\n| twitter-roberta-base-mar2022-15M-incr | 138.86M | 2022-03-31 | [Hub Link](https://huggingface.co/cardiffnlp/twitter-roberta-base-mar2022-15M-incr) | [v050422](https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v050422.txt) |\n| twitter-roberta-base-jun2022-15M-incr | 153.86M | 2022-06-30 | [Hub Link](https://huggingface.co/cardiffnlp/twitter-roberta-base-jun2022-15M-incr) | [v050422](https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v050422.txt) |\n| twitter-roberta-base-sep2022 | 168.86M | 2022-09-30 | [Hub Link](https://huggingface.co/cardiffnlp/twitter-roberta-base-sep2022) | [v050422](https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v050422.txt) |\n| twitter-roberta-base-2022-154m | 154M | 2022-12-31 | [Hub Link](https://huggingface.co/cardiffnlp/twitter-roberta-base-2022-154m) | [v091122](https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v091122.txt) |\n| twitter-roberta-large-2022-154m | 154M | 2022-12-31 | [Hub Link](https://huggingface.co/cardiffnlp/twitter-roberta-large-2022-154m) | [v091122](https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v091122.txt) |\n\n\n\n# Getting Started\n\n## Environment and Dependencies\n\nYou may create a new environment using conda and install dependencies following the commands below.\nWe assume you already have PyTorch with CUDA support installed (tested with torch==1.8.2+cu111 and CUDA 11.2).\n\n```bash\n$ conda create -n timelms python=3.7\n$ conda activate timelms\n$ pip install -r requirements.txt\n```\n\n\n## Loading TimeLMs\n\nYou can load our interface simply with these two lines, importing the TimeLMs class from the [timelms.py](timelms.py) file in this repository.\n\n```python\nfrom timelms import TimeLMs\ntlms = TimeLMs(device='cuda:0')\n```\n\n\n## Operating Modes\n\nTimeLMs currently supports the following temporal modes for determining which models are employed for different tweets.\n1. 'latest': using our most recently trained Twitter model.\n2. 'YYYY-MM' (custom): using the model closest to a custom date provided by the user (e.g., '2020-11').\n3. 'corresponding': using the model that was trained only until to each tweet's date (i.e., its specific quarter).\n4. 'quarterly': using all available models trained over time in quarterly intervals.\n\nThe `corresponding` mode requires tweets with a `created_at` field with dates under any format that begins with YYYY-MM.\n\n\n## Computing Perplexity\n\n```python\ntweets = [{'text': 'She is pure heart #SanaTheBBWinner', 'created_at': '2020-02-09T05:55:00.000Z'},\n          {'text': 'Looking forward to watching Squid Game tonight !', 'created_at': '2021-10-11T12:34:56.000Z'}]\n\npseudo_ppls = tlms.get_pseudo_ppl(tweets, mode='corresponding')\n```\n\nTo get pseudo-perplexity (PPPL) scores for a set of tweets, you just need to pass a list of tweets to `tlms.get_pseudo_ppl()`, specifying your desired mode. Depending on the chosen mode, you'll get a score from each applicable model (2 models for this example). Besides PPPL scores by model, this method also returns the input tweets with their specific pseudo-log likelihood (PLL) values.\n\n\n## Masked Predictions\n\n```python\ntweets = [{\"text\": \"So glad I'm <mask> vaccinated .\"},\n          {\"text\": \"Looking forward to watching <mask> Game tonight !\"}]\n\npreds = tlms.get_masked_predictions(tweets, mode='quarterly', top_k=3)\n```\n\nTo get masked predictions using our models, you just need to pass a list of tweets to `tlms.get_masked_predictions()`, specifying your desired mode and number of predictions.\nIn the example above, we're choosing the `quarterly` mode, which does not require date fields.\n\n\n## Evaluating Models\n\n```python\ntlms.eval_model('roberta-base', 'data/tweets/tweets-2020-2021-subset-rnd.jl')\n```\n\nWe provide a method for evaluating other models supported by the Transformers package using PPPL.\nFor evaluating over the periods of 2020 to 2021, we recommend retrieving the tweets used for our evaluation (we provide tweet ids [here](data/test_ids.csv.zip)), or using the 50K subset provided in this repository as an alternative.\nFor the time being, we only support models based on RoBERTa (most Twitter LMs).\n\n# Creating Twitter Corpora\n\nBelow you find instructions for using our scripts to retrieve and preprocess Twitter data.\nThe same scripts were used for obtaining our training and testing corpora for TimeLMs.\n\n## Sampling Tweets from the API\n\n```bash\n$ python scripts/sampler_api.py 2020 01 35  # <YYYY> <MM> <MIN_MARK>\n```\n\nA generic sample of tweets from the Twitter API can be retrieved using the [sampler_api.py](scripts/sampler_api.py) script.\nBy 'generic' we mean tweets that are not targetting any specific content (we use stopwords as query terms, more details in the paper).\n\nThe MIN_MARK variable is the specific minute passed to the API request. You should set this value according to your preference for the time difference between requests. In our paper, we used several calls to this script in increments of 5 minutes.\n\nThis script retrieves tweets for every hour of every day of the given YYYY-MM at the specified MIN_MARK.\nEvery response is stored as its own file in `data/responses`. Requests for files already in that folder will be skipped.\n\nRequires the API BEARER_TOKEN available as an environment variable. You can set that up with:\n\n```bash\n$ export 'BEARER_TOKEN'='<your_bearer_token>'\n```\n\nThe script is set up to wait 7 seconds between requests of 500 results. In case of error, the script waits another 60 seconds before retrying (and increments time between requests by 0.01 seconds).\n\n## Compiling Tweets by Date\n\n```bash\n$ python scripts/combine.py tweets-2020-Q3.jl 2020-01 2020-02 2020-03  # <output_file> <months:YYYY-MM>\n```\n\nAfter populating `data/responses` with tweets retrieved from the API, you can use the [combine.py](scripts/combine.py) script to combine those responses into a single .jl file restricted to tweets for specified year-months.\n\nThis script also merges metrics and location info so that all data pertaining to a particular tweet is contained in a single-line JSON entry of the output .jl file.\n\nYou may specify any number of YYYY-MMs. If none are provided, the script will use all available tweets.\n\n## Cleaning, Filtering and Anonymizing\n\n```bash\n$ python scripts/preprocess.py --src tweets-2020-Q3.jl --out tweets-2020-Q3.cleaned.jl\n```\n\nFinally, the merged .jl file can be preprocessed using the [preprocess.py](scripts/preprocess.py) script. This step requires the following additional packages:\n\n```bash\n$ pip install datasketch==1.5.3\n$ pip install xxhash==2.0.2\n```\n\nThis script removes duplicates, near duplicates and tweets from most frequent users (likely bots, details in the paper) besides replacing user mentions with '@user' for anonymization, except for popular users (i.e., verified users).\n\nThe set of verified users was determined using the [get_verified.py](scripts/get_verified.py) script, producing the [verified_users.v310821.txt](data/verified_users.v310821.txt) file shared with this repository (and more recent versions).\n\n\n# Citing TimeLMs\n\nIf you use TimeLMs in your research, please use the following bib entry to cite the [reference paper](https://arxiv.org/pdf/2202.03829.pdf).\n\n```\n@inproceedings{loureiro-etal-2022-timelms,\n    title = \"{T}ime{LM}s: Diachronic Language Models from {T}witter\",\n    author = \"Loureiro, Daniel  and\n      Barbieri, Francesco  and\n      Neves, Leonardo  and\n      Espinosa Anke, Luis  and\n      Camacho-collados, Jose\",\n    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations\",\n    month = may,\n    year = \"2022\",\n    address = \"Dublin, Ireland\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://aclanthology.org/2022.acl-demo.25\",\n    doi = \"10.18653/v1/2022.acl-demo.25\",\n    pages = \"251--260\"\n}\n```\n\n# License\n\nTimeLMs is released without any restrictions, but our scoring code is based on the [https://github.com/awslabs/mlm-scoring](https://github.com/awslabs/mlm-scoring) repository, which is distributed under [Apache License 2.0](https://github.com/awslabs/mlm-scoring/blob/master/LICENSE). We also refer users to Twitter regulations regarding use of our models and test sets.\n",
  "external_links_in_readme": [
    "https://github.com/awslabs/mlm-scoring/blob/master/LICENSE",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-sep2021",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-jun2022-15M-incr",
    "https://arxiv.org/pdf/2202.03829.pdf",
    "https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v091122.txt",
    "https://github.com/awslabs/mlm-scoring](https://github.com/awslabs/mlm-scoring",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-sep2020",
    "https://aclanthology.org/2022.acl-demo.25\",",
    "https://arxiv.org/pdf/2202.03829.pdf](https://arxiv.org/pdf/2202.03829.pdf",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-jun2022",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-jun2020",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-sep2022",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-2019-90m",
    "https://colab.research.google.com/assets/colab-badge.svg",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-mar2022-15M-incr",
    "https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v050422.txt",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-dec2020",
    "https://github.com/cardiffnlp/tweetnlp",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-jun2021",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-2022-154m",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-mar2022",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-dec2021",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-mar2020",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-mar2021",
    "https://colab.research.google.com/drive/1pmHi5GU-5VHtXhbK_QV2A-7mDGwddjp8?usp=sharing",
    "https://github.com/cardiffnlp/timelms/raw/main/data/verified_users.v310821.txt",
    "https://huggingface.co/cardiffnlp/twitter-roberta-large-2022-154m",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-2021-124m"
  ]
}
```

</details>


---

## Repository 3: cardiffnlp/tweeteval

# GitHub Repository Data

**Repository:** [cardiffnlp/tweeteval](https://github.com/cardiffnlp/tweeteval)

## Basic Information

- **Description:** Repository for TweetEval
- **Created:** 2020-10-05T10:46:24+00:00
- **Last Updated:** 2025-06-06T12:46:29+00:00
- **Last Pushed:** 2022-07-08T07:23:43+00:00
- **Default Branch:** main
- **Size:** 8696 KB

## Statistics

- **Stars:** 377
- **Forks:** 81
- **Watchers:** 377
- **Open Issues:** 14
- **Total Issues:** 0
- **Pull Requests:** 4

## License

- **Type:** No license specified

## Languages

- **Jupyter Notebook:** 57,875 bytes
- **Python:** 4,670 bytes

## Top Contributors

1. **pedrada88** - 61 contributions
2. **luisespinosaanke** - 12 contributions
3. **chrisfalter** - 2 contributions
4. **fvancesco** - 2 contributions
5. **kornosk** - 1 contributions

## File Structure (Sample of 10 files)

Total files: 105

- `README.md` (blob)
- `TweetEval_Tutorial.ipynb` (blob)
- `datasets` (tree)
- `datasets/README.txt` (blob)
- `datasets/emoji` (tree)
- `datasets/emoji/mapping.txt` (blob)
- `datasets/emoji/test_labels.txt` (blob)
- `datasets/emoji/test_text.txt` (blob)
- `datasets/emoji/train_labels.txt` (blob)
- `datasets/emoji/train_text.txt` (blob)

## Recent Issues

- ðŸŸ¢ **#24** mismatched lengths in hate training dataset (open)
- ðŸŸ¢ **#23** Leftover utf-codes in emoji analysis texts (open)
- ðŸ”´ **#22** baseline scripts and predictions (closed)
- ðŸŸ¢ **#21** Warning when loading the sentiment-analysis model (open)
- ðŸŸ¢ **#20** RuntimeError: Error(s) in loading state_dict for RobertaForSequenceClassification: (open)

## Recent Pull Requests

- ðŸ”´ **#11** fix miscalculated results (closed)
- ðŸ”´ **#6** Sentiment dataset fixes (closed)
- ðŸ”´ **#4** Re-parse lines containing multiple tweets (closed)
- ðŸ”´ **#3** Fix link to Google Colab notebook (closed)

## Recent Commits

- **4fbd22cd** Add link - Jose Camacho-Collados (2022-07-08T07:23:43+00:00)
- **bff82c81** Update README.md - Francesco Barbieri (2022-03-14T23:22:17+00:00)
- **c6b806cc** Update README.md - Francesco Barbieri (2022-03-14T23:21:50+00:00)
- **778b0f65** Add TimeLMs to Leaderboard - Jose Camacho-Collados (2022-02-18T08:17:50+00:00)
- **876cb467** Add references TweetEval (on top) - Cardiff NLP (2022-01-27T11:25:08+00:00)
- **c50a16d4** Fix RoBERTa-Retrained sentiment results - Jose Camacho-Collados (2021-07-29T17:00:24+00:00)
- **f3a4d9e8** fix sentiment predictions - Jose Camacho Collados (2021-07-29T16:58:16+00:00)
- **851de61d** Merge pull request #11 from kornosk/main - Jose Camacho-Collados (2021-07-29T16:34:34+00:00)
- **b75cebfd** fix miscalculated results - Kornraphop Kawintiranon (2021-05-04T23:48:52+00:00)
- **1fab2265** Add XLM-T link - Jose Camacho-Collados (2021-04-28T20:05:02+00:00)

## External Links Found in README

- https://www.aclweb.org/anthology/S19-2007",
- https://www.aclweb.org/anthology/S19-2010/
- https://www.aclweb.org/anthology/S18-1001/
- https://github.com/cardiffnlp/tweeteval/tree/main/datasets
- https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion
- https://huggingface.co/cardiffnlp/twitter-roberta-base
- https://arxiv.org/pdf/2010.12421.pdf
- https://arxiv.org/abs/2005.10200
- https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment
- https://www.aclweb.org/anthology/S18-1003.pdf
- https://huggingface.co/cardiffnlp/twitter-roberta-base-irony
- https://github.com/cardiffnlp/xlm-t
- https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji
- https://arxiv.org/abs/2202.03829
- https://huggingface.co/cardiffnlp/twitter-roberta-base-offensive
- https://www.aclweb.org/anthology/S18-1005.pdf
- https://huggingface.co/cardiffnlp/twitter-roberta-base-hate
- https://colab.research.google.com/drive/18cNn4cJ-bAi-Luiqi8V6c09Tj-iiX0oG
- https://www.aclweb.org/anthology/S17-2088/
- https://www.aclweb.org/anthology/S16-1003/

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 301375115,
  "name": "tweeteval",
  "full_name": "cardiffnlp/tweeteval",
  "description": "Repository for TweetEval",
  "html_url": "https://github.com/cardiffnlp/tweeteval",
  "clone_url": "https://github.com/cardiffnlp/tweeteval.git",
  "ssh_url": "git@github.com:cardiffnlp/tweeteval.git",
  "homepage": null,
  "topics": [],
  "default_branch": "main",
  "created_at": "2020-10-05T10:46:24+00:00",
  "updated_at": "2025-06-06T12:46:29+00:00",
  "pushed_at": "2022-07-08T07:23:43+00:00",
  "size_kb": 8696,
  "watchers_count": 377,
  "stargazers_count": 377,
  "forks_count": 81,
  "open_issues_count": 14,
  "license": null,
  "languages": {
    "Jupyter Notebook": 57875,
    "Python": 4670
  },
  "top_contributors": [
    {
      "login": "pedrada88",
      "contributions": 61
    },
    {
      "login": "luisespinosaanke",
      "contributions": 12
    },
    {
      "login": "chrisfalter",
      "contributions": 2
    },
    {
      "login": "fvancesco",
      "contributions": 2
    },
    {
      "login": "kornosk",
      "contributions": 1
    }
  ],
  "file_tree_count": 105,
  "file_tree_sample": [
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "TweetEval_Tutorial.ipynb",
      "type": "blob"
    },
    {
      "path": "datasets",
      "type": "tree"
    },
    {
      "path": "datasets/README.txt",
      "type": "blob"
    },
    {
      "path": "datasets/emoji",
      "type": "tree"
    },
    {
      "path": "datasets/emoji/mapping.txt",
      "type": "blob"
    },
    {
      "path": "datasets/emoji/test_labels.txt",
      "type": "blob"
    },
    {
      "path": "datasets/emoji/test_text.txt",
      "type": "blob"
    },
    {
      "path": "datasets/emoji/train_labels.txt",
      "type": "blob"
    },
    {
      "path": "datasets/emoji/train_text.txt",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 4,
  "recent_issues": [
    {
      "number": 24,
      "title": "mismatched lengths in hate training dataset",
      "state": "open"
    },
    {
      "number": 23,
      "title": "Leftover utf-codes in emoji analysis texts",
      "state": "open"
    },
    {
      "number": 22,
      "title": "baseline scripts and predictions",
      "state": "closed"
    },
    {
      "number": 21,
      "title": "Warning when loading the sentiment-analysis model",
      "state": "open"
    },
    {
      "number": 20,
      "title": "RuntimeError: Error(s) in loading state_dict for RobertaForSequenceClassification:",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 11,
      "title": "fix miscalculated results",
      "state": "closed"
    },
    {
      "number": 6,
      "title": "Sentiment dataset fixes",
      "state": "closed"
    },
    {
      "number": 4,
      "title": "Re-parse lines containing multiple tweets",
      "state": "closed"
    },
    {
      "number": 3,
      "title": "Fix link to Google Colab notebook",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "4fbd22cd78421f05b1ecdb4fc5725bc7a7bd8f66",
      "author": "Jose Camacho-Collados",
      "date": "2022-07-08T07:23:43+00:00",
      "message": "Add link"
    },
    {
      "sha": "bff82c817cb2d15fc9a3cd5b1b5ac999afd3dcf5",
      "author": "Francesco Barbieri",
      "date": "2022-03-14T23:22:17+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "c6b806cceb449b3a889a4b622b3dc16186c45945",
      "author": "Francesco Barbieri",
      "date": "2022-03-14T23:21:50+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "778b0f651295a79e57f09cc6f48ac4f91a78bde0",
      "author": "Jose Camacho-Collados",
      "date": "2022-02-18T08:17:50+00:00",
      "message": "Add TimeLMs to Leaderboard"
    },
    {
      "sha": "876cb4673c2dc14b47e2bd7581f1236e33efd123",
      "author": "Cardiff NLP",
      "date": "2022-01-27T11:25:08+00:00",
      "message": "Add references TweetEval (on top)"
    },
    {
      "sha": "c50a16d4b6bf20931c8fc0d54674f3c655362385",
      "author": "Jose Camacho-Collados",
      "date": "2021-07-29T17:00:24+00:00",
      "message": "Fix RoBERTa-Retrained sentiment results"
    },
    {
      "sha": "f3a4d9e87d31d2bd9cdaf6781ee4fa524c1f8395",
      "author": "Jose Camacho Collados",
      "date": "2021-07-29T16:58:16+00:00",
      "message": "fix sentiment predictions"
    },
    {
      "sha": "851de61de346e3b9a9d5ec6c1b137c6b33fc6e9d",
      "author": "Jose Camacho-Collados",
      "date": "2021-07-29T16:34:34+00:00",
      "message": "Merge pull request #11 from kornosk/main"
    },
    {
      "sha": "b75cebfd0da00439fc991efad0f68877f9fc401b",
      "author": "Kornraphop Kawintiranon",
      "date": "2021-05-04T23:48:52+00:00",
      "message": "fix miscalculated results"
    },
    {
      "sha": "1fab2265b47f4e6f275984c83d652b78cc03fc0b",
      "author": "Jose Camacho-Collados",
      "date": "2021-04-28T20:05:02+00:00",
      "message": "Add XLM-T link"
    },
    {
      "sha": "3f3bcd3ae631851582fa0e63ef6fedae91b4939f",
      "author": "Jose Camacho-Collados",
      "date": "2021-01-12T21:35:37+00:00",
      "message": "Add BERTweet"
    },
    {
      "sha": "4932ed43836d2aaaf3d09740011ccc7535d905de",
      "author": "Jose Camacho-Collados",
      "date": "2020-12-17T10:25:36+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "dc32df5ecdc84128f728c5a370574d61dd867b30",
      "author": "Jose Camacho-Collados",
      "date": "2020-12-17T10:07:45+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "3eb5e91dd5070b5a5931df4e70cbab5978e3c50c",
      "author": "pedrada88",
      "date": "2020-12-17T10:05:54+00:00",
      "message": "fix sentiment dataset"
    },
    {
      "sha": "7c6acdaf2ee596eeb00e0ddf207e35bf860cac47",
      "author": "luisespinosa",
      "date": "2020-12-07T10:55:15+00:00",
      "message": "fix notebook"
    },
    {
      "sha": "fab9163e2f1d665286f6334b28a939b57705cae3",
      "author": "luisespinosaanke",
      "date": "2020-12-05T18:00:26+00:00",
      "message": "Merge pull request #3 from chrisfalter/main"
    },
    {
      "sha": "b05b68933a9134ab5f80aceb52bfb4e8381e37af",
      "author": "Christopher Falter",
      "date": "2020-11-28T00:30:37+00:00",
      "message": "Merge pull request #1 from chrisfalter/chrisfalter-issue-1"
    },
    {
      "sha": "88376b272fc8e0d4f642d56f73e0438fb988478b",
      "author": "Christopher Falter",
      "date": "2020-11-28T00:29:42+00:00",
      "message": "Fix Google Colab notebook link"
    },
    {
      "sha": "326c4e53b561e728966b07520e9d79d430202543",
      "author": "Jose Camacho-Collados",
      "date": "2020-11-24T22:14:12+00:00",
      "message": "Update README typo"
    },
    {
      "sha": "596ef9931cc713b980f4a3ec069631d37f90c1bc",
      "author": "luisespinosa",
      "date": "2020-11-13T12:37:00+00:00",
      "message": "add notebook"
    }
  ],
  "readme_text": "# TweetEval\nThis is the repository for the [_TweetEval_ benchmark (Findings of EMNLP 2020)](https://arxiv.org/pdf/2010.12421.pdf). _TweetEval_ consists of seven heterogenous tasks in Twitter, all framed as multi-class tweet classification. All tasks have been unified into the same benchmark, with each dataset presented in the same format and with fixed training, validation and test splits.\n\n# TweetEval: The Benchmark\n\nThese are the seven datasets of TweetEval, with its corresponding labels (more details about the format in the [datasets](https://github.com/cardiffnlp/tweeteval/tree/main/datasets) directory):\n\n- **Emotion Recognition**: [SemEval 2018 - Emotion Recognition](https://www.aclweb.org/anthology/S18-1001/) (Mohammad et al., 2018) - 4 labels: `anger`, `joy`,`sadness`, `optimism`\n\n- **Emoji Prediction**, [SemEval 2018 - Emoji Prediction](https://www.aclweb.org/anthology/S18-1003.pdf) (Barbieri et al., 2018) - 20 labels: :heart:, :heart_eyes:, :joy:  `...` :evergreen_tree:, :camera:, :stuck_out_tongue_winking_eye:\n\n- **Irony Detection**, [SemEval 2018 - Irony Detection](https://www.aclweb.org/anthology/S18-1005.pdf) (Van Hee et al., 2018) - 2 labels: `irony`, `not irony`\n\n- **Hate Speech Detection**, [SemEval 2019 - Hateval](https://www.aclweb.org/anthology/S19-2007.pdf) (Basile et al., 2019) - 2 labels: `hateful`, `not hateful`\n  \n- **Offensive Language Identification**, [SemEval 2019 - OffensEval](https://www.aclweb.org/anthology/S19-2010/) (Zampieri et al., 2019)- 2 labels: `offensive`, `not offensive`\n\n- **Sentiment Analysis***, [SemEval 2017 - Sentiment Analysis in Twitter](https://www.aclweb.org/anthology/S17-2088/) (Rosenthal et al., 2019) - 3 labels: `positive`, `neutral`, `negative`\n\n- **Stance Detection***, [SemEval 2016 - Detecting Stance in Tweets](https://www.aclweb.org/anthology/S16-1003/) (Mohammad et al., 2016) - 3 labels: `favour`, `neutral`, `against`\n\n**Note 1***: For stance there are five different target topics (Abortion, Atheism, Climate change, Feminism and Hillary Clinton), each of which contains its own training, validation and test data.\n\n**Note 2***: The sentiment dataset has been updated as of 17 December 2020. The update has been minimal and it was intended to fix a small number of sentences that were cropped.\n\n# TweetEval: Leaderboard (Test set)\n\n| Model | Emoji | Emotion | Hate | Irony | Offensive | Sentiment | Stance | ALL(TE) | Reference |\n|----------|------:|--------:|-----:|------:|----------:|----------:|-------:|----:|---------|\n| BERTweet   | 33.4     | 79.3       | **56.4**    | **82.1**     | 79.5         | 73.4         | 71.2     | **67.9**  | [BERTweet](https://arxiv.org/abs/2005.10200) |\n| TimeLMs-2021   | **34.0**     | **80.2**      | 55.1    | 64.5     | **82.2**         | **73.7**         | **72.9**     | 66.2  | [TimeLMs](https://arxiv.org/abs/2202.03829) |\n| RoBERTa-Retrained   | 31.4     | 78.5       | 52.3    | 61.7     | 80.5         | 72.8         | 69.3     | 65.2  | [TweetEval](https://arxiv.org/pdf/2010.12421.pdf) |\n| RoBERTa-Base   | 30.9     | 76.1       | 46.6    | 59.7     | 79.5         | 71.3         | 68      | 61.3  | [TweetEval](https://arxiv.org/pdf/2010.12421.pdf) |\n| RoBERTa-Twitter   | 29.3     | 72.0       | 49.9    | 65.4     | 77.1         | 69.1        | 66.7     | 61.4  | [TweetEval](https://arxiv.org/pdf/2010.12421.pdf) |\n| FastText | 25.8     | 65.2       | 50.6    | 63.1     | 73.4         | 62.9         | 65.4      | 58.1 | [TweetEval](https://arxiv.org/pdf/2010.12421.pdf) |\n| LSTM      | 24.7     | 66.0       | 52.6    | 62.8     | 71.7         | 58.3         | 59.4      | 56.5 | [TweetEval](https://arxiv.org/pdf/2010.12421.pdf) |\n| SVM      | 29.3     | 64.7       | 36.7    | 61.7     | 52.3         | 62.9         | 67.3      | 53.5 | [TweetEval](https://arxiv.org/pdf/2010.12421.pdf) |\n\n**Note***: Check the [reference paper](https://arxiv.org/pdf/2010.12421.pdf) for details on the official metrics for each task\n\nIf you would like to have your results added to the leaderboard you can either submit a pull request or send an email to any of the paper authors with results and the predictions of your model. Please also submit a reference to a paper describing your approach.\n\n# Evaluating your system\n\nFor evaluating your system, you simply need an individual predictions file for each of the tasks. The format of the predictions file should be the same as the output examples in the predictions folder (one output label per line as per the original test file). The predictions included as an example in this repo correspond to the best model evaluated in the paper, i.e., RoBERTa re-trained on Twitter (RoB-Rt in the paper).  \n\n### Example usage\n\n```bash\npython evaluation_script.py\n```\nThe script takes the TweetEval gold test labels and the predictions from the \"predictions\" folder by default, but you can set this to suit your needs as optional arguments.\n\n### Optional arguments\n\nThree optional arguments can be modified: \n\n*--tweeteval_path*: Path to TweetEval datasets. Default: *\"./datasets/\"*\n\n*--predictions_path*: Path to predictions directory. Default: *\"./predictions/\"*\n\n*--task*: Use this to get single task detailed results *(emoji|emotion|hate|irony|offensive|sentiment|stance)*. Default: \"\"\n\nEvaluation script sample usage from the terminal with parameters:\n\n```bash\npython evaluation_script.py --tweeteval_path ./datasets/ --predictions_path ./predictions/ --task emoji\n```\n(this script would output the breakdown of the results for the emoji prediction task only)\n\n# Pre-trained models and code\n\nYou can download the best Twitter masked language model (RoBERTa-retrained in the paper) from \ud83e\udd17HuggingFace [here](https://huggingface.co/cardiffnlp/twitter-roberta-base). We also provide task-specific models:\n\n- [Twitter-RoBERTa-emoji](https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji)\n- [Twitter-RoBERTa-emotion](https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion)\n- [Twitter-RoBERTa-hate](https://huggingface.co/cardiffnlp/twitter-roberta-base-hate)\n- [Twitter-RoBERTa-irony](https://huggingface.co/cardiffnlp/twitter-roberta-base-irony)\n- [Twitter-RoBERTa-offensive](https://huggingface.co/cardiffnlp/twitter-roberta-base-offensive)\n- [Twitter-RoBERTa-sentiment](https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment)\n- Twitter-RoBERTa-stance - Coming soon\n\nTo know how to use the pre-trained models, you can check our [Google Colab Notebook](https://colab.research.google.com/drive/18cNn4cJ-bAi-Luiqi8V6c09Tj-iiX0oG), with sample code for masked language modeling, extracting embeddings from tweets and tweet classification.\n\n**NEW!** A multilingual language model trained on Twitter for 30+ languages (XLM-T) is now available [here](https://github.com/cardiffnlp/xlm-t)\n\n**NEWx2!** You can now test TweetEval models as part of the [TweetNLP platform](https://tweetnlp.org).\n\n\n# Citing TweetEval\n\nIf you use TweetEval in your research, please use the following `bib` entry to cite the [reference paper](https://arxiv.org/pdf/2010.12421.pdf).\n\n```\n@inproceedings{barbieri2020tweeteval,\n  title={{TweetEval:Unified Benchmark and Comparative Evaluation for Tweet Classification}},\n  author={Barbieri, Francesco and Camacho-Collados, Jose and Espinosa-Anke, Luis and Neves, Leonardo},\n  booktitle={Proceedings of Findings of EMNLP},\n  year={2020}\n}\n```\n# License\n\nTweetEval is released without any restrictions but restrictions may apply to individual tasks (which are derived from existing datasets) or Twitter (main data source). We refer users to the original licenses accompanying each dataset and Twitter regulations.\n\n\n# Citing TweetEval datasets\n\nIf you use any of the TweetEval datasets, please cite their original publications:\n\n#### Emotion Recognition:\n```\n@inproceedings{mohammad2018semeval,\n  title={Semeval-2018 task 1: Affect in tweets},\n  author={Mohammad, Saif and Bravo-Marquez, Felipe and Salameh, Mohammad and Kiritchenko, Svetlana},\n  booktitle={Proceedings of the 12th international workshop on semantic evaluation},\n  pages={1--17},\n  year={2018}\n}\n\n```\n#### Emoji Prediction:\n```\n@inproceedings{barbieri2018semeval,\n  title={Semeval 2018 task 2: Multilingual emoji prediction},\n  author={Barbieri, Francesco and Camacho-Collados, Jose and Ronzano, Francesco and Espinosa-Anke, Luis and \n    Ballesteros, Miguel and Basile, Valerio and Patti, Viviana and Saggion, Horacio},\n  booktitle={Proceedings of The 12th International Workshop on Semantic Evaluation},\n  pages={24--33},\n  year={2018}\n}\n```\n\n#### Irony Detection:\n```\n@inproceedings{van2018semeval,\n  title={Semeval-2018 task 3: Irony detection in english tweets},\n  author={Van Hee, Cynthia and Lefever, Els and Hoste, V{\\'e}ronique},\n  booktitle={Proceedings of The 12th International Workshop on Semantic Evaluation},\n  pages={39--50},\n  year={2018}\n}\n```\n\n#### Hate Speech Detection:\n```\n@inproceedings{basile-etal-2019-semeval,\n    title = \"{S}em{E}val-2019 Task 5: Multilingual Detection of Hate Speech Against Immigrants and Women in {T}witter\",\n    author = \"Basile, Valerio  and Bosco, Cristina  and Fersini, Elisabetta  and Nozza, Debora and Patti, Viviana and\n      Rangel Pardo, Francisco Manuel  and Rosso, Paolo  and Sanguinetti, Manuela\",\n    booktitle = \"Proceedings of the 13th International Workshop on Semantic Evaluation\",\n    year = \"2019\",\n    address = \"Minneapolis, Minnesota, USA\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/S19-2007\",\n    doi = \"10.18653/v1/S19-2007\",\n    pages = \"54--63\"\n}\n```\n#### Offensive Language Identification:\n```\n@inproceedings{zampieri2019semeval,\n  title={SemEval-2019 Task 6: Identifying and Categorizing Offensive Language in Social Media (OffensEval)},\n  author={Zampieri, Marcos and Malmasi, Shervin and Nakov, Preslav and Rosenthal, Sara and Farra, Noura and Kumar, Ritesh},\n  booktitle={Proceedings of the 13th International Workshop on Semantic Evaluation},\n  pages={75--86},\n  year={2019}\n}\n```\n\n#### Sentiment Analysis:\n```\n@inproceedings{rosenthal2017semeval,\n  title={SemEval-2017 task 4: Sentiment analysis in Twitter},\n  author={Rosenthal, Sara and Farra, Noura and Nakov, Preslav},\n  booktitle={Proceedings of the 11th international workshop on semantic evaluation (SemEval-2017)},\n  pages={502--518},\n  year={2017}\n}\n```\n\n#### Stance Detection:\n```\n@inproceedings{mohammad2016semeval,\n  title={Semeval-2016 task 6: Detecting stance in tweets},\n  author={Mohammad, Saif and Kiritchenko, Svetlana and Sobhani, Parinaz and Zhu, Xiaodan and Cherry, Colin},\n  booktitle={Proceedings of the 10th International Workshop on Semantic Evaluation (SemEval-2016)},\n  pages={31--41},\n  year={2016}\n}\n```\n",
  "external_links_in_readme": [
    "https://www.aclweb.org/anthology/S19-2007\",",
    "https://www.aclweb.org/anthology/S19-2010/",
    "https://www.aclweb.org/anthology/S18-1001/",
    "https://github.com/cardiffnlp/tweeteval/tree/main/datasets",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-emotion",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base",
    "https://arxiv.org/pdf/2010.12421.pdf",
    "https://arxiv.org/abs/2005.10200",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-sentiment",
    "https://www.aclweb.org/anthology/S18-1003.pdf",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-irony",
    "https://github.com/cardiffnlp/xlm-t",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-emoji",
    "https://arxiv.org/abs/2202.03829",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-offensive",
    "https://www.aclweb.org/anthology/S18-1005.pdf",
    "https://huggingface.co/cardiffnlp/twitter-roberta-base-hate",
    "https://colab.research.google.com/drive/18cNn4cJ-bAi-Luiqi8V6c09Tj-iiX0oG",
    "https://www.aclweb.org/anthology/S17-2088/",
    "https://www.aclweb.org/anthology/S16-1003/",
    "https://www.aclweb.org/anthology/S19-2007.pdf",
    "https://tweetnlp.org"
  ]
}
```

</details>


---

