# GitHub Data for facebook_bart-large-cnn

**Task Category:** Summarization

## Repository 1: pytorch/fairseq

# GitHub Repository Data

**Repository:** [facebookresearch/fairseq](https://github.com/facebookresearch/fairseq)

## Basic Information

- **Description:** Facebook AI Research Sequence-to-Sequence Toolkit written in Python.
- **Created:** 2017-08-29T16:26:12+00:00
- **Last Updated:** 2025-06-22T01:39:25+00:00
- **Last Pushed:** 2025-06-10T21:41:39+00:00
- **Default Branch:** main
- **Size:** 26077 KB

## Statistics

- **Stars:** 31,553
- **Forks:** 6,545
- **Watchers:** 31,553
- **Open Issues:** 1,332
- **Total Issues:** 0
- **Pull Requests:** 1,367

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/facebookresearch/fairseq/blob/main/LICENSE)

## Languages

- **Python:** 4,294,275 bytes
- **Cuda:** 38,178 bytes
- **C++:** 21,106 bytes
- **Cython:** 13,294 bytes
- **Lua:** 4,210 bytes
- **Shell:** 2,182 bytes

## Topics

- `python`
- `pytorch`
- `artificial-intelligence`

## Top Contributors

1. **alexeib** - 177 contributions
2. **kahne** - 50 contributions
3. **cndn** - 36 contributions
4. **tangyuq** - 36 contributions
5. **dianaml0** - 36 contributions
6. **sshleifer** - 26 contributions
7. **theweiho** - 25 contributions
8. **sravyapopuri388** - 25 contributions
9. **louismartin** - 24 contributions
10. **huihuifan** - 22 contributions

## File Structure (Sample of 10 files)

Total files: 1,999

- `.github` (tree)
- `.github/CODEOWNERS` (blob)
- `.github/ISSUE_TEMPLATE.md` (blob)
- `.github/ISSUE_TEMPLATE` (tree)
- `.github/ISSUE_TEMPLATE/bug_report.md` (blob)
- `.github/ISSUE_TEMPLATE/documentation.md` (blob)
- `.github/ISSUE_TEMPLATE/feature_request.md` (blob)
- `.github/ISSUE_TEMPLATE/how-to-question.md` (blob)
- `.github/PULL_REQUEST_TEMPLATE.md` (blob)
- `.github/stale.yml` (blob)

## Recent Issues

- 游릭 **#5622** Question regarding the model size, token limits and model optimizaion? (open)
- 游릭 **#5621** Why am I running nllb-200 on A100 with only 30% GPU utilization? Can you help me solve it (open)
- 游릭 **#5620** Installation fails on Windows: Missing stddef.h while building fairseq C++ extension (libbleu.cpp) (open)
- 游댮 **#5619** Remove index caching support from fasta due to security concerns (closed)
- 游릭 **#5618** Using RoBERTa with more than 512 tokens (open)

## Recent Pull Requests

- 游댮 **#5619** Remove index caching support from fasta due to security concerns (closed)
- 游릭 **#5616** Add option to disable EOS appending in preprocessing (#5615) (open)
- 游릭 **#5612** Please Fix dependency issues? (open)
- 游릭 **#5611** Pt2.6 compatibility (open)
- 游댮 **#5604** Inference (closed)

## Recent Commits

- **d13e14a8** Remove index caching support from fasta due to security concerns (#5619) - Can Balioglu (2025-06-10T21:41:36+00:00)
- **ecbf110e** Add an `auto_expand` option to `SinusoidalPositionalEmbedding` (#5555) - Yun Wang (Maigo) (2024-10-18T16:40:02+00:00)
- **018621f3** update paper link + bug fix (#5547) - Brian Yan (2024-10-03T15:50:44+00:00)
- **c2145111** Add LID rerank for MMS (#5545) - Brian Yan (2024-09-27T02:13:41+00:00)
- **920a548c** Create README.md (#5529) - Vineel Pratap (2024-07-22T08:17:18+00:00)
- **d9a62708** Create depreview.yml (#5501) - Jon Janzen (2024-05-30T21:44:49+00:00)
- **bedb259b** Delete .circleci directory (#5458) - Jon Janzen (2024-03-13T13:24:17+00:00)
- **34973a94** Multires hubert (#5363) - Jiatong (2024-02-26T20:15:44+00:00)
- **3f0f20f2** MMS alignment README fixes (#5432) - Rapha칢l Merx (2024-01-24T18:54:38+00:00)
- **fad2c4d1** Update README.md (#5407) - Yoach Lacombe (2024-01-08T22:38:14+00:00)

## External Links Found in README

- https://github.com/pytorch/fairseq/blob/main/LICENSE"><img
- https://github.com/pytorch/fairseq/releases/tag/v0.10.0
- https://github.com/pytorch/fairseq/tree/main/examples/backtranslation#training-your-own-model-wmt18-english-german
- https://arxiv.org/pdf/2109.14084.pdf
- https://aclanthology.org/2021.findings-acl.370.pdf
- https://github.com/facebookresearch/hydra
- https://twitter.com/fairseq
- https://fairseq.readthedocs.io/en/latest/getting_started.html#training-with-half-precision-floating-point-fp16
- https://github.com/NVIDIA/nccl
- https://arxiv.org/abs/2105.11084
- https://arxiv.org/abs/1610.02424
- https://arxiv.org/abs/2010.11430
- https://app.circleci.com/pipelines/github/facebookresearch/fairseq/"><img
- https://pytorch.org/hub/pytorch_fairseq_translation/
- https://developer.nvidia.com/tensor-cores
- https://github.com/pytorch/fairseq/releases/tag/v0.9.0
- https://facebookresearch.github.io/vizseq/docs/getting_started/fairseq_example
- https://arrow.apache.org/docs/python/install.html#using-pip
- https://arxiv.org/abs/2006.13979
- https://github.com/facebookresearch/xformers

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 101782647,
  "name": "fairseq",
  "full_name": "facebookresearch/fairseq",
  "description": "Facebook AI Research Sequence-to-Sequence Toolkit written in Python.",
  "html_url": "https://github.com/facebookresearch/fairseq",
  "clone_url": "https://github.com/facebookresearch/fairseq.git",
  "ssh_url": "git@github.com:facebookresearch/fairseq.git",
  "homepage": null,
  "topics": [
    "python",
    "pytorch",
    "artificial-intelligence"
  ],
  "default_branch": "main",
  "created_at": "2017-08-29T16:26:12+00:00",
  "updated_at": "2025-06-22T01:39:25+00:00",
  "pushed_at": "2025-06-10T21:41:39+00:00",
  "size_kb": 26077,
  "watchers_count": 31553,
  "stargazers_count": 31553,
  "forks_count": 6545,
  "open_issues_count": 1332,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/facebookresearch/fairseq/blob/main/LICENSE"
  },
  "languages": {
    "Python": 4294275,
    "Cuda": 38178,
    "C++": 21106,
    "Cython": 13294,
    "Lua": 4210,
    "Shell": 2182
  },
  "top_contributors": [
    {
      "login": "alexeib",
      "contributions": 177
    },
    {
      "login": "kahne",
      "contributions": 50
    },
    {
      "login": "cndn",
      "contributions": 36
    },
    {
      "login": "tangyuq",
      "contributions": 36
    },
    {
      "login": "dianaml0",
      "contributions": 36
    },
    {
      "login": "sshleifer",
      "contributions": 26
    },
    {
      "login": "theweiho",
      "contributions": 25
    },
    {
      "login": "sravyapopuri388",
      "contributions": 25
    },
    {
      "login": "louismartin",
      "contributions": 24
    },
    {
      "login": "huihuifan",
      "contributions": 22
    },
    {
      "login": "liezl200",
      "contributions": 21
    },
    {
      "login": "freewym",
      "contributions": 21
    },
    {
      "login": "edunov",
      "contributions": 18
    },
    {
      "login": "xutaima",
      "contributions": 18
    },
    {
      "login": "MaigoAkisame",
      "contributions": 18
    },
    {
      "login": "xu-song",
      "contributions": 16
    },
    {
      "login": "jhcross",
      "contributions": 14
    },
    {
      "login": "pipibjc",
      "contributions": 14
    },
    {
      "login": "Mortimerp9",
      "contributions": 14
    },
    {
      "login": "vineelpratap",
      "contributions": 14
    }
  ],
  "file_tree_count": 1999,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/CODEOWNERS",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE.md",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/bug_report.md",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/documentation.md",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/feature_request.md",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/how-to-question.md",
      "type": "blob"
    },
    {
      "path": ".github/PULL_REQUEST_TEMPLATE.md",
      "type": "blob"
    },
    {
      "path": ".github/stale.yml",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 1367,
  "recent_issues": [
    {
      "number": 5622,
      "title": "Question regarding the model size, token limits and model optimizaion?",
      "state": "open"
    },
    {
      "number": 5621,
      "title": "Why am I running nllb-200 on A100 with only 30% GPU utilization? Can you help me solve it",
      "state": "open"
    },
    {
      "number": 5620,
      "title": "Installation fails on Windows: Missing stddef.h while building fairseq C++ extension (libbleu.cpp)",
      "state": "open"
    },
    {
      "number": 5619,
      "title": "Remove index caching support from fasta due to security concerns",
      "state": "closed"
    },
    {
      "number": 5618,
      "title": "Using RoBERTa with more than 512 tokens",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 5619,
      "title": "Remove index caching support from fasta due to security concerns",
      "state": "closed"
    },
    {
      "number": 5616,
      "title": "Add option to disable EOS appending in preprocessing (#5615)",
      "state": "open"
    },
    {
      "number": 5612,
      "title": "Please Fix dependency issues?",
      "state": "open"
    },
    {
      "number": 5611,
      "title": "Pt2.6 compatibility",
      "state": "open"
    },
    {
      "number": 5604,
      "title": "Inference",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "d13e14a800bb588e5a77fb4e551f554ff9b24a72",
      "author": "Can Balioglu",
      "date": "2025-06-10T21:41:36+00:00",
      "message": "Remove index caching support from fasta due to security concerns (#5619)"
    },
    {
      "sha": "ecbf110e1eb43861214b05fa001eff584954f65a",
      "author": "Yun Wang (Maigo)",
      "date": "2024-10-18T16:40:02+00:00",
      "message": "Add an `auto_expand` option to `SinusoidalPositionalEmbedding` (#5555)"
    },
    {
      "sha": "018621f3cca02ca9de945dc082c3fb1a7f9f2deb",
      "author": "Brian Yan",
      "date": "2024-10-03T15:50:44+00:00",
      "message": "update paper link + bug fix (#5547)"
    },
    {
      "sha": "c2145111e77312cb09df1d61114bb9a902afe57b",
      "author": "Brian Yan",
      "date": "2024-09-27T02:13:41+00:00",
      "message": "Add LID rerank for MMS (#5545)"
    },
    {
      "sha": "920a548ca770fb1a951f7f4289b4d3a0c1bc226f",
      "author": "Vineel Pratap",
      "date": "2024-07-22T08:17:18+00:00",
      "message": "Create README.md (#5529)"
    },
    {
      "sha": "d9a627082fd03ec72a27a31a4e56289bfcb2e4e4",
      "author": "Jon Janzen",
      "date": "2024-05-30T21:44:49+00:00",
      "message": "Create depreview.yml (#5501)"
    },
    {
      "sha": "bedb259bf34a9fc22073c13a1cee23192fa70ef3",
      "author": "Jon Janzen",
      "date": "2024-03-13T13:24:17+00:00",
      "message": "Delete .circleci directory (#5458)"
    },
    {
      "sha": "34973a94d09ecc12092a5ecc8afece5e536b7692",
      "author": "Jiatong",
      "date": "2024-02-26T20:15:44+00:00",
      "message": "Multires hubert (#5363)"
    },
    {
      "sha": "3f0f20f2d12403629224347664b3e75c13b2c8e0",
      "author": "Rapha\u00ebl Merx",
      "date": "2024-01-24T18:54:38+00:00",
      "message": "MMS alignment README fixes (#5432)"
    },
    {
      "sha": "fad2c4d1ebe14d974876de52dcb06db6d99b0b4a",
      "author": "Yoach Lacombe",
      "date": "2024-01-08T22:38:14+00:00",
      "message": "Update README.md (#5407)"
    },
    {
      "sha": "da8fb630880d529ab47e53381c30ddc8ad235216",
      "author": "Can Balioglu",
      "date": "2023-10-10T17:36:53+00:00",
      "message": "Change Meta AI to FAIR (#5346)"
    },
    {
      "sha": "c7c478b92fe135838a2b9ec8341495c732a92401",
      "author": "Junteng Jia",
      "date": "2023-10-09T21:13:06+00:00",
      "message": "fix iterator when loading from checkpoint (#5344)"
    },
    {
      "sha": "7409af7f9a7b6ddac4cbfe7cafccc715b3c1b21e",
      "author": "Piyush Kansal",
      "date": "2023-09-15T23:15:19+00:00",
      "message": "Keep task level checkpoint key name generic (#5330)"
    },
    {
      "sha": "e29f53bfea67fd9e81c3da374daac4b472ba6bda",
      "author": "Piyush Kansal",
      "date": "2023-09-15T19:01:49+00:00",
      "message": "initial revision (#5328)"
    },
    {
      "sha": "b5d89cddc9e4a0af831d2aafc1ba7dbf0f1b10d0",
      "author": "Vineel Pratap",
      "date": "2023-09-07T18:25:28+00:00",
      "message": "Update align_and_segment.py (#5317)"
    },
    {
      "sha": "4db264940f281a6f47558d17387b1455d4abd8d9",
      "author": "Nguyen Tu Anh",
      "date": "2023-08-18T15:10:40+00:00",
      "message": "Add batchnorm option to hubert/wav2vec2 positional convolution layer for hubert bf16 models (#5285)"
    },
    {
      "sha": "100cd91db19bb27277a06a25eb4154c805b10189",
      "author": "Egor Lakomkin",
      "date": "2023-07-07T06:08:01+00:00",
      "message": "Make RotaryPositionalEmbedding jit-compatible (#5237)"
    },
    {
      "sha": "31fba013a070eaff69dec8642e68e7134d60ab0f",
      "author": "Yun Wang (Maigo)",
      "date": "2023-06-23T17:31:52+00:00",
      "message": "Register `weights` as a non-persistent buffer of `SinusoidalPositionalEmbedding` (#5213)"
    },
    {
      "sha": "a29952ce6d313a4daf3e90647f8bf84cc6d4df6d",
      "author": "Patrick von Platen",
      "date": "2023-06-20T17:15:05+00:00",
      "message": "Update README.md (#5211)"
    },
    {
      "sha": "8deb43af8c54d6840e5ba6e057acf715c4491f9c",
      "author": "Andros Tjandra",
      "date": "2023-06-17T20:43:16+00:00",
      "message": "add new instructions on how to get manifest *.tsv file (#5207)"
    }
  ],
  "readme_text": "<p align=\"center\">\n  <img src=\"docs/fairseq_logo.png\" width=\"150\">\n  <br />\n  <br />\n  <a href=\"https://opensource.fb.com/support-ukraine\"><img alt=\"Support Ukraine\" src=\"https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB\" /></a>\n  <a href=\"https://github.com/pytorch/fairseq/blob/main/LICENSE\"><img alt=\"MIT License\" src=\"https://img.shields.io/badge/license-MIT-blue.svg\" /></a>\n  <a href=\"https://github.com/pytorch/fairseq/releases\"><img alt=\"Latest Release\" src=\"https://img.shields.io/github/release/pytorch/fairseq.svg\" /></a>\n  <a href=\"https://github.com/pytorch/fairseq/actions?query=workflow:build\"><img alt=\"Build Status\" src=\"https://github.com/pytorch/fairseq/workflows/build/badge.svg\" /></a>\n  <a href=\"https://fairseq.readthedocs.io/en/latest/?badge=latest\"><img alt=\"Documentation Status\" src=\"https://readthedocs.org/projects/fairseq/badge/?version=latest\" /></a>\n  <a href=\"https://app.circleci.com/pipelines/github/facebookresearch/fairseq/\"><img alt=\"CicleCI Status\" src=\"https://circleci.com/gh/facebookresearch/fairseq.svg?style=shield\" /></a>\n</p>\n\n--------------------------------------------------------------------------------\n\nFairseq(-py) is a sequence modeling toolkit that allows researchers and\ndevelopers to train custom models for translation, summarization, language\nmodeling and other text generation tasks.\n\nWe provide reference implementations of various sequence modeling papers:\n\n<details><summary>List of implemented papers</summary><p>\n\n* **Convolutional Neural Networks (CNN)**\n  + [Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)](examples/language_model/conv_lm/README.md)\n  + [Convolutional Sequence to Sequence Learning (Gehring et al., 2017)](examples/conv_seq2seq/README.md)\n  + [Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)](https://github.com/pytorch/fairseq/tree/classic_seqlevel)\n  + [Hierarchical Neural Story Generation (Fan et al., 2018)](examples/stories/README.md)\n  + [wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)](examples/wav2vec/README.md)\n* **LightConv and DynamicConv models**\n  + [Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)](examples/pay_less_attention_paper/README.md)\n* **Long Short-Term Memory (LSTM) networks**\n  + Effective Approaches to Attention-based Neural Machine Translation (Luong et al., 2015)\n* **Transformer (self-attention) networks**\n  + Attention Is All You Need (Vaswani et al., 2017)\n  + [Scaling Neural Machine Translation (Ott et al., 2018)](examples/scaling_nmt/README.md)\n  + [Understanding Back-Translation at Scale (Edunov et al., 2018)](examples/backtranslation/README.md)\n  + [Adaptive Input Representations for Neural Language Modeling (Baevski and Auli, 2018)](examples/language_model/README.adaptive_inputs.md)\n  + [Lexically constrained decoding with dynamic beam allocation (Post & Vilar, 2018)](examples/constrained_decoding/README.md)\n  + [Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Dai et al., 2019)](examples/truncated_bptt/README.md)\n  + [Adaptive Attention Span in Transformers (Sukhbaatar et al., 2019)](examples/adaptive_span/README.md)\n  + [Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)](examples/translation_moe/README.md)\n  + [RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)](examples/roberta/README.md)\n  + [Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)](examples/wmt19/README.md)\n  + [Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)](examples/joint_alignment_translation/README.md )\n  + [Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at., 2020)](examples/mbart/README.md)\n  + [Neural Machine Translation with Byte-Level Subwords (Wang et al., 2020)](examples/byte_level_bpe/README.md)\n  + [Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al., 2020)](examples/unsupervised_quality_estimation/README.md)\n  + [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., 2020)](examples/wav2vec/README.md)\n  + [Generating Medical Reports from Patient-Doctor Conversations Using Sequence-to-Sequence Models (Enarvi et al., 2020)](examples/pointer_generator/README.md)\n  + [Linformer: Self-Attention with Linear Complexity (Wang et al., 2020)](examples/linformer/README.md)\n  + [Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al., 2020)](examples/criss/README.md)\n  + [Deep Transformers with Latent Depth (Li et al., 2020)](examples/latent_depth/README.md)\n  + [Unsupervised Cross-lingual Representation Learning for Speech Recognition (Conneau et al., 2020)](https://arxiv.org/abs/2006.13979)\n  + [Self-training and Pre-training are Complementary for Speech Recognition (Xu et al., 2020)](https://arxiv.org/abs/2010.11430)\n  + [Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training (Hsu, et al., 2021)](https://arxiv.org/abs/2104.01027)\n  + [Unsupervised Speech Recognition (Baevski, et al., 2021)](https://arxiv.org/abs/2105.11084)\n  + [Simple and Effective Zero-shot Cross-lingual Phoneme Recognition (Xu et al., 2021)](https://arxiv.org/abs/2109.11680)\n  + [VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding (Xu et. al., 2021)](https://arxiv.org/pdf/2109.14084.pdf)\n  + [VLM: Task-agnostic Video-Language Model Pre-training for Video Understanding (Xu et. al., 2021)](https://aclanthology.org/2021.findings-acl.370.pdf)\n  + [NormFormer: Improved Transformer Pretraining with Extra Normalization (Shleifer et. al, 2021)](examples/normformer/README.md)\n* **Non-autoregressive Transformers**\n  + Non-Autoregressive Neural Machine Translation (Gu et al., 2017)\n  + Deterministic Non-Autoregressive Neural Sequence Modeling by Iterative Refinement (Lee et al. 2018)\n  + Insertion Transformer: Flexible Sequence Generation via Insertion Operations (Stern et al. 2019)\n  + Mask-Predict: Parallel Decoding of Conditional Masked Language Models (Ghazvininejad et al., 2019)\n  + [Levenshtein Transformer (Gu et al., 2019)](examples/nonautoregressive_translation/README.md)\n* **Finetuning**\n  + [Better Fine-Tuning by Reducing Representational Collapse (Aghajanyan et al. 2020)](examples/rxf/README.md)\n\n</p></details>\n\n### What's New:\n* May 2023 [Released models for Scaling Speech Technology to 1,000+ Languages  (Pratap, et al., 2023)](examples/mms/README.md)\n* June 2022 [Released code for wav2vec-U 2.0 from Towards End-to-end Unsupervised Speech Recognition (Liu, et al., 2022)](examples/wav2vec/unsupervised/README.md)\n* May 2022 [Integration with xFormers](https://github.com/facebookresearch/xformers)\n* December 2021 [Released Direct speech-to-speech translation code](examples/speech_to_speech/README.md)\n* October 2021 [Released VideoCLIP and VLM models](examples/MMPT/README.md)\n* October 2021 [Released multilingual finetuned XLSR-53 model](examples/wav2vec/README.md)\n* September 2021 [`master` branch renamed to `main`](https://github.com/github/renaming).\n* July 2021 [Released DrNMT code](examples/discriminative_reranking_nmt/README.md)\n* July 2021 [Released Robust wav2vec 2.0 model](examples/wav2vec/README.md)\n* June 2021 [Released XLMR-XL and XLMR-XXL models](examples/xlmr/README.md)\n* May 2021 [Released Unsupervised Speech Recognition code](examples/wav2vec/unsupervised/README.md)\n* March 2021 [Added full parameter and optimizer state sharding + CPU offloading](examples/fully_sharded_data_parallel/README.md)\n* February 2021 [Added LASER training code](examples/laser/README.md)\n* December 2020: [Added Adaptive Attention Span code](examples/adaptive_span/README.md)\n* December 2020: [GottBERT model and code released](examples/gottbert/README.md)\n* November 2020: Adopted the [Hydra](https://github.com/facebookresearch/hydra) configuration framework\n  * [see documentation explaining how to use it for new and existing projects](docs/hydra_integration.md)\n* November 2020: [fairseq 0.10.0 released](https://github.com/pytorch/fairseq/releases/tag/v0.10.0)\n* October 2020: [Added R3F/R4F (Better Fine-Tuning) code](examples/rxf/README.md)\n* October 2020: [Deep Transformer with Latent Depth code released](examples/latent_depth/README.md)\n* October 2020: [Added CRISS models and code](examples/criss/README.md)\n\n<details><summary>Previous updates</summary><p>\n\n* September 2020: [Added Linformer code](examples/linformer/README.md)\n* September 2020: [Added pointer-generator networks](examples/pointer_generator/README.md)\n* August 2020: [Added lexically constrained decoding](examples/constrained_decoding/README.md)\n* August 2020: [wav2vec2 models and code released](examples/wav2vec/README.md)\n* July 2020: [Unsupervised Quality Estimation code released](examples/unsupervised_quality_estimation/README.md)\n* May 2020: [Follow fairseq on Twitter](https://twitter.com/fairseq)\n* April 2020: [Monotonic Multihead Attention code released](examples/simultaneous_translation/README.md)\n* April 2020: [Quant-Noise code released](examples/quant_noise/README.md)\n* April 2020: [Initial model parallel support and 11B parameters unidirectional LM released](examples/megatron_11b/README.md)\n* March 2020: [Byte-level BPE code released](examples/byte_level_bpe/README.md)\n* February 2020: [mBART model and code released](examples/mbart/README.md)\n* February 2020: [Added tutorial for back-translation](https://github.com/pytorch/fairseq/tree/main/examples/backtranslation#training-your-own-model-wmt18-english-german)\n* December 2019: [fairseq 0.9.0 released](https://github.com/pytorch/fairseq/releases/tag/v0.9.0)\n* November 2019: [VizSeq released (a visual analysis toolkit for evaluating fairseq models)](https://facebookresearch.github.io/vizseq/docs/getting_started/fairseq_example)\n* November 2019: [CamemBERT model and code released](examples/camembert/README.md)\n* November 2019: [BART model and code released](examples/bart/README.md)\n* November 2019: [XLM-R models and code released](examples/xlmr/README.md)\n* September 2019: [Nonautoregressive translation code released](examples/nonautoregressive_translation/README.md)\n* August 2019: [WMT'19 models released](examples/wmt19/README.md)\n* July 2019: fairseq relicensed under MIT license\n* July 2019: [RoBERTa models and code released](examples/roberta/README.md)\n* June 2019: [wav2vec models and code released](examples/wav2vec/README.md)\n\n</p></details>\n\n### Features:\n\n* multi-GPU training on one machine or across multiple machines (data and model parallel)\n* fast generation on both CPU and GPU with multiple search algorithms implemented:\n  + beam search\n  + Diverse Beam Search ([Vijayakumar et al., 2016](https://arxiv.org/abs/1610.02424))\n  + sampling (unconstrained, top-k and top-p/nucleus)\n  + [lexically constrained decoding](examples/constrained_decoding/README.md) (Post & Vilar, 2018)\n* [gradient accumulation](https://fairseq.readthedocs.io/en/latest/getting_started.html#large-mini-batch-training-with-delayed-updates) enables training with large mini-batches even on a single GPU\n* [mixed precision training](https://fairseq.readthedocs.io/en/latest/getting_started.html#training-with-half-precision-floating-point-fp16) (trains faster with less GPU memory on [NVIDIA tensor cores](https://developer.nvidia.com/tensor-cores))\n* [extensible](https://fairseq.readthedocs.io/en/latest/overview.html): easily register new models, criterions, tasks, optimizers and learning rate schedulers\n* [flexible configuration](docs/hydra_integration.md) based on [Hydra](https://github.com/facebookresearch/hydra) allowing a combination of code, command-line and file based configuration\n* [full parameter and optimizer state sharding](examples/fully_sharded_data_parallel/README.md)\n* [offloading parameters to CPU](examples/fully_sharded_data_parallel/README.md)\n\nWe also provide [pre-trained models for translation and language modeling](#pre-trained-models-and-examples)\nwith a convenient `torch.hub` interface:\n\n``` python\nen2de = torch.hub.load('pytorch/fairseq', 'transformer.wmt19.en-de.single_model')\nen2de.translate('Hello world', beam=5)\n# 'Hallo Welt'\n```\n\nSee the PyTorch Hub tutorials for [translation](https://pytorch.org/hub/pytorch_fairseq_translation/)\nand [RoBERTa](https://pytorch.org/hub/pytorch_fairseq_roberta/) for more examples.\n\n# Requirements and Installation\n\n* [PyTorch](http://pytorch.org/) version >= 1.10.0\n* Python version >= 3.8\n* For training new models, you'll also need an NVIDIA GPU and [NCCL](https://github.com/NVIDIA/nccl)\n* **To install fairseq** and develop locally:\n\n``` bash\ngit clone https://github.com/pytorch/fairseq\ncd fairseq\npip install --editable ./\n\n# on MacOS:\n# CFLAGS=\"-stdlib=libc++\" pip install --editable ./\n\n# to install the latest stable release (0.10.x)\n# pip install fairseq\n```\n\n* **For faster training** install NVIDIA's [apex](https://github.com/NVIDIA/apex) library:\n\n``` bash\ngit clone https://github.com/NVIDIA/apex\ncd apex\npip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" \\\n  --global-option=\"--deprecated_fused_adam\" --global-option=\"--xentropy\" \\\n  --global-option=\"--fast_multihead_attn\" ./\n```\n\n* **For large datasets** install [PyArrow](https://arrow.apache.org/docs/python/install.html#using-pip): `pip install pyarrow`\n* If you use Docker make sure to increase the shared memory size either with `--ipc=host` or `--shm-size`\n as command line options to `nvidia-docker run` .\n\n# Getting Started\n\nThe [full documentation](https://fairseq.readthedocs.io/) contains instructions\nfor getting started, training new models and extending fairseq with new model\ntypes and tasks.\n\n# Pre-trained models and examples\n\nWe provide pre-trained models and pre-processed, binarized test sets for several tasks listed below,\nas well as example training and evaluation commands.\n\n* [Translation](examples/translation/README.md): convolutional and transformer models are available\n* [Language Modeling](examples/language_model/README.md): convolutional and transformer models are available\n\nWe also have more detailed READMEs to reproduce results from specific papers:\n\n* [XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale (Babu et al., 2021)](examples/wav2vec/xlsr/README.md)\n* [Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al., 2020)](examples/criss/README.md)\n* [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., 2020)](examples/wav2vec/README.md)\n* [Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al., 2020)](examples/unsupervised_quality_estimation/README.md)\n* [Training with Quantization Noise for Extreme Model Compression ({Fan*, Stock*} et al., 2020)](examples/quant_noise/README.md)\n* [Neural Machine Translation with Byte-Level Subwords (Wang et al., 2020)](examples/byte_level_bpe/README.md)\n* [Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at., 2020)](examples/mbart/README.md)\n* [Reducing Transformer Depth on Demand with Structured Dropout (Fan et al., 2019)](examples/layerdrop/README.md)\n* [Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)](examples/joint_alignment_translation/README.md)\n* [Levenshtein Transformer (Gu et al., 2019)](examples/nonautoregressive_translation/README.md)\n* [Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)](examples/wmt19/README.md)\n* [RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)](examples/roberta/README.md)\n* [wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)](examples/wav2vec/README.md)\n* [Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)](examples/translation_moe/README.md)\n* [Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)](examples/pay_less_attention_paper/README.md)\n* [Understanding Back-Translation at Scale (Edunov et al., 2018)](examples/backtranslation/README.md)\n* [Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018)](https://github.com/pytorch/fairseq/tree/classic_seqlevel)\n* [Hierarchical Neural Story Generation (Fan et al., 2018)](examples/stories/README.md)\n* [Scaling Neural Machine Translation (Ott et al., 2018)](examples/scaling_nmt/README.md)\n* [Convolutional Sequence to Sequence Learning (Gehring et al., 2017)](examples/conv_seq2seq/README.md)\n* [Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)](examples/language_model/README.conv.md)\n\n# Join the fairseq community\n\n* Twitter: https://twitter.com/fairseq\n* Facebook page: https://www.facebook.com/groups/fairseq.users\n* Google group: https://groups.google.com/forum/#!forum/fairseq-users\n\n# License\n\nfairseq(-py) is MIT-licensed.\nThe license applies to the pre-trained models as well.\n\n# Citation\n\nPlease cite as:\n\n``` bibtex\n@inproceedings{ott2019fairseq,\n  title = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},\n  author = {Myle Ott and Sergey Edunov and Alexei Baevski and Angela Fan and Sam Gross and Nathan Ng and David Grangier and Michael Auli},\n  booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},\n  year = {2019},\n}\n```\n",
  "external_links_in_readme": [
    "https://github.com/pytorch/fairseq/blob/main/LICENSE\"><img",
    "https://github.com/pytorch/fairseq/releases/tag/v0.10.0",
    "https://github.com/pytorch/fairseq/tree/main/examples/backtranslation#training-your-own-model-wmt18-english-german",
    "https://arxiv.org/pdf/2109.14084.pdf",
    "https://aclanthology.org/2021.findings-acl.370.pdf",
    "https://github.com/facebookresearch/hydra",
    "https://twitter.com/fairseq",
    "https://fairseq.readthedocs.io/en/latest/getting_started.html#training-with-half-precision-floating-point-fp16",
    "https://github.com/NVIDIA/nccl",
    "https://arxiv.org/abs/2105.11084",
    "https://arxiv.org/abs/1610.02424",
    "https://arxiv.org/abs/2010.11430",
    "https://app.circleci.com/pipelines/github/facebookresearch/fairseq/\"><img",
    "https://pytorch.org/hub/pytorch_fairseq_translation/",
    "https://developer.nvidia.com/tensor-cores",
    "https://github.com/pytorch/fairseq/releases/tag/v0.9.0",
    "https://facebookresearch.github.io/vizseq/docs/getting_started/fairseq_example",
    "https://arrow.apache.org/docs/python/install.html#using-pip",
    "https://arxiv.org/abs/2006.13979",
    "https://github.com/facebookresearch/xformers",
    "https://github.com/pytorch/fairseq/releases\"><img",
    "https://circleci.com/gh/facebookresearch/fairseq.svg?style=shield\"",
    "http://pytorch.org/",
    "https://fairseq.readthedocs.io/",
    "https://github.com/pytorch/fairseq",
    "https://img.shields.io/badge/license-MIT-blue.svg\"",
    "https://github.com/pytorch/fairseq/actions?query=workflow:build\"><img",
    "https://github.com/pytorch/fairseq/workflows/build/badge.svg\"",
    "https://groups.google.com/forum/#!forum/fairseq-users",
    "https://fairseq.readthedocs.io/en/latest/overview.html",
    "https://arxiv.org/abs/2109.11680",
    "https://arxiv.org/abs/2104.01027",
    "https://pytorch.org/hub/pytorch_fairseq_roberta/",
    "https://opensource.fb.com/support-ukraine\"><img",
    "https://fairseq.readthedocs.io/en/latest/getting_started.html#large-mini-batch-training-with-delayed-updates",
    "https://img.shields.io/github/release/pytorch/fairseq.svg\"",
    "https://github.com/NVIDIA/apex",
    "https://readthedocs.org/projects/fairseq/badge/?version=latest\"",
    "https://github.com/github/renaming",
    "https://www.facebook.com/groups/fairseq.users",
    "https://fairseq.readthedocs.io/en/latest/?badge=latest\"><img",
    "https://img.shields.io/badge/Support-Ukraine-FFD500?style=flat&labelColor=005BBB\"",
    "https://github.com/pytorch/fairseq/tree/classic_seqlevel"
  ]
}
```

</details>


---

