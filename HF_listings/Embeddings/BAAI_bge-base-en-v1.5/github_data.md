# GitHub Data for BAAI_bge-base-en-v1.5

**Task Category:** Embeddings

## Repository 1: FlagOpen/FlagEmbedding

# GitHub Repository Data

**Repository:** [FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## Basic Information

- **Description:** Retrieval and Retrieval-augmented LLMs
- **Created:** 2023-08-02T02:08:11+00:00
- **Last Updated:** 2025-06-21T10:44:22+00:00
- **Last Pushed:** 2025-06-04T10:00:26+00:00
- **Default Branch:** master
- **Size:** 52310 KB

## Statistics

- **Stars:** 9,974
- **Forks:** 737
- **Watchers:** 9,974
- **Open Issues:** 847
- **Total Issues:** 0
- **Pull Requests:** 217

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE)

## Languages

- **Python:** 3,871,341 bytes
- **Jupyter Notebook:** 2,607,506 bytes
- **Shell:** 7,322 bytes

## Topics

- `embeddings`
- `information-retrieval`
- `llm`
- `sentence-embeddings`
- `text-semantic-similarity`
- `retrieval-augmented-generation`

## Top Contributors

1. **hanhainebula** - 350 contributions
2. **545999961** - 327 contributions
3. **staoxiao** - 182 contributions
4. **ZiyiXia** - 113 contributions
5. **namespace-Pt** - 101 contributions
6. **JUNJIE99** - 78 contributions
7. **ftgreat** - 20 contributions
8. **ZhengLiu101** - 15 contributions
9. **Muennighoff** - 12 contributions
10. **shuyansy** - 10 contributions

## File Structure (Sample of 10 files)

Total files: 1,395

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.gitignore` (blob)
- `FlagEmbedding` (tree)
- `FlagEmbedding/__init__.py` (blob)
- `FlagEmbedding/abc` (tree)
- `FlagEmbedding/abc/__init__.py` (blob)
- `FlagEmbedding/abc/evaluation` (tree)
- `FlagEmbedding/abc/evaluation/__init__.py` (blob)

## Recent Issues

- 🟢 **#1480** Continued pretraining using RetroMAE (open)
- 🟢 **#1479** 微调bge-reranker-base一直卡顿，无日志输出 (open)
- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1477** 训练过程中train_loss和eval_loss都是先下降后上升再缓慢下降 (open)
- 🟢 **#1476** How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline (open)

## Recent Pull Requests

- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1475** Add inline comment to setup.py (open)
- 🟢 **#1469** Extend tutorials to include cloud deployment options (open)
- 🔴 **#1466** update reranker inference (closed)
- 🔴 **#1465** update environment (closed)

## Recent Commits

- **875fd4ff** update docs - ZiyiXia (2025-06-04T09:57:10+00:00)
- **235f967a** update tutorials - ZiyiXia (2025-06-04T09:27:36+00:00)
- **5e64baa6** Merge pull request #1466 from 545999961/master - chaofan (2025-05-28T08:15:17+00:00)
- **018b984e** update reranker inference - cfli (2025-05-28T08:14:32+00:00)
- **d04f7989** release version 1.3.5 - chaofan (2025-05-28T07:12:19+00:00)
- **8576fde7** Merge pull request #1465 from 545999961/master - chaofan (2025-05-28T06:58:53+00:00)
- **bf410d51** update environment - cfli (2025-05-28T06:55:54+00:00)
- **97ca0732** Merge pull request #1464 from 545999961/master - chaofan (2025-05-28T06:18:23+00:00)
- **b51706d6** upload coder eval script - cfli (2025-05-28T06:17:19+00:00)
- **821c3879** upload coder eval script - cfli (2025-05-28T06:12:16+00:00)

## External Links Found in README

- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation
- https://arxiv.org/pdf/2310.07554.pdf
- https://huggingface.co/BAAI/bge-reranker-base
- https://arxiv.org/pdf/2409.05591v1
- https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset
- https://img.shields.io/badge/BGE_series-🤗-yellow">
- https://huggingface.co/Shitao
- https://huggingface.co/BAAI/bge-base-en
- https://huggingface.co/BAAI/llm-embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder
- https://huggingface.co/BAAI/bge-reranker-large
- https://img.shields.io/badge/Contribution-Welcome-blue">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder
- https://huggingface.co/BAAI/bge-small-en-v1.5
- https://huggingface.co/BAAI/bge-m3
- https://github.com/VectorSpaceLab/OmniGen
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge
- https://github.com/VectorSpaceLab/MegaPairs

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673596675,
  "name": "FlagEmbedding",
  "full_name": "FlagOpen/FlagEmbedding",
  "description": "Retrieval and Retrieval-augmented LLMs",
  "html_url": "https://github.com/FlagOpen/FlagEmbedding",
  "clone_url": "https://github.com/FlagOpen/FlagEmbedding.git",
  "ssh_url": "git@github.com:FlagOpen/FlagEmbedding.git",
  "homepage": "http://www.bge-model.com/",
  "topics": [
    "embeddings",
    "information-retrieval",
    "llm",
    "sentence-embeddings",
    "text-semantic-similarity",
    "retrieval-augmented-generation"
  ],
  "default_branch": "master",
  "created_at": "2023-08-02T02:08:11+00:00",
  "updated_at": "2025-06-21T10:44:22+00:00",
  "pushed_at": "2025-06-04T10:00:26+00:00",
  "size_kb": 52310,
  "watchers_count": 9974,
  "stargazers_count": 9974,
  "forks_count": 737,
  "open_issues_count": 847,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"
  },
  "languages": {
    "Python": 3871341,
    "Jupyter Notebook": 2607506,
    "Shell": 7322
  },
  "top_contributors": [
    {
      "login": "hanhainebula",
      "contributions": 350
    },
    {
      "login": "545999961",
      "contributions": 327
    },
    {
      "login": "staoxiao",
      "contributions": 182
    },
    {
      "login": "ZiyiXia",
      "contributions": 113
    },
    {
      "login": "namespace-Pt",
      "contributions": 101
    },
    {
      "login": "JUNJIE99",
      "contributions": 78
    },
    {
      "login": "ftgreat",
      "contributions": 20
    },
    {
      "login": "ZhengLiu101",
      "contributions": 15
    },
    {
      "login": "Muennighoff",
      "contributions": 12
    },
    {
      "login": "shuyansy",
      "contributions": 10
    },
    {
      "login": "rainym00d",
      "contributions": 3
    },
    {
      "login": "chinainfant",
      "contributions": 3
    },
    {
      "login": "GeoloeG-IsT",
      "contributions": 3
    },
    {
      "login": "blue-vision0",
      "contributions": 2
    },
    {
      "login": "muazhari",
      "contributions": 2
    },
    {
      "login": "luisegarduno",
      "contributions": 2
    },
    {
      "login": "khazic",
      "contributions": 2
    },
    {
      "login": "shtdbb",
      "contributions": 2
    },
    {
      "login": "pengjunfeng11",
      "contributions": 2
    },
    {
      "login": "ycjcl868",
      "contributions": 2
    }
  ],
  "file_tree_count": 1395,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc/evaluation",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/evaluation/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 217,
  "recent_issues": [
    {
      "number": 1480,
      "title": "Continued pretraining using RetroMAE",
      "state": "open"
    },
    {
      "number": 1479,
      "title": "\u5fae\u8c03bge-reranker-base\u4e00\u76f4\u5361\u987f\uff0c\u65e0\u65e5\u5fd7\u8f93\u51fa",
      "state": "open"
    },
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1477,
      "title": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2dtrain_loss\u548ceval_loss\u90fd\u662f\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u518d\u7f13\u6162\u4e0b\u964d",
      "state": "open"
    },
    {
      "number": 1476,
      "title": "How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1475,
      "title": "Add inline comment to setup.py",
      "state": "open"
    },
    {
      "number": 1469,
      "title": "Extend tutorials to include cloud deployment options",
      "state": "open"
    },
    {
      "number": 1466,
      "title": "update reranker inference",
      "state": "closed"
    },
    {
      "number": 1465,
      "title": "update environment",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "875fd4ffcb894313a918e8bb0c9b7ead82f37e64",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:57:10+00:00",
      "message": "update docs"
    },
    {
      "sha": "235f967a0122b9d5b85104fd4e8f36dc15ee85a8",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:27:36+00:00",
      "message": "update tutorials"
    },
    {
      "sha": "5e64baa61e75df23105a66d1e9d09ad799366e2a",
      "author": "chaofan",
      "date": "2025-05-28T08:15:17+00:00",
      "message": "Merge pull request #1466 from 545999961/master"
    },
    {
      "sha": "018b984e23e12d9793e59f2a1ef07fb9153ab08e",
      "author": "cfli",
      "date": "2025-05-28T08:14:32+00:00",
      "message": "update reranker inference"
    },
    {
      "sha": "d04f7989eeec2c6db4ead7c77ddef656f9c9d3aa",
      "author": "chaofan",
      "date": "2025-05-28T07:12:19+00:00",
      "message": "release version 1.3.5"
    },
    {
      "sha": "8576fde79b84224ecbe493857115552fcfb72ed7",
      "author": "chaofan",
      "date": "2025-05-28T06:58:53+00:00",
      "message": "Merge pull request #1465 from 545999961/master"
    },
    {
      "sha": "bf410d51e9e685e7f1b2e3816f09f3dc64ad8292",
      "author": "cfli",
      "date": "2025-05-28T06:55:54+00:00",
      "message": "update environment"
    },
    {
      "sha": "97ca07325d9e786d668f9a038502ad736dded9d5",
      "author": "chaofan",
      "date": "2025-05-28T06:18:23+00:00",
      "message": "Merge pull request #1464 from 545999961/master"
    },
    {
      "sha": "b51706d63ae8a1d86443df8b0e5ff6e45eb08fdc",
      "author": "cfli",
      "date": "2025-05-28T06:17:19+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "821c387961b5597fe672a4386232110ef54d5107",
      "author": "cfli",
      "date": "2025-05-28T06:12:16+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "3332a1956e2d5af7c52628a60e5c9788bd1b5ed9",
      "author": "chaofan",
      "date": "2025-05-22T10:07:40+00:00",
      "message": "Merge pull request #1459 from 545999961/master"
    },
    {
      "sha": "d412884a4e96c45e4a9e83a43e507e9beaa04713",
      "author": "cfli",
      "date": "2025-05-22T10:06:43+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "5c11feffc104497953a333036aa56b6b543e7cee",
      "author": "chaofan",
      "date": "2025-05-22T09:56:48+00:00",
      "message": "Merge pull request #1458 from 545999961/master"
    },
    {
      "sha": "41b526441f13242b49126261c219b6b9f14e17cb",
      "author": "cfli",
      "date": "2025-05-22T09:55:44+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "52bb9fddb5def2074b277f6e9a2f02ed8aa17dee",
      "author": "chaofan",
      "date": "2025-05-22T09:52:21+00:00",
      "message": "Merge pull request #1457 from 545999961/master"
    },
    {
      "sha": "f05d4b3fbeff5df9dabeb71ab16225fe5e1e8c95",
      "author": "cfli",
      "date": "2025-05-22T09:51:20+00:00",
      "message": "upload reinforced_ir"
    },
    {
      "sha": "25ee99c228e9df63a2be79e8d795d549ab5001e9",
      "author": "chaofan",
      "date": "2025-05-21T04:46:03+00:00",
      "message": "Merge pull request #1453 from 545999961/master"
    },
    {
      "sha": "436210bbed3ca352aac82a9be66cd4e665408944",
      "author": "cfli",
      "date": "2025-05-21T04:45:10+00:00",
      "message": "upload Matroyshka_reranker"
    },
    {
      "sha": "1946f89d5a6926871861c9ed9f4acbb31665acfe",
      "author": "cfli",
      "date": "2025-05-20T08:13:30+00:00",
      "message": "upload Matroyshka Re-Ranker"
    },
    {
      "sha": "f6924351bba09620938b69a2c933c6712be829c4",
      "author": "cfli",
      "date": "2025-05-20T08:12:08+00:00",
      "message": "upload Matroyshka Re-Ranker"
    }
  ],
  "readme_text": "[<img src=\"./imgs/FlagOpen.png\">](https://flagopen.baai.ac.cn/)\n\n<h1 align=\"center\">\u26a1\ufe0fBGE: One-Stop Retrieval Toolkit For Search and RAG</h1>\n\n![bge_logo](./imgs/bge_logo.jpg)\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Contribution-Welcome-blue\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\">\n    </a>\n    <a href=\"https://huggingface.co/C-MTEB\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=#news>News</a> |\n        <a href=#installation>Installation</a> |\n        <a href=#quick-start>Quick Start</a> |\n        <a href=#community>Community</a> |\n        <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a> |\n        <a href=#model-list>Model List</a> |\n        <a href=\"#contributors\">Contributor</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\n\n## News\n\n- 3/6/2025: :fire::fire: Introduce **BGE-VL** ([HF repo](https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92)), State-Of-The-Art multimodal embedding models to support Any visual search applications (everything, including text-to-image, image-to-text, image&prompt-to-image, text-to-image&text, and more)! They are released under the MIT license and are completely free for both academic and commercial use. We also release **MegaPairs** ([repo](https://github.com/VectorSpaceLab/MegaPairs), [paper](https://arxiv.org/abs/2412.14475)), a massive synthetic dataset which empowers BGE-VL!\n- 12/5/2024: :book: We built the [BGE documentation](https://www.bge-model.com) for centralized BGE information and materials!\n- 10/29/2024: :earth_asia: We created WeChat group for BGE. Scan the [QR code](./imgs/BGE_WeChat_Group.png) to join the group chat! To get the first hand message about our updates and new release, or having any questions or ideas, join us now!\n- <img src=\"./imgs/BGE_WeChat_Group.png\" alt=\"bge_wechat_group\" class=\"center\" width=\"200\">\n\n- 10/22/2024: We release another interesting model: [OmniGen](https://github.com/VectorSpaceLab/OmniGen), which is a unified image generation model supporting various tasks. OmniGen can accomplish complex image generation tasks without the need for additional plugins like ControlNet, IP-Adapter, or auxiliary models such as pose detection and face detection.\n- 9/10/2024: Introducing **MemoRAG**, a step forward towards RAG 2.0 on top of memory-inspired knowledge discovery (repo: https://github.com/qhjqhj00/MemoRAG, paper: https://arxiv.org/pdf/2409.05591v1) \n- 9/2/2024: Start to maintain the [tutorials](./Tutorials/). The contents within will be actively updated and eariched, stay tuned! :books:\n- 7/26/2024: Release a new embedding model [bge-en-icl](https://huggingface.co/BAAI/bge-en-icl), an embedding model that incorporates in-context learning capabilities, which, by providing task-relevant query-response examples, can encode semantically richer queries, further enhancing the semantic representation ability of the embeddings.\n- 7/26/2024: Release a new embedding model [bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2), a multilingual embedding model based on gemma-2-9b, which supports multiple languages and diverse downstream tasks, achieving new SOTA on multilingual benchmarks (MIRACL, MTEB-fr, and MTEB-pl). \n- 7/26/2024: Release a new lightweight reranker [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight), a lightweight reranker based on gemma-2-9b, which supports token compression and layerwise lightweight operations, can still ensure good performance while saving a significant amount of resources. :fire:\n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n\n- 6/7/2024: Release a new benchmark [MLVU](https://github.com/JUNJIE99/MLVU), the first comprehensive benchmark specifically designed for long video understanding. MLVU features an extensive range of video durations, a diverse collection of video sources, and a set of evaluation tasks uniquely tailored for long-form video understanding. :fire:\n- 5/21/2024: Release a new benchmark [AIR-Bench](https://github.com/AIR-Bench/AIR-Bench) together with Jina AI, Zilliz, HuggingFace, and other partners. AIR-Bench focuses on a fair out-of-distribution evaluation for Neural IR & RAG. It generates the synthetic data for benchmarking w.r.t. diverse domains and languages. It is dynamic and will be updated on regular basis. [Leaderboard](https://huggingface.co/spaces/AIR-Bench/leaderboard) :fire:\n- 4/30/2024: Release [Llama-3-8B-Instruct-80K-QLoRA](https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA), extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA training on a few synthesized long-context data. The model achieves remarkable performance on various long-context benchmarks. [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora) :fire:\n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually :smiley:) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation :fire:\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data. :fire:\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://arxiv.org/pdf/2402.03216.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) \n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n\n</details>\n\n\nBGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n![projects](./imgs/projects.png)\n\n- **Inference**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker)\n- **Finetune**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)\n- **[Evaluation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation)**\n- **[Dataset](https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset)**\n- **[Tutorials](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)**\n- **[research](https://github.com/FlagOpen/FlagEmbedding/tree/master/research)**\n\n\n## Installation\n### Using pip:\nIf you do not want to finetune the models, you can install the package without the finetune dependency:\n```\npip install -U FlagEmbedding\n```\nIf you want to finetune the models, you can install the package with the finetune dependency:\n```\npip install -U FlagEmbedding[finetune]\n```\n### Install from sources:\n\nClone the repository and install\n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install  .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install  .[finetune]\n```\nFor development in editable mode:\n```\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install -e .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install -e .[finetune]\n```\n\n## Quick Start\nFirst, load one of the BGE embedding model:\n```\nfrom FlagEmbedding import FlagAutoModel\n\nmodel = FlagAutoModel.from_finetuned('BAAI/bge-base-en-v1.5',\n                                      query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n                                      use_fp16=True)\n```\nThen, feed some sentences to the model and get their embeddings:\n```\nsentences_1 = [\"I love NLP\", \"I love machine learning\"]\nsentences_2 = [\"I love BGE\", \"I love text retrieval\"]\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\n```\nOnce we get the embeddings, we can compute similarity by inner product:\n```\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\n\nFor more details, you can refer to [embedder inference](./examples/inference/embedder), [reranker inference](./examples/inference/reranker), [embedder finetune](./examples/finetune/embedder), [reranker fintune](./examples/finetune/reranker), [evaluation](./examples/evaluation).\n\nIf you're unfamiliar with any of related concepts, please check out the [tutorial](./Tutorials/). If it's not there, let us know.\n\nFor more interesting topics related to BGE, take a look at [research](./research).\n\n## Community\n\nWe are actively maintaining the community of BGE and FlagEmbedding. Let us know if you have any suggessions or ideas!\n\nCurrently we are updating the [tutorials](./Tutorials/), we aim to create a comprehensive and detailed tutorial for beginners on text retrieval and RAG. Stay tuned!\n\nThe following contents are releasing in the upcoming weeks:\n\n- Evaluation\n- BGE-EN-ICL\n\n<details>\n  <summary>The whole tutorial roadmap</summary>\n    <img src=\"./Tutorials/tutorial_map.png\"/>\n</details>\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n| Model                                                                     | Language |                                                             Description                                                             |                                query instruction for retrieval                                 |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Provide instructions and few-shot examples freely based on the given task. |\n| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |    Multilingual     | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |        Provide instructions based on the given task.         |\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n| [LM-Cocktail](https://huggingface.co/Shitao)                   |   English |                     fine-tuned models (Llama and BGE) which can be used to reproduce the results of LM-Cocktail                     |  |\n| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English |                         a unified embedding model to support diverse retrieval augmentation needs for LLMs                          | See [README](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder) |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | Multilingual | a lightweight cross-encoder model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |  |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |  |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers, compress ratio and compress layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)             |   English |                                             Embedding Model which map text into vector                                              |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)               |   English |                                    a base-scale model but with similar ability to `bge-large-en`                                    |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)             |   English |                                        a small-scale model but with competitive performance                                         |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)             |   Chinese |                                             Embedding Model which map text into vector                                              |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)               |   Chinese |                                    a base-scale model but with similar ability to `bge-large-zh`                                    |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)             |   Chinese |                                        a small-scale model but with competitive performance                                         |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n\n\n\n\n### Contributors:\nThank all our contributors for their efforts and warmly welcome new members to join in!\n\n<a href=\"https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\" />\n</a>\n\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_m3,\n  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},\n  year={2023},\n  eprint={2309.07597},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n@misc{cocktail,\n      title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Xingrun Xing},\n      year={2023},\n      eprint={2311.13534},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{llm_embedder,\n      title={Retrieve Anything To Augment Large Language Models}, \n      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\n      year={2023},\n      eprint={2310.07554},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). \n\n",
  "external_links_in_readme": [
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation",
    "https://arxiv.org/pdf/2310.07554.pdf",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://arxiv.org/pdf/2409.05591v1",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset",
    "https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">",
    "https://huggingface.co/Shitao",
    "https://huggingface.co/BAAI/bge-base-en",
    "https://huggingface.co/BAAI/llm-embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://img.shields.io/badge/Contribution-Welcome-blue\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://huggingface.co/BAAI/bge-m3",
    "https://github.com/VectorSpaceLab/OmniGen",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge",
    "https://github.com/VectorSpaceLab/MegaPairs",
    "https://github.com/FlagOpen/FlagEmbedding.git",
    "https://huggingface.co/spaces/AIR-Bench/leaderboard",
    "https://github.com/AIR-Bench/AIR-Bench",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker",
    "https://github.com/JUNJIE99/MLVU",
    "https://github.com/qhjqhj00/MemoRAG,",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail",
    "https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research",
    "https://huggingface.co/BAAI/bge-small-zh-v1.5",
    "https://arxiv.org/abs/2412.14475",
    "https://img.shields.io/badge/LICENSE-MIT-green\">",
    "https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">",
    "https://huggingface.co/BAAI/bge-reranker-v2-gemma",
    "https://huggingface.co/spaces/mteb/leaderboard",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker",
    "https://arxiv.org/pdf/2402.03216.pdf",
    "https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\"",
    "https://huggingface.co/BAAI/bge-base-zh",
    "https://arxiv.org/abs/2311.13534",
    "https://huggingface.co/C-MTEB\">",
    "https://flagopen.baai.ac.cn/",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding",
    "https://arxiv.org/abs/2401.03462",
    "https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">",
    "https://arxiv.org/pdf/2309.07597.pdf",
    "https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a>",
    "https://huggingface.co/BAAI/bge-en-icl",
    "https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight",
    "https://www.bge-model.com",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://arxiv.org/abs/2312.15503",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials",
    "https://huggingface.co/BAAI/bge-multilingual-gemma2",
    "https://github.com/FlagOpen/FlagEmbedding\">",
    "https://huggingface.co/BAAI/bge-small-zh",
    "https://huggingface.co/BAAI/bge-small-en",
    "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE",
    "https://data.baai.ac.cn/details/BAAI-MTP",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker",
    "https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise",
    "https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92",
    "https://huggingface.co/BAAI/bge-large-en",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md",
    "https://huggingface.co/BAAI/bge-large-zh",
    "https://huggingface.co/BAAI/bge-base-zh-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora",
    "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA"
  ]
}
```

</details>


---

## Repository 2: FlagOpen/FlagEmbedding

# GitHub Repository Data

**Repository:** [FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## Basic Information

- **Description:** Retrieval and Retrieval-augmented LLMs
- **Created:** 2023-08-02T02:08:11+00:00
- **Last Updated:** 2025-06-21T10:44:22+00:00
- **Last Pushed:** 2025-06-04T10:00:26+00:00
- **Default Branch:** master
- **Size:** 52310 KB

## Statistics

- **Stars:** 9,974
- **Forks:** 737
- **Watchers:** 9,974
- **Open Issues:** 847
- **Total Issues:** 0
- **Pull Requests:** 217

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE)

## Languages

- **Python:** 3,871,341 bytes
- **Jupyter Notebook:** 2,607,506 bytes
- **Shell:** 7,322 bytes

## Topics

- `embeddings`
- `information-retrieval`
- `llm`
- `sentence-embeddings`
- `text-semantic-similarity`
- `retrieval-augmented-generation`

## Top Contributors

1. **hanhainebula** - 350 contributions
2. **545999961** - 327 contributions
3. **staoxiao** - 182 contributions
4. **ZiyiXia** - 113 contributions
5. **namespace-Pt** - 101 contributions
6. **JUNJIE99** - 78 contributions
7. **ftgreat** - 20 contributions
8. **ZhengLiu101** - 15 contributions
9. **Muennighoff** - 12 contributions
10. **shuyansy** - 10 contributions

## File Structure (Sample of 10 files)

Total files: 1,395

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.gitignore` (blob)
- `FlagEmbedding` (tree)
- `FlagEmbedding/__init__.py` (blob)
- `FlagEmbedding/abc` (tree)
- `FlagEmbedding/abc/__init__.py` (blob)
- `FlagEmbedding/abc/evaluation` (tree)
- `FlagEmbedding/abc/evaluation/__init__.py` (blob)

## Recent Issues

- 🟢 **#1480** Continued pretraining using RetroMAE (open)
- 🟢 **#1479** 微调bge-reranker-base一直卡顿，无日志输出 (open)
- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1477** 训练过程中train_loss和eval_loss都是先下降后上升再缓慢下降 (open)
- 🟢 **#1476** How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline (open)

## Recent Pull Requests

- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1475** Add inline comment to setup.py (open)
- 🟢 **#1469** Extend tutorials to include cloud deployment options (open)
- 🔴 **#1466** update reranker inference (closed)
- 🔴 **#1465** update environment (closed)

## Recent Commits

- **875fd4ff** update docs - ZiyiXia (2025-06-04T09:57:10+00:00)
- **235f967a** update tutorials - ZiyiXia (2025-06-04T09:27:36+00:00)
- **5e64baa6** Merge pull request #1466 from 545999961/master - chaofan (2025-05-28T08:15:17+00:00)
- **018b984e** update reranker inference - cfli (2025-05-28T08:14:32+00:00)
- **d04f7989** release version 1.3.5 - chaofan (2025-05-28T07:12:19+00:00)
- **8576fde7** Merge pull request #1465 from 545999961/master - chaofan (2025-05-28T06:58:53+00:00)
- **bf410d51** update environment - cfli (2025-05-28T06:55:54+00:00)
- **97ca0732** Merge pull request #1464 from 545999961/master - chaofan (2025-05-28T06:18:23+00:00)
- **b51706d6** upload coder eval script - cfli (2025-05-28T06:17:19+00:00)
- **821c3879** upload coder eval script - cfli (2025-05-28T06:12:16+00:00)

## External Links Found in README

- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation
- https://arxiv.org/pdf/2310.07554.pdf
- https://huggingface.co/BAAI/bge-reranker-base
- https://arxiv.org/pdf/2409.05591v1
- https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset
- https://img.shields.io/badge/BGE_series-🤗-yellow">
- https://huggingface.co/Shitao
- https://huggingface.co/BAAI/bge-base-en
- https://huggingface.co/BAAI/llm-embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder
- https://huggingface.co/BAAI/bge-reranker-large
- https://img.shields.io/badge/Contribution-Welcome-blue">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder
- https://huggingface.co/BAAI/bge-small-en-v1.5
- https://huggingface.co/BAAI/bge-m3
- https://github.com/VectorSpaceLab/OmniGen
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge
- https://github.com/VectorSpaceLab/MegaPairs

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673596675,
  "name": "FlagEmbedding",
  "full_name": "FlagOpen/FlagEmbedding",
  "description": "Retrieval and Retrieval-augmented LLMs",
  "html_url": "https://github.com/FlagOpen/FlagEmbedding",
  "clone_url": "https://github.com/FlagOpen/FlagEmbedding.git",
  "ssh_url": "git@github.com:FlagOpen/FlagEmbedding.git",
  "homepage": "http://www.bge-model.com/",
  "topics": [
    "embeddings",
    "information-retrieval",
    "llm",
    "sentence-embeddings",
    "text-semantic-similarity",
    "retrieval-augmented-generation"
  ],
  "default_branch": "master",
  "created_at": "2023-08-02T02:08:11+00:00",
  "updated_at": "2025-06-21T10:44:22+00:00",
  "pushed_at": "2025-06-04T10:00:26+00:00",
  "size_kb": 52310,
  "watchers_count": 9974,
  "stargazers_count": 9974,
  "forks_count": 737,
  "open_issues_count": 847,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"
  },
  "languages": {
    "Python": 3871341,
    "Jupyter Notebook": 2607506,
    "Shell": 7322
  },
  "top_contributors": [
    {
      "login": "hanhainebula",
      "contributions": 350
    },
    {
      "login": "545999961",
      "contributions": 327
    },
    {
      "login": "staoxiao",
      "contributions": 182
    },
    {
      "login": "ZiyiXia",
      "contributions": 113
    },
    {
      "login": "namespace-Pt",
      "contributions": 101
    },
    {
      "login": "JUNJIE99",
      "contributions": 78
    },
    {
      "login": "ftgreat",
      "contributions": 20
    },
    {
      "login": "ZhengLiu101",
      "contributions": 15
    },
    {
      "login": "Muennighoff",
      "contributions": 12
    },
    {
      "login": "shuyansy",
      "contributions": 10
    },
    {
      "login": "rainym00d",
      "contributions": 3
    },
    {
      "login": "chinainfant",
      "contributions": 3
    },
    {
      "login": "GeoloeG-IsT",
      "contributions": 3
    },
    {
      "login": "blue-vision0",
      "contributions": 2
    },
    {
      "login": "muazhari",
      "contributions": 2
    },
    {
      "login": "luisegarduno",
      "contributions": 2
    },
    {
      "login": "khazic",
      "contributions": 2
    },
    {
      "login": "shtdbb",
      "contributions": 2
    },
    {
      "login": "pengjunfeng11",
      "contributions": 2
    },
    {
      "login": "ycjcl868",
      "contributions": 2
    }
  ],
  "file_tree_count": 1395,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc/evaluation",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/evaluation/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 217,
  "recent_issues": [
    {
      "number": 1480,
      "title": "Continued pretraining using RetroMAE",
      "state": "open"
    },
    {
      "number": 1479,
      "title": "\u5fae\u8c03bge-reranker-base\u4e00\u76f4\u5361\u987f\uff0c\u65e0\u65e5\u5fd7\u8f93\u51fa",
      "state": "open"
    },
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1477,
      "title": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2dtrain_loss\u548ceval_loss\u90fd\u662f\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u518d\u7f13\u6162\u4e0b\u964d",
      "state": "open"
    },
    {
      "number": 1476,
      "title": "How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1475,
      "title": "Add inline comment to setup.py",
      "state": "open"
    },
    {
      "number": 1469,
      "title": "Extend tutorials to include cloud deployment options",
      "state": "open"
    },
    {
      "number": 1466,
      "title": "update reranker inference",
      "state": "closed"
    },
    {
      "number": 1465,
      "title": "update environment",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "875fd4ffcb894313a918e8bb0c9b7ead82f37e64",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:57:10+00:00",
      "message": "update docs"
    },
    {
      "sha": "235f967a0122b9d5b85104fd4e8f36dc15ee85a8",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:27:36+00:00",
      "message": "update tutorials"
    },
    {
      "sha": "5e64baa61e75df23105a66d1e9d09ad799366e2a",
      "author": "chaofan",
      "date": "2025-05-28T08:15:17+00:00",
      "message": "Merge pull request #1466 from 545999961/master"
    },
    {
      "sha": "018b984e23e12d9793e59f2a1ef07fb9153ab08e",
      "author": "cfli",
      "date": "2025-05-28T08:14:32+00:00",
      "message": "update reranker inference"
    },
    {
      "sha": "d04f7989eeec2c6db4ead7c77ddef656f9c9d3aa",
      "author": "chaofan",
      "date": "2025-05-28T07:12:19+00:00",
      "message": "release version 1.3.5"
    },
    {
      "sha": "8576fde79b84224ecbe493857115552fcfb72ed7",
      "author": "chaofan",
      "date": "2025-05-28T06:58:53+00:00",
      "message": "Merge pull request #1465 from 545999961/master"
    },
    {
      "sha": "bf410d51e9e685e7f1b2e3816f09f3dc64ad8292",
      "author": "cfli",
      "date": "2025-05-28T06:55:54+00:00",
      "message": "update environment"
    },
    {
      "sha": "97ca07325d9e786d668f9a038502ad736dded9d5",
      "author": "chaofan",
      "date": "2025-05-28T06:18:23+00:00",
      "message": "Merge pull request #1464 from 545999961/master"
    },
    {
      "sha": "b51706d63ae8a1d86443df8b0e5ff6e45eb08fdc",
      "author": "cfli",
      "date": "2025-05-28T06:17:19+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "821c387961b5597fe672a4386232110ef54d5107",
      "author": "cfli",
      "date": "2025-05-28T06:12:16+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "3332a1956e2d5af7c52628a60e5c9788bd1b5ed9",
      "author": "chaofan",
      "date": "2025-05-22T10:07:40+00:00",
      "message": "Merge pull request #1459 from 545999961/master"
    },
    {
      "sha": "d412884a4e96c45e4a9e83a43e507e9beaa04713",
      "author": "cfli",
      "date": "2025-05-22T10:06:43+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "5c11feffc104497953a333036aa56b6b543e7cee",
      "author": "chaofan",
      "date": "2025-05-22T09:56:48+00:00",
      "message": "Merge pull request #1458 from 545999961/master"
    },
    {
      "sha": "41b526441f13242b49126261c219b6b9f14e17cb",
      "author": "cfli",
      "date": "2025-05-22T09:55:44+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "52bb9fddb5def2074b277f6e9a2f02ed8aa17dee",
      "author": "chaofan",
      "date": "2025-05-22T09:52:21+00:00",
      "message": "Merge pull request #1457 from 545999961/master"
    },
    {
      "sha": "f05d4b3fbeff5df9dabeb71ab16225fe5e1e8c95",
      "author": "cfli",
      "date": "2025-05-22T09:51:20+00:00",
      "message": "upload reinforced_ir"
    },
    {
      "sha": "25ee99c228e9df63a2be79e8d795d549ab5001e9",
      "author": "chaofan",
      "date": "2025-05-21T04:46:03+00:00",
      "message": "Merge pull request #1453 from 545999961/master"
    },
    {
      "sha": "436210bbed3ca352aac82a9be66cd4e665408944",
      "author": "cfli",
      "date": "2025-05-21T04:45:10+00:00",
      "message": "upload Matroyshka_reranker"
    },
    {
      "sha": "1946f89d5a6926871861c9ed9f4acbb31665acfe",
      "author": "cfli",
      "date": "2025-05-20T08:13:30+00:00",
      "message": "upload Matroyshka Re-Ranker"
    },
    {
      "sha": "f6924351bba09620938b69a2c933c6712be829c4",
      "author": "cfli",
      "date": "2025-05-20T08:12:08+00:00",
      "message": "upload Matroyshka Re-Ranker"
    }
  ],
  "readme_text": "[<img src=\"./imgs/FlagOpen.png\">](https://flagopen.baai.ac.cn/)\n\n<h1 align=\"center\">\u26a1\ufe0fBGE: One-Stop Retrieval Toolkit For Search and RAG</h1>\n\n![bge_logo](./imgs/bge_logo.jpg)\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Contribution-Welcome-blue\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\">\n    </a>\n    <a href=\"https://huggingface.co/C-MTEB\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=#news>News</a> |\n        <a href=#installation>Installation</a> |\n        <a href=#quick-start>Quick Start</a> |\n        <a href=#community>Community</a> |\n        <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a> |\n        <a href=#model-list>Model List</a> |\n        <a href=\"#contributors\">Contributor</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\n\n## News\n\n- 3/6/2025: :fire::fire: Introduce **BGE-VL** ([HF repo](https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92)), State-Of-The-Art multimodal embedding models to support Any visual search applications (everything, including text-to-image, image-to-text, image&prompt-to-image, text-to-image&text, and more)! They are released under the MIT license and are completely free for both academic and commercial use. We also release **MegaPairs** ([repo](https://github.com/VectorSpaceLab/MegaPairs), [paper](https://arxiv.org/abs/2412.14475)), a massive synthetic dataset which empowers BGE-VL!\n- 12/5/2024: :book: We built the [BGE documentation](https://www.bge-model.com) for centralized BGE information and materials!\n- 10/29/2024: :earth_asia: We created WeChat group for BGE. Scan the [QR code](./imgs/BGE_WeChat_Group.png) to join the group chat! To get the first hand message about our updates and new release, or having any questions or ideas, join us now!\n- <img src=\"./imgs/BGE_WeChat_Group.png\" alt=\"bge_wechat_group\" class=\"center\" width=\"200\">\n\n- 10/22/2024: We release another interesting model: [OmniGen](https://github.com/VectorSpaceLab/OmniGen), which is a unified image generation model supporting various tasks. OmniGen can accomplish complex image generation tasks without the need for additional plugins like ControlNet, IP-Adapter, or auxiliary models such as pose detection and face detection.\n- 9/10/2024: Introducing **MemoRAG**, a step forward towards RAG 2.0 on top of memory-inspired knowledge discovery (repo: https://github.com/qhjqhj00/MemoRAG, paper: https://arxiv.org/pdf/2409.05591v1) \n- 9/2/2024: Start to maintain the [tutorials](./Tutorials/). The contents within will be actively updated and eariched, stay tuned! :books:\n- 7/26/2024: Release a new embedding model [bge-en-icl](https://huggingface.co/BAAI/bge-en-icl), an embedding model that incorporates in-context learning capabilities, which, by providing task-relevant query-response examples, can encode semantically richer queries, further enhancing the semantic representation ability of the embeddings.\n- 7/26/2024: Release a new embedding model [bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2), a multilingual embedding model based on gemma-2-9b, which supports multiple languages and diverse downstream tasks, achieving new SOTA on multilingual benchmarks (MIRACL, MTEB-fr, and MTEB-pl). \n- 7/26/2024: Release a new lightweight reranker [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight), a lightweight reranker based on gemma-2-9b, which supports token compression and layerwise lightweight operations, can still ensure good performance while saving a significant amount of resources. :fire:\n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n\n- 6/7/2024: Release a new benchmark [MLVU](https://github.com/JUNJIE99/MLVU), the first comprehensive benchmark specifically designed for long video understanding. MLVU features an extensive range of video durations, a diverse collection of video sources, and a set of evaluation tasks uniquely tailored for long-form video understanding. :fire:\n- 5/21/2024: Release a new benchmark [AIR-Bench](https://github.com/AIR-Bench/AIR-Bench) together with Jina AI, Zilliz, HuggingFace, and other partners. AIR-Bench focuses on a fair out-of-distribution evaluation for Neural IR & RAG. It generates the synthetic data for benchmarking w.r.t. diverse domains and languages. It is dynamic and will be updated on regular basis. [Leaderboard](https://huggingface.co/spaces/AIR-Bench/leaderboard) :fire:\n- 4/30/2024: Release [Llama-3-8B-Instruct-80K-QLoRA](https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA), extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA training on a few synthesized long-context data. The model achieves remarkable performance on various long-context benchmarks. [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora) :fire:\n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually :smiley:) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation :fire:\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data. :fire:\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://arxiv.org/pdf/2402.03216.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) \n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n\n</details>\n\n\nBGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n![projects](./imgs/projects.png)\n\n- **Inference**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker)\n- **Finetune**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)\n- **[Evaluation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation)**\n- **[Dataset](https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset)**\n- **[Tutorials](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)**\n- **[research](https://github.com/FlagOpen/FlagEmbedding/tree/master/research)**\n\n\n## Installation\n### Using pip:\nIf you do not want to finetune the models, you can install the package without the finetune dependency:\n```\npip install -U FlagEmbedding\n```\nIf you want to finetune the models, you can install the package with the finetune dependency:\n```\npip install -U FlagEmbedding[finetune]\n```\n### Install from sources:\n\nClone the repository and install\n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install  .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install  .[finetune]\n```\nFor development in editable mode:\n```\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install -e .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install -e .[finetune]\n```\n\n## Quick Start\nFirst, load one of the BGE embedding model:\n```\nfrom FlagEmbedding import FlagAutoModel\n\nmodel = FlagAutoModel.from_finetuned('BAAI/bge-base-en-v1.5',\n                                      query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n                                      use_fp16=True)\n```\nThen, feed some sentences to the model and get their embeddings:\n```\nsentences_1 = [\"I love NLP\", \"I love machine learning\"]\nsentences_2 = [\"I love BGE\", \"I love text retrieval\"]\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\n```\nOnce we get the embeddings, we can compute similarity by inner product:\n```\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\n\nFor more details, you can refer to [embedder inference](./examples/inference/embedder), [reranker inference](./examples/inference/reranker), [embedder finetune](./examples/finetune/embedder), [reranker fintune](./examples/finetune/reranker), [evaluation](./examples/evaluation).\n\nIf you're unfamiliar with any of related concepts, please check out the [tutorial](./Tutorials/). If it's not there, let us know.\n\nFor more interesting topics related to BGE, take a look at [research](./research).\n\n## Community\n\nWe are actively maintaining the community of BGE and FlagEmbedding. Let us know if you have any suggessions or ideas!\n\nCurrently we are updating the [tutorials](./Tutorials/), we aim to create a comprehensive and detailed tutorial for beginners on text retrieval and RAG. Stay tuned!\n\nThe following contents are releasing in the upcoming weeks:\n\n- Evaluation\n- BGE-EN-ICL\n\n<details>\n  <summary>The whole tutorial roadmap</summary>\n    <img src=\"./Tutorials/tutorial_map.png\"/>\n</details>\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n| Model                                                                     | Language |                                                             Description                                                             |                                query instruction for retrieval                                 |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Provide instructions and few-shot examples freely based on the given task. |\n| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |    Multilingual     | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |        Provide instructions based on the given task.         |\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n| [LM-Cocktail](https://huggingface.co/Shitao)                   |   English |                     fine-tuned models (Llama and BGE) which can be used to reproduce the results of LM-Cocktail                     |  |\n| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English |                         a unified embedding model to support diverse retrieval augmentation needs for LLMs                          | See [README](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder) |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | Multilingual | a lightweight cross-encoder model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |  |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |  |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers, compress ratio and compress layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)             |   English |                                             Embedding Model which map text into vector                                              |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)               |   English |                                    a base-scale model but with similar ability to `bge-large-en`                                    |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)             |   English |                                        a small-scale model but with competitive performance                                         |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)             |   Chinese |                                             Embedding Model which map text into vector                                              |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)               |   Chinese |                                    a base-scale model but with similar ability to `bge-large-zh`                                    |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)             |   Chinese |                                        a small-scale model but with competitive performance                                         |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n\n\n\n\n### Contributors:\nThank all our contributors for their efforts and warmly welcome new members to join in!\n\n<a href=\"https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\" />\n</a>\n\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_m3,\n  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},\n  year={2023},\n  eprint={2309.07597},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n@misc{cocktail,\n      title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Xingrun Xing},\n      year={2023},\n      eprint={2311.13534},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{llm_embedder,\n      title={Retrieve Anything To Augment Large Language Models}, \n      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\n      year={2023},\n      eprint={2310.07554},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). \n\n",
  "external_links_in_readme": [
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation",
    "https://arxiv.org/pdf/2310.07554.pdf",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://arxiv.org/pdf/2409.05591v1",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset",
    "https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">",
    "https://huggingface.co/Shitao",
    "https://huggingface.co/BAAI/bge-base-en",
    "https://huggingface.co/BAAI/llm-embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://img.shields.io/badge/Contribution-Welcome-blue\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://huggingface.co/BAAI/bge-m3",
    "https://github.com/VectorSpaceLab/OmniGen",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge",
    "https://github.com/VectorSpaceLab/MegaPairs",
    "https://github.com/FlagOpen/FlagEmbedding.git",
    "https://huggingface.co/spaces/AIR-Bench/leaderboard",
    "https://github.com/AIR-Bench/AIR-Bench",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker",
    "https://github.com/JUNJIE99/MLVU",
    "https://github.com/qhjqhj00/MemoRAG,",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail",
    "https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research",
    "https://huggingface.co/BAAI/bge-small-zh-v1.5",
    "https://arxiv.org/abs/2412.14475",
    "https://img.shields.io/badge/LICENSE-MIT-green\">",
    "https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">",
    "https://huggingface.co/BAAI/bge-reranker-v2-gemma",
    "https://huggingface.co/spaces/mteb/leaderboard",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker",
    "https://arxiv.org/pdf/2402.03216.pdf",
    "https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\"",
    "https://huggingface.co/BAAI/bge-base-zh",
    "https://arxiv.org/abs/2311.13534",
    "https://huggingface.co/C-MTEB\">",
    "https://flagopen.baai.ac.cn/",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding",
    "https://arxiv.org/abs/2401.03462",
    "https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">",
    "https://arxiv.org/pdf/2309.07597.pdf",
    "https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a>",
    "https://huggingface.co/BAAI/bge-en-icl",
    "https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight",
    "https://www.bge-model.com",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://arxiv.org/abs/2312.15503",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials",
    "https://huggingface.co/BAAI/bge-multilingual-gemma2",
    "https://github.com/FlagOpen/FlagEmbedding\">",
    "https://huggingface.co/BAAI/bge-small-zh",
    "https://huggingface.co/BAAI/bge-small-en",
    "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE",
    "https://data.baai.ac.cn/details/BAAI-MTP",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker",
    "https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise",
    "https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92",
    "https://huggingface.co/BAAI/bge-large-en",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md",
    "https://huggingface.co/BAAI/bge-large-zh",
    "https://huggingface.co/BAAI/bge-base-zh-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora",
    "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA"
  ]
}
```

</details>


---

## Repository 3: FlagOpen/FlagEmbedding

# GitHub Repository Data

**Repository:** [FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## Basic Information

- **Description:** Retrieval and Retrieval-augmented LLMs
- **Created:** 2023-08-02T02:08:11+00:00
- **Last Updated:** 2025-06-21T10:44:22+00:00
- **Last Pushed:** 2025-06-04T10:00:26+00:00
- **Default Branch:** master
- **Size:** 52310 KB

## Statistics

- **Stars:** 9,974
- **Forks:** 737
- **Watchers:** 9,974
- **Open Issues:** 847
- **Total Issues:** 0
- **Pull Requests:** 217

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE)

## Languages

- **Python:** 3,871,341 bytes
- **Jupyter Notebook:** 2,607,506 bytes
- **Shell:** 7,322 bytes

## Topics

- `embeddings`
- `information-retrieval`
- `llm`
- `sentence-embeddings`
- `text-semantic-similarity`
- `retrieval-augmented-generation`

## Top Contributors

1. **hanhainebula** - 350 contributions
2. **545999961** - 327 contributions
3. **staoxiao** - 182 contributions
4. **ZiyiXia** - 113 contributions
5. **namespace-Pt** - 101 contributions
6. **JUNJIE99** - 78 contributions
7. **ftgreat** - 20 contributions
8. **ZhengLiu101** - 15 contributions
9. **Muennighoff** - 12 contributions
10. **shuyansy** - 10 contributions

## File Structure (Sample of 10 files)

Total files: 1,395

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.gitignore` (blob)
- `FlagEmbedding` (tree)
- `FlagEmbedding/__init__.py` (blob)
- `FlagEmbedding/abc` (tree)
- `FlagEmbedding/abc/__init__.py` (blob)
- `FlagEmbedding/abc/evaluation` (tree)
- `FlagEmbedding/abc/evaluation/__init__.py` (blob)

## Recent Issues

- 🟢 **#1480** Continued pretraining using RetroMAE (open)
- 🟢 **#1479** 微调bge-reranker-base一直卡顿，无日志输出 (open)
- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1477** 训练过程中train_loss和eval_loss都是先下降后上升再缓慢下降 (open)
- 🟢 **#1476** How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline (open)

## Recent Pull Requests

- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1475** Add inline comment to setup.py (open)
- 🟢 **#1469** Extend tutorials to include cloud deployment options (open)
- 🔴 **#1466** update reranker inference (closed)
- 🔴 **#1465** update environment (closed)

## Recent Commits

- **875fd4ff** update docs - ZiyiXia (2025-06-04T09:57:10+00:00)
- **235f967a** update tutorials - ZiyiXia (2025-06-04T09:27:36+00:00)
- **5e64baa6** Merge pull request #1466 from 545999961/master - chaofan (2025-05-28T08:15:17+00:00)
- **018b984e** update reranker inference - cfli (2025-05-28T08:14:32+00:00)
- **d04f7989** release version 1.3.5 - chaofan (2025-05-28T07:12:19+00:00)
- **8576fde7** Merge pull request #1465 from 545999961/master - chaofan (2025-05-28T06:58:53+00:00)
- **bf410d51** update environment - cfli (2025-05-28T06:55:54+00:00)
- **97ca0732** Merge pull request #1464 from 545999961/master - chaofan (2025-05-28T06:18:23+00:00)
- **b51706d6** upload coder eval script - cfli (2025-05-28T06:17:19+00:00)
- **821c3879** upload coder eval script - cfli (2025-05-28T06:12:16+00:00)

## External Links Found in README

- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation
- https://arxiv.org/pdf/2310.07554.pdf
- https://huggingface.co/BAAI/bge-reranker-base
- https://arxiv.org/pdf/2409.05591v1
- https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset
- https://img.shields.io/badge/BGE_series-🤗-yellow">
- https://huggingface.co/Shitao
- https://huggingface.co/BAAI/bge-base-en
- https://huggingface.co/BAAI/llm-embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder
- https://huggingface.co/BAAI/bge-reranker-large
- https://img.shields.io/badge/Contribution-Welcome-blue">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder
- https://huggingface.co/BAAI/bge-small-en-v1.5
- https://huggingface.co/BAAI/bge-m3
- https://github.com/VectorSpaceLab/OmniGen
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge
- https://github.com/VectorSpaceLab/MegaPairs

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673596675,
  "name": "FlagEmbedding",
  "full_name": "FlagOpen/FlagEmbedding",
  "description": "Retrieval and Retrieval-augmented LLMs",
  "html_url": "https://github.com/FlagOpen/FlagEmbedding",
  "clone_url": "https://github.com/FlagOpen/FlagEmbedding.git",
  "ssh_url": "git@github.com:FlagOpen/FlagEmbedding.git",
  "homepage": "http://www.bge-model.com/",
  "topics": [
    "embeddings",
    "information-retrieval",
    "llm",
    "sentence-embeddings",
    "text-semantic-similarity",
    "retrieval-augmented-generation"
  ],
  "default_branch": "master",
  "created_at": "2023-08-02T02:08:11+00:00",
  "updated_at": "2025-06-21T10:44:22+00:00",
  "pushed_at": "2025-06-04T10:00:26+00:00",
  "size_kb": 52310,
  "watchers_count": 9974,
  "stargazers_count": 9974,
  "forks_count": 737,
  "open_issues_count": 847,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"
  },
  "languages": {
    "Python": 3871341,
    "Jupyter Notebook": 2607506,
    "Shell": 7322
  },
  "top_contributors": [
    {
      "login": "hanhainebula",
      "contributions": 350
    },
    {
      "login": "545999961",
      "contributions": 327
    },
    {
      "login": "staoxiao",
      "contributions": 182
    },
    {
      "login": "ZiyiXia",
      "contributions": 113
    },
    {
      "login": "namespace-Pt",
      "contributions": 101
    },
    {
      "login": "JUNJIE99",
      "contributions": 78
    },
    {
      "login": "ftgreat",
      "contributions": 20
    },
    {
      "login": "ZhengLiu101",
      "contributions": 15
    },
    {
      "login": "Muennighoff",
      "contributions": 12
    },
    {
      "login": "shuyansy",
      "contributions": 10
    },
    {
      "login": "rainym00d",
      "contributions": 3
    },
    {
      "login": "chinainfant",
      "contributions": 3
    },
    {
      "login": "GeoloeG-IsT",
      "contributions": 3
    },
    {
      "login": "blue-vision0",
      "contributions": 2
    },
    {
      "login": "muazhari",
      "contributions": 2
    },
    {
      "login": "luisegarduno",
      "contributions": 2
    },
    {
      "login": "khazic",
      "contributions": 2
    },
    {
      "login": "shtdbb",
      "contributions": 2
    },
    {
      "login": "pengjunfeng11",
      "contributions": 2
    },
    {
      "login": "ycjcl868",
      "contributions": 2
    }
  ],
  "file_tree_count": 1395,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc/evaluation",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/evaluation/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 217,
  "recent_issues": [
    {
      "number": 1480,
      "title": "Continued pretraining using RetroMAE",
      "state": "open"
    },
    {
      "number": 1479,
      "title": "\u5fae\u8c03bge-reranker-base\u4e00\u76f4\u5361\u987f\uff0c\u65e0\u65e5\u5fd7\u8f93\u51fa",
      "state": "open"
    },
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1477,
      "title": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2dtrain_loss\u548ceval_loss\u90fd\u662f\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u518d\u7f13\u6162\u4e0b\u964d",
      "state": "open"
    },
    {
      "number": 1476,
      "title": "How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1475,
      "title": "Add inline comment to setup.py",
      "state": "open"
    },
    {
      "number": 1469,
      "title": "Extend tutorials to include cloud deployment options",
      "state": "open"
    },
    {
      "number": 1466,
      "title": "update reranker inference",
      "state": "closed"
    },
    {
      "number": 1465,
      "title": "update environment",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "875fd4ffcb894313a918e8bb0c9b7ead82f37e64",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:57:10+00:00",
      "message": "update docs"
    },
    {
      "sha": "235f967a0122b9d5b85104fd4e8f36dc15ee85a8",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:27:36+00:00",
      "message": "update tutorials"
    },
    {
      "sha": "5e64baa61e75df23105a66d1e9d09ad799366e2a",
      "author": "chaofan",
      "date": "2025-05-28T08:15:17+00:00",
      "message": "Merge pull request #1466 from 545999961/master"
    },
    {
      "sha": "018b984e23e12d9793e59f2a1ef07fb9153ab08e",
      "author": "cfli",
      "date": "2025-05-28T08:14:32+00:00",
      "message": "update reranker inference"
    },
    {
      "sha": "d04f7989eeec2c6db4ead7c77ddef656f9c9d3aa",
      "author": "chaofan",
      "date": "2025-05-28T07:12:19+00:00",
      "message": "release version 1.3.5"
    },
    {
      "sha": "8576fde79b84224ecbe493857115552fcfb72ed7",
      "author": "chaofan",
      "date": "2025-05-28T06:58:53+00:00",
      "message": "Merge pull request #1465 from 545999961/master"
    },
    {
      "sha": "bf410d51e9e685e7f1b2e3816f09f3dc64ad8292",
      "author": "cfli",
      "date": "2025-05-28T06:55:54+00:00",
      "message": "update environment"
    },
    {
      "sha": "97ca07325d9e786d668f9a038502ad736dded9d5",
      "author": "chaofan",
      "date": "2025-05-28T06:18:23+00:00",
      "message": "Merge pull request #1464 from 545999961/master"
    },
    {
      "sha": "b51706d63ae8a1d86443df8b0e5ff6e45eb08fdc",
      "author": "cfli",
      "date": "2025-05-28T06:17:19+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "821c387961b5597fe672a4386232110ef54d5107",
      "author": "cfli",
      "date": "2025-05-28T06:12:16+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "3332a1956e2d5af7c52628a60e5c9788bd1b5ed9",
      "author": "chaofan",
      "date": "2025-05-22T10:07:40+00:00",
      "message": "Merge pull request #1459 from 545999961/master"
    },
    {
      "sha": "d412884a4e96c45e4a9e83a43e507e9beaa04713",
      "author": "cfli",
      "date": "2025-05-22T10:06:43+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "5c11feffc104497953a333036aa56b6b543e7cee",
      "author": "chaofan",
      "date": "2025-05-22T09:56:48+00:00",
      "message": "Merge pull request #1458 from 545999961/master"
    },
    {
      "sha": "41b526441f13242b49126261c219b6b9f14e17cb",
      "author": "cfli",
      "date": "2025-05-22T09:55:44+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "52bb9fddb5def2074b277f6e9a2f02ed8aa17dee",
      "author": "chaofan",
      "date": "2025-05-22T09:52:21+00:00",
      "message": "Merge pull request #1457 from 545999961/master"
    },
    {
      "sha": "f05d4b3fbeff5df9dabeb71ab16225fe5e1e8c95",
      "author": "cfli",
      "date": "2025-05-22T09:51:20+00:00",
      "message": "upload reinforced_ir"
    },
    {
      "sha": "25ee99c228e9df63a2be79e8d795d549ab5001e9",
      "author": "chaofan",
      "date": "2025-05-21T04:46:03+00:00",
      "message": "Merge pull request #1453 from 545999961/master"
    },
    {
      "sha": "436210bbed3ca352aac82a9be66cd4e665408944",
      "author": "cfli",
      "date": "2025-05-21T04:45:10+00:00",
      "message": "upload Matroyshka_reranker"
    },
    {
      "sha": "1946f89d5a6926871861c9ed9f4acbb31665acfe",
      "author": "cfli",
      "date": "2025-05-20T08:13:30+00:00",
      "message": "upload Matroyshka Re-Ranker"
    },
    {
      "sha": "f6924351bba09620938b69a2c933c6712be829c4",
      "author": "cfli",
      "date": "2025-05-20T08:12:08+00:00",
      "message": "upload Matroyshka Re-Ranker"
    }
  ],
  "readme_text": "[<img src=\"./imgs/FlagOpen.png\">](https://flagopen.baai.ac.cn/)\n\n<h1 align=\"center\">\u26a1\ufe0fBGE: One-Stop Retrieval Toolkit For Search and RAG</h1>\n\n![bge_logo](./imgs/bge_logo.jpg)\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Contribution-Welcome-blue\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\">\n    </a>\n    <a href=\"https://huggingface.co/C-MTEB\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=#news>News</a> |\n        <a href=#installation>Installation</a> |\n        <a href=#quick-start>Quick Start</a> |\n        <a href=#community>Community</a> |\n        <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a> |\n        <a href=#model-list>Model List</a> |\n        <a href=\"#contributors\">Contributor</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\n\n## News\n\n- 3/6/2025: :fire::fire: Introduce **BGE-VL** ([HF repo](https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92)), State-Of-The-Art multimodal embedding models to support Any visual search applications (everything, including text-to-image, image-to-text, image&prompt-to-image, text-to-image&text, and more)! They are released under the MIT license and are completely free for both academic and commercial use. We also release **MegaPairs** ([repo](https://github.com/VectorSpaceLab/MegaPairs), [paper](https://arxiv.org/abs/2412.14475)), a massive synthetic dataset which empowers BGE-VL!\n- 12/5/2024: :book: We built the [BGE documentation](https://www.bge-model.com) for centralized BGE information and materials!\n- 10/29/2024: :earth_asia: We created WeChat group for BGE. Scan the [QR code](./imgs/BGE_WeChat_Group.png) to join the group chat! To get the first hand message about our updates and new release, or having any questions or ideas, join us now!\n- <img src=\"./imgs/BGE_WeChat_Group.png\" alt=\"bge_wechat_group\" class=\"center\" width=\"200\">\n\n- 10/22/2024: We release another interesting model: [OmniGen](https://github.com/VectorSpaceLab/OmniGen), which is a unified image generation model supporting various tasks. OmniGen can accomplish complex image generation tasks without the need for additional plugins like ControlNet, IP-Adapter, or auxiliary models such as pose detection and face detection.\n- 9/10/2024: Introducing **MemoRAG**, a step forward towards RAG 2.0 on top of memory-inspired knowledge discovery (repo: https://github.com/qhjqhj00/MemoRAG, paper: https://arxiv.org/pdf/2409.05591v1) \n- 9/2/2024: Start to maintain the [tutorials](./Tutorials/). The contents within will be actively updated and eariched, stay tuned! :books:\n- 7/26/2024: Release a new embedding model [bge-en-icl](https://huggingface.co/BAAI/bge-en-icl), an embedding model that incorporates in-context learning capabilities, which, by providing task-relevant query-response examples, can encode semantically richer queries, further enhancing the semantic representation ability of the embeddings.\n- 7/26/2024: Release a new embedding model [bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2), a multilingual embedding model based on gemma-2-9b, which supports multiple languages and diverse downstream tasks, achieving new SOTA on multilingual benchmarks (MIRACL, MTEB-fr, and MTEB-pl). \n- 7/26/2024: Release a new lightweight reranker [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight), a lightweight reranker based on gemma-2-9b, which supports token compression and layerwise lightweight operations, can still ensure good performance while saving a significant amount of resources. :fire:\n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n\n- 6/7/2024: Release a new benchmark [MLVU](https://github.com/JUNJIE99/MLVU), the first comprehensive benchmark specifically designed for long video understanding. MLVU features an extensive range of video durations, a diverse collection of video sources, and a set of evaluation tasks uniquely tailored for long-form video understanding. :fire:\n- 5/21/2024: Release a new benchmark [AIR-Bench](https://github.com/AIR-Bench/AIR-Bench) together with Jina AI, Zilliz, HuggingFace, and other partners. AIR-Bench focuses on a fair out-of-distribution evaluation for Neural IR & RAG. It generates the synthetic data for benchmarking w.r.t. diverse domains and languages. It is dynamic and will be updated on regular basis. [Leaderboard](https://huggingface.co/spaces/AIR-Bench/leaderboard) :fire:\n- 4/30/2024: Release [Llama-3-8B-Instruct-80K-QLoRA](https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA), extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA training on a few synthesized long-context data. The model achieves remarkable performance on various long-context benchmarks. [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora) :fire:\n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually :smiley:) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation :fire:\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data. :fire:\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://arxiv.org/pdf/2402.03216.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) \n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n\n</details>\n\n\nBGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n![projects](./imgs/projects.png)\n\n- **Inference**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker)\n- **Finetune**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)\n- **[Evaluation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation)**\n- **[Dataset](https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset)**\n- **[Tutorials](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)**\n- **[research](https://github.com/FlagOpen/FlagEmbedding/tree/master/research)**\n\n\n## Installation\n### Using pip:\nIf you do not want to finetune the models, you can install the package without the finetune dependency:\n```\npip install -U FlagEmbedding\n```\nIf you want to finetune the models, you can install the package with the finetune dependency:\n```\npip install -U FlagEmbedding[finetune]\n```\n### Install from sources:\n\nClone the repository and install\n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install  .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install  .[finetune]\n```\nFor development in editable mode:\n```\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install -e .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install -e .[finetune]\n```\n\n## Quick Start\nFirst, load one of the BGE embedding model:\n```\nfrom FlagEmbedding import FlagAutoModel\n\nmodel = FlagAutoModel.from_finetuned('BAAI/bge-base-en-v1.5',\n                                      query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n                                      use_fp16=True)\n```\nThen, feed some sentences to the model and get their embeddings:\n```\nsentences_1 = [\"I love NLP\", \"I love machine learning\"]\nsentences_2 = [\"I love BGE\", \"I love text retrieval\"]\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\n```\nOnce we get the embeddings, we can compute similarity by inner product:\n```\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\n\nFor more details, you can refer to [embedder inference](./examples/inference/embedder), [reranker inference](./examples/inference/reranker), [embedder finetune](./examples/finetune/embedder), [reranker fintune](./examples/finetune/reranker), [evaluation](./examples/evaluation).\n\nIf you're unfamiliar with any of related concepts, please check out the [tutorial](./Tutorials/). If it's not there, let us know.\n\nFor more interesting topics related to BGE, take a look at [research](./research).\n\n## Community\n\nWe are actively maintaining the community of BGE and FlagEmbedding. Let us know if you have any suggessions or ideas!\n\nCurrently we are updating the [tutorials](./Tutorials/), we aim to create a comprehensive and detailed tutorial for beginners on text retrieval and RAG. Stay tuned!\n\nThe following contents are releasing in the upcoming weeks:\n\n- Evaluation\n- BGE-EN-ICL\n\n<details>\n  <summary>The whole tutorial roadmap</summary>\n    <img src=\"./Tutorials/tutorial_map.png\"/>\n</details>\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n| Model                                                                     | Language |                                                             Description                                                             |                                query instruction for retrieval                                 |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Provide instructions and few-shot examples freely based on the given task. |\n| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |    Multilingual     | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |        Provide instructions based on the given task.         |\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n| [LM-Cocktail](https://huggingface.co/Shitao)                   |   English |                     fine-tuned models (Llama and BGE) which can be used to reproduce the results of LM-Cocktail                     |  |\n| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English |                         a unified embedding model to support diverse retrieval augmentation needs for LLMs                          | See [README](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder) |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | Multilingual | a lightweight cross-encoder model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |  |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |  |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers, compress ratio and compress layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)             |   English |                                             Embedding Model which map text into vector                                              |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)               |   English |                                    a base-scale model but with similar ability to `bge-large-en`                                    |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)             |   English |                                        a small-scale model but with competitive performance                                         |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)             |   Chinese |                                             Embedding Model which map text into vector                                              |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)               |   Chinese |                                    a base-scale model but with similar ability to `bge-large-zh`                                    |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)             |   Chinese |                                        a small-scale model but with competitive performance                                         |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n\n\n\n\n### Contributors:\nThank all our contributors for their efforts and warmly welcome new members to join in!\n\n<a href=\"https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\" />\n</a>\n\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_m3,\n  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},\n  year={2023},\n  eprint={2309.07597},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n@misc{cocktail,\n      title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Xingrun Xing},\n      year={2023},\n      eprint={2311.13534},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{llm_embedder,\n      title={Retrieve Anything To Augment Large Language Models}, \n      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\n      year={2023},\n      eprint={2310.07554},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). \n\n",
  "external_links_in_readme": [
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation",
    "https://arxiv.org/pdf/2310.07554.pdf",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://arxiv.org/pdf/2409.05591v1",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset",
    "https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">",
    "https://huggingface.co/Shitao",
    "https://huggingface.co/BAAI/bge-base-en",
    "https://huggingface.co/BAAI/llm-embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://img.shields.io/badge/Contribution-Welcome-blue\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://huggingface.co/BAAI/bge-m3",
    "https://github.com/VectorSpaceLab/OmniGen",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge",
    "https://github.com/VectorSpaceLab/MegaPairs",
    "https://github.com/FlagOpen/FlagEmbedding.git",
    "https://huggingface.co/spaces/AIR-Bench/leaderboard",
    "https://github.com/AIR-Bench/AIR-Bench",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker",
    "https://github.com/JUNJIE99/MLVU",
    "https://github.com/qhjqhj00/MemoRAG,",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail",
    "https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research",
    "https://huggingface.co/BAAI/bge-small-zh-v1.5",
    "https://arxiv.org/abs/2412.14475",
    "https://img.shields.io/badge/LICENSE-MIT-green\">",
    "https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">",
    "https://huggingface.co/BAAI/bge-reranker-v2-gemma",
    "https://huggingface.co/spaces/mteb/leaderboard",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker",
    "https://arxiv.org/pdf/2402.03216.pdf",
    "https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\"",
    "https://huggingface.co/BAAI/bge-base-zh",
    "https://arxiv.org/abs/2311.13534",
    "https://huggingface.co/C-MTEB\">",
    "https://flagopen.baai.ac.cn/",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding",
    "https://arxiv.org/abs/2401.03462",
    "https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">",
    "https://arxiv.org/pdf/2309.07597.pdf",
    "https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a>",
    "https://huggingface.co/BAAI/bge-en-icl",
    "https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight",
    "https://www.bge-model.com",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://arxiv.org/abs/2312.15503",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials",
    "https://huggingface.co/BAAI/bge-multilingual-gemma2",
    "https://github.com/FlagOpen/FlagEmbedding\">",
    "https://huggingface.co/BAAI/bge-small-zh",
    "https://huggingface.co/BAAI/bge-small-en",
    "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE",
    "https://data.baai.ac.cn/details/BAAI-MTP",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker",
    "https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise",
    "https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92",
    "https://huggingface.co/BAAI/bge-large-en",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md",
    "https://huggingface.co/BAAI/bge-large-zh",
    "https://huggingface.co/BAAI/bge-base-zh-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora",
    "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA"
  ]
}
```

</details>


---

## Repository 4: FlagOpen/FlagEmbedding

# GitHub Repository Data

**Repository:** [FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## Basic Information

- **Description:** Retrieval and Retrieval-augmented LLMs
- **Created:** 2023-08-02T02:08:11+00:00
- **Last Updated:** 2025-06-21T10:44:22+00:00
- **Last Pushed:** 2025-06-04T10:00:26+00:00
- **Default Branch:** master
- **Size:** 52310 KB

## Statistics

- **Stars:** 9,974
- **Forks:** 737
- **Watchers:** 9,974
- **Open Issues:** 847
- **Total Issues:** 0
- **Pull Requests:** 217

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE)

## Languages

- **Python:** 3,871,341 bytes
- **Jupyter Notebook:** 2,607,506 bytes
- **Shell:** 7,322 bytes

## Topics

- `embeddings`
- `information-retrieval`
- `llm`
- `sentence-embeddings`
- `text-semantic-similarity`
- `retrieval-augmented-generation`

## Top Contributors

1. **hanhainebula** - 350 contributions
2. **545999961** - 327 contributions
3. **staoxiao** - 182 contributions
4. **ZiyiXia** - 113 contributions
5. **namespace-Pt** - 101 contributions
6. **JUNJIE99** - 78 contributions
7. **ftgreat** - 20 contributions
8. **ZhengLiu101** - 15 contributions
9. **Muennighoff** - 12 contributions
10. **shuyansy** - 10 contributions

## File Structure (Sample of 10 files)

Total files: 1,395

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.gitignore` (blob)
- `FlagEmbedding` (tree)
- `FlagEmbedding/__init__.py` (blob)
- `FlagEmbedding/abc` (tree)
- `FlagEmbedding/abc/__init__.py` (blob)
- `FlagEmbedding/abc/evaluation` (tree)
- `FlagEmbedding/abc/evaluation/__init__.py` (blob)

## Recent Issues

- 🟢 **#1480** Continued pretraining using RetroMAE (open)
- 🟢 **#1479** 微调bge-reranker-base一直卡顿，无日志输出 (open)
- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1477** 训练过程中train_loss和eval_loss都是先下降后上升再缓慢下降 (open)
- 🟢 **#1476** How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline (open)

## Recent Pull Requests

- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1475** Add inline comment to setup.py (open)
- 🟢 **#1469** Extend tutorials to include cloud deployment options (open)
- 🔴 **#1466** update reranker inference (closed)
- 🔴 **#1465** update environment (closed)

## Recent Commits

- **875fd4ff** update docs - ZiyiXia (2025-06-04T09:57:10+00:00)
- **235f967a** update tutorials - ZiyiXia (2025-06-04T09:27:36+00:00)
- **5e64baa6** Merge pull request #1466 from 545999961/master - chaofan (2025-05-28T08:15:17+00:00)
- **018b984e** update reranker inference - cfli (2025-05-28T08:14:32+00:00)
- **d04f7989** release version 1.3.5 - chaofan (2025-05-28T07:12:19+00:00)
- **8576fde7** Merge pull request #1465 from 545999961/master - chaofan (2025-05-28T06:58:53+00:00)
- **bf410d51** update environment - cfli (2025-05-28T06:55:54+00:00)
- **97ca0732** Merge pull request #1464 from 545999961/master - chaofan (2025-05-28T06:18:23+00:00)
- **b51706d6** upload coder eval script - cfli (2025-05-28T06:17:19+00:00)
- **821c3879** upload coder eval script - cfli (2025-05-28T06:12:16+00:00)

## External Links Found in README

- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation
- https://arxiv.org/pdf/2310.07554.pdf
- https://huggingface.co/BAAI/bge-reranker-base
- https://arxiv.org/pdf/2409.05591v1
- https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset
- https://img.shields.io/badge/BGE_series-🤗-yellow">
- https://huggingface.co/Shitao
- https://huggingface.co/BAAI/bge-base-en
- https://huggingface.co/BAAI/llm-embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder
- https://huggingface.co/BAAI/bge-reranker-large
- https://img.shields.io/badge/Contribution-Welcome-blue">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder
- https://huggingface.co/BAAI/bge-small-en-v1.5
- https://huggingface.co/BAAI/bge-m3
- https://github.com/VectorSpaceLab/OmniGen
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge
- https://github.com/VectorSpaceLab/MegaPairs

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673596675,
  "name": "FlagEmbedding",
  "full_name": "FlagOpen/FlagEmbedding",
  "description": "Retrieval and Retrieval-augmented LLMs",
  "html_url": "https://github.com/FlagOpen/FlagEmbedding",
  "clone_url": "https://github.com/FlagOpen/FlagEmbedding.git",
  "ssh_url": "git@github.com:FlagOpen/FlagEmbedding.git",
  "homepage": "http://www.bge-model.com/",
  "topics": [
    "embeddings",
    "information-retrieval",
    "llm",
    "sentence-embeddings",
    "text-semantic-similarity",
    "retrieval-augmented-generation"
  ],
  "default_branch": "master",
  "created_at": "2023-08-02T02:08:11+00:00",
  "updated_at": "2025-06-21T10:44:22+00:00",
  "pushed_at": "2025-06-04T10:00:26+00:00",
  "size_kb": 52310,
  "watchers_count": 9974,
  "stargazers_count": 9974,
  "forks_count": 737,
  "open_issues_count": 847,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"
  },
  "languages": {
    "Python": 3871341,
    "Jupyter Notebook": 2607506,
    "Shell": 7322
  },
  "top_contributors": [
    {
      "login": "hanhainebula",
      "contributions": 350
    },
    {
      "login": "545999961",
      "contributions": 327
    },
    {
      "login": "staoxiao",
      "contributions": 182
    },
    {
      "login": "ZiyiXia",
      "contributions": 113
    },
    {
      "login": "namespace-Pt",
      "contributions": 101
    },
    {
      "login": "JUNJIE99",
      "contributions": 78
    },
    {
      "login": "ftgreat",
      "contributions": 20
    },
    {
      "login": "ZhengLiu101",
      "contributions": 15
    },
    {
      "login": "Muennighoff",
      "contributions": 12
    },
    {
      "login": "shuyansy",
      "contributions": 10
    },
    {
      "login": "rainym00d",
      "contributions": 3
    },
    {
      "login": "chinainfant",
      "contributions": 3
    },
    {
      "login": "GeoloeG-IsT",
      "contributions": 3
    },
    {
      "login": "blue-vision0",
      "contributions": 2
    },
    {
      "login": "muazhari",
      "contributions": 2
    },
    {
      "login": "luisegarduno",
      "contributions": 2
    },
    {
      "login": "khazic",
      "contributions": 2
    },
    {
      "login": "shtdbb",
      "contributions": 2
    },
    {
      "login": "pengjunfeng11",
      "contributions": 2
    },
    {
      "login": "ycjcl868",
      "contributions": 2
    }
  ],
  "file_tree_count": 1395,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc/evaluation",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/evaluation/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 217,
  "recent_issues": [
    {
      "number": 1480,
      "title": "Continued pretraining using RetroMAE",
      "state": "open"
    },
    {
      "number": 1479,
      "title": "\u5fae\u8c03bge-reranker-base\u4e00\u76f4\u5361\u987f\uff0c\u65e0\u65e5\u5fd7\u8f93\u51fa",
      "state": "open"
    },
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1477,
      "title": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2dtrain_loss\u548ceval_loss\u90fd\u662f\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u518d\u7f13\u6162\u4e0b\u964d",
      "state": "open"
    },
    {
      "number": 1476,
      "title": "How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1475,
      "title": "Add inline comment to setup.py",
      "state": "open"
    },
    {
      "number": 1469,
      "title": "Extend tutorials to include cloud deployment options",
      "state": "open"
    },
    {
      "number": 1466,
      "title": "update reranker inference",
      "state": "closed"
    },
    {
      "number": 1465,
      "title": "update environment",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "875fd4ffcb894313a918e8bb0c9b7ead82f37e64",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:57:10+00:00",
      "message": "update docs"
    },
    {
      "sha": "235f967a0122b9d5b85104fd4e8f36dc15ee85a8",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:27:36+00:00",
      "message": "update tutorials"
    },
    {
      "sha": "5e64baa61e75df23105a66d1e9d09ad799366e2a",
      "author": "chaofan",
      "date": "2025-05-28T08:15:17+00:00",
      "message": "Merge pull request #1466 from 545999961/master"
    },
    {
      "sha": "018b984e23e12d9793e59f2a1ef07fb9153ab08e",
      "author": "cfli",
      "date": "2025-05-28T08:14:32+00:00",
      "message": "update reranker inference"
    },
    {
      "sha": "d04f7989eeec2c6db4ead7c77ddef656f9c9d3aa",
      "author": "chaofan",
      "date": "2025-05-28T07:12:19+00:00",
      "message": "release version 1.3.5"
    },
    {
      "sha": "8576fde79b84224ecbe493857115552fcfb72ed7",
      "author": "chaofan",
      "date": "2025-05-28T06:58:53+00:00",
      "message": "Merge pull request #1465 from 545999961/master"
    },
    {
      "sha": "bf410d51e9e685e7f1b2e3816f09f3dc64ad8292",
      "author": "cfli",
      "date": "2025-05-28T06:55:54+00:00",
      "message": "update environment"
    },
    {
      "sha": "97ca07325d9e786d668f9a038502ad736dded9d5",
      "author": "chaofan",
      "date": "2025-05-28T06:18:23+00:00",
      "message": "Merge pull request #1464 from 545999961/master"
    },
    {
      "sha": "b51706d63ae8a1d86443df8b0e5ff6e45eb08fdc",
      "author": "cfli",
      "date": "2025-05-28T06:17:19+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "821c387961b5597fe672a4386232110ef54d5107",
      "author": "cfli",
      "date": "2025-05-28T06:12:16+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "3332a1956e2d5af7c52628a60e5c9788bd1b5ed9",
      "author": "chaofan",
      "date": "2025-05-22T10:07:40+00:00",
      "message": "Merge pull request #1459 from 545999961/master"
    },
    {
      "sha": "d412884a4e96c45e4a9e83a43e507e9beaa04713",
      "author": "cfli",
      "date": "2025-05-22T10:06:43+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "5c11feffc104497953a333036aa56b6b543e7cee",
      "author": "chaofan",
      "date": "2025-05-22T09:56:48+00:00",
      "message": "Merge pull request #1458 from 545999961/master"
    },
    {
      "sha": "41b526441f13242b49126261c219b6b9f14e17cb",
      "author": "cfli",
      "date": "2025-05-22T09:55:44+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "52bb9fddb5def2074b277f6e9a2f02ed8aa17dee",
      "author": "chaofan",
      "date": "2025-05-22T09:52:21+00:00",
      "message": "Merge pull request #1457 from 545999961/master"
    },
    {
      "sha": "f05d4b3fbeff5df9dabeb71ab16225fe5e1e8c95",
      "author": "cfli",
      "date": "2025-05-22T09:51:20+00:00",
      "message": "upload reinforced_ir"
    },
    {
      "sha": "25ee99c228e9df63a2be79e8d795d549ab5001e9",
      "author": "chaofan",
      "date": "2025-05-21T04:46:03+00:00",
      "message": "Merge pull request #1453 from 545999961/master"
    },
    {
      "sha": "436210bbed3ca352aac82a9be66cd4e665408944",
      "author": "cfli",
      "date": "2025-05-21T04:45:10+00:00",
      "message": "upload Matroyshka_reranker"
    },
    {
      "sha": "1946f89d5a6926871861c9ed9f4acbb31665acfe",
      "author": "cfli",
      "date": "2025-05-20T08:13:30+00:00",
      "message": "upload Matroyshka Re-Ranker"
    },
    {
      "sha": "f6924351bba09620938b69a2c933c6712be829c4",
      "author": "cfli",
      "date": "2025-05-20T08:12:08+00:00",
      "message": "upload Matroyshka Re-Ranker"
    }
  ],
  "readme_text": "[<img src=\"./imgs/FlagOpen.png\">](https://flagopen.baai.ac.cn/)\n\n<h1 align=\"center\">\u26a1\ufe0fBGE: One-Stop Retrieval Toolkit For Search and RAG</h1>\n\n![bge_logo](./imgs/bge_logo.jpg)\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Contribution-Welcome-blue\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\">\n    </a>\n    <a href=\"https://huggingface.co/C-MTEB\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=#news>News</a> |\n        <a href=#installation>Installation</a> |\n        <a href=#quick-start>Quick Start</a> |\n        <a href=#community>Community</a> |\n        <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a> |\n        <a href=#model-list>Model List</a> |\n        <a href=\"#contributors\">Contributor</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\n\n## News\n\n- 3/6/2025: :fire::fire: Introduce **BGE-VL** ([HF repo](https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92)), State-Of-The-Art multimodal embedding models to support Any visual search applications (everything, including text-to-image, image-to-text, image&prompt-to-image, text-to-image&text, and more)! They are released under the MIT license and are completely free for both academic and commercial use. We also release **MegaPairs** ([repo](https://github.com/VectorSpaceLab/MegaPairs), [paper](https://arxiv.org/abs/2412.14475)), a massive synthetic dataset which empowers BGE-VL!\n- 12/5/2024: :book: We built the [BGE documentation](https://www.bge-model.com) for centralized BGE information and materials!\n- 10/29/2024: :earth_asia: We created WeChat group for BGE. Scan the [QR code](./imgs/BGE_WeChat_Group.png) to join the group chat! To get the first hand message about our updates and new release, or having any questions or ideas, join us now!\n- <img src=\"./imgs/BGE_WeChat_Group.png\" alt=\"bge_wechat_group\" class=\"center\" width=\"200\">\n\n- 10/22/2024: We release another interesting model: [OmniGen](https://github.com/VectorSpaceLab/OmniGen), which is a unified image generation model supporting various tasks. OmniGen can accomplish complex image generation tasks without the need for additional plugins like ControlNet, IP-Adapter, or auxiliary models such as pose detection and face detection.\n- 9/10/2024: Introducing **MemoRAG**, a step forward towards RAG 2.0 on top of memory-inspired knowledge discovery (repo: https://github.com/qhjqhj00/MemoRAG, paper: https://arxiv.org/pdf/2409.05591v1) \n- 9/2/2024: Start to maintain the [tutorials](./Tutorials/). The contents within will be actively updated and eariched, stay tuned! :books:\n- 7/26/2024: Release a new embedding model [bge-en-icl](https://huggingface.co/BAAI/bge-en-icl), an embedding model that incorporates in-context learning capabilities, which, by providing task-relevant query-response examples, can encode semantically richer queries, further enhancing the semantic representation ability of the embeddings.\n- 7/26/2024: Release a new embedding model [bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2), a multilingual embedding model based on gemma-2-9b, which supports multiple languages and diverse downstream tasks, achieving new SOTA on multilingual benchmarks (MIRACL, MTEB-fr, and MTEB-pl). \n- 7/26/2024: Release a new lightweight reranker [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight), a lightweight reranker based on gemma-2-9b, which supports token compression and layerwise lightweight operations, can still ensure good performance while saving a significant amount of resources. :fire:\n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n\n- 6/7/2024: Release a new benchmark [MLVU](https://github.com/JUNJIE99/MLVU), the first comprehensive benchmark specifically designed for long video understanding. MLVU features an extensive range of video durations, a diverse collection of video sources, and a set of evaluation tasks uniquely tailored for long-form video understanding. :fire:\n- 5/21/2024: Release a new benchmark [AIR-Bench](https://github.com/AIR-Bench/AIR-Bench) together with Jina AI, Zilliz, HuggingFace, and other partners. AIR-Bench focuses on a fair out-of-distribution evaluation for Neural IR & RAG. It generates the synthetic data for benchmarking w.r.t. diverse domains and languages. It is dynamic and will be updated on regular basis. [Leaderboard](https://huggingface.co/spaces/AIR-Bench/leaderboard) :fire:\n- 4/30/2024: Release [Llama-3-8B-Instruct-80K-QLoRA](https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA), extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA training on a few synthesized long-context data. The model achieves remarkable performance on various long-context benchmarks. [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora) :fire:\n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually :smiley:) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation :fire:\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data. :fire:\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://arxiv.org/pdf/2402.03216.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) \n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n\n</details>\n\n\nBGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n![projects](./imgs/projects.png)\n\n- **Inference**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker)\n- **Finetune**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)\n- **[Evaluation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation)**\n- **[Dataset](https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset)**\n- **[Tutorials](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)**\n- **[research](https://github.com/FlagOpen/FlagEmbedding/tree/master/research)**\n\n\n## Installation\n### Using pip:\nIf you do not want to finetune the models, you can install the package without the finetune dependency:\n```\npip install -U FlagEmbedding\n```\nIf you want to finetune the models, you can install the package with the finetune dependency:\n```\npip install -U FlagEmbedding[finetune]\n```\n### Install from sources:\n\nClone the repository and install\n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install  .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install  .[finetune]\n```\nFor development in editable mode:\n```\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install -e .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install -e .[finetune]\n```\n\n## Quick Start\nFirst, load one of the BGE embedding model:\n```\nfrom FlagEmbedding import FlagAutoModel\n\nmodel = FlagAutoModel.from_finetuned('BAAI/bge-base-en-v1.5',\n                                      query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n                                      use_fp16=True)\n```\nThen, feed some sentences to the model and get their embeddings:\n```\nsentences_1 = [\"I love NLP\", \"I love machine learning\"]\nsentences_2 = [\"I love BGE\", \"I love text retrieval\"]\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\n```\nOnce we get the embeddings, we can compute similarity by inner product:\n```\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\n\nFor more details, you can refer to [embedder inference](./examples/inference/embedder), [reranker inference](./examples/inference/reranker), [embedder finetune](./examples/finetune/embedder), [reranker fintune](./examples/finetune/reranker), [evaluation](./examples/evaluation).\n\nIf you're unfamiliar with any of related concepts, please check out the [tutorial](./Tutorials/). If it's not there, let us know.\n\nFor more interesting topics related to BGE, take a look at [research](./research).\n\n## Community\n\nWe are actively maintaining the community of BGE and FlagEmbedding. Let us know if you have any suggessions or ideas!\n\nCurrently we are updating the [tutorials](./Tutorials/), we aim to create a comprehensive and detailed tutorial for beginners on text retrieval and RAG. Stay tuned!\n\nThe following contents are releasing in the upcoming weeks:\n\n- Evaluation\n- BGE-EN-ICL\n\n<details>\n  <summary>The whole tutorial roadmap</summary>\n    <img src=\"./Tutorials/tutorial_map.png\"/>\n</details>\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n| Model                                                                     | Language |                                                             Description                                                             |                                query instruction for retrieval                                 |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Provide instructions and few-shot examples freely based on the given task. |\n| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |    Multilingual     | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |        Provide instructions based on the given task.         |\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n| [LM-Cocktail](https://huggingface.co/Shitao)                   |   English |                     fine-tuned models (Llama and BGE) which can be used to reproduce the results of LM-Cocktail                     |  |\n| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English |                         a unified embedding model to support diverse retrieval augmentation needs for LLMs                          | See [README](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder) |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | Multilingual | a lightweight cross-encoder model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |  |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |  |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers, compress ratio and compress layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)             |   English |                                             Embedding Model which map text into vector                                              |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)               |   English |                                    a base-scale model but with similar ability to `bge-large-en`                                    |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)             |   English |                                        a small-scale model but with competitive performance                                         |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)             |   Chinese |                                             Embedding Model which map text into vector                                              |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)               |   Chinese |                                    a base-scale model but with similar ability to `bge-large-zh`                                    |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)             |   Chinese |                                        a small-scale model but with competitive performance                                         |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n\n\n\n\n### Contributors:\nThank all our contributors for their efforts and warmly welcome new members to join in!\n\n<a href=\"https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\" />\n</a>\n\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_m3,\n  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},\n  year={2023},\n  eprint={2309.07597},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n@misc{cocktail,\n      title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Xingrun Xing},\n      year={2023},\n      eprint={2311.13534},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{llm_embedder,\n      title={Retrieve Anything To Augment Large Language Models}, \n      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\n      year={2023},\n      eprint={2310.07554},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). \n\n",
  "external_links_in_readme": [
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation",
    "https://arxiv.org/pdf/2310.07554.pdf",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://arxiv.org/pdf/2409.05591v1",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset",
    "https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">",
    "https://huggingface.co/Shitao",
    "https://huggingface.co/BAAI/bge-base-en",
    "https://huggingface.co/BAAI/llm-embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://img.shields.io/badge/Contribution-Welcome-blue\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://huggingface.co/BAAI/bge-m3",
    "https://github.com/VectorSpaceLab/OmniGen",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge",
    "https://github.com/VectorSpaceLab/MegaPairs",
    "https://github.com/FlagOpen/FlagEmbedding.git",
    "https://huggingface.co/spaces/AIR-Bench/leaderboard",
    "https://github.com/AIR-Bench/AIR-Bench",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker",
    "https://github.com/JUNJIE99/MLVU",
    "https://github.com/qhjqhj00/MemoRAG,",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail",
    "https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research",
    "https://huggingface.co/BAAI/bge-small-zh-v1.5",
    "https://arxiv.org/abs/2412.14475",
    "https://img.shields.io/badge/LICENSE-MIT-green\">",
    "https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">",
    "https://huggingface.co/BAAI/bge-reranker-v2-gemma",
    "https://huggingface.co/spaces/mteb/leaderboard",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker",
    "https://arxiv.org/pdf/2402.03216.pdf",
    "https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\"",
    "https://huggingface.co/BAAI/bge-base-zh",
    "https://arxiv.org/abs/2311.13534",
    "https://huggingface.co/C-MTEB\">",
    "https://flagopen.baai.ac.cn/",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding",
    "https://arxiv.org/abs/2401.03462",
    "https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">",
    "https://arxiv.org/pdf/2309.07597.pdf",
    "https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a>",
    "https://huggingface.co/BAAI/bge-en-icl",
    "https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight",
    "https://www.bge-model.com",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://arxiv.org/abs/2312.15503",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials",
    "https://huggingface.co/BAAI/bge-multilingual-gemma2",
    "https://github.com/FlagOpen/FlagEmbedding\">",
    "https://huggingface.co/BAAI/bge-small-zh",
    "https://huggingface.co/BAAI/bge-small-en",
    "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE",
    "https://data.baai.ac.cn/details/BAAI-MTP",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker",
    "https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise",
    "https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92",
    "https://huggingface.co/BAAI/bge-large-en",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md",
    "https://huggingface.co/BAAI/bge-large-zh",
    "https://huggingface.co/BAAI/bge-base-zh-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora",
    "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA"
  ]
}
```

</details>


---

## Repository 5: FlagOpen/FlagEmbedding

# GitHub Repository Data

**Repository:** [FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## Basic Information

- **Description:** Retrieval and Retrieval-augmented LLMs
- **Created:** 2023-08-02T02:08:11+00:00
- **Last Updated:** 2025-06-21T10:44:22+00:00
- **Last Pushed:** 2025-06-04T10:00:26+00:00
- **Default Branch:** master
- **Size:** 52310 KB

## Statistics

- **Stars:** 9,974
- **Forks:** 737
- **Watchers:** 9,974
- **Open Issues:** 847
- **Total Issues:** 0
- **Pull Requests:** 217

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE)

## Languages

- **Python:** 3,871,341 bytes
- **Jupyter Notebook:** 2,607,506 bytes
- **Shell:** 7,322 bytes

## Topics

- `embeddings`
- `information-retrieval`
- `llm`
- `sentence-embeddings`
- `text-semantic-similarity`
- `retrieval-augmented-generation`

## Top Contributors

1. **hanhainebula** - 350 contributions
2. **545999961** - 327 contributions
3. **staoxiao** - 182 contributions
4. **ZiyiXia** - 113 contributions
5. **namespace-Pt** - 101 contributions
6. **JUNJIE99** - 78 contributions
7. **ftgreat** - 20 contributions
8. **ZhengLiu101** - 15 contributions
9. **Muennighoff** - 12 contributions
10. **shuyansy** - 10 contributions

## File Structure (Sample of 10 files)

Total files: 1,395

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.gitignore` (blob)
- `FlagEmbedding` (tree)
- `FlagEmbedding/__init__.py` (blob)
- `FlagEmbedding/abc` (tree)
- `FlagEmbedding/abc/__init__.py` (blob)
- `FlagEmbedding/abc/evaluation` (tree)
- `FlagEmbedding/abc/evaluation/__init__.py` (blob)

## Recent Issues

- 🟢 **#1480** Continued pretraining using RetroMAE (open)
- 🟢 **#1479** 微调bge-reranker-base一直卡顿，无日志输出 (open)
- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1477** 训练过程中train_loss和eval_loss都是先下降后上升再缓慢下降 (open)
- 🟢 **#1476** How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline (open)

## Recent Pull Requests

- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1475** Add inline comment to setup.py (open)
- 🟢 **#1469** Extend tutorials to include cloud deployment options (open)
- 🔴 **#1466** update reranker inference (closed)
- 🔴 **#1465** update environment (closed)

## Recent Commits

- **875fd4ff** update docs - ZiyiXia (2025-06-04T09:57:10+00:00)
- **235f967a** update tutorials - ZiyiXia (2025-06-04T09:27:36+00:00)
- **5e64baa6** Merge pull request #1466 from 545999961/master - chaofan (2025-05-28T08:15:17+00:00)
- **018b984e** update reranker inference - cfli (2025-05-28T08:14:32+00:00)
- **d04f7989** release version 1.3.5 - chaofan (2025-05-28T07:12:19+00:00)
- **8576fde7** Merge pull request #1465 from 545999961/master - chaofan (2025-05-28T06:58:53+00:00)
- **bf410d51** update environment - cfli (2025-05-28T06:55:54+00:00)
- **97ca0732** Merge pull request #1464 from 545999961/master - chaofan (2025-05-28T06:18:23+00:00)
- **b51706d6** upload coder eval script - cfli (2025-05-28T06:17:19+00:00)
- **821c3879** upload coder eval script - cfli (2025-05-28T06:12:16+00:00)

## External Links Found in README

- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation
- https://arxiv.org/pdf/2310.07554.pdf
- https://huggingface.co/BAAI/bge-reranker-base
- https://arxiv.org/pdf/2409.05591v1
- https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset
- https://img.shields.io/badge/BGE_series-🤗-yellow">
- https://huggingface.co/Shitao
- https://huggingface.co/BAAI/bge-base-en
- https://huggingface.co/BAAI/llm-embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder
- https://huggingface.co/BAAI/bge-reranker-large
- https://img.shields.io/badge/Contribution-Welcome-blue">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder
- https://huggingface.co/BAAI/bge-small-en-v1.5
- https://huggingface.co/BAAI/bge-m3
- https://github.com/VectorSpaceLab/OmniGen
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge
- https://github.com/VectorSpaceLab/MegaPairs

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673596675,
  "name": "FlagEmbedding",
  "full_name": "FlagOpen/FlagEmbedding",
  "description": "Retrieval and Retrieval-augmented LLMs",
  "html_url": "https://github.com/FlagOpen/FlagEmbedding",
  "clone_url": "https://github.com/FlagOpen/FlagEmbedding.git",
  "ssh_url": "git@github.com:FlagOpen/FlagEmbedding.git",
  "homepage": "http://www.bge-model.com/",
  "topics": [
    "embeddings",
    "information-retrieval",
    "llm",
    "sentence-embeddings",
    "text-semantic-similarity",
    "retrieval-augmented-generation"
  ],
  "default_branch": "master",
  "created_at": "2023-08-02T02:08:11+00:00",
  "updated_at": "2025-06-21T10:44:22+00:00",
  "pushed_at": "2025-06-04T10:00:26+00:00",
  "size_kb": 52310,
  "watchers_count": 9974,
  "stargazers_count": 9974,
  "forks_count": 737,
  "open_issues_count": 847,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"
  },
  "languages": {
    "Python": 3871341,
    "Jupyter Notebook": 2607506,
    "Shell": 7322
  },
  "top_contributors": [
    {
      "login": "hanhainebula",
      "contributions": 350
    },
    {
      "login": "545999961",
      "contributions": 327
    },
    {
      "login": "staoxiao",
      "contributions": 182
    },
    {
      "login": "ZiyiXia",
      "contributions": 113
    },
    {
      "login": "namespace-Pt",
      "contributions": 101
    },
    {
      "login": "JUNJIE99",
      "contributions": 78
    },
    {
      "login": "ftgreat",
      "contributions": 20
    },
    {
      "login": "ZhengLiu101",
      "contributions": 15
    },
    {
      "login": "Muennighoff",
      "contributions": 12
    },
    {
      "login": "shuyansy",
      "contributions": 10
    },
    {
      "login": "rainym00d",
      "contributions": 3
    },
    {
      "login": "chinainfant",
      "contributions": 3
    },
    {
      "login": "GeoloeG-IsT",
      "contributions": 3
    },
    {
      "login": "blue-vision0",
      "contributions": 2
    },
    {
      "login": "muazhari",
      "contributions": 2
    },
    {
      "login": "luisegarduno",
      "contributions": 2
    },
    {
      "login": "khazic",
      "contributions": 2
    },
    {
      "login": "shtdbb",
      "contributions": 2
    },
    {
      "login": "pengjunfeng11",
      "contributions": 2
    },
    {
      "login": "ycjcl868",
      "contributions": 2
    }
  ],
  "file_tree_count": 1395,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc/evaluation",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/evaluation/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 217,
  "recent_issues": [
    {
      "number": 1480,
      "title": "Continued pretraining using RetroMAE",
      "state": "open"
    },
    {
      "number": 1479,
      "title": "\u5fae\u8c03bge-reranker-base\u4e00\u76f4\u5361\u987f\uff0c\u65e0\u65e5\u5fd7\u8f93\u51fa",
      "state": "open"
    },
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1477,
      "title": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2dtrain_loss\u548ceval_loss\u90fd\u662f\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u518d\u7f13\u6162\u4e0b\u964d",
      "state": "open"
    },
    {
      "number": 1476,
      "title": "How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1475,
      "title": "Add inline comment to setup.py",
      "state": "open"
    },
    {
      "number": 1469,
      "title": "Extend tutorials to include cloud deployment options",
      "state": "open"
    },
    {
      "number": 1466,
      "title": "update reranker inference",
      "state": "closed"
    },
    {
      "number": 1465,
      "title": "update environment",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "875fd4ffcb894313a918e8bb0c9b7ead82f37e64",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:57:10+00:00",
      "message": "update docs"
    },
    {
      "sha": "235f967a0122b9d5b85104fd4e8f36dc15ee85a8",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:27:36+00:00",
      "message": "update tutorials"
    },
    {
      "sha": "5e64baa61e75df23105a66d1e9d09ad799366e2a",
      "author": "chaofan",
      "date": "2025-05-28T08:15:17+00:00",
      "message": "Merge pull request #1466 from 545999961/master"
    },
    {
      "sha": "018b984e23e12d9793e59f2a1ef07fb9153ab08e",
      "author": "cfli",
      "date": "2025-05-28T08:14:32+00:00",
      "message": "update reranker inference"
    },
    {
      "sha": "d04f7989eeec2c6db4ead7c77ddef656f9c9d3aa",
      "author": "chaofan",
      "date": "2025-05-28T07:12:19+00:00",
      "message": "release version 1.3.5"
    },
    {
      "sha": "8576fde79b84224ecbe493857115552fcfb72ed7",
      "author": "chaofan",
      "date": "2025-05-28T06:58:53+00:00",
      "message": "Merge pull request #1465 from 545999961/master"
    },
    {
      "sha": "bf410d51e9e685e7f1b2e3816f09f3dc64ad8292",
      "author": "cfli",
      "date": "2025-05-28T06:55:54+00:00",
      "message": "update environment"
    },
    {
      "sha": "97ca07325d9e786d668f9a038502ad736dded9d5",
      "author": "chaofan",
      "date": "2025-05-28T06:18:23+00:00",
      "message": "Merge pull request #1464 from 545999961/master"
    },
    {
      "sha": "b51706d63ae8a1d86443df8b0e5ff6e45eb08fdc",
      "author": "cfli",
      "date": "2025-05-28T06:17:19+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "821c387961b5597fe672a4386232110ef54d5107",
      "author": "cfli",
      "date": "2025-05-28T06:12:16+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "3332a1956e2d5af7c52628a60e5c9788bd1b5ed9",
      "author": "chaofan",
      "date": "2025-05-22T10:07:40+00:00",
      "message": "Merge pull request #1459 from 545999961/master"
    },
    {
      "sha": "d412884a4e96c45e4a9e83a43e507e9beaa04713",
      "author": "cfli",
      "date": "2025-05-22T10:06:43+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "5c11feffc104497953a333036aa56b6b543e7cee",
      "author": "chaofan",
      "date": "2025-05-22T09:56:48+00:00",
      "message": "Merge pull request #1458 from 545999961/master"
    },
    {
      "sha": "41b526441f13242b49126261c219b6b9f14e17cb",
      "author": "cfli",
      "date": "2025-05-22T09:55:44+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "52bb9fddb5def2074b277f6e9a2f02ed8aa17dee",
      "author": "chaofan",
      "date": "2025-05-22T09:52:21+00:00",
      "message": "Merge pull request #1457 from 545999961/master"
    },
    {
      "sha": "f05d4b3fbeff5df9dabeb71ab16225fe5e1e8c95",
      "author": "cfli",
      "date": "2025-05-22T09:51:20+00:00",
      "message": "upload reinforced_ir"
    },
    {
      "sha": "25ee99c228e9df63a2be79e8d795d549ab5001e9",
      "author": "chaofan",
      "date": "2025-05-21T04:46:03+00:00",
      "message": "Merge pull request #1453 from 545999961/master"
    },
    {
      "sha": "436210bbed3ca352aac82a9be66cd4e665408944",
      "author": "cfli",
      "date": "2025-05-21T04:45:10+00:00",
      "message": "upload Matroyshka_reranker"
    },
    {
      "sha": "1946f89d5a6926871861c9ed9f4acbb31665acfe",
      "author": "cfli",
      "date": "2025-05-20T08:13:30+00:00",
      "message": "upload Matroyshka Re-Ranker"
    },
    {
      "sha": "f6924351bba09620938b69a2c933c6712be829c4",
      "author": "cfli",
      "date": "2025-05-20T08:12:08+00:00",
      "message": "upload Matroyshka Re-Ranker"
    }
  ],
  "readme_text": "[<img src=\"./imgs/FlagOpen.png\">](https://flagopen.baai.ac.cn/)\n\n<h1 align=\"center\">\u26a1\ufe0fBGE: One-Stop Retrieval Toolkit For Search and RAG</h1>\n\n![bge_logo](./imgs/bge_logo.jpg)\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Contribution-Welcome-blue\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\">\n    </a>\n    <a href=\"https://huggingface.co/C-MTEB\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=#news>News</a> |\n        <a href=#installation>Installation</a> |\n        <a href=#quick-start>Quick Start</a> |\n        <a href=#community>Community</a> |\n        <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a> |\n        <a href=#model-list>Model List</a> |\n        <a href=\"#contributors\">Contributor</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\n\n## News\n\n- 3/6/2025: :fire::fire: Introduce **BGE-VL** ([HF repo](https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92)), State-Of-The-Art multimodal embedding models to support Any visual search applications (everything, including text-to-image, image-to-text, image&prompt-to-image, text-to-image&text, and more)! They are released under the MIT license and are completely free for both academic and commercial use. We also release **MegaPairs** ([repo](https://github.com/VectorSpaceLab/MegaPairs), [paper](https://arxiv.org/abs/2412.14475)), a massive synthetic dataset which empowers BGE-VL!\n- 12/5/2024: :book: We built the [BGE documentation](https://www.bge-model.com) for centralized BGE information and materials!\n- 10/29/2024: :earth_asia: We created WeChat group for BGE. Scan the [QR code](./imgs/BGE_WeChat_Group.png) to join the group chat! To get the first hand message about our updates and new release, or having any questions or ideas, join us now!\n- <img src=\"./imgs/BGE_WeChat_Group.png\" alt=\"bge_wechat_group\" class=\"center\" width=\"200\">\n\n- 10/22/2024: We release another interesting model: [OmniGen](https://github.com/VectorSpaceLab/OmniGen), which is a unified image generation model supporting various tasks. OmniGen can accomplish complex image generation tasks without the need for additional plugins like ControlNet, IP-Adapter, or auxiliary models such as pose detection and face detection.\n- 9/10/2024: Introducing **MemoRAG**, a step forward towards RAG 2.0 on top of memory-inspired knowledge discovery (repo: https://github.com/qhjqhj00/MemoRAG, paper: https://arxiv.org/pdf/2409.05591v1) \n- 9/2/2024: Start to maintain the [tutorials](./Tutorials/). The contents within will be actively updated and eariched, stay tuned! :books:\n- 7/26/2024: Release a new embedding model [bge-en-icl](https://huggingface.co/BAAI/bge-en-icl), an embedding model that incorporates in-context learning capabilities, which, by providing task-relevant query-response examples, can encode semantically richer queries, further enhancing the semantic representation ability of the embeddings.\n- 7/26/2024: Release a new embedding model [bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2), a multilingual embedding model based on gemma-2-9b, which supports multiple languages and diverse downstream tasks, achieving new SOTA on multilingual benchmarks (MIRACL, MTEB-fr, and MTEB-pl). \n- 7/26/2024: Release a new lightweight reranker [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight), a lightweight reranker based on gemma-2-9b, which supports token compression and layerwise lightweight operations, can still ensure good performance while saving a significant amount of resources. :fire:\n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n\n- 6/7/2024: Release a new benchmark [MLVU](https://github.com/JUNJIE99/MLVU), the first comprehensive benchmark specifically designed for long video understanding. MLVU features an extensive range of video durations, a diverse collection of video sources, and a set of evaluation tasks uniquely tailored for long-form video understanding. :fire:\n- 5/21/2024: Release a new benchmark [AIR-Bench](https://github.com/AIR-Bench/AIR-Bench) together with Jina AI, Zilliz, HuggingFace, and other partners. AIR-Bench focuses on a fair out-of-distribution evaluation for Neural IR & RAG. It generates the synthetic data for benchmarking w.r.t. diverse domains and languages. It is dynamic and will be updated on regular basis. [Leaderboard](https://huggingface.co/spaces/AIR-Bench/leaderboard) :fire:\n- 4/30/2024: Release [Llama-3-8B-Instruct-80K-QLoRA](https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA), extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA training on a few synthesized long-context data. The model achieves remarkable performance on various long-context benchmarks. [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora) :fire:\n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually :smiley:) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation :fire:\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data. :fire:\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://arxiv.org/pdf/2402.03216.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) \n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n\n</details>\n\n\nBGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n![projects](./imgs/projects.png)\n\n- **Inference**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker)\n- **Finetune**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)\n- **[Evaluation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation)**\n- **[Dataset](https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset)**\n- **[Tutorials](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)**\n- **[research](https://github.com/FlagOpen/FlagEmbedding/tree/master/research)**\n\n\n## Installation\n### Using pip:\nIf you do not want to finetune the models, you can install the package without the finetune dependency:\n```\npip install -U FlagEmbedding\n```\nIf you want to finetune the models, you can install the package with the finetune dependency:\n```\npip install -U FlagEmbedding[finetune]\n```\n### Install from sources:\n\nClone the repository and install\n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install  .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install  .[finetune]\n```\nFor development in editable mode:\n```\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install -e .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install -e .[finetune]\n```\n\n## Quick Start\nFirst, load one of the BGE embedding model:\n```\nfrom FlagEmbedding import FlagAutoModel\n\nmodel = FlagAutoModel.from_finetuned('BAAI/bge-base-en-v1.5',\n                                      query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n                                      use_fp16=True)\n```\nThen, feed some sentences to the model and get their embeddings:\n```\nsentences_1 = [\"I love NLP\", \"I love machine learning\"]\nsentences_2 = [\"I love BGE\", \"I love text retrieval\"]\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\n```\nOnce we get the embeddings, we can compute similarity by inner product:\n```\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\n\nFor more details, you can refer to [embedder inference](./examples/inference/embedder), [reranker inference](./examples/inference/reranker), [embedder finetune](./examples/finetune/embedder), [reranker fintune](./examples/finetune/reranker), [evaluation](./examples/evaluation).\n\nIf you're unfamiliar with any of related concepts, please check out the [tutorial](./Tutorials/). If it's not there, let us know.\n\nFor more interesting topics related to BGE, take a look at [research](./research).\n\n## Community\n\nWe are actively maintaining the community of BGE and FlagEmbedding. Let us know if you have any suggessions or ideas!\n\nCurrently we are updating the [tutorials](./Tutorials/), we aim to create a comprehensive and detailed tutorial for beginners on text retrieval and RAG. Stay tuned!\n\nThe following contents are releasing in the upcoming weeks:\n\n- Evaluation\n- BGE-EN-ICL\n\n<details>\n  <summary>The whole tutorial roadmap</summary>\n    <img src=\"./Tutorials/tutorial_map.png\"/>\n</details>\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n| Model                                                                     | Language |                                                             Description                                                             |                                query instruction for retrieval                                 |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Provide instructions and few-shot examples freely based on the given task. |\n| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |    Multilingual     | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |        Provide instructions based on the given task.         |\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n| [LM-Cocktail](https://huggingface.co/Shitao)                   |   English |                     fine-tuned models (Llama and BGE) which can be used to reproduce the results of LM-Cocktail                     |  |\n| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English |                         a unified embedding model to support diverse retrieval augmentation needs for LLMs                          | See [README](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder) |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | Multilingual | a lightweight cross-encoder model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |  |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |  |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers, compress ratio and compress layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)             |   English |                                             Embedding Model which map text into vector                                              |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)               |   English |                                    a base-scale model but with similar ability to `bge-large-en`                                    |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)             |   English |                                        a small-scale model but with competitive performance                                         |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)             |   Chinese |                                             Embedding Model which map text into vector                                              |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)               |   Chinese |                                    a base-scale model but with similar ability to `bge-large-zh`                                    |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)             |   Chinese |                                        a small-scale model but with competitive performance                                         |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n\n\n\n\n### Contributors:\nThank all our contributors for their efforts and warmly welcome new members to join in!\n\n<a href=\"https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\" />\n</a>\n\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_m3,\n  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},\n  year={2023},\n  eprint={2309.07597},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n@misc{cocktail,\n      title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Xingrun Xing},\n      year={2023},\n      eprint={2311.13534},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{llm_embedder,\n      title={Retrieve Anything To Augment Large Language Models}, \n      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\n      year={2023},\n      eprint={2310.07554},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). \n\n",
  "external_links_in_readme": [
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation",
    "https://arxiv.org/pdf/2310.07554.pdf",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://arxiv.org/pdf/2409.05591v1",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset",
    "https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">",
    "https://huggingface.co/Shitao",
    "https://huggingface.co/BAAI/bge-base-en",
    "https://huggingface.co/BAAI/llm-embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://img.shields.io/badge/Contribution-Welcome-blue\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://huggingface.co/BAAI/bge-m3",
    "https://github.com/VectorSpaceLab/OmniGen",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge",
    "https://github.com/VectorSpaceLab/MegaPairs",
    "https://github.com/FlagOpen/FlagEmbedding.git",
    "https://huggingface.co/spaces/AIR-Bench/leaderboard",
    "https://github.com/AIR-Bench/AIR-Bench",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker",
    "https://github.com/JUNJIE99/MLVU",
    "https://github.com/qhjqhj00/MemoRAG,",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail",
    "https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research",
    "https://huggingface.co/BAAI/bge-small-zh-v1.5",
    "https://arxiv.org/abs/2412.14475",
    "https://img.shields.io/badge/LICENSE-MIT-green\">",
    "https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">",
    "https://huggingface.co/BAAI/bge-reranker-v2-gemma",
    "https://huggingface.co/spaces/mteb/leaderboard",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker",
    "https://arxiv.org/pdf/2402.03216.pdf",
    "https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\"",
    "https://huggingface.co/BAAI/bge-base-zh",
    "https://arxiv.org/abs/2311.13534",
    "https://huggingface.co/C-MTEB\">",
    "https://flagopen.baai.ac.cn/",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding",
    "https://arxiv.org/abs/2401.03462",
    "https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">",
    "https://arxiv.org/pdf/2309.07597.pdf",
    "https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a>",
    "https://huggingface.co/BAAI/bge-en-icl",
    "https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight",
    "https://www.bge-model.com",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://arxiv.org/abs/2312.15503",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials",
    "https://huggingface.co/BAAI/bge-multilingual-gemma2",
    "https://github.com/FlagOpen/FlagEmbedding\">",
    "https://huggingface.co/BAAI/bge-small-zh",
    "https://huggingface.co/BAAI/bge-small-en",
    "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE",
    "https://data.baai.ac.cn/details/BAAI-MTP",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker",
    "https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise",
    "https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92",
    "https://huggingface.co/BAAI/bge-large-en",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md",
    "https://huggingface.co/BAAI/bge-large-zh",
    "https://huggingface.co/BAAI/bge-base-zh-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora",
    "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA"
  ]
}
```

</details>


---

## Repository 6: FlagOpen/FlagEmbedding

# GitHub Repository Data

**Repository:** [FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## Basic Information

- **Description:** Retrieval and Retrieval-augmented LLMs
- **Created:** 2023-08-02T02:08:11+00:00
- **Last Updated:** 2025-06-21T10:44:22+00:00
- **Last Pushed:** 2025-06-04T10:00:26+00:00
- **Default Branch:** master
- **Size:** 52310 KB

## Statistics

- **Stars:** 9,974
- **Forks:** 737
- **Watchers:** 9,974
- **Open Issues:** 847
- **Total Issues:** 0
- **Pull Requests:** 217

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE)

## Languages

- **Python:** 3,871,341 bytes
- **Jupyter Notebook:** 2,607,506 bytes
- **Shell:** 7,322 bytes

## Topics

- `embeddings`
- `information-retrieval`
- `llm`
- `sentence-embeddings`
- `text-semantic-similarity`
- `retrieval-augmented-generation`

## Top Contributors

1. **hanhainebula** - 350 contributions
2. **545999961** - 327 contributions
3. **staoxiao** - 182 contributions
4. **ZiyiXia** - 113 contributions
5. **namespace-Pt** - 101 contributions
6. **JUNJIE99** - 78 contributions
7. **ftgreat** - 20 contributions
8. **ZhengLiu101** - 15 contributions
9. **Muennighoff** - 12 contributions
10. **shuyansy** - 10 contributions

## File Structure (Sample of 10 files)

Total files: 1,395

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.gitignore` (blob)
- `FlagEmbedding` (tree)
- `FlagEmbedding/__init__.py` (blob)
- `FlagEmbedding/abc` (tree)
- `FlagEmbedding/abc/__init__.py` (blob)
- `FlagEmbedding/abc/evaluation` (tree)
- `FlagEmbedding/abc/evaluation/__init__.py` (blob)

## Recent Issues

- 🟢 **#1480** Continued pretraining using RetroMAE (open)
- 🟢 **#1479** 微调bge-reranker-base一直卡顿，无日志输出 (open)
- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1477** 训练过程中train_loss和eval_loss都是先下降后上升再缓慢下降 (open)
- 🟢 **#1476** How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline (open)

## Recent Pull Requests

- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1475** Add inline comment to setup.py (open)
- 🟢 **#1469** Extend tutorials to include cloud deployment options (open)
- 🔴 **#1466** update reranker inference (closed)
- 🔴 **#1465** update environment (closed)

## Recent Commits

- **875fd4ff** update docs - ZiyiXia (2025-06-04T09:57:10+00:00)
- **235f967a** update tutorials - ZiyiXia (2025-06-04T09:27:36+00:00)
- **5e64baa6** Merge pull request #1466 from 545999961/master - chaofan (2025-05-28T08:15:17+00:00)
- **018b984e** update reranker inference - cfli (2025-05-28T08:14:32+00:00)
- **d04f7989** release version 1.3.5 - chaofan (2025-05-28T07:12:19+00:00)
- **8576fde7** Merge pull request #1465 from 545999961/master - chaofan (2025-05-28T06:58:53+00:00)
- **bf410d51** update environment - cfli (2025-05-28T06:55:54+00:00)
- **97ca0732** Merge pull request #1464 from 545999961/master - chaofan (2025-05-28T06:18:23+00:00)
- **b51706d6** upload coder eval script - cfli (2025-05-28T06:17:19+00:00)
- **821c3879** upload coder eval script - cfli (2025-05-28T06:12:16+00:00)

## External Links Found in README

- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation
- https://arxiv.org/pdf/2310.07554.pdf
- https://huggingface.co/BAAI/bge-reranker-base
- https://arxiv.org/pdf/2409.05591v1
- https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset
- https://img.shields.io/badge/BGE_series-🤗-yellow">
- https://huggingface.co/Shitao
- https://huggingface.co/BAAI/bge-base-en
- https://huggingface.co/BAAI/llm-embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder
- https://huggingface.co/BAAI/bge-reranker-large
- https://img.shields.io/badge/Contribution-Welcome-blue">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder
- https://huggingface.co/BAAI/bge-small-en-v1.5
- https://huggingface.co/BAAI/bge-m3
- https://github.com/VectorSpaceLab/OmniGen
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge
- https://github.com/VectorSpaceLab/MegaPairs

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673596675,
  "name": "FlagEmbedding",
  "full_name": "FlagOpen/FlagEmbedding",
  "description": "Retrieval and Retrieval-augmented LLMs",
  "html_url": "https://github.com/FlagOpen/FlagEmbedding",
  "clone_url": "https://github.com/FlagOpen/FlagEmbedding.git",
  "ssh_url": "git@github.com:FlagOpen/FlagEmbedding.git",
  "homepage": "http://www.bge-model.com/",
  "topics": [
    "embeddings",
    "information-retrieval",
    "llm",
    "sentence-embeddings",
    "text-semantic-similarity",
    "retrieval-augmented-generation"
  ],
  "default_branch": "master",
  "created_at": "2023-08-02T02:08:11+00:00",
  "updated_at": "2025-06-21T10:44:22+00:00",
  "pushed_at": "2025-06-04T10:00:26+00:00",
  "size_kb": 52310,
  "watchers_count": 9974,
  "stargazers_count": 9974,
  "forks_count": 737,
  "open_issues_count": 847,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"
  },
  "languages": {
    "Python": 3871341,
    "Jupyter Notebook": 2607506,
    "Shell": 7322
  },
  "top_contributors": [
    {
      "login": "hanhainebula",
      "contributions": 350
    },
    {
      "login": "545999961",
      "contributions": 327
    },
    {
      "login": "staoxiao",
      "contributions": 182
    },
    {
      "login": "ZiyiXia",
      "contributions": 113
    },
    {
      "login": "namespace-Pt",
      "contributions": 101
    },
    {
      "login": "JUNJIE99",
      "contributions": 78
    },
    {
      "login": "ftgreat",
      "contributions": 20
    },
    {
      "login": "ZhengLiu101",
      "contributions": 15
    },
    {
      "login": "Muennighoff",
      "contributions": 12
    },
    {
      "login": "shuyansy",
      "contributions": 10
    },
    {
      "login": "rainym00d",
      "contributions": 3
    },
    {
      "login": "chinainfant",
      "contributions": 3
    },
    {
      "login": "GeoloeG-IsT",
      "contributions": 3
    },
    {
      "login": "blue-vision0",
      "contributions": 2
    },
    {
      "login": "muazhari",
      "contributions": 2
    },
    {
      "login": "luisegarduno",
      "contributions": 2
    },
    {
      "login": "khazic",
      "contributions": 2
    },
    {
      "login": "shtdbb",
      "contributions": 2
    },
    {
      "login": "pengjunfeng11",
      "contributions": 2
    },
    {
      "login": "ycjcl868",
      "contributions": 2
    }
  ],
  "file_tree_count": 1395,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc/evaluation",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/evaluation/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 217,
  "recent_issues": [
    {
      "number": 1480,
      "title": "Continued pretraining using RetroMAE",
      "state": "open"
    },
    {
      "number": 1479,
      "title": "\u5fae\u8c03bge-reranker-base\u4e00\u76f4\u5361\u987f\uff0c\u65e0\u65e5\u5fd7\u8f93\u51fa",
      "state": "open"
    },
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1477,
      "title": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2dtrain_loss\u548ceval_loss\u90fd\u662f\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u518d\u7f13\u6162\u4e0b\u964d",
      "state": "open"
    },
    {
      "number": 1476,
      "title": "How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1475,
      "title": "Add inline comment to setup.py",
      "state": "open"
    },
    {
      "number": 1469,
      "title": "Extend tutorials to include cloud deployment options",
      "state": "open"
    },
    {
      "number": 1466,
      "title": "update reranker inference",
      "state": "closed"
    },
    {
      "number": 1465,
      "title": "update environment",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "875fd4ffcb894313a918e8bb0c9b7ead82f37e64",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:57:10+00:00",
      "message": "update docs"
    },
    {
      "sha": "235f967a0122b9d5b85104fd4e8f36dc15ee85a8",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:27:36+00:00",
      "message": "update tutorials"
    },
    {
      "sha": "5e64baa61e75df23105a66d1e9d09ad799366e2a",
      "author": "chaofan",
      "date": "2025-05-28T08:15:17+00:00",
      "message": "Merge pull request #1466 from 545999961/master"
    },
    {
      "sha": "018b984e23e12d9793e59f2a1ef07fb9153ab08e",
      "author": "cfli",
      "date": "2025-05-28T08:14:32+00:00",
      "message": "update reranker inference"
    },
    {
      "sha": "d04f7989eeec2c6db4ead7c77ddef656f9c9d3aa",
      "author": "chaofan",
      "date": "2025-05-28T07:12:19+00:00",
      "message": "release version 1.3.5"
    },
    {
      "sha": "8576fde79b84224ecbe493857115552fcfb72ed7",
      "author": "chaofan",
      "date": "2025-05-28T06:58:53+00:00",
      "message": "Merge pull request #1465 from 545999961/master"
    },
    {
      "sha": "bf410d51e9e685e7f1b2e3816f09f3dc64ad8292",
      "author": "cfli",
      "date": "2025-05-28T06:55:54+00:00",
      "message": "update environment"
    },
    {
      "sha": "97ca07325d9e786d668f9a038502ad736dded9d5",
      "author": "chaofan",
      "date": "2025-05-28T06:18:23+00:00",
      "message": "Merge pull request #1464 from 545999961/master"
    },
    {
      "sha": "b51706d63ae8a1d86443df8b0e5ff6e45eb08fdc",
      "author": "cfli",
      "date": "2025-05-28T06:17:19+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "821c387961b5597fe672a4386232110ef54d5107",
      "author": "cfli",
      "date": "2025-05-28T06:12:16+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "3332a1956e2d5af7c52628a60e5c9788bd1b5ed9",
      "author": "chaofan",
      "date": "2025-05-22T10:07:40+00:00",
      "message": "Merge pull request #1459 from 545999961/master"
    },
    {
      "sha": "d412884a4e96c45e4a9e83a43e507e9beaa04713",
      "author": "cfli",
      "date": "2025-05-22T10:06:43+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "5c11feffc104497953a333036aa56b6b543e7cee",
      "author": "chaofan",
      "date": "2025-05-22T09:56:48+00:00",
      "message": "Merge pull request #1458 from 545999961/master"
    },
    {
      "sha": "41b526441f13242b49126261c219b6b9f14e17cb",
      "author": "cfli",
      "date": "2025-05-22T09:55:44+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "52bb9fddb5def2074b277f6e9a2f02ed8aa17dee",
      "author": "chaofan",
      "date": "2025-05-22T09:52:21+00:00",
      "message": "Merge pull request #1457 from 545999961/master"
    },
    {
      "sha": "f05d4b3fbeff5df9dabeb71ab16225fe5e1e8c95",
      "author": "cfli",
      "date": "2025-05-22T09:51:20+00:00",
      "message": "upload reinforced_ir"
    },
    {
      "sha": "25ee99c228e9df63a2be79e8d795d549ab5001e9",
      "author": "chaofan",
      "date": "2025-05-21T04:46:03+00:00",
      "message": "Merge pull request #1453 from 545999961/master"
    },
    {
      "sha": "436210bbed3ca352aac82a9be66cd4e665408944",
      "author": "cfli",
      "date": "2025-05-21T04:45:10+00:00",
      "message": "upload Matroyshka_reranker"
    },
    {
      "sha": "1946f89d5a6926871861c9ed9f4acbb31665acfe",
      "author": "cfli",
      "date": "2025-05-20T08:13:30+00:00",
      "message": "upload Matroyshka Re-Ranker"
    },
    {
      "sha": "f6924351bba09620938b69a2c933c6712be829c4",
      "author": "cfli",
      "date": "2025-05-20T08:12:08+00:00",
      "message": "upload Matroyshka Re-Ranker"
    }
  ],
  "readme_text": "[<img src=\"./imgs/FlagOpen.png\">](https://flagopen.baai.ac.cn/)\n\n<h1 align=\"center\">\u26a1\ufe0fBGE: One-Stop Retrieval Toolkit For Search and RAG</h1>\n\n![bge_logo](./imgs/bge_logo.jpg)\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Contribution-Welcome-blue\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\">\n    </a>\n    <a href=\"https://huggingface.co/C-MTEB\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=#news>News</a> |\n        <a href=#installation>Installation</a> |\n        <a href=#quick-start>Quick Start</a> |\n        <a href=#community>Community</a> |\n        <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a> |\n        <a href=#model-list>Model List</a> |\n        <a href=\"#contributors\">Contributor</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\n\n## News\n\n- 3/6/2025: :fire::fire: Introduce **BGE-VL** ([HF repo](https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92)), State-Of-The-Art multimodal embedding models to support Any visual search applications (everything, including text-to-image, image-to-text, image&prompt-to-image, text-to-image&text, and more)! They are released under the MIT license and are completely free for both academic and commercial use. We also release **MegaPairs** ([repo](https://github.com/VectorSpaceLab/MegaPairs), [paper](https://arxiv.org/abs/2412.14475)), a massive synthetic dataset which empowers BGE-VL!\n- 12/5/2024: :book: We built the [BGE documentation](https://www.bge-model.com) for centralized BGE information and materials!\n- 10/29/2024: :earth_asia: We created WeChat group for BGE. Scan the [QR code](./imgs/BGE_WeChat_Group.png) to join the group chat! To get the first hand message about our updates and new release, or having any questions or ideas, join us now!\n- <img src=\"./imgs/BGE_WeChat_Group.png\" alt=\"bge_wechat_group\" class=\"center\" width=\"200\">\n\n- 10/22/2024: We release another interesting model: [OmniGen](https://github.com/VectorSpaceLab/OmniGen), which is a unified image generation model supporting various tasks. OmniGen can accomplish complex image generation tasks without the need for additional plugins like ControlNet, IP-Adapter, or auxiliary models such as pose detection and face detection.\n- 9/10/2024: Introducing **MemoRAG**, a step forward towards RAG 2.0 on top of memory-inspired knowledge discovery (repo: https://github.com/qhjqhj00/MemoRAG, paper: https://arxiv.org/pdf/2409.05591v1) \n- 9/2/2024: Start to maintain the [tutorials](./Tutorials/). The contents within will be actively updated and eariched, stay tuned! :books:\n- 7/26/2024: Release a new embedding model [bge-en-icl](https://huggingface.co/BAAI/bge-en-icl), an embedding model that incorporates in-context learning capabilities, which, by providing task-relevant query-response examples, can encode semantically richer queries, further enhancing the semantic representation ability of the embeddings.\n- 7/26/2024: Release a new embedding model [bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2), a multilingual embedding model based on gemma-2-9b, which supports multiple languages and diverse downstream tasks, achieving new SOTA on multilingual benchmarks (MIRACL, MTEB-fr, and MTEB-pl). \n- 7/26/2024: Release a new lightweight reranker [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight), a lightweight reranker based on gemma-2-9b, which supports token compression and layerwise lightweight operations, can still ensure good performance while saving a significant amount of resources. :fire:\n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n\n- 6/7/2024: Release a new benchmark [MLVU](https://github.com/JUNJIE99/MLVU), the first comprehensive benchmark specifically designed for long video understanding. MLVU features an extensive range of video durations, a diverse collection of video sources, and a set of evaluation tasks uniquely tailored for long-form video understanding. :fire:\n- 5/21/2024: Release a new benchmark [AIR-Bench](https://github.com/AIR-Bench/AIR-Bench) together with Jina AI, Zilliz, HuggingFace, and other partners. AIR-Bench focuses on a fair out-of-distribution evaluation for Neural IR & RAG. It generates the synthetic data for benchmarking w.r.t. diverse domains and languages. It is dynamic and will be updated on regular basis. [Leaderboard](https://huggingface.co/spaces/AIR-Bench/leaderboard) :fire:\n- 4/30/2024: Release [Llama-3-8B-Instruct-80K-QLoRA](https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA), extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA training on a few synthesized long-context data. The model achieves remarkable performance on various long-context benchmarks. [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora) :fire:\n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually :smiley:) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation :fire:\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data. :fire:\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://arxiv.org/pdf/2402.03216.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) \n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n\n</details>\n\n\nBGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n![projects](./imgs/projects.png)\n\n- **Inference**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker)\n- **Finetune**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)\n- **[Evaluation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation)**\n- **[Dataset](https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset)**\n- **[Tutorials](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)**\n- **[research](https://github.com/FlagOpen/FlagEmbedding/tree/master/research)**\n\n\n## Installation\n### Using pip:\nIf you do not want to finetune the models, you can install the package without the finetune dependency:\n```\npip install -U FlagEmbedding\n```\nIf you want to finetune the models, you can install the package with the finetune dependency:\n```\npip install -U FlagEmbedding[finetune]\n```\n### Install from sources:\n\nClone the repository and install\n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install  .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install  .[finetune]\n```\nFor development in editable mode:\n```\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install -e .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install -e .[finetune]\n```\n\n## Quick Start\nFirst, load one of the BGE embedding model:\n```\nfrom FlagEmbedding import FlagAutoModel\n\nmodel = FlagAutoModel.from_finetuned('BAAI/bge-base-en-v1.5',\n                                      query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n                                      use_fp16=True)\n```\nThen, feed some sentences to the model and get their embeddings:\n```\nsentences_1 = [\"I love NLP\", \"I love machine learning\"]\nsentences_2 = [\"I love BGE\", \"I love text retrieval\"]\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\n```\nOnce we get the embeddings, we can compute similarity by inner product:\n```\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\n\nFor more details, you can refer to [embedder inference](./examples/inference/embedder), [reranker inference](./examples/inference/reranker), [embedder finetune](./examples/finetune/embedder), [reranker fintune](./examples/finetune/reranker), [evaluation](./examples/evaluation).\n\nIf you're unfamiliar with any of related concepts, please check out the [tutorial](./Tutorials/). If it's not there, let us know.\n\nFor more interesting topics related to BGE, take a look at [research](./research).\n\n## Community\n\nWe are actively maintaining the community of BGE and FlagEmbedding. Let us know if you have any suggessions or ideas!\n\nCurrently we are updating the [tutorials](./Tutorials/), we aim to create a comprehensive and detailed tutorial for beginners on text retrieval and RAG. Stay tuned!\n\nThe following contents are releasing in the upcoming weeks:\n\n- Evaluation\n- BGE-EN-ICL\n\n<details>\n  <summary>The whole tutorial roadmap</summary>\n    <img src=\"./Tutorials/tutorial_map.png\"/>\n</details>\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n| Model                                                                     | Language |                                                             Description                                                             |                                query instruction for retrieval                                 |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Provide instructions and few-shot examples freely based on the given task. |\n| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |    Multilingual     | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |        Provide instructions based on the given task.         |\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n| [LM-Cocktail](https://huggingface.co/Shitao)                   |   English |                     fine-tuned models (Llama and BGE) which can be used to reproduce the results of LM-Cocktail                     |  |\n| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English |                         a unified embedding model to support diverse retrieval augmentation needs for LLMs                          | See [README](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder) |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | Multilingual | a lightweight cross-encoder model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |  |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |  |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers, compress ratio and compress layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)             |   English |                                             Embedding Model which map text into vector                                              |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)               |   English |                                    a base-scale model but with similar ability to `bge-large-en`                                    |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)             |   English |                                        a small-scale model but with competitive performance                                         |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)             |   Chinese |                                             Embedding Model which map text into vector                                              |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)               |   Chinese |                                    a base-scale model but with similar ability to `bge-large-zh`                                    |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)             |   Chinese |                                        a small-scale model but with competitive performance                                         |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n\n\n\n\n### Contributors:\nThank all our contributors for their efforts and warmly welcome new members to join in!\n\n<a href=\"https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\" />\n</a>\n\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_m3,\n  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},\n  year={2023},\n  eprint={2309.07597},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n@misc{cocktail,\n      title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Xingrun Xing},\n      year={2023},\n      eprint={2311.13534},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{llm_embedder,\n      title={Retrieve Anything To Augment Large Language Models}, \n      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\n      year={2023},\n      eprint={2310.07554},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). \n\n",
  "external_links_in_readme": [
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation",
    "https://arxiv.org/pdf/2310.07554.pdf",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://arxiv.org/pdf/2409.05591v1",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset",
    "https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">",
    "https://huggingface.co/Shitao",
    "https://huggingface.co/BAAI/bge-base-en",
    "https://huggingface.co/BAAI/llm-embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://img.shields.io/badge/Contribution-Welcome-blue\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://huggingface.co/BAAI/bge-m3",
    "https://github.com/VectorSpaceLab/OmniGen",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge",
    "https://github.com/VectorSpaceLab/MegaPairs",
    "https://github.com/FlagOpen/FlagEmbedding.git",
    "https://huggingface.co/spaces/AIR-Bench/leaderboard",
    "https://github.com/AIR-Bench/AIR-Bench",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker",
    "https://github.com/JUNJIE99/MLVU",
    "https://github.com/qhjqhj00/MemoRAG,",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail",
    "https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research",
    "https://huggingface.co/BAAI/bge-small-zh-v1.5",
    "https://arxiv.org/abs/2412.14475",
    "https://img.shields.io/badge/LICENSE-MIT-green\">",
    "https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">",
    "https://huggingface.co/BAAI/bge-reranker-v2-gemma",
    "https://huggingface.co/spaces/mteb/leaderboard",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker",
    "https://arxiv.org/pdf/2402.03216.pdf",
    "https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\"",
    "https://huggingface.co/BAAI/bge-base-zh",
    "https://arxiv.org/abs/2311.13534",
    "https://huggingface.co/C-MTEB\">",
    "https://flagopen.baai.ac.cn/",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding",
    "https://arxiv.org/abs/2401.03462",
    "https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">",
    "https://arxiv.org/pdf/2309.07597.pdf",
    "https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a>",
    "https://huggingface.co/BAAI/bge-en-icl",
    "https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight",
    "https://www.bge-model.com",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://arxiv.org/abs/2312.15503",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials",
    "https://huggingface.co/BAAI/bge-multilingual-gemma2",
    "https://github.com/FlagOpen/FlagEmbedding\">",
    "https://huggingface.co/BAAI/bge-small-zh",
    "https://huggingface.co/BAAI/bge-small-en",
    "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE",
    "https://data.baai.ac.cn/details/BAAI-MTP",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker",
    "https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise",
    "https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92",
    "https://huggingface.co/BAAI/bge-large-en",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md",
    "https://huggingface.co/BAAI/bge-large-zh",
    "https://huggingface.co/BAAI/bge-base-zh-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora",
    "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA"
  ]
}
```

</details>


---

## Repository 7: FlagOpen/FlagEmbedding

# GitHub Repository Data

**Repository:** [FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## Basic Information

- **Description:** Retrieval and Retrieval-augmented LLMs
- **Created:** 2023-08-02T02:08:11+00:00
- **Last Updated:** 2025-06-21T10:44:22+00:00
- **Last Pushed:** 2025-06-04T10:00:26+00:00
- **Default Branch:** master
- **Size:** 52310 KB

## Statistics

- **Stars:** 9,974
- **Forks:** 737
- **Watchers:** 9,974
- **Open Issues:** 847
- **Total Issues:** 0
- **Pull Requests:** 217

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE)

## Languages

- **Python:** 3,871,341 bytes
- **Jupyter Notebook:** 2,607,506 bytes
- **Shell:** 7,322 bytes

## Topics

- `embeddings`
- `information-retrieval`
- `llm`
- `sentence-embeddings`
- `text-semantic-similarity`
- `retrieval-augmented-generation`

## Top Contributors

1. **hanhainebula** - 350 contributions
2. **545999961** - 327 contributions
3. **staoxiao** - 182 contributions
4. **ZiyiXia** - 113 contributions
5. **namespace-Pt** - 101 contributions
6. **JUNJIE99** - 78 contributions
7. **ftgreat** - 20 contributions
8. **ZhengLiu101** - 15 contributions
9. **Muennighoff** - 12 contributions
10. **shuyansy** - 10 contributions

## File Structure (Sample of 10 files)

Total files: 1,395

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.gitignore` (blob)
- `FlagEmbedding` (tree)
- `FlagEmbedding/__init__.py` (blob)
- `FlagEmbedding/abc` (tree)
- `FlagEmbedding/abc/__init__.py` (blob)
- `FlagEmbedding/abc/evaluation` (tree)
- `FlagEmbedding/abc/evaluation/__init__.py` (blob)

## Recent Issues

- 🟢 **#1480** Continued pretraining using RetroMAE (open)
- 🟢 **#1479** 微调bge-reranker-base一直卡顿，无日志输出 (open)
- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1477** 训练过程中train_loss和eval_loss都是先下降后上升再缓慢下降 (open)
- 🟢 **#1476** How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline (open)

## Recent Pull Requests

- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1475** Add inline comment to setup.py (open)
- 🟢 **#1469** Extend tutorials to include cloud deployment options (open)
- 🔴 **#1466** update reranker inference (closed)
- 🔴 **#1465** update environment (closed)

## Recent Commits

- **875fd4ff** update docs - ZiyiXia (2025-06-04T09:57:10+00:00)
- **235f967a** update tutorials - ZiyiXia (2025-06-04T09:27:36+00:00)
- **5e64baa6** Merge pull request #1466 from 545999961/master - chaofan (2025-05-28T08:15:17+00:00)
- **018b984e** update reranker inference - cfli (2025-05-28T08:14:32+00:00)
- **d04f7989** release version 1.3.5 - chaofan (2025-05-28T07:12:19+00:00)
- **8576fde7** Merge pull request #1465 from 545999961/master - chaofan (2025-05-28T06:58:53+00:00)
- **bf410d51** update environment - cfli (2025-05-28T06:55:54+00:00)
- **97ca0732** Merge pull request #1464 from 545999961/master - chaofan (2025-05-28T06:18:23+00:00)
- **b51706d6** upload coder eval script - cfli (2025-05-28T06:17:19+00:00)
- **821c3879** upload coder eval script - cfli (2025-05-28T06:12:16+00:00)

## External Links Found in README

- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation
- https://arxiv.org/pdf/2310.07554.pdf
- https://huggingface.co/BAAI/bge-reranker-base
- https://arxiv.org/pdf/2409.05591v1
- https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset
- https://img.shields.io/badge/BGE_series-🤗-yellow">
- https://huggingface.co/Shitao
- https://huggingface.co/BAAI/bge-base-en
- https://huggingface.co/BAAI/llm-embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder
- https://huggingface.co/BAAI/bge-reranker-large
- https://img.shields.io/badge/Contribution-Welcome-blue">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder
- https://huggingface.co/BAAI/bge-small-en-v1.5
- https://huggingface.co/BAAI/bge-m3
- https://github.com/VectorSpaceLab/OmniGen
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge
- https://github.com/VectorSpaceLab/MegaPairs

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673596675,
  "name": "FlagEmbedding",
  "full_name": "FlagOpen/FlagEmbedding",
  "description": "Retrieval and Retrieval-augmented LLMs",
  "html_url": "https://github.com/FlagOpen/FlagEmbedding",
  "clone_url": "https://github.com/FlagOpen/FlagEmbedding.git",
  "ssh_url": "git@github.com:FlagOpen/FlagEmbedding.git",
  "homepage": "http://www.bge-model.com/",
  "topics": [
    "embeddings",
    "information-retrieval",
    "llm",
    "sentence-embeddings",
    "text-semantic-similarity",
    "retrieval-augmented-generation"
  ],
  "default_branch": "master",
  "created_at": "2023-08-02T02:08:11+00:00",
  "updated_at": "2025-06-21T10:44:22+00:00",
  "pushed_at": "2025-06-04T10:00:26+00:00",
  "size_kb": 52310,
  "watchers_count": 9974,
  "stargazers_count": 9974,
  "forks_count": 737,
  "open_issues_count": 847,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"
  },
  "languages": {
    "Python": 3871341,
    "Jupyter Notebook": 2607506,
    "Shell": 7322
  },
  "top_contributors": [
    {
      "login": "hanhainebula",
      "contributions": 350
    },
    {
      "login": "545999961",
      "contributions": 327
    },
    {
      "login": "staoxiao",
      "contributions": 182
    },
    {
      "login": "ZiyiXia",
      "contributions": 113
    },
    {
      "login": "namespace-Pt",
      "contributions": 101
    },
    {
      "login": "JUNJIE99",
      "contributions": 78
    },
    {
      "login": "ftgreat",
      "contributions": 20
    },
    {
      "login": "ZhengLiu101",
      "contributions": 15
    },
    {
      "login": "Muennighoff",
      "contributions": 12
    },
    {
      "login": "shuyansy",
      "contributions": 10
    },
    {
      "login": "rainym00d",
      "contributions": 3
    },
    {
      "login": "chinainfant",
      "contributions": 3
    },
    {
      "login": "GeoloeG-IsT",
      "contributions": 3
    },
    {
      "login": "blue-vision0",
      "contributions": 2
    },
    {
      "login": "muazhari",
      "contributions": 2
    },
    {
      "login": "luisegarduno",
      "contributions": 2
    },
    {
      "login": "khazic",
      "contributions": 2
    },
    {
      "login": "shtdbb",
      "contributions": 2
    },
    {
      "login": "pengjunfeng11",
      "contributions": 2
    },
    {
      "login": "ycjcl868",
      "contributions": 2
    }
  ],
  "file_tree_count": 1395,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc/evaluation",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/evaluation/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 217,
  "recent_issues": [
    {
      "number": 1480,
      "title": "Continued pretraining using RetroMAE",
      "state": "open"
    },
    {
      "number": 1479,
      "title": "\u5fae\u8c03bge-reranker-base\u4e00\u76f4\u5361\u987f\uff0c\u65e0\u65e5\u5fd7\u8f93\u51fa",
      "state": "open"
    },
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1477,
      "title": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2dtrain_loss\u548ceval_loss\u90fd\u662f\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u518d\u7f13\u6162\u4e0b\u964d",
      "state": "open"
    },
    {
      "number": 1476,
      "title": "How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1475,
      "title": "Add inline comment to setup.py",
      "state": "open"
    },
    {
      "number": 1469,
      "title": "Extend tutorials to include cloud deployment options",
      "state": "open"
    },
    {
      "number": 1466,
      "title": "update reranker inference",
      "state": "closed"
    },
    {
      "number": 1465,
      "title": "update environment",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "875fd4ffcb894313a918e8bb0c9b7ead82f37e64",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:57:10+00:00",
      "message": "update docs"
    },
    {
      "sha": "235f967a0122b9d5b85104fd4e8f36dc15ee85a8",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:27:36+00:00",
      "message": "update tutorials"
    },
    {
      "sha": "5e64baa61e75df23105a66d1e9d09ad799366e2a",
      "author": "chaofan",
      "date": "2025-05-28T08:15:17+00:00",
      "message": "Merge pull request #1466 from 545999961/master"
    },
    {
      "sha": "018b984e23e12d9793e59f2a1ef07fb9153ab08e",
      "author": "cfli",
      "date": "2025-05-28T08:14:32+00:00",
      "message": "update reranker inference"
    },
    {
      "sha": "d04f7989eeec2c6db4ead7c77ddef656f9c9d3aa",
      "author": "chaofan",
      "date": "2025-05-28T07:12:19+00:00",
      "message": "release version 1.3.5"
    },
    {
      "sha": "8576fde79b84224ecbe493857115552fcfb72ed7",
      "author": "chaofan",
      "date": "2025-05-28T06:58:53+00:00",
      "message": "Merge pull request #1465 from 545999961/master"
    },
    {
      "sha": "bf410d51e9e685e7f1b2e3816f09f3dc64ad8292",
      "author": "cfli",
      "date": "2025-05-28T06:55:54+00:00",
      "message": "update environment"
    },
    {
      "sha": "97ca07325d9e786d668f9a038502ad736dded9d5",
      "author": "chaofan",
      "date": "2025-05-28T06:18:23+00:00",
      "message": "Merge pull request #1464 from 545999961/master"
    },
    {
      "sha": "b51706d63ae8a1d86443df8b0e5ff6e45eb08fdc",
      "author": "cfli",
      "date": "2025-05-28T06:17:19+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "821c387961b5597fe672a4386232110ef54d5107",
      "author": "cfli",
      "date": "2025-05-28T06:12:16+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "3332a1956e2d5af7c52628a60e5c9788bd1b5ed9",
      "author": "chaofan",
      "date": "2025-05-22T10:07:40+00:00",
      "message": "Merge pull request #1459 from 545999961/master"
    },
    {
      "sha": "d412884a4e96c45e4a9e83a43e507e9beaa04713",
      "author": "cfli",
      "date": "2025-05-22T10:06:43+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "5c11feffc104497953a333036aa56b6b543e7cee",
      "author": "chaofan",
      "date": "2025-05-22T09:56:48+00:00",
      "message": "Merge pull request #1458 from 545999961/master"
    },
    {
      "sha": "41b526441f13242b49126261c219b6b9f14e17cb",
      "author": "cfli",
      "date": "2025-05-22T09:55:44+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "52bb9fddb5def2074b277f6e9a2f02ed8aa17dee",
      "author": "chaofan",
      "date": "2025-05-22T09:52:21+00:00",
      "message": "Merge pull request #1457 from 545999961/master"
    },
    {
      "sha": "f05d4b3fbeff5df9dabeb71ab16225fe5e1e8c95",
      "author": "cfli",
      "date": "2025-05-22T09:51:20+00:00",
      "message": "upload reinforced_ir"
    },
    {
      "sha": "25ee99c228e9df63a2be79e8d795d549ab5001e9",
      "author": "chaofan",
      "date": "2025-05-21T04:46:03+00:00",
      "message": "Merge pull request #1453 from 545999961/master"
    },
    {
      "sha": "436210bbed3ca352aac82a9be66cd4e665408944",
      "author": "cfli",
      "date": "2025-05-21T04:45:10+00:00",
      "message": "upload Matroyshka_reranker"
    },
    {
      "sha": "1946f89d5a6926871861c9ed9f4acbb31665acfe",
      "author": "cfli",
      "date": "2025-05-20T08:13:30+00:00",
      "message": "upload Matroyshka Re-Ranker"
    },
    {
      "sha": "f6924351bba09620938b69a2c933c6712be829c4",
      "author": "cfli",
      "date": "2025-05-20T08:12:08+00:00",
      "message": "upload Matroyshka Re-Ranker"
    }
  ],
  "readme_text": "[<img src=\"./imgs/FlagOpen.png\">](https://flagopen.baai.ac.cn/)\n\n<h1 align=\"center\">\u26a1\ufe0fBGE: One-Stop Retrieval Toolkit For Search and RAG</h1>\n\n![bge_logo](./imgs/bge_logo.jpg)\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Contribution-Welcome-blue\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\">\n    </a>\n    <a href=\"https://huggingface.co/C-MTEB\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=#news>News</a> |\n        <a href=#installation>Installation</a> |\n        <a href=#quick-start>Quick Start</a> |\n        <a href=#community>Community</a> |\n        <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a> |\n        <a href=#model-list>Model List</a> |\n        <a href=\"#contributors\">Contributor</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\n\n## News\n\n- 3/6/2025: :fire::fire: Introduce **BGE-VL** ([HF repo](https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92)), State-Of-The-Art multimodal embedding models to support Any visual search applications (everything, including text-to-image, image-to-text, image&prompt-to-image, text-to-image&text, and more)! They are released under the MIT license and are completely free for both academic and commercial use. We also release **MegaPairs** ([repo](https://github.com/VectorSpaceLab/MegaPairs), [paper](https://arxiv.org/abs/2412.14475)), a massive synthetic dataset which empowers BGE-VL!\n- 12/5/2024: :book: We built the [BGE documentation](https://www.bge-model.com) for centralized BGE information and materials!\n- 10/29/2024: :earth_asia: We created WeChat group for BGE. Scan the [QR code](./imgs/BGE_WeChat_Group.png) to join the group chat! To get the first hand message about our updates and new release, or having any questions or ideas, join us now!\n- <img src=\"./imgs/BGE_WeChat_Group.png\" alt=\"bge_wechat_group\" class=\"center\" width=\"200\">\n\n- 10/22/2024: We release another interesting model: [OmniGen](https://github.com/VectorSpaceLab/OmniGen), which is a unified image generation model supporting various tasks. OmniGen can accomplish complex image generation tasks without the need for additional plugins like ControlNet, IP-Adapter, or auxiliary models such as pose detection and face detection.\n- 9/10/2024: Introducing **MemoRAG**, a step forward towards RAG 2.0 on top of memory-inspired knowledge discovery (repo: https://github.com/qhjqhj00/MemoRAG, paper: https://arxiv.org/pdf/2409.05591v1) \n- 9/2/2024: Start to maintain the [tutorials](./Tutorials/). The contents within will be actively updated and eariched, stay tuned! :books:\n- 7/26/2024: Release a new embedding model [bge-en-icl](https://huggingface.co/BAAI/bge-en-icl), an embedding model that incorporates in-context learning capabilities, which, by providing task-relevant query-response examples, can encode semantically richer queries, further enhancing the semantic representation ability of the embeddings.\n- 7/26/2024: Release a new embedding model [bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2), a multilingual embedding model based on gemma-2-9b, which supports multiple languages and diverse downstream tasks, achieving new SOTA on multilingual benchmarks (MIRACL, MTEB-fr, and MTEB-pl). \n- 7/26/2024: Release a new lightweight reranker [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight), a lightweight reranker based on gemma-2-9b, which supports token compression and layerwise lightweight operations, can still ensure good performance while saving a significant amount of resources. :fire:\n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n\n- 6/7/2024: Release a new benchmark [MLVU](https://github.com/JUNJIE99/MLVU), the first comprehensive benchmark specifically designed for long video understanding. MLVU features an extensive range of video durations, a diverse collection of video sources, and a set of evaluation tasks uniquely tailored for long-form video understanding. :fire:\n- 5/21/2024: Release a new benchmark [AIR-Bench](https://github.com/AIR-Bench/AIR-Bench) together with Jina AI, Zilliz, HuggingFace, and other partners. AIR-Bench focuses on a fair out-of-distribution evaluation for Neural IR & RAG. It generates the synthetic data for benchmarking w.r.t. diverse domains and languages. It is dynamic and will be updated on regular basis. [Leaderboard](https://huggingface.co/spaces/AIR-Bench/leaderboard) :fire:\n- 4/30/2024: Release [Llama-3-8B-Instruct-80K-QLoRA](https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA), extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA training on a few synthesized long-context data. The model achieves remarkable performance on various long-context benchmarks. [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora) :fire:\n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually :smiley:) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation :fire:\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data. :fire:\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://arxiv.org/pdf/2402.03216.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) \n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n\n</details>\n\n\nBGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n![projects](./imgs/projects.png)\n\n- **Inference**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker)\n- **Finetune**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)\n- **[Evaluation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation)**\n- **[Dataset](https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset)**\n- **[Tutorials](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)**\n- **[research](https://github.com/FlagOpen/FlagEmbedding/tree/master/research)**\n\n\n## Installation\n### Using pip:\nIf you do not want to finetune the models, you can install the package without the finetune dependency:\n```\npip install -U FlagEmbedding\n```\nIf you want to finetune the models, you can install the package with the finetune dependency:\n```\npip install -U FlagEmbedding[finetune]\n```\n### Install from sources:\n\nClone the repository and install\n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install  .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install  .[finetune]\n```\nFor development in editable mode:\n```\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install -e .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install -e .[finetune]\n```\n\n## Quick Start\nFirst, load one of the BGE embedding model:\n```\nfrom FlagEmbedding import FlagAutoModel\n\nmodel = FlagAutoModel.from_finetuned('BAAI/bge-base-en-v1.5',\n                                      query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n                                      use_fp16=True)\n```\nThen, feed some sentences to the model and get their embeddings:\n```\nsentences_1 = [\"I love NLP\", \"I love machine learning\"]\nsentences_2 = [\"I love BGE\", \"I love text retrieval\"]\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\n```\nOnce we get the embeddings, we can compute similarity by inner product:\n```\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\n\nFor more details, you can refer to [embedder inference](./examples/inference/embedder), [reranker inference](./examples/inference/reranker), [embedder finetune](./examples/finetune/embedder), [reranker fintune](./examples/finetune/reranker), [evaluation](./examples/evaluation).\n\nIf you're unfamiliar with any of related concepts, please check out the [tutorial](./Tutorials/). If it's not there, let us know.\n\nFor more interesting topics related to BGE, take a look at [research](./research).\n\n## Community\n\nWe are actively maintaining the community of BGE and FlagEmbedding. Let us know if you have any suggessions or ideas!\n\nCurrently we are updating the [tutorials](./Tutorials/), we aim to create a comprehensive and detailed tutorial for beginners on text retrieval and RAG. Stay tuned!\n\nThe following contents are releasing in the upcoming weeks:\n\n- Evaluation\n- BGE-EN-ICL\n\n<details>\n  <summary>The whole tutorial roadmap</summary>\n    <img src=\"./Tutorials/tutorial_map.png\"/>\n</details>\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n| Model                                                                     | Language |                                                             Description                                                             |                                query instruction for retrieval                                 |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Provide instructions and few-shot examples freely based on the given task. |\n| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |    Multilingual     | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |        Provide instructions based on the given task.         |\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n| [LM-Cocktail](https://huggingface.co/Shitao)                   |   English |                     fine-tuned models (Llama and BGE) which can be used to reproduce the results of LM-Cocktail                     |  |\n| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English |                         a unified embedding model to support diverse retrieval augmentation needs for LLMs                          | See [README](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder) |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | Multilingual | a lightweight cross-encoder model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |  |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |  |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers, compress ratio and compress layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)             |   English |                                             Embedding Model which map text into vector                                              |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)               |   English |                                    a base-scale model but with similar ability to `bge-large-en`                                    |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)             |   English |                                        a small-scale model but with competitive performance                                         |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)             |   Chinese |                                             Embedding Model which map text into vector                                              |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)               |   Chinese |                                    a base-scale model but with similar ability to `bge-large-zh`                                    |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)             |   Chinese |                                        a small-scale model but with competitive performance                                         |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n\n\n\n\n### Contributors:\nThank all our contributors for their efforts and warmly welcome new members to join in!\n\n<a href=\"https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\" />\n</a>\n\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_m3,\n  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},\n  year={2023},\n  eprint={2309.07597},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n@misc{cocktail,\n      title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Xingrun Xing},\n      year={2023},\n      eprint={2311.13534},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{llm_embedder,\n      title={Retrieve Anything To Augment Large Language Models}, \n      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\n      year={2023},\n      eprint={2310.07554},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). \n\n",
  "external_links_in_readme": [
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation",
    "https://arxiv.org/pdf/2310.07554.pdf",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://arxiv.org/pdf/2409.05591v1",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset",
    "https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">",
    "https://huggingface.co/Shitao",
    "https://huggingface.co/BAAI/bge-base-en",
    "https://huggingface.co/BAAI/llm-embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://img.shields.io/badge/Contribution-Welcome-blue\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://huggingface.co/BAAI/bge-m3",
    "https://github.com/VectorSpaceLab/OmniGen",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge",
    "https://github.com/VectorSpaceLab/MegaPairs",
    "https://github.com/FlagOpen/FlagEmbedding.git",
    "https://huggingface.co/spaces/AIR-Bench/leaderboard",
    "https://github.com/AIR-Bench/AIR-Bench",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker",
    "https://github.com/JUNJIE99/MLVU",
    "https://github.com/qhjqhj00/MemoRAG,",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail",
    "https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research",
    "https://huggingface.co/BAAI/bge-small-zh-v1.5",
    "https://arxiv.org/abs/2412.14475",
    "https://img.shields.io/badge/LICENSE-MIT-green\">",
    "https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">",
    "https://huggingface.co/BAAI/bge-reranker-v2-gemma",
    "https://huggingface.co/spaces/mteb/leaderboard",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker",
    "https://arxiv.org/pdf/2402.03216.pdf",
    "https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\"",
    "https://huggingface.co/BAAI/bge-base-zh",
    "https://arxiv.org/abs/2311.13534",
    "https://huggingface.co/C-MTEB\">",
    "https://flagopen.baai.ac.cn/",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding",
    "https://arxiv.org/abs/2401.03462",
    "https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">",
    "https://arxiv.org/pdf/2309.07597.pdf",
    "https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a>",
    "https://huggingface.co/BAAI/bge-en-icl",
    "https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight",
    "https://www.bge-model.com",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://arxiv.org/abs/2312.15503",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials",
    "https://huggingface.co/BAAI/bge-multilingual-gemma2",
    "https://github.com/FlagOpen/FlagEmbedding\">",
    "https://huggingface.co/BAAI/bge-small-zh",
    "https://huggingface.co/BAAI/bge-small-en",
    "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE",
    "https://data.baai.ac.cn/details/BAAI-MTP",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker",
    "https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise",
    "https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92",
    "https://huggingface.co/BAAI/bge-large-en",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md",
    "https://huggingface.co/BAAI/bge-large-zh",
    "https://huggingface.co/BAAI/bge-base-zh-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora",
    "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA"
  ]
}
```

</details>


---

## Repository 8: FlagOpen/FlagEmbedding

# GitHub Repository Data

**Repository:** [FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## Basic Information

- **Description:** Retrieval and Retrieval-augmented LLMs
- **Created:** 2023-08-02T02:08:11+00:00
- **Last Updated:** 2025-06-21T10:44:22+00:00
- **Last Pushed:** 2025-06-04T10:00:26+00:00
- **Default Branch:** master
- **Size:** 52310 KB

## Statistics

- **Stars:** 9,974
- **Forks:** 737
- **Watchers:** 9,974
- **Open Issues:** 847
- **Total Issues:** 0
- **Pull Requests:** 217

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE)

## Languages

- **Python:** 3,871,341 bytes
- **Jupyter Notebook:** 2,607,506 bytes
- **Shell:** 7,322 bytes

## Topics

- `embeddings`
- `information-retrieval`
- `llm`
- `sentence-embeddings`
- `text-semantic-similarity`
- `retrieval-augmented-generation`

## Top Contributors

1. **hanhainebula** - 350 contributions
2. **545999961** - 327 contributions
3. **staoxiao** - 182 contributions
4. **ZiyiXia** - 113 contributions
5. **namespace-Pt** - 101 contributions
6. **JUNJIE99** - 78 contributions
7. **ftgreat** - 20 contributions
8. **ZhengLiu101** - 15 contributions
9. **Muennighoff** - 12 contributions
10. **shuyansy** - 10 contributions

## File Structure (Sample of 10 files)

Total files: 1,395

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.gitignore` (blob)
- `FlagEmbedding` (tree)
- `FlagEmbedding/__init__.py` (blob)
- `FlagEmbedding/abc` (tree)
- `FlagEmbedding/abc/__init__.py` (blob)
- `FlagEmbedding/abc/evaluation` (tree)
- `FlagEmbedding/abc/evaluation/__init__.py` (blob)

## Recent Issues

- 🟢 **#1480** Continued pretraining using RetroMAE (open)
- 🟢 **#1479** 微调bge-reranker-base一直卡顿，无日志输出 (open)
- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1477** 训练过程中train_loss和eval_loss都是先下降后上升再缓慢下降 (open)
- 🟢 **#1476** How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline (open)

## Recent Pull Requests

- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1475** Add inline comment to setup.py (open)
- 🟢 **#1469** Extend tutorials to include cloud deployment options (open)
- 🔴 **#1466** update reranker inference (closed)
- 🔴 **#1465** update environment (closed)

## Recent Commits

- **875fd4ff** update docs - ZiyiXia (2025-06-04T09:57:10+00:00)
- **235f967a** update tutorials - ZiyiXia (2025-06-04T09:27:36+00:00)
- **5e64baa6** Merge pull request #1466 from 545999961/master - chaofan (2025-05-28T08:15:17+00:00)
- **018b984e** update reranker inference - cfli (2025-05-28T08:14:32+00:00)
- **d04f7989** release version 1.3.5 - chaofan (2025-05-28T07:12:19+00:00)
- **8576fde7** Merge pull request #1465 from 545999961/master - chaofan (2025-05-28T06:58:53+00:00)
- **bf410d51** update environment - cfli (2025-05-28T06:55:54+00:00)
- **97ca0732** Merge pull request #1464 from 545999961/master - chaofan (2025-05-28T06:18:23+00:00)
- **b51706d6** upload coder eval script - cfli (2025-05-28T06:17:19+00:00)
- **821c3879** upload coder eval script - cfli (2025-05-28T06:12:16+00:00)

## External Links Found in README

- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation
- https://arxiv.org/pdf/2310.07554.pdf
- https://huggingface.co/BAAI/bge-reranker-base
- https://arxiv.org/pdf/2409.05591v1
- https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset
- https://img.shields.io/badge/BGE_series-🤗-yellow">
- https://huggingface.co/Shitao
- https://huggingface.co/BAAI/bge-base-en
- https://huggingface.co/BAAI/llm-embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder
- https://huggingface.co/BAAI/bge-reranker-large
- https://img.shields.io/badge/Contribution-Welcome-blue">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder
- https://huggingface.co/BAAI/bge-small-en-v1.5
- https://huggingface.co/BAAI/bge-m3
- https://github.com/VectorSpaceLab/OmniGen
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge
- https://github.com/VectorSpaceLab/MegaPairs

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673596675,
  "name": "FlagEmbedding",
  "full_name": "FlagOpen/FlagEmbedding",
  "description": "Retrieval and Retrieval-augmented LLMs",
  "html_url": "https://github.com/FlagOpen/FlagEmbedding",
  "clone_url": "https://github.com/FlagOpen/FlagEmbedding.git",
  "ssh_url": "git@github.com:FlagOpen/FlagEmbedding.git",
  "homepage": "http://www.bge-model.com/",
  "topics": [
    "embeddings",
    "information-retrieval",
    "llm",
    "sentence-embeddings",
    "text-semantic-similarity",
    "retrieval-augmented-generation"
  ],
  "default_branch": "master",
  "created_at": "2023-08-02T02:08:11+00:00",
  "updated_at": "2025-06-21T10:44:22+00:00",
  "pushed_at": "2025-06-04T10:00:26+00:00",
  "size_kb": 52310,
  "watchers_count": 9974,
  "stargazers_count": 9974,
  "forks_count": 737,
  "open_issues_count": 847,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"
  },
  "languages": {
    "Python": 3871341,
    "Jupyter Notebook": 2607506,
    "Shell": 7322
  },
  "top_contributors": [
    {
      "login": "hanhainebula",
      "contributions": 350
    },
    {
      "login": "545999961",
      "contributions": 327
    },
    {
      "login": "staoxiao",
      "contributions": 182
    },
    {
      "login": "ZiyiXia",
      "contributions": 113
    },
    {
      "login": "namespace-Pt",
      "contributions": 101
    },
    {
      "login": "JUNJIE99",
      "contributions": 78
    },
    {
      "login": "ftgreat",
      "contributions": 20
    },
    {
      "login": "ZhengLiu101",
      "contributions": 15
    },
    {
      "login": "Muennighoff",
      "contributions": 12
    },
    {
      "login": "shuyansy",
      "contributions": 10
    },
    {
      "login": "rainym00d",
      "contributions": 3
    },
    {
      "login": "chinainfant",
      "contributions": 3
    },
    {
      "login": "GeoloeG-IsT",
      "contributions": 3
    },
    {
      "login": "blue-vision0",
      "contributions": 2
    },
    {
      "login": "muazhari",
      "contributions": 2
    },
    {
      "login": "luisegarduno",
      "contributions": 2
    },
    {
      "login": "khazic",
      "contributions": 2
    },
    {
      "login": "shtdbb",
      "contributions": 2
    },
    {
      "login": "pengjunfeng11",
      "contributions": 2
    },
    {
      "login": "ycjcl868",
      "contributions": 2
    }
  ],
  "file_tree_count": 1395,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc/evaluation",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/evaluation/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 217,
  "recent_issues": [
    {
      "number": 1480,
      "title": "Continued pretraining using RetroMAE",
      "state": "open"
    },
    {
      "number": 1479,
      "title": "\u5fae\u8c03bge-reranker-base\u4e00\u76f4\u5361\u987f\uff0c\u65e0\u65e5\u5fd7\u8f93\u51fa",
      "state": "open"
    },
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1477,
      "title": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2dtrain_loss\u548ceval_loss\u90fd\u662f\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u518d\u7f13\u6162\u4e0b\u964d",
      "state": "open"
    },
    {
      "number": 1476,
      "title": "How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1475,
      "title": "Add inline comment to setup.py",
      "state": "open"
    },
    {
      "number": 1469,
      "title": "Extend tutorials to include cloud deployment options",
      "state": "open"
    },
    {
      "number": 1466,
      "title": "update reranker inference",
      "state": "closed"
    },
    {
      "number": 1465,
      "title": "update environment",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "875fd4ffcb894313a918e8bb0c9b7ead82f37e64",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:57:10+00:00",
      "message": "update docs"
    },
    {
      "sha": "235f967a0122b9d5b85104fd4e8f36dc15ee85a8",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:27:36+00:00",
      "message": "update tutorials"
    },
    {
      "sha": "5e64baa61e75df23105a66d1e9d09ad799366e2a",
      "author": "chaofan",
      "date": "2025-05-28T08:15:17+00:00",
      "message": "Merge pull request #1466 from 545999961/master"
    },
    {
      "sha": "018b984e23e12d9793e59f2a1ef07fb9153ab08e",
      "author": "cfli",
      "date": "2025-05-28T08:14:32+00:00",
      "message": "update reranker inference"
    },
    {
      "sha": "d04f7989eeec2c6db4ead7c77ddef656f9c9d3aa",
      "author": "chaofan",
      "date": "2025-05-28T07:12:19+00:00",
      "message": "release version 1.3.5"
    },
    {
      "sha": "8576fde79b84224ecbe493857115552fcfb72ed7",
      "author": "chaofan",
      "date": "2025-05-28T06:58:53+00:00",
      "message": "Merge pull request #1465 from 545999961/master"
    },
    {
      "sha": "bf410d51e9e685e7f1b2e3816f09f3dc64ad8292",
      "author": "cfli",
      "date": "2025-05-28T06:55:54+00:00",
      "message": "update environment"
    },
    {
      "sha": "97ca07325d9e786d668f9a038502ad736dded9d5",
      "author": "chaofan",
      "date": "2025-05-28T06:18:23+00:00",
      "message": "Merge pull request #1464 from 545999961/master"
    },
    {
      "sha": "b51706d63ae8a1d86443df8b0e5ff6e45eb08fdc",
      "author": "cfli",
      "date": "2025-05-28T06:17:19+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "821c387961b5597fe672a4386232110ef54d5107",
      "author": "cfli",
      "date": "2025-05-28T06:12:16+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "3332a1956e2d5af7c52628a60e5c9788bd1b5ed9",
      "author": "chaofan",
      "date": "2025-05-22T10:07:40+00:00",
      "message": "Merge pull request #1459 from 545999961/master"
    },
    {
      "sha": "d412884a4e96c45e4a9e83a43e507e9beaa04713",
      "author": "cfli",
      "date": "2025-05-22T10:06:43+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "5c11feffc104497953a333036aa56b6b543e7cee",
      "author": "chaofan",
      "date": "2025-05-22T09:56:48+00:00",
      "message": "Merge pull request #1458 from 545999961/master"
    },
    {
      "sha": "41b526441f13242b49126261c219b6b9f14e17cb",
      "author": "cfli",
      "date": "2025-05-22T09:55:44+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "52bb9fddb5def2074b277f6e9a2f02ed8aa17dee",
      "author": "chaofan",
      "date": "2025-05-22T09:52:21+00:00",
      "message": "Merge pull request #1457 from 545999961/master"
    },
    {
      "sha": "f05d4b3fbeff5df9dabeb71ab16225fe5e1e8c95",
      "author": "cfli",
      "date": "2025-05-22T09:51:20+00:00",
      "message": "upload reinforced_ir"
    },
    {
      "sha": "25ee99c228e9df63a2be79e8d795d549ab5001e9",
      "author": "chaofan",
      "date": "2025-05-21T04:46:03+00:00",
      "message": "Merge pull request #1453 from 545999961/master"
    },
    {
      "sha": "436210bbed3ca352aac82a9be66cd4e665408944",
      "author": "cfli",
      "date": "2025-05-21T04:45:10+00:00",
      "message": "upload Matroyshka_reranker"
    },
    {
      "sha": "1946f89d5a6926871861c9ed9f4acbb31665acfe",
      "author": "cfli",
      "date": "2025-05-20T08:13:30+00:00",
      "message": "upload Matroyshka Re-Ranker"
    },
    {
      "sha": "f6924351bba09620938b69a2c933c6712be829c4",
      "author": "cfli",
      "date": "2025-05-20T08:12:08+00:00",
      "message": "upload Matroyshka Re-Ranker"
    }
  ],
  "readme_text": "[<img src=\"./imgs/FlagOpen.png\">](https://flagopen.baai.ac.cn/)\n\n<h1 align=\"center\">\u26a1\ufe0fBGE: One-Stop Retrieval Toolkit For Search and RAG</h1>\n\n![bge_logo](./imgs/bge_logo.jpg)\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Contribution-Welcome-blue\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\">\n    </a>\n    <a href=\"https://huggingface.co/C-MTEB\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=#news>News</a> |\n        <a href=#installation>Installation</a> |\n        <a href=#quick-start>Quick Start</a> |\n        <a href=#community>Community</a> |\n        <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a> |\n        <a href=#model-list>Model List</a> |\n        <a href=\"#contributors\">Contributor</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\n\n## News\n\n- 3/6/2025: :fire::fire: Introduce **BGE-VL** ([HF repo](https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92)), State-Of-The-Art multimodal embedding models to support Any visual search applications (everything, including text-to-image, image-to-text, image&prompt-to-image, text-to-image&text, and more)! They are released under the MIT license and are completely free for both academic and commercial use. We also release **MegaPairs** ([repo](https://github.com/VectorSpaceLab/MegaPairs), [paper](https://arxiv.org/abs/2412.14475)), a massive synthetic dataset which empowers BGE-VL!\n- 12/5/2024: :book: We built the [BGE documentation](https://www.bge-model.com) for centralized BGE information and materials!\n- 10/29/2024: :earth_asia: We created WeChat group for BGE. Scan the [QR code](./imgs/BGE_WeChat_Group.png) to join the group chat! To get the first hand message about our updates and new release, or having any questions or ideas, join us now!\n- <img src=\"./imgs/BGE_WeChat_Group.png\" alt=\"bge_wechat_group\" class=\"center\" width=\"200\">\n\n- 10/22/2024: We release another interesting model: [OmniGen](https://github.com/VectorSpaceLab/OmniGen), which is a unified image generation model supporting various tasks. OmniGen can accomplish complex image generation tasks without the need for additional plugins like ControlNet, IP-Adapter, or auxiliary models such as pose detection and face detection.\n- 9/10/2024: Introducing **MemoRAG**, a step forward towards RAG 2.0 on top of memory-inspired knowledge discovery (repo: https://github.com/qhjqhj00/MemoRAG, paper: https://arxiv.org/pdf/2409.05591v1) \n- 9/2/2024: Start to maintain the [tutorials](./Tutorials/). The contents within will be actively updated and eariched, stay tuned! :books:\n- 7/26/2024: Release a new embedding model [bge-en-icl](https://huggingface.co/BAAI/bge-en-icl), an embedding model that incorporates in-context learning capabilities, which, by providing task-relevant query-response examples, can encode semantically richer queries, further enhancing the semantic representation ability of the embeddings.\n- 7/26/2024: Release a new embedding model [bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2), a multilingual embedding model based on gemma-2-9b, which supports multiple languages and diverse downstream tasks, achieving new SOTA on multilingual benchmarks (MIRACL, MTEB-fr, and MTEB-pl). \n- 7/26/2024: Release a new lightweight reranker [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight), a lightweight reranker based on gemma-2-9b, which supports token compression and layerwise lightweight operations, can still ensure good performance while saving a significant amount of resources. :fire:\n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n\n- 6/7/2024: Release a new benchmark [MLVU](https://github.com/JUNJIE99/MLVU), the first comprehensive benchmark specifically designed for long video understanding. MLVU features an extensive range of video durations, a diverse collection of video sources, and a set of evaluation tasks uniquely tailored for long-form video understanding. :fire:\n- 5/21/2024: Release a new benchmark [AIR-Bench](https://github.com/AIR-Bench/AIR-Bench) together with Jina AI, Zilliz, HuggingFace, and other partners. AIR-Bench focuses on a fair out-of-distribution evaluation for Neural IR & RAG. It generates the synthetic data for benchmarking w.r.t. diverse domains and languages. It is dynamic and will be updated on regular basis. [Leaderboard](https://huggingface.co/spaces/AIR-Bench/leaderboard) :fire:\n- 4/30/2024: Release [Llama-3-8B-Instruct-80K-QLoRA](https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA), extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA training on a few synthesized long-context data. The model achieves remarkable performance on various long-context benchmarks. [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora) :fire:\n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually :smiley:) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation :fire:\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data. :fire:\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://arxiv.org/pdf/2402.03216.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) \n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n\n</details>\n\n\nBGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n![projects](./imgs/projects.png)\n\n- **Inference**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker)\n- **Finetune**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)\n- **[Evaluation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation)**\n- **[Dataset](https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset)**\n- **[Tutorials](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)**\n- **[research](https://github.com/FlagOpen/FlagEmbedding/tree/master/research)**\n\n\n## Installation\n### Using pip:\nIf you do not want to finetune the models, you can install the package without the finetune dependency:\n```\npip install -U FlagEmbedding\n```\nIf you want to finetune the models, you can install the package with the finetune dependency:\n```\npip install -U FlagEmbedding[finetune]\n```\n### Install from sources:\n\nClone the repository and install\n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install  .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install  .[finetune]\n```\nFor development in editable mode:\n```\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install -e .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install -e .[finetune]\n```\n\n## Quick Start\nFirst, load one of the BGE embedding model:\n```\nfrom FlagEmbedding import FlagAutoModel\n\nmodel = FlagAutoModel.from_finetuned('BAAI/bge-base-en-v1.5',\n                                      query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n                                      use_fp16=True)\n```\nThen, feed some sentences to the model and get their embeddings:\n```\nsentences_1 = [\"I love NLP\", \"I love machine learning\"]\nsentences_2 = [\"I love BGE\", \"I love text retrieval\"]\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\n```\nOnce we get the embeddings, we can compute similarity by inner product:\n```\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\n\nFor more details, you can refer to [embedder inference](./examples/inference/embedder), [reranker inference](./examples/inference/reranker), [embedder finetune](./examples/finetune/embedder), [reranker fintune](./examples/finetune/reranker), [evaluation](./examples/evaluation).\n\nIf you're unfamiliar with any of related concepts, please check out the [tutorial](./Tutorials/). If it's not there, let us know.\n\nFor more interesting topics related to BGE, take a look at [research](./research).\n\n## Community\n\nWe are actively maintaining the community of BGE and FlagEmbedding. Let us know if you have any suggessions or ideas!\n\nCurrently we are updating the [tutorials](./Tutorials/), we aim to create a comprehensive and detailed tutorial for beginners on text retrieval and RAG. Stay tuned!\n\nThe following contents are releasing in the upcoming weeks:\n\n- Evaluation\n- BGE-EN-ICL\n\n<details>\n  <summary>The whole tutorial roadmap</summary>\n    <img src=\"./Tutorials/tutorial_map.png\"/>\n</details>\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n| Model                                                                     | Language |                                                             Description                                                             |                                query instruction for retrieval                                 |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Provide instructions and few-shot examples freely based on the given task. |\n| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |    Multilingual     | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |        Provide instructions based on the given task.         |\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n| [LM-Cocktail](https://huggingface.co/Shitao)                   |   English |                     fine-tuned models (Llama and BGE) which can be used to reproduce the results of LM-Cocktail                     |  |\n| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English |                         a unified embedding model to support diverse retrieval augmentation needs for LLMs                          | See [README](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder) |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | Multilingual | a lightweight cross-encoder model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |  |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |  |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers, compress ratio and compress layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)             |   English |                                             Embedding Model which map text into vector                                              |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)               |   English |                                    a base-scale model but with similar ability to `bge-large-en`                                    |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)             |   English |                                        a small-scale model but with competitive performance                                         |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)             |   Chinese |                                             Embedding Model which map text into vector                                              |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)               |   Chinese |                                    a base-scale model but with similar ability to `bge-large-zh`                                    |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)             |   Chinese |                                        a small-scale model but with competitive performance                                         |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n\n\n\n\n### Contributors:\nThank all our contributors for their efforts and warmly welcome new members to join in!\n\n<a href=\"https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\" />\n</a>\n\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_m3,\n  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},\n  year={2023},\n  eprint={2309.07597},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n@misc{cocktail,\n      title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Xingrun Xing},\n      year={2023},\n      eprint={2311.13534},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{llm_embedder,\n      title={Retrieve Anything To Augment Large Language Models}, \n      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\n      year={2023},\n      eprint={2310.07554},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). \n\n",
  "external_links_in_readme": [
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation",
    "https://arxiv.org/pdf/2310.07554.pdf",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://arxiv.org/pdf/2409.05591v1",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset",
    "https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">",
    "https://huggingface.co/Shitao",
    "https://huggingface.co/BAAI/bge-base-en",
    "https://huggingface.co/BAAI/llm-embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://img.shields.io/badge/Contribution-Welcome-blue\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://huggingface.co/BAAI/bge-m3",
    "https://github.com/VectorSpaceLab/OmniGen",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge",
    "https://github.com/VectorSpaceLab/MegaPairs",
    "https://github.com/FlagOpen/FlagEmbedding.git",
    "https://huggingface.co/spaces/AIR-Bench/leaderboard",
    "https://github.com/AIR-Bench/AIR-Bench",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker",
    "https://github.com/JUNJIE99/MLVU",
    "https://github.com/qhjqhj00/MemoRAG,",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail",
    "https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research",
    "https://huggingface.co/BAAI/bge-small-zh-v1.5",
    "https://arxiv.org/abs/2412.14475",
    "https://img.shields.io/badge/LICENSE-MIT-green\">",
    "https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">",
    "https://huggingface.co/BAAI/bge-reranker-v2-gemma",
    "https://huggingface.co/spaces/mteb/leaderboard",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker",
    "https://arxiv.org/pdf/2402.03216.pdf",
    "https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\"",
    "https://huggingface.co/BAAI/bge-base-zh",
    "https://arxiv.org/abs/2311.13534",
    "https://huggingface.co/C-MTEB\">",
    "https://flagopen.baai.ac.cn/",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding",
    "https://arxiv.org/abs/2401.03462",
    "https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">",
    "https://arxiv.org/pdf/2309.07597.pdf",
    "https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a>",
    "https://huggingface.co/BAAI/bge-en-icl",
    "https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight",
    "https://www.bge-model.com",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://arxiv.org/abs/2312.15503",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials",
    "https://huggingface.co/BAAI/bge-multilingual-gemma2",
    "https://github.com/FlagOpen/FlagEmbedding\">",
    "https://huggingface.co/BAAI/bge-small-zh",
    "https://huggingface.co/BAAI/bge-small-en",
    "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE",
    "https://data.baai.ac.cn/details/BAAI-MTP",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker",
    "https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise",
    "https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92",
    "https://huggingface.co/BAAI/bge-large-en",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md",
    "https://huggingface.co/BAAI/bge-large-zh",
    "https://huggingface.co/BAAI/bge-base-zh-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora",
    "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA"
  ]
}
```

</details>


---

## Repository 9: FlagOpen/FlagEmbedding

# GitHub Repository Data

**Repository:** [FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## Basic Information

- **Description:** Retrieval and Retrieval-augmented LLMs
- **Created:** 2023-08-02T02:08:11+00:00
- **Last Updated:** 2025-06-21T10:44:22+00:00
- **Last Pushed:** 2025-06-04T10:00:26+00:00
- **Default Branch:** master
- **Size:** 52310 KB

## Statistics

- **Stars:** 9,974
- **Forks:** 737
- **Watchers:** 9,974
- **Open Issues:** 847
- **Total Issues:** 0
- **Pull Requests:** 217

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE)

## Languages

- **Python:** 3,871,341 bytes
- **Jupyter Notebook:** 2,607,506 bytes
- **Shell:** 7,322 bytes

## Topics

- `embeddings`
- `information-retrieval`
- `llm`
- `sentence-embeddings`
- `text-semantic-similarity`
- `retrieval-augmented-generation`

## Top Contributors

1. **hanhainebula** - 350 contributions
2. **545999961** - 327 contributions
3. **staoxiao** - 182 contributions
4. **ZiyiXia** - 113 contributions
5. **namespace-Pt** - 101 contributions
6. **JUNJIE99** - 78 contributions
7. **ftgreat** - 20 contributions
8. **ZhengLiu101** - 15 contributions
9. **Muennighoff** - 12 contributions
10. **shuyansy** - 10 contributions

## File Structure (Sample of 10 files)

Total files: 1,395

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.gitignore` (blob)
- `FlagEmbedding` (tree)
- `FlagEmbedding/__init__.py` (blob)
- `FlagEmbedding/abc` (tree)
- `FlagEmbedding/abc/__init__.py` (blob)
- `FlagEmbedding/abc/evaluation` (tree)
- `FlagEmbedding/abc/evaluation/__init__.py` (blob)

## Recent Issues

- 🟢 **#1480** Continued pretraining using RetroMAE (open)
- 🟢 **#1479** 微调bge-reranker-base一直卡顿，无日志输出 (open)
- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1477** 训练过程中train_loss和eval_loss都是先下降后上升再缓慢下降 (open)
- 🟢 **#1476** How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline (open)

## Recent Pull Requests

- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1475** Add inline comment to setup.py (open)
- 🟢 **#1469** Extend tutorials to include cloud deployment options (open)
- 🔴 **#1466** update reranker inference (closed)
- 🔴 **#1465** update environment (closed)

## Recent Commits

- **875fd4ff** update docs - ZiyiXia (2025-06-04T09:57:10+00:00)
- **235f967a** update tutorials - ZiyiXia (2025-06-04T09:27:36+00:00)
- **5e64baa6** Merge pull request #1466 from 545999961/master - chaofan (2025-05-28T08:15:17+00:00)
- **018b984e** update reranker inference - cfli (2025-05-28T08:14:32+00:00)
- **d04f7989** release version 1.3.5 - chaofan (2025-05-28T07:12:19+00:00)
- **8576fde7** Merge pull request #1465 from 545999961/master - chaofan (2025-05-28T06:58:53+00:00)
- **bf410d51** update environment - cfli (2025-05-28T06:55:54+00:00)
- **97ca0732** Merge pull request #1464 from 545999961/master - chaofan (2025-05-28T06:18:23+00:00)
- **b51706d6** upload coder eval script - cfli (2025-05-28T06:17:19+00:00)
- **821c3879** upload coder eval script - cfli (2025-05-28T06:12:16+00:00)

## External Links Found in README

- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation
- https://arxiv.org/pdf/2310.07554.pdf
- https://huggingface.co/BAAI/bge-reranker-base
- https://arxiv.org/pdf/2409.05591v1
- https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset
- https://img.shields.io/badge/BGE_series-🤗-yellow">
- https://huggingface.co/Shitao
- https://huggingface.co/BAAI/bge-base-en
- https://huggingface.co/BAAI/llm-embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder
- https://huggingface.co/BAAI/bge-reranker-large
- https://img.shields.io/badge/Contribution-Welcome-blue">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder
- https://huggingface.co/BAAI/bge-small-en-v1.5
- https://huggingface.co/BAAI/bge-m3
- https://github.com/VectorSpaceLab/OmniGen
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge
- https://github.com/VectorSpaceLab/MegaPairs

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673596675,
  "name": "FlagEmbedding",
  "full_name": "FlagOpen/FlagEmbedding",
  "description": "Retrieval and Retrieval-augmented LLMs",
  "html_url": "https://github.com/FlagOpen/FlagEmbedding",
  "clone_url": "https://github.com/FlagOpen/FlagEmbedding.git",
  "ssh_url": "git@github.com:FlagOpen/FlagEmbedding.git",
  "homepage": "http://www.bge-model.com/",
  "topics": [
    "embeddings",
    "information-retrieval",
    "llm",
    "sentence-embeddings",
    "text-semantic-similarity",
    "retrieval-augmented-generation"
  ],
  "default_branch": "master",
  "created_at": "2023-08-02T02:08:11+00:00",
  "updated_at": "2025-06-21T10:44:22+00:00",
  "pushed_at": "2025-06-04T10:00:26+00:00",
  "size_kb": 52310,
  "watchers_count": 9974,
  "stargazers_count": 9974,
  "forks_count": 737,
  "open_issues_count": 847,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"
  },
  "languages": {
    "Python": 3871341,
    "Jupyter Notebook": 2607506,
    "Shell": 7322
  },
  "top_contributors": [
    {
      "login": "hanhainebula",
      "contributions": 350
    },
    {
      "login": "545999961",
      "contributions": 327
    },
    {
      "login": "staoxiao",
      "contributions": 182
    },
    {
      "login": "ZiyiXia",
      "contributions": 113
    },
    {
      "login": "namespace-Pt",
      "contributions": 101
    },
    {
      "login": "JUNJIE99",
      "contributions": 78
    },
    {
      "login": "ftgreat",
      "contributions": 20
    },
    {
      "login": "ZhengLiu101",
      "contributions": 15
    },
    {
      "login": "Muennighoff",
      "contributions": 12
    },
    {
      "login": "shuyansy",
      "contributions": 10
    },
    {
      "login": "rainym00d",
      "contributions": 3
    },
    {
      "login": "chinainfant",
      "contributions": 3
    },
    {
      "login": "GeoloeG-IsT",
      "contributions": 3
    },
    {
      "login": "blue-vision0",
      "contributions": 2
    },
    {
      "login": "muazhari",
      "contributions": 2
    },
    {
      "login": "luisegarduno",
      "contributions": 2
    },
    {
      "login": "khazic",
      "contributions": 2
    },
    {
      "login": "shtdbb",
      "contributions": 2
    },
    {
      "login": "pengjunfeng11",
      "contributions": 2
    },
    {
      "login": "ycjcl868",
      "contributions": 2
    }
  ],
  "file_tree_count": 1395,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc/evaluation",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/evaluation/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 217,
  "recent_issues": [
    {
      "number": 1480,
      "title": "Continued pretraining using RetroMAE",
      "state": "open"
    },
    {
      "number": 1479,
      "title": "\u5fae\u8c03bge-reranker-base\u4e00\u76f4\u5361\u987f\uff0c\u65e0\u65e5\u5fd7\u8f93\u51fa",
      "state": "open"
    },
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1477,
      "title": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2dtrain_loss\u548ceval_loss\u90fd\u662f\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u518d\u7f13\u6162\u4e0b\u964d",
      "state": "open"
    },
    {
      "number": 1476,
      "title": "How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1475,
      "title": "Add inline comment to setup.py",
      "state": "open"
    },
    {
      "number": 1469,
      "title": "Extend tutorials to include cloud deployment options",
      "state": "open"
    },
    {
      "number": 1466,
      "title": "update reranker inference",
      "state": "closed"
    },
    {
      "number": 1465,
      "title": "update environment",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "875fd4ffcb894313a918e8bb0c9b7ead82f37e64",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:57:10+00:00",
      "message": "update docs"
    },
    {
      "sha": "235f967a0122b9d5b85104fd4e8f36dc15ee85a8",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:27:36+00:00",
      "message": "update tutorials"
    },
    {
      "sha": "5e64baa61e75df23105a66d1e9d09ad799366e2a",
      "author": "chaofan",
      "date": "2025-05-28T08:15:17+00:00",
      "message": "Merge pull request #1466 from 545999961/master"
    },
    {
      "sha": "018b984e23e12d9793e59f2a1ef07fb9153ab08e",
      "author": "cfli",
      "date": "2025-05-28T08:14:32+00:00",
      "message": "update reranker inference"
    },
    {
      "sha": "d04f7989eeec2c6db4ead7c77ddef656f9c9d3aa",
      "author": "chaofan",
      "date": "2025-05-28T07:12:19+00:00",
      "message": "release version 1.3.5"
    },
    {
      "sha": "8576fde79b84224ecbe493857115552fcfb72ed7",
      "author": "chaofan",
      "date": "2025-05-28T06:58:53+00:00",
      "message": "Merge pull request #1465 from 545999961/master"
    },
    {
      "sha": "bf410d51e9e685e7f1b2e3816f09f3dc64ad8292",
      "author": "cfli",
      "date": "2025-05-28T06:55:54+00:00",
      "message": "update environment"
    },
    {
      "sha": "97ca07325d9e786d668f9a038502ad736dded9d5",
      "author": "chaofan",
      "date": "2025-05-28T06:18:23+00:00",
      "message": "Merge pull request #1464 from 545999961/master"
    },
    {
      "sha": "b51706d63ae8a1d86443df8b0e5ff6e45eb08fdc",
      "author": "cfli",
      "date": "2025-05-28T06:17:19+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "821c387961b5597fe672a4386232110ef54d5107",
      "author": "cfli",
      "date": "2025-05-28T06:12:16+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "3332a1956e2d5af7c52628a60e5c9788bd1b5ed9",
      "author": "chaofan",
      "date": "2025-05-22T10:07:40+00:00",
      "message": "Merge pull request #1459 from 545999961/master"
    },
    {
      "sha": "d412884a4e96c45e4a9e83a43e507e9beaa04713",
      "author": "cfli",
      "date": "2025-05-22T10:06:43+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "5c11feffc104497953a333036aa56b6b543e7cee",
      "author": "chaofan",
      "date": "2025-05-22T09:56:48+00:00",
      "message": "Merge pull request #1458 from 545999961/master"
    },
    {
      "sha": "41b526441f13242b49126261c219b6b9f14e17cb",
      "author": "cfli",
      "date": "2025-05-22T09:55:44+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "52bb9fddb5def2074b277f6e9a2f02ed8aa17dee",
      "author": "chaofan",
      "date": "2025-05-22T09:52:21+00:00",
      "message": "Merge pull request #1457 from 545999961/master"
    },
    {
      "sha": "f05d4b3fbeff5df9dabeb71ab16225fe5e1e8c95",
      "author": "cfli",
      "date": "2025-05-22T09:51:20+00:00",
      "message": "upload reinforced_ir"
    },
    {
      "sha": "25ee99c228e9df63a2be79e8d795d549ab5001e9",
      "author": "chaofan",
      "date": "2025-05-21T04:46:03+00:00",
      "message": "Merge pull request #1453 from 545999961/master"
    },
    {
      "sha": "436210bbed3ca352aac82a9be66cd4e665408944",
      "author": "cfli",
      "date": "2025-05-21T04:45:10+00:00",
      "message": "upload Matroyshka_reranker"
    },
    {
      "sha": "1946f89d5a6926871861c9ed9f4acbb31665acfe",
      "author": "cfli",
      "date": "2025-05-20T08:13:30+00:00",
      "message": "upload Matroyshka Re-Ranker"
    },
    {
      "sha": "f6924351bba09620938b69a2c933c6712be829c4",
      "author": "cfli",
      "date": "2025-05-20T08:12:08+00:00",
      "message": "upload Matroyshka Re-Ranker"
    }
  ],
  "readme_text": "[<img src=\"./imgs/FlagOpen.png\">](https://flagopen.baai.ac.cn/)\n\n<h1 align=\"center\">\u26a1\ufe0fBGE: One-Stop Retrieval Toolkit For Search and RAG</h1>\n\n![bge_logo](./imgs/bge_logo.jpg)\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Contribution-Welcome-blue\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\">\n    </a>\n    <a href=\"https://huggingface.co/C-MTEB\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=#news>News</a> |\n        <a href=#installation>Installation</a> |\n        <a href=#quick-start>Quick Start</a> |\n        <a href=#community>Community</a> |\n        <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a> |\n        <a href=#model-list>Model List</a> |\n        <a href=\"#contributors\">Contributor</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\n\n## News\n\n- 3/6/2025: :fire::fire: Introduce **BGE-VL** ([HF repo](https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92)), State-Of-The-Art multimodal embedding models to support Any visual search applications (everything, including text-to-image, image-to-text, image&prompt-to-image, text-to-image&text, and more)! They are released under the MIT license and are completely free for both academic and commercial use. We also release **MegaPairs** ([repo](https://github.com/VectorSpaceLab/MegaPairs), [paper](https://arxiv.org/abs/2412.14475)), a massive synthetic dataset which empowers BGE-VL!\n- 12/5/2024: :book: We built the [BGE documentation](https://www.bge-model.com) for centralized BGE information and materials!\n- 10/29/2024: :earth_asia: We created WeChat group for BGE. Scan the [QR code](./imgs/BGE_WeChat_Group.png) to join the group chat! To get the first hand message about our updates and new release, or having any questions or ideas, join us now!\n- <img src=\"./imgs/BGE_WeChat_Group.png\" alt=\"bge_wechat_group\" class=\"center\" width=\"200\">\n\n- 10/22/2024: We release another interesting model: [OmniGen](https://github.com/VectorSpaceLab/OmniGen), which is a unified image generation model supporting various tasks. OmniGen can accomplish complex image generation tasks without the need for additional plugins like ControlNet, IP-Adapter, or auxiliary models such as pose detection and face detection.\n- 9/10/2024: Introducing **MemoRAG**, a step forward towards RAG 2.0 on top of memory-inspired knowledge discovery (repo: https://github.com/qhjqhj00/MemoRAG, paper: https://arxiv.org/pdf/2409.05591v1) \n- 9/2/2024: Start to maintain the [tutorials](./Tutorials/). The contents within will be actively updated and eariched, stay tuned! :books:\n- 7/26/2024: Release a new embedding model [bge-en-icl](https://huggingface.co/BAAI/bge-en-icl), an embedding model that incorporates in-context learning capabilities, which, by providing task-relevant query-response examples, can encode semantically richer queries, further enhancing the semantic representation ability of the embeddings.\n- 7/26/2024: Release a new embedding model [bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2), a multilingual embedding model based on gemma-2-9b, which supports multiple languages and diverse downstream tasks, achieving new SOTA on multilingual benchmarks (MIRACL, MTEB-fr, and MTEB-pl). \n- 7/26/2024: Release a new lightweight reranker [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight), a lightweight reranker based on gemma-2-9b, which supports token compression and layerwise lightweight operations, can still ensure good performance while saving a significant amount of resources. :fire:\n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n\n- 6/7/2024: Release a new benchmark [MLVU](https://github.com/JUNJIE99/MLVU), the first comprehensive benchmark specifically designed for long video understanding. MLVU features an extensive range of video durations, a diverse collection of video sources, and a set of evaluation tasks uniquely tailored for long-form video understanding. :fire:\n- 5/21/2024: Release a new benchmark [AIR-Bench](https://github.com/AIR-Bench/AIR-Bench) together with Jina AI, Zilliz, HuggingFace, and other partners. AIR-Bench focuses on a fair out-of-distribution evaluation for Neural IR & RAG. It generates the synthetic data for benchmarking w.r.t. diverse domains and languages. It is dynamic and will be updated on regular basis. [Leaderboard](https://huggingface.co/spaces/AIR-Bench/leaderboard) :fire:\n- 4/30/2024: Release [Llama-3-8B-Instruct-80K-QLoRA](https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA), extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA training on a few synthesized long-context data. The model achieves remarkable performance on various long-context benchmarks. [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora) :fire:\n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually :smiley:) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation :fire:\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data. :fire:\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://arxiv.org/pdf/2402.03216.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) \n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n\n</details>\n\n\nBGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n![projects](./imgs/projects.png)\n\n- **Inference**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker)\n- **Finetune**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)\n- **[Evaluation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation)**\n- **[Dataset](https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset)**\n- **[Tutorials](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)**\n- **[research](https://github.com/FlagOpen/FlagEmbedding/tree/master/research)**\n\n\n## Installation\n### Using pip:\nIf you do not want to finetune the models, you can install the package without the finetune dependency:\n```\npip install -U FlagEmbedding\n```\nIf you want to finetune the models, you can install the package with the finetune dependency:\n```\npip install -U FlagEmbedding[finetune]\n```\n### Install from sources:\n\nClone the repository and install\n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install  .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install  .[finetune]\n```\nFor development in editable mode:\n```\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install -e .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install -e .[finetune]\n```\n\n## Quick Start\nFirst, load one of the BGE embedding model:\n```\nfrom FlagEmbedding import FlagAutoModel\n\nmodel = FlagAutoModel.from_finetuned('BAAI/bge-base-en-v1.5',\n                                      query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n                                      use_fp16=True)\n```\nThen, feed some sentences to the model and get their embeddings:\n```\nsentences_1 = [\"I love NLP\", \"I love machine learning\"]\nsentences_2 = [\"I love BGE\", \"I love text retrieval\"]\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\n```\nOnce we get the embeddings, we can compute similarity by inner product:\n```\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\n\nFor more details, you can refer to [embedder inference](./examples/inference/embedder), [reranker inference](./examples/inference/reranker), [embedder finetune](./examples/finetune/embedder), [reranker fintune](./examples/finetune/reranker), [evaluation](./examples/evaluation).\n\nIf you're unfamiliar with any of related concepts, please check out the [tutorial](./Tutorials/). If it's not there, let us know.\n\nFor more interesting topics related to BGE, take a look at [research](./research).\n\n## Community\n\nWe are actively maintaining the community of BGE and FlagEmbedding. Let us know if you have any suggessions or ideas!\n\nCurrently we are updating the [tutorials](./Tutorials/), we aim to create a comprehensive and detailed tutorial for beginners on text retrieval and RAG. Stay tuned!\n\nThe following contents are releasing in the upcoming weeks:\n\n- Evaluation\n- BGE-EN-ICL\n\n<details>\n  <summary>The whole tutorial roadmap</summary>\n    <img src=\"./Tutorials/tutorial_map.png\"/>\n</details>\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n| Model                                                                     | Language |                                                             Description                                                             |                                query instruction for retrieval                                 |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Provide instructions and few-shot examples freely based on the given task. |\n| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |    Multilingual     | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |        Provide instructions based on the given task.         |\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n| [LM-Cocktail](https://huggingface.co/Shitao)                   |   English |                     fine-tuned models (Llama and BGE) which can be used to reproduce the results of LM-Cocktail                     |  |\n| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English |                         a unified embedding model to support diverse retrieval augmentation needs for LLMs                          | See [README](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder) |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | Multilingual | a lightweight cross-encoder model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |  |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |  |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers, compress ratio and compress layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)             |   English |                                             Embedding Model which map text into vector                                              |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)               |   English |                                    a base-scale model but with similar ability to `bge-large-en`                                    |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)             |   English |                                        a small-scale model but with competitive performance                                         |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)             |   Chinese |                                             Embedding Model which map text into vector                                              |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)               |   Chinese |                                    a base-scale model but with similar ability to `bge-large-zh`                                    |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)             |   Chinese |                                        a small-scale model but with competitive performance                                         |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n\n\n\n\n### Contributors:\nThank all our contributors for their efforts and warmly welcome new members to join in!\n\n<a href=\"https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\" />\n</a>\n\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_m3,\n  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},\n  year={2023},\n  eprint={2309.07597},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n@misc{cocktail,\n      title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Xingrun Xing},\n      year={2023},\n      eprint={2311.13534},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{llm_embedder,\n      title={Retrieve Anything To Augment Large Language Models}, \n      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\n      year={2023},\n      eprint={2310.07554},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). \n\n",
  "external_links_in_readme": [
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation",
    "https://arxiv.org/pdf/2310.07554.pdf",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://arxiv.org/pdf/2409.05591v1",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset",
    "https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">",
    "https://huggingface.co/Shitao",
    "https://huggingface.co/BAAI/bge-base-en",
    "https://huggingface.co/BAAI/llm-embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://img.shields.io/badge/Contribution-Welcome-blue\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://huggingface.co/BAAI/bge-m3",
    "https://github.com/VectorSpaceLab/OmniGen",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge",
    "https://github.com/VectorSpaceLab/MegaPairs",
    "https://github.com/FlagOpen/FlagEmbedding.git",
    "https://huggingface.co/spaces/AIR-Bench/leaderboard",
    "https://github.com/AIR-Bench/AIR-Bench",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker",
    "https://github.com/JUNJIE99/MLVU",
    "https://github.com/qhjqhj00/MemoRAG,",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail",
    "https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research",
    "https://huggingface.co/BAAI/bge-small-zh-v1.5",
    "https://arxiv.org/abs/2412.14475",
    "https://img.shields.io/badge/LICENSE-MIT-green\">",
    "https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">",
    "https://huggingface.co/BAAI/bge-reranker-v2-gemma",
    "https://huggingface.co/spaces/mteb/leaderboard",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker",
    "https://arxiv.org/pdf/2402.03216.pdf",
    "https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\"",
    "https://huggingface.co/BAAI/bge-base-zh",
    "https://arxiv.org/abs/2311.13534",
    "https://huggingface.co/C-MTEB\">",
    "https://flagopen.baai.ac.cn/",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding",
    "https://arxiv.org/abs/2401.03462",
    "https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">",
    "https://arxiv.org/pdf/2309.07597.pdf",
    "https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a>",
    "https://huggingface.co/BAAI/bge-en-icl",
    "https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight",
    "https://www.bge-model.com",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://arxiv.org/abs/2312.15503",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials",
    "https://huggingface.co/BAAI/bge-multilingual-gemma2",
    "https://github.com/FlagOpen/FlagEmbedding\">",
    "https://huggingface.co/BAAI/bge-small-zh",
    "https://huggingface.co/BAAI/bge-small-en",
    "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE",
    "https://data.baai.ac.cn/details/BAAI-MTP",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker",
    "https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise",
    "https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92",
    "https://huggingface.co/BAAI/bge-large-en",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md",
    "https://huggingface.co/BAAI/bge-large-zh",
    "https://huggingface.co/BAAI/bge-base-zh-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora",
    "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA"
  ]
}
```

</details>


---

## Repository 10: staoxiao/RetroMAE

# GitHub Repository Data

**Repository:** [staoxiao/RetroMAE](https://github.com/staoxiao/RetroMAE)

## Basic Information

- **Description:** Codebase for RetroMAE and beyond.
- **Created:** 2022-10-18T08:01:24+00:00
- **Last Updated:** 2025-06-17T02:49:24+00:00
- **Last Pushed:** 2024-06-07T10:45:29+00:00
- **Default Branch:** master
- **Size:** 105 KB

## Statistics

- **Stars:** 262
- **Forks:** 22
- **Watchers:** 262
- **Open Issues:** 19
- **Total Issues:** 0
- **Pull Requests:** 3

## License

- **Type:** Apache License 2.0
- **SPDX ID:** Apache-2.0
- **URL:** [License](https://github.com/staoxiao/RetroMAE/blob/master/LICENSE)

## Languages

- **Python:** 138,696 bytes

## Topics

- `information-retrieval`
- `pretrained-models`
- `language-model`
- `text-embedding`

## Top Contributors

1. **ZhengLiu101** - 2 contributions
2. **staoxiao** - 1 contributions

## File Structure (Sample of 10 files)

Total files: 60

- `.gitignore` (blob)
- `LICENSE` (blob)
- `README.md` (blob)
- `examples` (tree)
- `examples/pretrain` (tree)
- `examples/pretrain/README.md` (blob)
- `examples/pretrain/preprocess.py` (blob)
- `examples/reranker` (tree)
- `examples/reranker/README.md` (blob)
- `examples/reranker/msmarco_eval.py` (blob)

## Recent Issues

- 🟢 **#35** Dupmae for modernbert (open)
- 🟢 **#34** Regarding hyper-parameters used to train the BEIR model (open)
- 🟢 **#33** RetroMAEv2 and checkpoint names (open)
- 🟢 **#32** one GPU with zero GPU-util (open)
- 🔴 **#31** About the evaluation after pre-training on the msmarco dataset (closed)

## Recent Pull Requests

- 🔴 **#30** fix(preprocess.py): compatible with `wikipedia` 20220301 (closed)
- 🔴 **#4** minor updates (closed)
- 🔴 **#3** adding news about v2 (closed)

## Recent Commits

- **ee375876** fix transformers version - shitao (2024-01-12T15:51:04+00:00)
- **67a06a2a** update v2 - shitao (2023-05-21T10:52:14+00:00)
- **264346fd** v2 code - shitao (2023-05-21T09:14:21+00:00)
- **87878f09** update loss func - shitao (2023-02-28T03:42:02+00:00)
- **78d6c028** update readme - shitao (2023-02-26T06:39:22+00:00)
- **a15da7a3** fix typo - shitao (2023-02-26T06:32:22+00:00)
- **a9eb3490** fix typo - shitao (2023-02-26T06:28:02+00:00)
- **8d089928** v2 performance - shitao (2023-02-16T06:09:49+00:00)
- **524ca274** reformat - shitao (2023-01-15T13:19:37+00:00)
- **6a3a092b** typo - shitao (2023-01-15T13:17:46+00:00)

## External Links Found in README

- https://arxiv.org/abs/2211.08769
- https://github.com/staoxiao/RetroMAE.git
- https://huggingface.co/Shitao/RetroMAE_BEIR
- https://huggingface.co/Shitao/RetroMAE
- https://huggingface.co/Shitao/RetroMAE_MSMARCO_finetune
- https://arxiv.org/abs/2205.12035
- https://huggingface.co/Shitao/RetroMAE_MSMARCO_distill
- https://arxiv.org/abs/2205.12035},
- https://huggingface.co/Shitao/RetroMAE_MSMARCO

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 553436055,
  "name": "RetroMAE",
  "full_name": "staoxiao/RetroMAE",
  "description": "Codebase for RetroMAE and beyond.",
  "html_url": "https://github.com/staoxiao/RetroMAE",
  "clone_url": "https://github.com/staoxiao/RetroMAE.git",
  "ssh_url": "git@github.com:staoxiao/RetroMAE.git",
  "homepage": "",
  "topics": [
    "information-retrieval",
    "pretrained-models",
    "language-model",
    "text-embedding"
  ],
  "default_branch": "master",
  "created_at": "2022-10-18T08:01:24+00:00",
  "updated_at": "2025-06-17T02:49:24+00:00",
  "pushed_at": "2024-06-07T10:45:29+00:00",
  "size_kb": 105,
  "watchers_count": 262,
  "stargazers_count": 262,
  "forks_count": 22,
  "open_issues_count": 19,
  "license": {
    "key": "apache-2.0",
    "name": "Apache License 2.0",
    "spdx_id": "Apache-2.0",
    "url": "https://github.com/staoxiao/RetroMAE/blob/master/LICENSE"
  },
  "languages": {
    "Python": 138696
  },
  "top_contributors": [
    {
      "login": "ZhengLiu101",
      "contributions": 2
    },
    {
      "login": "staoxiao",
      "contributions": 1
    }
  ],
  "file_tree_count": 60,
  "file_tree_sample": [
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "examples",
      "type": "tree"
    },
    {
      "path": "examples/pretrain",
      "type": "tree"
    },
    {
      "path": "examples/pretrain/README.md",
      "type": "blob"
    },
    {
      "path": "examples/pretrain/preprocess.py",
      "type": "blob"
    },
    {
      "path": "examples/reranker",
      "type": "tree"
    },
    {
      "path": "examples/reranker/README.md",
      "type": "blob"
    },
    {
      "path": "examples/reranker/msmarco_eval.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 3,
  "recent_issues": [
    {
      "number": 35,
      "title": "Dupmae for modernbert",
      "state": "open"
    },
    {
      "number": 34,
      "title": "Regarding hyper-parameters used to train the BEIR model",
      "state": "open"
    },
    {
      "number": 33,
      "title": "RetroMAEv2 and checkpoint names",
      "state": "open"
    },
    {
      "number": 32,
      "title": "one GPU with zero GPU-util",
      "state": "open"
    },
    {
      "number": 31,
      "title": "About the evaluation after pre-training on the msmarco dataset",
      "state": "closed"
    }
  ],
  "recent_pulls": [
    {
      "number": 30,
      "title": "fix(preprocess.py): compatible with `wikipedia` 20220301",
      "state": "closed"
    },
    {
      "number": 4,
      "title": "minor updates",
      "state": "closed"
    },
    {
      "number": 3,
      "title": "adding news about v2",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "ee375876a5e571617e37dd43450decc7865e2a07",
      "author": "shitao",
      "date": "2024-01-12T15:51:04+00:00",
      "message": "fix transformers version"
    },
    {
      "sha": "67a06a2ade9065c7d7cd6c2934f9a745b8fcae2b",
      "author": "shitao",
      "date": "2023-05-21T10:52:14+00:00",
      "message": "update v2"
    },
    {
      "sha": "264346fd9b8f9d6fb2e0f36d58f8f7bb97c843b7",
      "author": "shitao",
      "date": "2023-05-21T09:14:21+00:00",
      "message": "v2 code"
    },
    {
      "sha": "87878f09b5d33889b75f55dd7b5ff5dabe4ba919",
      "author": "shitao",
      "date": "2023-02-28T03:42:02+00:00",
      "message": "update loss func"
    },
    {
      "sha": "78d6c0280583897d0a7e27d476cfd91d3811163c",
      "author": "shitao",
      "date": "2023-02-26T06:39:22+00:00",
      "message": "update readme"
    },
    {
      "sha": "a15da7a39396d5cb8f4df36bbfdfd936283a6013",
      "author": "shitao",
      "date": "2023-02-26T06:32:22+00:00",
      "message": "fix typo"
    },
    {
      "sha": "a9eb3490c235f46b57d9304997d5c5a54cc9a964",
      "author": "shitao",
      "date": "2023-02-26T06:28:02+00:00",
      "message": "fix typo"
    },
    {
      "sha": "8d0899282c354f2726649336a5991271b1d43cbe",
      "author": "shitao",
      "date": "2023-02-16T06:09:49+00:00",
      "message": "v2 performance"
    },
    {
      "sha": "524ca27451364b2f3f96d0feca854c36c1d1cb45",
      "author": "shitao",
      "date": "2023-01-15T13:19:37+00:00",
      "message": "reformat"
    },
    {
      "sha": "6a3a092b322bf50e2f177c6080d361857484b954",
      "author": "shitao",
      "date": "2023-01-15T13:17:46+00:00",
      "message": "typo"
    },
    {
      "sha": "ba53e9efdaff6218ae4e93d9eb02f4f616813bcf",
      "author": "shitao",
      "date": "2023-01-15T13:14:47+00:00",
      "message": "add cross-encoder and hard negative"
    },
    {
      "sha": "e13ca70e5a1a599d4db8144dd12f388c61428877",
      "author": "shitao",
      "date": "2023-01-11T09:57:16+00:00",
      "message": "add beir_test"
    },
    {
      "sha": "aafe711dcf5c890509c8ad17b982475ea0fcbffc",
      "author": "shitao",
      "date": "2023-01-11T07:34:19+00:00",
      "message": "debug"
    },
    {
      "sha": "db8fbca4e7d5bc38f914c20b3053f16ed0670327",
      "author": "shitao",
      "date": "2022-11-30T15:01:58+00:00",
      "message": "delete ipynb"
    },
    {
      "sha": "2b861815e2ed2a40f046b45e374d4c66fe1379e5",
      "author": "shitao",
      "date": "2022-11-30T14:59:59+00:00",
      "message": "upload new model"
    },
    {
      "sha": "7fd3dbd43bae77b6297913e58d9721334874d570",
      "author": "zhengliu",
      "date": "2022-11-20T05:28:39+00:00",
      "message": "Merge pull request #4 from staoxiao/lz"
    },
    {
      "sha": "1e7d21921a9bfe7c137d2790fc9c898e162ec69e",
      "author": "zhengliu",
      "date": "2022-11-20T05:27:52+00:00",
      "message": "minor updates"
    },
    {
      "sha": "bce4c7e67a81f829903c08f9082c8fa298e6c074",
      "author": "zhengliu",
      "date": "2022-11-19T10:30:13+00:00",
      "message": "Merge pull request #3 from staoxiao/lz"
    },
    {
      "sha": "c502cffaf7d6850c82f1f7ef360ae3b0f6e0796c",
      "author": "zhengliu",
      "date": "2022-11-19T10:17:02+00:00",
      "message": "adding news about v2"
    },
    {
      "sha": "08b0dad75ce9fdd13e0fafb46394cea76d4f482b",
      "author": "shitao",
      "date": "2022-11-17T11:41:22+00:00",
      "message": "typo"
    }
  ],
  "readme_text": "# RetroMAE\nCodebase for **[RetroMAE](https://arxiv.org/abs/2205.12035)** and beyond.\n\n# What's New\n- :fire: Oct. 2022, [RetroMAE: Pre-Training Retrieval-oriented Language Models Via\nMasked Auto-Encoder](https://arxiv.org/abs/2205.12035) is accepted to **EMNLP 2022**; SOTA performances on MS MARCO and BEIR from a BERT-base scale dense retriever!\n- :fire: Nov. 2022, [RetroMAE v2: Duplex Masked Auto-Encoder For Pre-Training Retrieval-Oriented Language Models](https://arxiv.org/abs/2211.08769) is now on ArXiv. Another big stride forward from v1 and major improvements on MS MARCO and BEIR! Models and code are coming soon!\n\n\n## Released Models\nWe have uploaded some checkpoints to Huggingface Hub. \n\n| Model | Description | Link  |\n|---|---|---|\n|RetroMAE | Pre-trianed on the wikipedia and bookcorpus | [Shitao/RetroMAE](https://huggingface.co/Shitao/RetroMAE) | \n|RetroMAE_MSMARCO | Pre-trianed on the MSMARCO passage | [Shitao/RetroMAE_MSMARCO](https://huggingface.co/Shitao/RetroMAE_MSMARCO) | \n|RetroMAE_MSMARCO_finetune |Finetune the RetroMAE_MSMARCO on the MSMARCO passage data | [Shitao/RetroMAE_MSMARCO_finetune](https://huggingface.co/Shitao/RetroMAE_MSMARCO_finetune) | \n|RetroMAE_MSMARCO_distill | Finetune the RetroMAE_MSMARCO on the MSMARCO passage data by minimizing the KL-divergence with the cross-encoder\u3000| [Shitao/RetroMAE_MSMARCO_distill](https://huggingface.co/Shitao/RetroMAE_MSMARCO_distill) | \n|RetroMAE_BEIR | Finetune the RetroMAE on the MSMARCO passage data for BEIR (use the official negatives provided by BEIR)\u3000| [Shitao/RetroMAE_BEIR](https://huggingface.co/Shitao/RetroMAE_BEIR) | \n\nYou can load them easily using the identifier strings. For example:\n```python\nfrom transformers import AutoModel\nmodel = AutoModel.from_pretrained('Shitao/RetroMAE')\n```\n\n## State of the Art Performance\nRetroMAE can provide a strong initialization of dense retriever; after fine-tuned with in-domain data, it\ngives rise to a high-quality supervised retrieval performance in the corresponding scenario. \nBesides, It substantially improves the pre-trained model's transferability, which helps to result in superior zero-shot performances on out-of-domain datasets.\n\n### MSMARCO Passage\n- Model pre-trained on wikipedia and bookcorpus:\n\n| Model | MRR@10 | Recall@1000 |\n|---|---|---|\n|Bert | 0.346 | 0.964 |\n|RetroMAE | **0.382** | **0.981** |\n\n- Model pre-trained on MSMARCO:\n\n| Model             | MRR@10 | Recall@1000 |\n|-------------------|---|---|\n| coCondenser         | 0.382 | 0.984 | \n| RetroMAE          | 0.393 | 0.985 | \n| RetroMAE(distillation) | **0.416** | **0.988** | \n\n### BEIR Benchemark\n\n| Model             | Avg NDCG@10 (18 datasets) |\n|-------------------|---|\n| Bert         | 0.371 | \n| Condenser       | 0.407 | \n| RetroMAE       | **0.452** | \n| RetroMAE v2      | **0.491** | \n\n\n## Installation\n```\ngit clone https://github.com/staoxiao/RetroMAE.git\ncd RetroMAE\npip install .\n```\nFor development, install as editable:\n\n```\npip install -e .\n```\n\n## Workflow\nThis repo includes two functions: pre-train and finetune. Firstly, train the RetroMAE on general dataset\n (or downstream dataset) with mask language modeling loss. Then finetune the RetroMAE on \n downstream dataset with contrastive loss. To achieve a better performance, you also can finetune the \n RetroMAE by distillation the scores provided by cross-encoder. **Detailed workflow please refer to our examples.** \n\n### Pretrain\n```\ntorchrun --nproc_per_node 8 \\\n  -m pretrain.run \\\n  --output_dir {path to save ckpt} \\\n  --data_dir {your data} \\\n  --do_train True \\\n  --model_name_or_path bert-base-uncased \\\n  --pretrain_method {retromae or dupmae}\n```\n\n### Finetune\n```\ntorchrun --nproc_per_node 8 \\\n-m bi_encoder.run \\\n--output_dir {path to save ckpt} \\\n--model_name_or_path Shitao/RetroMAE \\\n--do_train  \\\n--corpus_file ./data/BertTokenizer_data/corpus \\\n--train_query_file ./data/BertTokenizer_data/train_query \\\n--train_qrels ./data/BertTokenizer_data/train_qrels.txt \\\n--neg_file ./data/train_negs.tsv \n```\n\n## Examples\n\n- Pre-train  \n    - [Pre-train on Wikipedia](examples/pretrain/README.md) \n    - [Pre-train on MSMARCO Passage](examples/pretrain/README.md) \n- Bi-encoder\n    - [Finetune on MSMARCO Passage](examples/retriever/msmarco/README.md)  \n    - [BEIR Benchemark](examples/retriever/BEIR/README.md)\n- Cross-encoder\n    - [Reranker on MSMARCO Passage](examples/reranker/README.md)\n\n\n## Citation\nIf you find our work helpful, please consider citing us:\n```\n@inproceedings{RetroMAE,\n  title={RetroMAE: Pre-Training Retrieval-oriented Language Models Via Masked Auto-Encoder},\n  author={Shitao Xiao, Zheng Liu, Yingxia Shao, Zhao Cao},\n  url={https://arxiv.org/abs/2205.12035},\n  booktitle ={EMNLP},\n  year={2022},\n}\n```\n\n\n\n",
  "external_links_in_readme": [
    "https://arxiv.org/abs/2211.08769",
    "https://github.com/staoxiao/RetroMAE.git",
    "https://huggingface.co/Shitao/RetroMAE_BEIR",
    "https://huggingface.co/Shitao/RetroMAE",
    "https://huggingface.co/Shitao/RetroMAE_MSMARCO_finetune",
    "https://arxiv.org/abs/2205.12035",
    "https://huggingface.co/Shitao/RetroMAE_MSMARCO_distill",
    "https://arxiv.org/abs/2205.12035},",
    "https://huggingface.co/Shitao/RetroMAE_MSMARCO"
  ]
}
```

</details>


---

## Repository 11: FlagOpen/FlagEmbedding

# GitHub Repository Data

**Repository:** [FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## Basic Information

- **Description:** Retrieval and Retrieval-augmented LLMs
- **Created:** 2023-08-02T02:08:11+00:00
- **Last Updated:** 2025-06-21T10:44:22+00:00
- **Last Pushed:** 2025-06-04T10:00:26+00:00
- **Default Branch:** master
- **Size:** 52310 KB

## Statistics

- **Stars:** 9,974
- **Forks:** 737
- **Watchers:** 9,974
- **Open Issues:** 847
- **Total Issues:** 0
- **Pull Requests:** 217

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE)

## Languages

- **Python:** 3,871,341 bytes
- **Jupyter Notebook:** 2,607,506 bytes
- **Shell:** 7,322 bytes

## Topics

- `embeddings`
- `information-retrieval`
- `llm`
- `sentence-embeddings`
- `text-semantic-similarity`
- `retrieval-augmented-generation`

## Top Contributors

1. **hanhainebula** - 350 contributions
2. **545999961** - 327 contributions
3. **staoxiao** - 182 contributions
4. **ZiyiXia** - 113 contributions
5. **namespace-Pt** - 101 contributions
6. **JUNJIE99** - 78 contributions
7. **ftgreat** - 20 contributions
8. **ZhengLiu101** - 15 contributions
9. **Muennighoff** - 12 contributions
10. **shuyansy** - 10 contributions

## File Structure (Sample of 10 files)

Total files: 1,395

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.gitignore` (blob)
- `FlagEmbedding` (tree)
- `FlagEmbedding/__init__.py` (blob)
- `FlagEmbedding/abc` (tree)
- `FlagEmbedding/abc/__init__.py` (blob)
- `FlagEmbedding/abc/evaluation` (tree)
- `FlagEmbedding/abc/evaluation/__init__.py` (blob)

## Recent Issues

- 🟢 **#1480** Continued pretraining using RetroMAE (open)
- 🟢 **#1479** 微调bge-reranker-base一直卡顿，无日志输出 (open)
- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1477** 训练过程中train_loss和eval_loss都是先下降后上升再缓慢下降 (open)
- 🟢 **#1476** How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline (open)

## Recent Pull Requests

- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1475** Add inline comment to setup.py (open)
- 🟢 **#1469** Extend tutorials to include cloud deployment options (open)
- 🔴 **#1466** update reranker inference (closed)
- 🔴 **#1465** update environment (closed)

## Recent Commits

- **875fd4ff** update docs - ZiyiXia (2025-06-04T09:57:10+00:00)
- **235f967a** update tutorials - ZiyiXia (2025-06-04T09:27:36+00:00)
- **5e64baa6** Merge pull request #1466 from 545999961/master - chaofan (2025-05-28T08:15:17+00:00)
- **018b984e** update reranker inference - cfli (2025-05-28T08:14:32+00:00)
- **d04f7989** release version 1.3.5 - chaofan (2025-05-28T07:12:19+00:00)
- **8576fde7** Merge pull request #1465 from 545999961/master - chaofan (2025-05-28T06:58:53+00:00)
- **bf410d51** update environment - cfli (2025-05-28T06:55:54+00:00)
- **97ca0732** Merge pull request #1464 from 545999961/master - chaofan (2025-05-28T06:18:23+00:00)
- **b51706d6** upload coder eval script - cfli (2025-05-28T06:17:19+00:00)
- **821c3879** upload coder eval script - cfli (2025-05-28T06:12:16+00:00)

## External Links Found in README

- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation
- https://arxiv.org/pdf/2310.07554.pdf
- https://huggingface.co/BAAI/bge-reranker-base
- https://arxiv.org/pdf/2409.05591v1
- https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset
- https://img.shields.io/badge/BGE_series-🤗-yellow">
- https://huggingface.co/Shitao
- https://huggingface.co/BAAI/bge-base-en
- https://huggingface.co/BAAI/llm-embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder
- https://huggingface.co/BAAI/bge-reranker-large
- https://img.shields.io/badge/Contribution-Welcome-blue">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder
- https://huggingface.co/BAAI/bge-small-en-v1.5
- https://huggingface.co/BAAI/bge-m3
- https://github.com/VectorSpaceLab/OmniGen
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge
- https://github.com/VectorSpaceLab/MegaPairs

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673596675,
  "name": "FlagEmbedding",
  "full_name": "FlagOpen/FlagEmbedding",
  "description": "Retrieval and Retrieval-augmented LLMs",
  "html_url": "https://github.com/FlagOpen/FlagEmbedding",
  "clone_url": "https://github.com/FlagOpen/FlagEmbedding.git",
  "ssh_url": "git@github.com:FlagOpen/FlagEmbedding.git",
  "homepage": "http://www.bge-model.com/",
  "topics": [
    "embeddings",
    "information-retrieval",
    "llm",
    "sentence-embeddings",
    "text-semantic-similarity",
    "retrieval-augmented-generation"
  ],
  "default_branch": "master",
  "created_at": "2023-08-02T02:08:11+00:00",
  "updated_at": "2025-06-21T10:44:22+00:00",
  "pushed_at": "2025-06-04T10:00:26+00:00",
  "size_kb": 52310,
  "watchers_count": 9974,
  "stargazers_count": 9974,
  "forks_count": 737,
  "open_issues_count": 847,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"
  },
  "languages": {
    "Python": 3871341,
    "Jupyter Notebook": 2607506,
    "Shell": 7322
  },
  "top_contributors": [
    {
      "login": "hanhainebula",
      "contributions": 350
    },
    {
      "login": "545999961",
      "contributions": 327
    },
    {
      "login": "staoxiao",
      "contributions": 182
    },
    {
      "login": "ZiyiXia",
      "contributions": 113
    },
    {
      "login": "namespace-Pt",
      "contributions": 101
    },
    {
      "login": "JUNJIE99",
      "contributions": 78
    },
    {
      "login": "ftgreat",
      "contributions": 20
    },
    {
      "login": "ZhengLiu101",
      "contributions": 15
    },
    {
      "login": "Muennighoff",
      "contributions": 12
    },
    {
      "login": "shuyansy",
      "contributions": 10
    },
    {
      "login": "rainym00d",
      "contributions": 3
    },
    {
      "login": "chinainfant",
      "contributions": 3
    },
    {
      "login": "GeoloeG-IsT",
      "contributions": 3
    },
    {
      "login": "blue-vision0",
      "contributions": 2
    },
    {
      "login": "muazhari",
      "contributions": 2
    },
    {
      "login": "luisegarduno",
      "contributions": 2
    },
    {
      "login": "khazic",
      "contributions": 2
    },
    {
      "login": "shtdbb",
      "contributions": 2
    },
    {
      "login": "pengjunfeng11",
      "contributions": 2
    },
    {
      "login": "ycjcl868",
      "contributions": 2
    }
  ],
  "file_tree_count": 1395,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc/evaluation",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/evaluation/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 217,
  "recent_issues": [
    {
      "number": 1480,
      "title": "Continued pretraining using RetroMAE",
      "state": "open"
    },
    {
      "number": 1479,
      "title": "\u5fae\u8c03bge-reranker-base\u4e00\u76f4\u5361\u987f\uff0c\u65e0\u65e5\u5fd7\u8f93\u51fa",
      "state": "open"
    },
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1477,
      "title": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2dtrain_loss\u548ceval_loss\u90fd\u662f\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u518d\u7f13\u6162\u4e0b\u964d",
      "state": "open"
    },
    {
      "number": 1476,
      "title": "How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1475,
      "title": "Add inline comment to setup.py",
      "state": "open"
    },
    {
      "number": 1469,
      "title": "Extend tutorials to include cloud deployment options",
      "state": "open"
    },
    {
      "number": 1466,
      "title": "update reranker inference",
      "state": "closed"
    },
    {
      "number": 1465,
      "title": "update environment",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "875fd4ffcb894313a918e8bb0c9b7ead82f37e64",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:57:10+00:00",
      "message": "update docs"
    },
    {
      "sha": "235f967a0122b9d5b85104fd4e8f36dc15ee85a8",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:27:36+00:00",
      "message": "update tutorials"
    },
    {
      "sha": "5e64baa61e75df23105a66d1e9d09ad799366e2a",
      "author": "chaofan",
      "date": "2025-05-28T08:15:17+00:00",
      "message": "Merge pull request #1466 from 545999961/master"
    },
    {
      "sha": "018b984e23e12d9793e59f2a1ef07fb9153ab08e",
      "author": "cfli",
      "date": "2025-05-28T08:14:32+00:00",
      "message": "update reranker inference"
    },
    {
      "sha": "d04f7989eeec2c6db4ead7c77ddef656f9c9d3aa",
      "author": "chaofan",
      "date": "2025-05-28T07:12:19+00:00",
      "message": "release version 1.3.5"
    },
    {
      "sha": "8576fde79b84224ecbe493857115552fcfb72ed7",
      "author": "chaofan",
      "date": "2025-05-28T06:58:53+00:00",
      "message": "Merge pull request #1465 from 545999961/master"
    },
    {
      "sha": "bf410d51e9e685e7f1b2e3816f09f3dc64ad8292",
      "author": "cfli",
      "date": "2025-05-28T06:55:54+00:00",
      "message": "update environment"
    },
    {
      "sha": "97ca07325d9e786d668f9a038502ad736dded9d5",
      "author": "chaofan",
      "date": "2025-05-28T06:18:23+00:00",
      "message": "Merge pull request #1464 from 545999961/master"
    },
    {
      "sha": "b51706d63ae8a1d86443df8b0e5ff6e45eb08fdc",
      "author": "cfli",
      "date": "2025-05-28T06:17:19+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "821c387961b5597fe672a4386232110ef54d5107",
      "author": "cfli",
      "date": "2025-05-28T06:12:16+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "3332a1956e2d5af7c52628a60e5c9788bd1b5ed9",
      "author": "chaofan",
      "date": "2025-05-22T10:07:40+00:00",
      "message": "Merge pull request #1459 from 545999961/master"
    },
    {
      "sha": "d412884a4e96c45e4a9e83a43e507e9beaa04713",
      "author": "cfli",
      "date": "2025-05-22T10:06:43+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "5c11feffc104497953a333036aa56b6b543e7cee",
      "author": "chaofan",
      "date": "2025-05-22T09:56:48+00:00",
      "message": "Merge pull request #1458 from 545999961/master"
    },
    {
      "sha": "41b526441f13242b49126261c219b6b9f14e17cb",
      "author": "cfli",
      "date": "2025-05-22T09:55:44+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "52bb9fddb5def2074b277f6e9a2f02ed8aa17dee",
      "author": "chaofan",
      "date": "2025-05-22T09:52:21+00:00",
      "message": "Merge pull request #1457 from 545999961/master"
    },
    {
      "sha": "f05d4b3fbeff5df9dabeb71ab16225fe5e1e8c95",
      "author": "cfli",
      "date": "2025-05-22T09:51:20+00:00",
      "message": "upload reinforced_ir"
    },
    {
      "sha": "25ee99c228e9df63a2be79e8d795d549ab5001e9",
      "author": "chaofan",
      "date": "2025-05-21T04:46:03+00:00",
      "message": "Merge pull request #1453 from 545999961/master"
    },
    {
      "sha": "436210bbed3ca352aac82a9be66cd4e665408944",
      "author": "cfli",
      "date": "2025-05-21T04:45:10+00:00",
      "message": "upload Matroyshka_reranker"
    },
    {
      "sha": "1946f89d5a6926871861c9ed9f4acbb31665acfe",
      "author": "cfli",
      "date": "2025-05-20T08:13:30+00:00",
      "message": "upload Matroyshka Re-Ranker"
    },
    {
      "sha": "f6924351bba09620938b69a2c933c6712be829c4",
      "author": "cfli",
      "date": "2025-05-20T08:12:08+00:00",
      "message": "upload Matroyshka Re-Ranker"
    }
  ],
  "readme_text": "[<img src=\"./imgs/FlagOpen.png\">](https://flagopen.baai.ac.cn/)\n\n<h1 align=\"center\">\u26a1\ufe0fBGE: One-Stop Retrieval Toolkit For Search and RAG</h1>\n\n![bge_logo](./imgs/bge_logo.jpg)\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Contribution-Welcome-blue\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\">\n    </a>\n    <a href=\"https://huggingface.co/C-MTEB\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=#news>News</a> |\n        <a href=#installation>Installation</a> |\n        <a href=#quick-start>Quick Start</a> |\n        <a href=#community>Community</a> |\n        <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a> |\n        <a href=#model-list>Model List</a> |\n        <a href=\"#contributors\">Contributor</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\n\n## News\n\n- 3/6/2025: :fire::fire: Introduce **BGE-VL** ([HF repo](https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92)), State-Of-The-Art multimodal embedding models to support Any visual search applications (everything, including text-to-image, image-to-text, image&prompt-to-image, text-to-image&text, and more)! They are released under the MIT license and are completely free for both academic and commercial use. We also release **MegaPairs** ([repo](https://github.com/VectorSpaceLab/MegaPairs), [paper](https://arxiv.org/abs/2412.14475)), a massive synthetic dataset which empowers BGE-VL!\n- 12/5/2024: :book: We built the [BGE documentation](https://www.bge-model.com) for centralized BGE information and materials!\n- 10/29/2024: :earth_asia: We created WeChat group for BGE. Scan the [QR code](./imgs/BGE_WeChat_Group.png) to join the group chat! To get the first hand message about our updates and new release, or having any questions or ideas, join us now!\n- <img src=\"./imgs/BGE_WeChat_Group.png\" alt=\"bge_wechat_group\" class=\"center\" width=\"200\">\n\n- 10/22/2024: We release another interesting model: [OmniGen](https://github.com/VectorSpaceLab/OmniGen), which is a unified image generation model supporting various tasks. OmniGen can accomplish complex image generation tasks without the need for additional plugins like ControlNet, IP-Adapter, or auxiliary models such as pose detection and face detection.\n- 9/10/2024: Introducing **MemoRAG**, a step forward towards RAG 2.0 on top of memory-inspired knowledge discovery (repo: https://github.com/qhjqhj00/MemoRAG, paper: https://arxiv.org/pdf/2409.05591v1) \n- 9/2/2024: Start to maintain the [tutorials](./Tutorials/). The contents within will be actively updated and eariched, stay tuned! :books:\n- 7/26/2024: Release a new embedding model [bge-en-icl](https://huggingface.co/BAAI/bge-en-icl), an embedding model that incorporates in-context learning capabilities, which, by providing task-relevant query-response examples, can encode semantically richer queries, further enhancing the semantic representation ability of the embeddings.\n- 7/26/2024: Release a new embedding model [bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2), a multilingual embedding model based on gemma-2-9b, which supports multiple languages and diverse downstream tasks, achieving new SOTA on multilingual benchmarks (MIRACL, MTEB-fr, and MTEB-pl). \n- 7/26/2024: Release a new lightweight reranker [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight), a lightweight reranker based on gemma-2-9b, which supports token compression and layerwise lightweight operations, can still ensure good performance while saving a significant amount of resources. :fire:\n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n\n- 6/7/2024: Release a new benchmark [MLVU](https://github.com/JUNJIE99/MLVU), the first comprehensive benchmark specifically designed for long video understanding. MLVU features an extensive range of video durations, a diverse collection of video sources, and a set of evaluation tasks uniquely tailored for long-form video understanding. :fire:\n- 5/21/2024: Release a new benchmark [AIR-Bench](https://github.com/AIR-Bench/AIR-Bench) together with Jina AI, Zilliz, HuggingFace, and other partners. AIR-Bench focuses on a fair out-of-distribution evaluation for Neural IR & RAG. It generates the synthetic data for benchmarking w.r.t. diverse domains and languages. It is dynamic and will be updated on regular basis. [Leaderboard](https://huggingface.co/spaces/AIR-Bench/leaderboard) :fire:\n- 4/30/2024: Release [Llama-3-8B-Instruct-80K-QLoRA](https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA), extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA training on a few synthesized long-context data. The model achieves remarkable performance on various long-context benchmarks. [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora) :fire:\n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually :smiley:) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation :fire:\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data. :fire:\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://arxiv.org/pdf/2402.03216.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) \n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n\n</details>\n\n\nBGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n![projects](./imgs/projects.png)\n\n- **Inference**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker)\n- **Finetune**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)\n- **[Evaluation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation)**\n- **[Dataset](https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset)**\n- **[Tutorials](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)**\n- **[research](https://github.com/FlagOpen/FlagEmbedding/tree/master/research)**\n\n\n## Installation\n### Using pip:\nIf you do not want to finetune the models, you can install the package without the finetune dependency:\n```\npip install -U FlagEmbedding\n```\nIf you want to finetune the models, you can install the package with the finetune dependency:\n```\npip install -U FlagEmbedding[finetune]\n```\n### Install from sources:\n\nClone the repository and install\n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install  .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install  .[finetune]\n```\nFor development in editable mode:\n```\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install -e .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install -e .[finetune]\n```\n\n## Quick Start\nFirst, load one of the BGE embedding model:\n```\nfrom FlagEmbedding import FlagAutoModel\n\nmodel = FlagAutoModel.from_finetuned('BAAI/bge-base-en-v1.5',\n                                      query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n                                      use_fp16=True)\n```\nThen, feed some sentences to the model and get their embeddings:\n```\nsentences_1 = [\"I love NLP\", \"I love machine learning\"]\nsentences_2 = [\"I love BGE\", \"I love text retrieval\"]\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\n```\nOnce we get the embeddings, we can compute similarity by inner product:\n```\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\n\nFor more details, you can refer to [embedder inference](./examples/inference/embedder), [reranker inference](./examples/inference/reranker), [embedder finetune](./examples/finetune/embedder), [reranker fintune](./examples/finetune/reranker), [evaluation](./examples/evaluation).\n\nIf you're unfamiliar with any of related concepts, please check out the [tutorial](./Tutorials/). If it's not there, let us know.\n\nFor more interesting topics related to BGE, take a look at [research](./research).\n\n## Community\n\nWe are actively maintaining the community of BGE and FlagEmbedding. Let us know if you have any suggessions or ideas!\n\nCurrently we are updating the [tutorials](./Tutorials/), we aim to create a comprehensive and detailed tutorial for beginners on text retrieval and RAG. Stay tuned!\n\nThe following contents are releasing in the upcoming weeks:\n\n- Evaluation\n- BGE-EN-ICL\n\n<details>\n  <summary>The whole tutorial roadmap</summary>\n    <img src=\"./Tutorials/tutorial_map.png\"/>\n</details>\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n| Model                                                                     | Language |                                                             Description                                                             |                                query instruction for retrieval                                 |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Provide instructions and few-shot examples freely based on the given task. |\n| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |    Multilingual     | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |        Provide instructions based on the given task.         |\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n| [LM-Cocktail](https://huggingface.co/Shitao)                   |   English |                     fine-tuned models (Llama and BGE) which can be used to reproduce the results of LM-Cocktail                     |  |\n| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English |                         a unified embedding model to support diverse retrieval augmentation needs for LLMs                          | See [README](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder) |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | Multilingual | a lightweight cross-encoder model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |  |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |  |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers, compress ratio and compress layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)             |   English |                                             Embedding Model which map text into vector                                              |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)               |   English |                                    a base-scale model but with similar ability to `bge-large-en`                                    |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)             |   English |                                        a small-scale model but with competitive performance                                         |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)             |   Chinese |                                             Embedding Model which map text into vector                                              |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)               |   Chinese |                                    a base-scale model but with similar ability to `bge-large-zh`                                    |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)             |   Chinese |                                        a small-scale model but with competitive performance                                         |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n\n\n\n\n### Contributors:\nThank all our contributors for their efforts and warmly welcome new members to join in!\n\n<a href=\"https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\" />\n</a>\n\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_m3,\n  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},\n  year={2023},\n  eprint={2309.07597},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n@misc{cocktail,\n      title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Xingrun Xing},\n      year={2023},\n      eprint={2311.13534},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{llm_embedder,\n      title={Retrieve Anything To Augment Large Language Models}, \n      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\n      year={2023},\n      eprint={2310.07554},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). \n\n",
  "external_links_in_readme": [
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation",
    "https://arxiv.org/pdf/2310.07554.pdf",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://arxiv.org/pdf/2409.05591v1",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset",
    "https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">",
    "https://huggingface.co/Shitao",
    "https://huggingface.co/BAAI/bge-base-en",
    "https://huggingface.co/BAAI/llm-embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://img.shields.io/badge/Contribution-Welcome-blue\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://huggingface.co/BAAI/bge-m3",
    "https://github.com/VectorSpaceLab/OmniGen",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge",
    "https://github.com/VectorSpaceLab/MegaPairs",
    "https://github.com/FlagOpen/FlagEmbedding.git",
    "https://huggingface.co/spaces/AIR-Bench/leaderboard",
    "https://github.com/AIR-Bench/AIR-Bench",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker",
    "https://github.com/JUNJIE99/MLVU",
    "https://github.com/qhjqhj00/MemoRAG,",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail",
    "https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research",
    "https://huggingface.co/BAAI/bge-small-zh-v1.5",
    "https://arxiv.org/abs/2412.14475",
    "https://img.shields.io/badge/LICENSE-MIT-green\">",
    "https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">",
    "https://huggingface.co/BAAI/bge-reranker-v2-gemma",
    "https://huggingface.co/spaces/mteb/leaderboard",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker",
    "https://arxiv.org/pdf/2402.03216.pdf",
    "https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\"",
    "https://huggingface.co/BAAI/bge-base-zh",
    "https://arxiv.org/abs/2311.13534",
    "https://huggingface.co/C-MTEB\">",
    "https://flagopen.baai.ac.cn/",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding",
    "https://arxiv.org/abs/2401.03462",
    "https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">",
    "https://arxiv.org/pdf/2309.07597.pdf",
    "https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a>",
    "https://huggingface.co/BAAI/bge-en-icl",
    "https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight",
    "https://www.bge-model.com",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://arxiv.org/abs/2312.15503",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials",
    "https://huggingface.co/BAAI/bge-multilingual-gemma2",
    "https://github.com/FlagOpen/FlagEmbedding\">",
    "https://huggingface.co/BAAI/bge-small-zh",
    "https://huggingface.co/BAAI/bge-small-en",
    "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE",
    "https://data.baai.ac.cn/details/BAAI-MTP",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker",
    "https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise",
    "https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92",
    "https://huggingface.co/BAAI/bge-large-en",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md",
    "https://huggingface.co/BAAI/bge-large-zh",
    "https://huggingface.co/BAAI/bge-base-zh-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora",
    "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA"
  ]
}
```

</details>


---

## Repository 12: FlagOpen/FlagEmbedding

# GitHub Repository Data

**Repository:** [FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## Basic Information

- **Description:** Retrieval and Retrieval-augmented LLMs
- **Created:** 2023-08-02T02:08:11+00:00
- **Last Updated:** 2025-06-21T10:44:22+00:00
- **Last Pushed:** 2025-06-04T10:00:26+00:00
- **Default Branch:** master
- **Size:** 52310 KB

## Statistics

- **Stars:** 9,974
- **Forks:** 737
- **Watchers:** 9,974
- **Open Issues:** 847
- **Total Issues:** 0
- **Pull Requests:** 217

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE)

## Languages

- **Python:** 3,871,341 bytes
- **Jupyter Notebook:** 2,607,506 bytes
- **Shell:** 7,322 bytes

## Topics

- `embeddings`
- `information-retrieval`
- `llm`
- `sentence-embeddings`
- `text-semantic-similarity`
- `retrieval-augmented-generation`

## Top Contributors

1. **hanhainebula** - 350 contributions
2. **545999961** - 327 contributions
3. **staoxiao** - 182 contributions
4. **ZiyiXia** - 113 contributions
5. **namespace-Pt** - 101 contributions
6. **JUNJIE99** - 78 contributions
7. **ftgreat** - 20 contributions
8. **ZhengLiu101** - 15 contributions
9. **Muennighoff** - 12 contributions
10. **shuyansy** - 10 contributions

## File Structure (Sample of 10 files)

Total files: 1,395

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.gitignore` (blob)
- `FlagEmbedding` (tree)
- `FlagEmbedding/__init__.py` (blob)
- `FlagEmbedding/abc` (tree)
- `FlagEmbedding/abc/__init__.py` (blob)
- `FlagEmbedding/abc/evaluation` (tree)
- `FlagEmbedding/abc/evaluation/__init__.py` (blob)

## Recent Issues

- 🟢 **#1480** Continued pretraining using RetroMAE (open)
- 🟢 **#1479** 微调bge-reranker-base一直卡顿，无日志输出 (open)
- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1477** 训练过程中train_loss和eval_loss都是先下降后上升再缓慢下降 (open)
- 🟢 **#1476** How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline (open)

## Recent Pull Requests

- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1475** Add inline comment to setup.py (open)
- 🟢 **#1469** Extend tutorials to include cloud deployment options (open)
- 🔴 **#1466** update reranker inference (closed)
- 🔴 **#1465** update environment (closed)

## Recent Commits

- **875fd4ff** update docs - ZiyiXia (2025-06-04T09:57:10+00:00)
- **235f967a** update tutorials - ZiyiXia (2025-06-04T09:27:36+00:00)
- **5e64baa6** Merge pull request #1466 from 545999961/master - chaofan (2025-05-28T08:15:17+00:00)
- **018b984e** update reranker inference - cfli (2025-05-28T08:14:32+00:00)
- **d04f7989** release version 1.3.5 - chaofan (2025-05-28T07:12:19+00:00)
- **8576fde7** Merge pull request #1465 from 545999961/master - chaofan (2025-05-28T06:58:53+00:00)
- **bf410d51** update environment - cfli (2025-05-28T06:55:54+00:00)
- **97ca0732** Merge pull request #1464 from 545999961/master - chaofan (2025-05-28T06:18:23+00:00)
- **b51706d6** upload coder eval script - cfli (2025-05-28T06:17:19+00:00)
- **821c3879** upload coder eval script - cfli (2025-05-28T06:12:16+00:00)

## External Links Found in README

- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation
- https://arxiv.org/pdf/2310.07554.pdf
- https://huggingface.co/BAAI/bge-reranker-base
- https://arxiv.org/pdf/2409.05591v1
- https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset
- https://img.shields.io/badge/BGE_series-🤗-yellow">
- https://huggingface.co/Shitao
- https://huggingface.co/BAAI/bge-base-en
- https://huggingface.co/BAAI/llm-embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder
- https://huggingface.co/BAAI/bge-reranker-large
- https://img.shields.io/badge/Contribution-Welcome-blue">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder
- https://huggingface.co/BAAI/bge-small-en-v1.5
- https://huggingface.co/BAAI/bge-m3
- https://github.com/VectorSpaceLab/OmniGen
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge
- https://github.com/VectorSpaceLab/MegaPairs

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673596675,
  "name": "FlagEmbedding",
  "full_name": "FlagOpen/FlagEmbedding",
  "description": "Retrieval and Retrieval-augmented LLMs",
  "html_url": "https://github.com/FlagOpen/FlagEmbedding",
  "clone_url": "https://github.com/FlagOpen/FlagEmbedding.git",
  "ssh_url": "git@github.com:FlagOpen/FlagEmbedding.git",
  "homepage": "http://www.bge-model.com/",
  "topics": [
    "embeddings",
    "information-retrieval",
    "llm",
    "sentence-embeddings",
    "text-semantic-similarity",
    "retrieval-augmented-generation"
  ],
  "default_branch": "master",
  "created_at": "2023-08-02T02:08:11+00:00",
  "updated_at": "2025-06-21T10:44:22+00:00",
  "pushed_at": "2025-06-04T10:00:26+00:00",
  "size_kb": 52310,
  "watchers_count": 9974,
  "stargazers_count": 9974,
  "forks_count": 737,
  "open_issues_count": 847,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"
  },
  "languages": {
    "Python": 3871341,
    "Jupyter Notebook": 2607506,
    "Shell": 7322
  },
  "top_contributors": [
    {
      "login": "hanhainebula",
      "contributions": 350
    },
    {
      "login": "545999961",
      "contributions": 327
    },
    {
      "login": "staoxiao",
      "contributions": 182
    },
    {
      "login": "ZiyiXia",
      "contributions": 113
    },
    {
      "login": "namespace-Pt",
      "contributions": 101
    },
    {
      "login": "JUNJIE99",
      "contributions": 78
    },
    {
      "login": "ftgreat",
      "contributions": 20
    },
    {
      "login": "ZhengLiu101",
      "contributions": 15
    },
    {
      "login": "Muennighoff",
      "contributions": 12
    },
    {
      "login": "shuyansy",
      "contributions": 10
    },
    {
      "login": "rainym00d",
      "contributions": 3
    },
    {
      "login": "chinainfant",
      "contributions": 3
    },
    {
      "login": "GeoloeG-IsT",
      "contributions": 3
    },
    {
      "login": "blue-vision0",
      "contributions": 2
    },
    {
      "login": "muazhari",
      "contributions": 2
    },
    {
      "login": "luisegarduno",
      "contributions": 2
    },
    {
      "login": "khazic",
      "contributions": 2
    },
    {
      "login": "shtdbb",
      "contributions": 2
    },
    {
      "login": "pengjunfeng11",
      "contributions": 2
    },
    {
      "login": "ycjcl868",
      "contributions": 2
    }
  ],
  "file_tree_count": 1395,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc/evaluation",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/evaluation/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 217,
  "recent_issues": [
    {
      "number": 1480,
      "title": "Continued pretraining using RetroMAE",
      "state": "open"
    },
    {
      "number": 1479,
      "title": "\u5fae\u8c03bge-reranker-base\u4e00\u76f4\u5361\u987f\uff0c\u65e0\u65e5\u5fd7\u8f93\u51fa",
      "state": "open"
    },
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1477,
      "title": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2dtrain_loss\u548ceval_loss\u90fd\u662f\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u518d\u7f13\u6162\u4e0b\u964d",
      "state": "open"
    },
    {
      "number": 1476,
      "title": "How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1475,
      "title": "Add inline comment to setup.py",
      "state": "open"
    },
    {
      "number": 1469,
      "title": "Extend tutorials to include cloud deployment options",
      "state": "open"
    },
    {
      "number": 1466,
      "title": "update reranker inference",
      "state": "closed"
    },
    {
      "number": 1465,
      "title": "update environment",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "875fd4ffcb894313a918e8bb0c9b7ead82f37e64",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:57:10+00:00",
      "message": "update docs"
    },
    {
      "sha": "235f967a0122b9d5b85104fd4e8f36dc15ee85a8",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:27:36+00:00",
      "message": "update tutorials"
    },
    {
      "sha": "5e64baa61e75df23105a66d1e9d09ad799366e2a",
      "author": "chaofan",
      "date": "2025-05-28T08:15:17+00:00",
      "message": "Merge pull request #1466 from 545999961/master"
    },
    {
      "sha": "018b984e23e12d9793e59f2a1ef07fb9153ab08e",
      "author": "cfli",
      "date": "2025-05-28T08:14:32+00:00",
      "message": "update reranker inference"
    },
    {
      "sha": "d04f7989eeec2c6db4ead7c77ddef656f9c9d3aa",
      "author": "chaofan",
      "date": "2025-05-28T07:12:19+00:00",
      "message": "release version 1.3.5"
    },
    {
      "sha": "8576fde79b84224ecbe493857115552fcfb72ed7",
      "author": "chaofan",
      "date": "2025-05-28T06:58:53+00:00",
      "message": "Merge pull request #1465 from 545999961/master"
    },
    {
      "sha": "bf410d51e9e685e7f1b2e3816f09f3dc64ad8292",
      "author": "cfli",
      "date": "2025-05-28T06:55:54+00:00",
      "message": "update environment"
    },
    {
      "sha": "97ca07325d9e786d668f9a038502ad736dded9d5",
      "author": "chaofan",
      "date": "2025-05-28T06:18:23+00:00",
      "message": "Merge pull request #1464 from 545999961/master"
    },
    {
      "sha": "b51706d63ae8a1d86443df8b0e5ff6e45eb08fdc",
      "author": "cfli",
      "date": "2025-05-28T06:17:19+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "821c387961b5597fe672a4386232110ef54d5107",
      "author": "cfli",
      "date": "2025-05-28T06:12:16+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "3332a1956e2d5af7c52628a60e5c9788bd1b5ed9",
      "author": "chaofan",
      "date": "2025-05-22T10:07:40+00:00",
      "message": "Merge pull request #1459 from 545999961/master"
    },
    {
      "sha": "d412884a4e96c45e4a9e83a43e507e9beaa04713",
      "author": "cfli",
      "date": "2025-05-22T10:06:43+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "5c11feffc104497953a333036aa56b6b543e7cee",
      "author": "chaofan",
      "date": "2025-05-22T09:56:48+00:00",
      "message": "Merge pull request #1458 from 545999961/master"
    },
    {
      "sha": "41b526441f13242b49126261c219b6b9f14e17cb",
      "author": "cfli",
      "date": "2025-05-22T09:55:44+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "52bb9fddb5def2074b277f6e9a2f02ed8aa17dee",
      "author": "chaofan",
      "date": "2025-05-22T09:52:21+00:00",
      "message": "Merge pull request #1457 from 545999961/master"
    },
    {
      "sha": "f05d4b3fbeff5df9dabeb71ab16225fe5e1e8c95",
      "author": "cfli",
      "date": "2025-05-22T09:51:20+00:00",
      "message": "upload reinforced_ir"
    },
    {
      "sha": "25ee99c228e9df63a2be79e8d795d549ab5001e9",
      "author": "chaofan",
      "date": "2025-05-21T04:46:03+00:00",
      "message": "Merge pull request #1453 from 545999961/master"
    },
    {
      "sha": "436210bbed3ca352aac82a9be66cd4e665408944",
      "author": "cfli",
      "date": "2025-05-21T04:45:10+00:00",
      "message": "upload Matroyshka_reranker"
    },
    {
      "sha": "1946f89d5a6926871861c9ed9f4acbb31665acfe",
      "author": "cfli",
      "date": "2025-05-20T08:13:30+00:00",
      "message": "upload Matroyshka Re-Ranker"
    },
    {
      "sha": "f6924351bba09620938b69a2c933c6712be829c4",
      "author": "cfli",
      "date": "2025-05-20T08:12:08+00:00",
      "message": "upload Matroyshka Re-Ranker"
    }
  ],
  "readme_text": "[<img src=\"./imgs/FlagOpen.png\">](https://flagopen.baai.ac.cn/)\n\n<h1 align=\"center\">\u26a1\ufe0fBGE: One-Stop Retrieval Toolkit For Search and RAG</h1>\n\n![bge_logo](./imgs/bge_logo.jpg)\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Contribution-Welcome-blue\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\">\n    </a>\n    <a href=\"https://huggingface.co/C-MTEB\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=#news>News</a> |\n        <a href=#installation>Installation</a> |\n        <a href=#quick-start>Quick Start</a> |\n        <a href=#community>Community</a> |\n        <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a> |\n        <a href=#model-list>Model List</a> |\n        <a href=\"#contributors\">Contributor</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\n\n## News\n\n- 3/6/2025: :fire::fire: Introduce **BGE-VL** ([HF repo](https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92)), State-Of-The-Art multimodal embedding models to support Any visual search applications (everything, including text-to-image, image-to-text, image&prompt-to-image, text-to-image&text, and more)! They are released under the MIT license and are completely free for both academic and commercial use. We also release **MegaPairs** ([repo](https://github.com/VectorSpaceLab/MegaPairs), [paper](https://arxiv.org/abs/2412.14475)), a massive synthetic dataset which empowers BGE-VL!\n- 12/5/2024: :book: We built the [BGE documentation](https://www.bge-model.com) for centralized BGE information and materials!\n- 10/29/2024: :earth_asia: We created WeChat group for BGE. Scan the [QR code](./imgs/BGE_WeChat_Group.png) to join the group chat! To get the first hand message about our updates and new release, or having any questions or ideas, join us now!\n- <img src=\"./imgs/BGE_WeChat_Group.png\" alt=\"bge_wechat_group\" class=\"center\" width=\"200\">\n\n- 10/22/2024: We release another interesting model: [OmniGen](https://github.com/VectorSpaceLab/OmniGen), which is a unified image generation model supporting various tasks. OmniGen can accomplish complex image generation tasks without the need for additional plugins like ControlNet, IP-Adapter, or auxiliary models such as pose detection and face detection.\n- 9/10/2024: Introducing **MemoRAG**, a step forward towards RAG 2.0 on top of memory-inspired knowledge discovery (repo: https://github.com/qhjqhj00/MemoRAG, paper: https://arxiv.org/pdf/2409.05591v1) \n- 9/2/2024: Start to maintain the [tutorials](./Tutorials/). The contents within will be actively updated and eariched, stay tuned! :books:\n- 7/26/2024: Release a new embedding model [bge-en-icl](https://huggingface.co/BAAI/bge-en-icl), an embedding model that incorporates in-context learning capabilities, which, by providing task-relevant query-response examples, can encode semantically richer queries, further enhancing the semantic representation ability of the embeddings.\n- 7/26/2024: Release a new embedding model [bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2), a multilingual embedding model based on gemma-2-9b, which supports multiple languages and diverse downstream tasks, achieving new SOTA on multilingual benchmarks (MIRACL, MTEB-fr, and MTEB-pl). \n- 7/26/2024: Release a new lightweight reranker [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight), a lightweight reranker based on gemma-2-9b, which supports token compression and layerwise lightweight operations, can still ensure good performance while saving a significant amount of resources. :fire:\n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n\n- 6/7/2024: Release a new benchmark [MLVU](https://github.com/JUNJIE99/MLVU), the first comprehensive benchmark specifically designed for long video understanding. MLVU features an extensive range of video durations, a diverse collection of video sources, and a set of evaluation tasks uniquely tailored for long-form video understanding. :fire:\n- 5/21/2024: Release a new benchmark [AIR-Bench](https://github.com/AIR-Bench/AIR-Bench) together with Jina AI, Zilliz, HuggingFace, and other partners. AIR-Bench focuses on a fair out-of-distribution evaluation for Neural IR & RAG. It generates the synthetic data for benchmarking w.r.t. diverse domains and languages. It is dynamic and will be updated on regular basis. [Leaderboard](https://huggingface.co/spaces/AIR-Bench/leaderboard) :fire:\n- 4/30/2024: Release [Llama-3-8B-Instruct-80K-QLoRA](https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA), extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA training on a few synthesized long-context data. The model achieves remarkable performance on various long-context benchmarks. [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora) :fire:\n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually :smiley:) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation :fire:\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data. :fire:\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://arxiv.org/pdf/2402.03216.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) \n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n\n</details>\n\n\nBGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n![projects](./imgs/projects.png)\n\n- **Inference**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker)\n- **Finetune**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)\n- **[Evaluation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation)**\n- **[Dataset](https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset)**\n- **[Tutorials](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)**\n- **[research](https://github.com/FlagOpen/FlagEmbedding/tree/master/research)**\n\n\n## Installation\n### Using pip:\nIf you do not want to finetune the models, you can install the package without the finetune dependency:\n```\npip install -U FlagEmbedding\n```\nIf you want to finetune the models, you can install the package with the finetune dependency:\n```\npip install -U FlagEmbedding[finetune]\n```\n### Install from sources:\n\nClone the repository and install\n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install  .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install  .[finetune]\n```\nFor development in editable mode:\n```\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install -e .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install -e .[finetune]\n```\n\n## Quick Start\nFirst, load one of the BGE embedding model:\n```\nfrom FlagEmbedding import FlagAutoModel\n\nmodel = FlagAutoModel.from_finetuned('BAAI/bge-base-en-v1.5',\n                                      query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n                                      use_fp16=True)\n```\nThen, feed some sentences to the model and get their embeddings:\n```\nsentences_1 = [\"I love NLP\", \"I love machine learning\"]\nsentences_2 = [\"I love BGE\", \"I love text retrieval\"]\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\n```\nOnce we get the embeddings, we can compute similarity by inner product:\n```\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\n\nFor more details, you can refer to [embedder inference](./examples/inference/embedder), [reranker inference](./examples/inference/reranker), [embedder finetune](./examples/finetune/embedder), [reranker fintune](./examples/finetune/reranker), [evaluation](./examples/evaluation).\n\nIf you're unfamiliar with any of related concepts, please check out the [tutorial](./Tutorials/). If it's not there, let us know.\n\nFor more interesting topics related to BGE, take a look at [research](./research).\n\n## Community\n\nWe are actively maintaining the community of BGE and FlagEmbedding. Let us know if you have any suggessions or ideas!\n\nCurrently we are updating the [tutorials](./Tutorials/), we aim to create a comprehensive and detailed tutorial for beginners on text retrieval and RAG. Stay tuned!\n\nThe following contents are releasing in the upcoming weeks:\n\n- Evaluation\n- BGE-EN-ICL\n\n<details>\n  <summary>The whole tutorial roadmap</summary>\n    <img src=\"./Tutorials/tutorial_map.png\"/>\n</details>\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n| Model                                                                     | Language |                                                             Description                                                             |                                query instruction for retrieval                                 |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Provide instructions and few-shot examples freely based on the given task. |\n| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |    Multilingual     | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |        Provide instructions based on the given task.         |\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n| [LM-Cocktail](https://huggingface.co/Shitao)                   |   English |                     fine-tuned models (Llama and BGE) which can be used to reproduce the results of LM-Cocktail                     |  |\n| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English |                         a unified embedding model to support diverse retrieval augmentation needs for LLMs                          | See [README](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder) |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | Multilingual | a lightweight cross-encoder model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |  |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |  |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers, compress ratio and compress layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)             |   English |                                             Embedding Model which map text into vector                                              |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)               |   English |                                    a base-scale model but with similar ability to `bge-large-en`                                    |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)             |   English |                                        a small-scale model but with competitive performance                                         |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)             |   Chinese |                                             Embedding Model which map text into vector                                              |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)               |   Chinese |                                    a base-scale model but with similar ability to `bge-large-zh`                                    |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)             |   Chinese |                                        a small-scale model but with competitive performance                                         |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n\n\n\n\n### Contributors:\nThank all our contributors for their efforts and warmly welcome new members to join in!\n\n<a href=\"https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\" />\n</a>\n\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_m3,\n  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},\n  year={2023},\n  eprint={2309.07597},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n@misc{cocktail,\n      title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Xingrun Xing},\n      year={2023},\n      eprint={2311.13534},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{llm_embedder,\n      title={Retrieve Anything To Augment Large Language Models}, \n      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\n      year={2023},\n      eprint={2310.07554},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). \n\n",
  "external_links_in_readme": [
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation",
    "https://arxiv.org/pdf/2310.07554.pdf",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://arxiv.org/pdf/2409.05591v1",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset",
    "https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">",
    "https://huggingface.co/Shitao",
    "https://huggingface.co/BAAI/bge-base-en",
    "https://huggingface.co/BAAI/llm-embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://img.shields.io/badge/Contribution-Welcome-blue\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://huggingface.co/BAAI/bge-m3",
    "https://github.com/VectorSpaceLab/OmniGen",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge",
    "https://github.com/VectorSpaceLab/MegaPairs",
    "https://github.com/FlagOpen/FlagEmbedding.git",
    "https://huggingface.co/spaces/AIR-Bench/leaderboard",
    "https://github.com/AIR-Bench/AIR-Bench",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker",
    "https://github.com/JUNJIE99/MLVU",
    "https://github.com/qhjqhj00/MemoRAG,",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail",
    "https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research",
    "https://huggingface.co/BAAI/bge-small-zh-v1.5",
    "https://arxiv.org/abs/2412.14475",
    "https://img.shields.io/badge/LICENSE-MIT-green\">",
    "https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">",
    "https://huggingface.co/BAAI/bge-reranker-v2-gemma",
    "https://huggingface.co/spaces/mteb/leaderboard",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker",
    "https://arxiv.org/pdf/2402.03216.pdf",
    "https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\"",
    "https://huggingface.co/BAAI/bge-base-zh",
    "https://arxiv.org/abs/2311.13534",
    "https://huggingface.co/C-MTEB\">",
    "https://flagopen.baai.ac.cn/",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding",
    "https://arxiv.org/abs/2401.03462",
    "https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">",
    "https://arxiv.org/pdf/2309.07597.pdf",
    "https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a>",
    "https://huggingface.co/BAAI/bge-en-icl",
    "https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight",
    "https://www.bge-model.com",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://arxiv.org/abs/2312.15503",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials",
    "https://huggingface.co/BAAI/bge-multilingual-gemma2",
    "https://github.com/FlagOpen/FlagEmbedding\">",
    "https://huggingface.co/BAAI/bge-small-zh",
    "https://huggingface.co/BAAI/bge-small-en",
    "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE",
    "https://data.baai.ac.cn/details/BAAI-MTP",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker",
    "https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise",
    "https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92",
    "https://huggingface.co/BAAI/bge-large-en",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md",
    "https://huggingface.co/BAAI/bge-large-zh",
    "https://huggingface.co/BAAI/bge-base-zh-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora",
    "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA"
  ]
}
```

</details>


---

## Repository 13: FlagOpen/FlagEmbedding

# GitHub Repository Data

**Repository:** [FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## Basic Information

- **Description:** Retrieval and Retrieval-augmented LLMs
- **Created:** 2023-08-02T02:08:11+00:00
- **Last Updated:** 2025-06-21T10:44:22+00:00
- **Last Pushed:** 2025-06-04T10:00:26+00:00
- **Default Branch:** master
- **Size:** 52310 KB

## Statistics

- **Stars:** 9,974
- **Forks:** 737
- **Watchers:** 9,974
- **Open Issues:** 847
- **Total Issues:** 0
- **Pull Requests:** 217

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE)

## Languages

- **Python:** 3,871,341 bytes
- **Jupyter Notebook:** 2,607,506 bytes
- **Shell:** 7,322 bytes

## Topics

- `embeddings`
- `information-retrieval`
- `llm`
- `sentence-embeddings`
- `text-semantic-similarity`
- `retrieval-augmented-generation`

## Top Contributors

1. **hanhainebula** - 350 contributions
2. **545999961** - 327 contributions
3. **staoxiao** - 182 contributions
4. **ZiyiXia** - 113 contributions
5. **namespace-Pt** - 101 contributions
6. **JUNJIE99** - 78 contributions
7. **ftgreat** - 20 contributions
8. **ZhengLiu101** - 15 contributions
9. **Muennighoff** - 12 contributions
10. **shuyansy** - 10 contributions

## File Structure (Sample of 10 files)

Total files: 1,395

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.gitignore` (blob)
- `FlagEmbedding` (tree)
- `FlagEmbedding/__init__.py` (blob)
- `FlagEmbedding/abc` (tree)
- `FlagEmbedding/abc/__init__.py` (blob)
- `FlagEmbedding/abc/evaluation` (tree)
- `FlagEmbedding/abc/evaluation/__init__.py` (blob)

## Recent Issues

- 🟢 **#1480** Continued pretraining using RetroMAE (open)
- 🟢 **#1479** 微调bge-reranker-base一直卡顿，无日志输出 (open)
- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1477** 训练过程中train_loss和eval_loss都是先下降后上升再缓慢下降 (open)
- 🟢 **#1476** How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline (open)

## Recent Pull Requests

- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1475** Add inline comment to setup.py (open)
- 🟢 **#1469** Extend tutorials to include cloud deployment options (open)
- 🔴 **#1466** update reranker inference (closed)
- 🔴 **#1465** update environment (closed)

## Recent Commits

- **875fd4ff** update docs - ZiyiXia (2025-06-04T09:57:10+00:00)
- **235f967a** update tutorials - ZiyiXia (2025-06-04T09:27:36+00:00)
- **5e64baa6** Merge pull request #1466 from 545999961/master - chaofan (2025-05-28T08:15:17+00:00)
- **018b984e** update reranker inference - cfli (2025-05-28T08:14:32+00:00)
- **d04f7989** release version 1.3.5 - chaofan (2025-05-28T07:12:19+00:00)
- **8576fde7** Merge pull request #1465 from 545999961/master - chaofan (2025-05-28T06:58:53+00:00)
- **bf410d51** update environment - cfli (2025-05-28T06:55:54+00:00)
- **97ca0732** Merge pull request #1464 from 545999961/master - chaofan (2025-05-28T06:18:23+00:00)
- **b51706d6** upload coder eval script - cfli (2025-05-28T06:17:19+00:00)
- **821c3879** upload coder eval script - cfli (2025-05-28T06:12:16+00:00)

## External Links Found in README

- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation
- https://arxiv.org/pdf/2310.07554.pdf
- https://huggingface.co/BAAI/bge-reranker-base
- https://arxiv.org/pdf/2409.05591v1
- https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset
- https://img.shields.io/badge/BGE_series-🤗-yellow">
- https://huggingface.co/Shitao
- https://huggingface.co/BAAI/bge-base-en
- https://huggingface.co/BAAI/llm-embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder
- https://huggingface.co/BAAI/bge-reranker-large
- https://img.shields.io/badge/Contribution-Welcome-blue">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder
- https://huggingface.co/BAAI/bge-small-en-v1.5
- https://huggingface.co/BAAI/bge-m3
- https://github.com/VectorSpaceLab/OmniGen
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge
- https://github.com/VectorSpaceLab/MegaPairs

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673596675,
  "name": "FlagEmbedding",
  "full_name": "FlagOpen/FlagEmbedding",
  "description": "Retrieval and Retrieval-augmented LLMs",
  "html_url": "https://github.com/FlagOpen/FlagEmbedding",
  "clone_url": "https://github.com/FlagOpen/FlagEmbedding.git",
  "ssh_url": "git@github.com:FlagOpen/FlagEmbedding.git",
  "homepage": "http://www.bge-model.com/",
  "topics": [
    "embeddings",
    "information-retrieval",
    "llm",
    "sentence-embeddings",
    "text-semantic-similarity",
    "retrieval-augmented-generation"
  ],
  "default_branch": "master",
  "created_at": "2023-08-02T02:08:11+00:00",
  "updated_at": "2025-06-21T10:44:22+00:00",
  "pushed_at": "2025-06-04T10:00:26+00:00",
  "size_kb": 52310,
  "watchers_count": 9974,
  "stargazers_count": 9974,
  "forks_count": 737,
  "open_issues_count": 847,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"
  },
  "languages": {
    "Python": 3871341,
    "Jupyter Notebook": 2607506,
    "Shell": 7322
  },
  "top_contributors": [
    {
      "login": "hanhainebula",
      "contributions": 350
    },
    {
      "login": "545999961",
      "contributions": 327
    },
    {
      "login": "staoxiao",
      "contributions": 182
    },
    {
      "login": "ZiyiXia",
      "contributions": 113
    },
    {
      "login": "namespace-Pt",
      "contributions": 101
    },
    {
      "login": "JUNJIE99",
      "contributions": 78
    },
    {
      "login": "ftgreat",
      "contributions": 20
    },
    {
      "login": "ZhengLiu101",
      "contributions": 15
    },
    {
      "login": "Muennighoff",
      "contributions": 12
    },
    {
      "login": "shuyansy",
      "contributions": 10
    },
    {
      "login": "rainym00d",
      "contributions": 3
    },
    {
      "login": "chinainfant",
      "contributions": 3
    },
    {
      "login": "GeoloeG-IsT",
      "contributions": 3
    },
    {
      "login": "blue-vision0",
      "contributions": 2
    },
    {
      "login": "muazhari",
      "contributions": 2
    },
    {
      "login": "luisegarduno",
      "contributions": 2
    },
    {
      "login": "khazic",
      "contributions": 2
    },
    {
      "login": "shtdbb",
      "contributions": 2
    },
    {
      "login": "pengjunfeng11",
      "contributions": 2
    },
    {
      "login": "ycjcl868",
      "contributions": 2
    }
  ],
  "file_tree_count": 1395,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc/evaluation",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/evaluation/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 217,
  "recent_issues": [
    {
      "number": 1480,
      "title": "Continued pretraining using RetroMAE",
      "state": "open"
    },
    {
      "number": 1479,
      "title": "\u5fae\u8c03bge-reranker-base\u4e00\u76f4\u5361\u987f\uff0c\u65e0\u65e5\u5fd7\u8f93\u51fa",
      "state": "open"
    },
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1477,
      "title": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2dtrain_loss\u548ceval_loss\u90fd\u662f\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u518d\u7f13\u6162\u4e0b\u964d",
      "state": "open"
    },
    {
      "number": 1476,
      "title": "How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1475,
      "title": "Add inline comment to setup.py",
      "state": "open"
    },
    {
      "number": 1469,
      "title": "Extend tutorials to include cloud deployment options",
      "state": "open"
    },
    {
      "number": 1466,
      "title": "update reranker inference",
      "state": "closed"
    },
    {
      "number": 1465,
      "title": "update environment",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "875fd4ffcb894313a918e8bb0c9b7ead82f37e64",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:57:10+00:00",
      "message": "update docs"
    },
    {
      "sha": "235f967a0122b9d5b85104fd4e8f36dc15ee85a8",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:27:36+00:00",
      "message": "update tutorials"
    },
    {
      "sha": "5e64baa61e75df23105a66d1e9d09ad799366e2a",
      "author": "chaofan",
      "date": "2025-05-28T08:15:17+00:00",
      "message": "Merge pull request #1466 from 545999961/master"
    },
    {
      "sha": "018b984e23e12d9793e59f2a1ef07fb9153ab08e",
      "author": "cfli",
      "date": "2025-05-28T08:14:32+00:00",
      "message": "update reranker inference"
    },
    {
      "sha": "d04f7989eeec2c6db4ead7c77ddef656f9c9d3aa",
      "author": "chaofan",
      "date": "2025-05-28T07:12:19+00:00",
      "message": "release version 1.3.5"
    },
    {
      "sha": "8576fde79b84224ecbe493857115552fcfb72ed7",
      "author": "chaofan",
      "date": "2025-05-28T06:58:53+00:00",
      "message": "Merge pull request #1465 from 545999961/master"
    },
    {
      "sha": "bf410d51e9e685e7f1b2e3816f09f3dc64ad8292",
      "author": "cfli",
      "date": "2025-05-28T06:55:54+00:00",
      "message": "update environment"
    },
    {
      "sha": "97ca07325d9e786d668f9a038502ad736dded9d5",
      "author": "chaofan",
      "date": "2025-05-28T06:18:23+00:00",
      "message": "Merge pull request #1464 from 545999961/master"
    },
    {
      "sha": "b51706d63ae8a1d86443df8b0e5ff6e45eb08fdc",
      "author": "cfli",
      "date": "2025-05-28T06:17:19+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "821c387961b5597fe672a4386232110ef54d5107",
      "author": "cfli",
      "date": "2025-05-28T06:12:16+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "3332a1956e2d5af7c52628a60e5c9788bd1b5ed9",
      "author": "chaofan",
      "date": "2025-05-22T10:07:40+00:00",
      "message": "Merge pull request #1459 from 545999961/master"
    },
    {
      "sha": "d412884a4e96c45e4a9e83a43e507e9beaa04713",
      "author": "cfli",
      "date": "2025-05-22T10:06:43+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "5c11feffc104497953a333036aa56b6b543e7cee",
      "author": "chaofan",
      "date": "2025-05-22T09:56:48+00:00",
      "message": "Merge pull request #1458 from 545999961/master"
    },
    {
      "sha": "41b526441f13242b49126261c219b6b9f14e17cb",
      "author": "cfli",
      "date": "2025-05-22T09:55:44+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "52bb9fddb5def2074b277f6e9a2f02ed8aa17dee",
      "author": "chaofan",
      "date": "2025-05-22T09:52:21+00:00",
      "message": "Merge pull request #1457 from 545999961/master"
    },
    {
      "sha": "f05d4b3fbeff5df9dabeb71ab16225fe5e1e8c95",
      "author": "cfli",
      "date": "2025-05-22T09:51:20+00:00",
      "message": "upload reinforced_ir"
    },
    {
      "sha": "25ee99c228e9df63a2be79e8d795d549ab5001e9",
      "author": "chaofan",
      "date": "2025-05-21T04:46:03+00:00",
      "message": "Merge pull request #1453 from 545999961/master"
    },
    {
      "sha": "436210bbed3ca352aac82a9be66cd4e665408944",
      "author": "cfli",
      "date": "2025-05-21T04:45:10+00:00",
      "message": "upload Matroyshka_reranker"
    },
    {
      "sha": "1946f89d5a6926871861c9ed9f4acbb31665acfe",
      "author": "cfli",
      "date": "2025-05-20T08:13:30+00:00",
      "message": "upload Matroyshka Re-Ranker"
    },
    {
      "sha": "f6924351bba09620938b69a2c933c6712be829c4",
      "author": "cfli",
      "date": "2025-05-20T08:12:08+00:00",
      "message": "upload Matroyshka Re-Ranker"
    }
  ],
  "readme_text": "[<img src=\"./imgs/FlagOpen.png\">](https://flagopen.baai.ac.cn/)\n\n<h1 align=\"center\">\u26a1\ufe0fBGE: One-Stop Retrieval Toolkit For Search and RAG</h1>\n\n![bge_logo](./imgs/bge_logo.jpg)\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Contribution-Welcome-blue\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\">\n    </a>\n    <a href=\"https://huggingface.co/C-MTEB\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=#news>News</a> |\n        <a href=#installation>Installation</a> |\n        <a href=#quick-start>Quick Start</a> |\n        <a href=#community>Community</a> |\n        <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a> |\n        <a href=#model-list>Model List</a> |\n        <a href=\"#contributors\">Contributor</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\n\n## News\n\n- 3/6/2025: :fire::fire: Introduce **BGE-VL** ([HF repo](https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92)), State-Of-The-Art multimodal embedding models to support Any visual search applications (everything, including text-to-image, image-to-text, image&prompt-to-image, text-to-image&text, and more)! They are released under the MIT license and are completely free for both academic and commercial use. We also release **MegaPairs** ([repo](https://github.com/VectorSpaceLab/MegaPairs), [paper](https://arxiv.org/abs/2412.14475)), a massive synthetic dataset which empowers BGE-VL!\n- 12/5/2024: :book: We built the [BGE documentation](https://www.bge-model.com) for centralized BGE information and materials!\n- 10/29/2024: :earth_asia: We created WeChat group for BGE. Scan the [QR code](./imgs/BGE_WeChat_Group.png) to join the group chat! To get the first hand message about our updates and new release, or having any questions or ideas, join us now!\n- <img src=\"./imgs/BGE_WeChat_Group.png\" alt=\"bge_wechat_group\" class=\"center\" width=\"200\">\n\n- 10/22/2024: We release another interesting model: [OmniGen](https://github.com/VectorSpaceLab/OmniGen), which is a unified image generation model supporting various tasks. OmniGen can accomplish complex image generation tasks without the need for additional plugins like ControlNet, IP-Adapter, or auxiliary models such as pose detection and face detection.\n- 9/10/2024: Introducing **MemoRAG**, a step forward towards RAG 2.0 on top of memory-inspired knowledge discovery (repo: https://github.com/qhjqhj00/MemoRAG, paper: https://arxiv.org/pdf/2409.05591v1) \n- 9/2/2024: Start to maintain the [tutorials](./Tutorials/). The contents within will be actively updated and eariched, stay tuned! :books:\n- 7/26/2024: Release a new embedding model [bge-en-icl](https://huggingface.co/BAAI/bge-en-icl), an embedding model that incorporates in-context learning capabilities, which, by providing task-relevant query-response examples, can encode semantically richer queries, further enhancing the semantic representation ability of the embeddings.\n- 7/26/2024: Release a new embedding model [bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2), a multilingual embedding model based on gemma-2-9b, which supports multiple languages and diverse downstream tasks, achieving new SOTA on multilingual benchmarks (MIRACL, MTEB-fr, and MTEB-pl). \n- 7/26/2024: Release a new lightweight reranker [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight), a lightweight reranker based on gemma-2-9b, which supports token compression and layerwise lightweight operations, can still ensure good performance while saving a significant amount of resources. :fire:\n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n\n- 6/7/2024: Release a new benchmark [MLVU](https://github.com/JUNJIE99/MLVU), the first comprehensive benchmark specifically designed for long video understanding. MLVU features an extensive range of video durations, a diverse collection of video sources, and a set of evaluation tasks uniquely tailored for long-form video understanding. :fire:\n- 5/21/2024: Release a new benchmark [AIR-Bench](https://github.com/AIR-Bench/AIR-Bench) together with Jina AI, Zilliz, HuggingFace, and other partners. AIR-Bench focuses on a fair out-of-distribution evaluation for Neural IR & RAG. It generates the synthetic data for benchmarking w.r.t. diverse domains and languages. It is dynamic and will be updated on regular basis. [Leaderboard](https://huggingface.co/spaces/AIR-Bench/leaderboard) :fire:\n- 4/30/2024: Release [Llama-3-8B-Instruct-80K-QLoRA](https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA), extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA training on a few synthesized long-context data. The model achieves remarkable performance on various long-context benchmarks. [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora) :fire:\n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually :smiley:) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation :fire:\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data. :fire:\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://arxiv.org/pdf/2402.03216.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) \n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n\n</details>\n\n\nBGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n![projects](./imgs/projects.png)\n\n- **Inference**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker)\n- **Finetune**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)\n- **[Evaluation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation)**\n- **[Dataset](https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset)**\n- **[Tutorials](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)**\n- **[research](https://github.com/FlagOpen/FlagEmbedding/tree/master/research)**\n\n\n## Installation\n### Using pip:\nIf you do not want to finetune the models, you can install the package without the finetune dependency:\n```\npip install -U FlagEmbedding\n```\nIf you want to finetune the models, you can install the package with the finetune dependency:\n```\npip install -U FlagEmbedding[finetune]\n```\n### Install from sources:\n\nClone the repository and install\n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install  .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install  .[finetune]\n```\nFor development in editable mode:\n```\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install -e .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install -e .[finetune]\n```\n\n## Quick Start\nFirst, load one of the BGE embedding model:\n```\nfrom FlagEmbedding import FlagAutoModel\n\nmodel = FlagAutoModel.from_finetuned('BAAI/bge-base-en-v1.5',\n                                      query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n                                      use_fp16=True)\n```\nThen, feed some sentences to the model and get their embeddings:\n```\nsentences_1 = [\"I love NLP\", \"I love machine learning\"]\nsentences_2 = [\"I love BGE\", \"I love text retrieval\"]\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\n```\nOnce we get the embeddings, we can compute similarity by inner product:\n```\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\n\nFor more details, you can refer to [embedder inference](./examples/inference/embedder), [reranker inference](./examples/inference/reranker), [embedder finetune](./examples/finetune/embedder), [reranker fintune](./examples/finetune/reranker), [evaluation](./examples/evaluation).\n\nIf you're unfamiliar with any of related concepts, please check out the [tutorial](./Tutorials/). If it's not there, let us know.\n\nFor more interesting topics related to BGE, take a look at [research](./research).\n\n## Community\n\nWe are actively maintaining the community of BGE and FlagEmbedding. Let us know if you have any suggessions or ideas!\n\nCurrently we are updating the [tutorials](./Tutorials/), we aim to create a comprehensive and detailed tutorial for beginners on text retrieval and RAG. Stay tuned!\n\nThe following contents are releasing in the upcoming weeks:\n\n- Evaluation\n- BGE-EN-ICL\n\n<details>\n  <summary>The whole tutorial roadmap</summary>\n    <img src=\"./Tutorials/tutorial_map.png\"/>\n</details>\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n| Model                                                                     | Language |                                                             Description                                                             |                                query instruction for retrieval                                 |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Provide instructions and few-shot examples freely based on the given task. |\n| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |    Multilingual     | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |        Provide instructions based on the given task.         |\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n| [LM-Cocktail](https://huggingface.co/Shitao)                   |   English |                     fine-tuned models (Llama and BGE) which can be used to reproduce the results of LM-Cocktail                     |  |\n| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English |                         a unified embedding model to support diverse retrieval augmentation needs for LLMs                          | See [README](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder) |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | Multilingual | a lightweight cross-encoder model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |  |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |  |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers, compress ratio and compress layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)             |   English |                                             Embedding Model which map text into vector                                              |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)               |   English |                                    a base-scale model but with similar ability to `bge-large-en`                                    |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)             |   English |                                        a small-scale model but with competitive performance                                         |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)             |   Chinese |                                             Embedding Model which map text into vector                                              |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)               |   Chinese |                                    a base-scale model but with similar ability to `bge-large-zh`                                    |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)             |   Chinese |                                        a small-scale model but with competitive performance                                         |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n\n\n\n\n### Contributors:\nThank all our contributors for their efforts and warmly welcome new members to join in!\n\n<a href=\"https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\" />\n</a>\n\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_m3,\n  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},\n  year={2023},\n  eprint={2309.07597},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n@misc{cocktail,\n      title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Xingrun Xing},\n      year={2023},\n      eprint={2311.13534},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{llm_embedder,\n      title={Retrieve Anything To Augment Large Language Models}, \n      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\n      year={2023},\n      eprint={2310.07554},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). \n\n",
  "external_links_in_readme": [
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation",
    "https://arxiv.org/pdf/2310.07554.pdf",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://arxiv.org/pdf/2409.05591v1",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset",
    "https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">",
    "https://huggingface.co/Shitao",
    "https://huggingface.co/BAAI/bge-base-en",
    "https://huggingface.co/BAAI/llm-embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://img.shields.io/badge/Contribution-Welcome-blue\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://huggingface.co/BAAI/bge-m3",
    "https://github.com/VectorSpaceLab/OmniGen",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge",
    "https://github.com/VectorSpaceLab/MegaPairs",
    "https://github.com/FlagOpen/FlagEmbedding.git",
    "https://huggingface.co/spaces/AIR-Bench/leaderboard",
    "https://github.com/AIR-Bench/AIR-Bench",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker",
    "https://github.com/JUNJIE99/MLVU",
    "https://github.com/qhjqhj00/MemoRAG,",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail",
    "https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research",
    "https://huggingface.co/BAAI/bge-small-zh-v1.5",
    "https://arxiv.org/abs/2412.14475",
    "https://img.shields.io/badge/LICENSE-MIT-green\">",
    "https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">",
    "https://huggingface.co/BAAI/bge-reranker-v2-gemma",
    "https://huggingface.co/spaces/mteb/leaderboard",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker",
    "https://arxiv.org/pdf/2402.03216.pdf",
    "https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\"",
    "https://huggingface.co/BAAI/bge-base-zh",
    "https://arxiv.org/abs/2311.13534",
    "https://huggingface.co/C-MTEB\">",
    "https://flagopen.baai.ac.cn/",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding",
    "https://arxiv.org/abs/2401.03462",
    "https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">",
    "https://arxiv.org/pdf/2309.07597.pdf",
    "https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a>",
    "https://huggingface.co/BAAI/bge-en-icl",
    "https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight",
    "https://www.bge-model.com",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://arxiv.org/abs/2312.15503",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials",
    "https://huggingface.co/BAAI/bge-multilingual-gemma2",
    "https://github.com/FlagOpen/FlagEmbedding\">",
    "https://huggingface.co/BAAI/bge-small-zh",
    "https://huggingface.co/BAAI/bge-small-en",
    "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE",
    "https://data.baai.ac.cn/details/BAAI-MTP",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker",
    "https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise",
    "https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92",
    "https://huggingface.co/BAAI/bge-large-en",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md",
    "https://huggingface.co/BAAI/bge-large-zh",
    "https://huggingface.co/BAAI/bge-base-zh-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora",
    "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA"
  ]
}
```

</details>


---

## Repository 14: FlagOpen/FlagEmbedding

# GitHub Repository Data

**Repository:** [FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## Basic Information

- **Description:** Retrieval and Retrieval-augmented LLMs
- **Created:** 2023-08-02T02:08:11+00:00
- **Last Updated:** 2025-06-21T10:44:22+00:00
- **Last Pushed:** 2025-06-04T10:00:26+00:00
- **Default Branch:** master
- **Size:** 52310 KB

## Statistics

- **Stars:** 9,974
- **Forks:** 737
- **Watchers:** 9,974
- **Open Issues:** 847
- **Total Issues:** 0
- **Pull Requests:** 217

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE)

## Languages

- **Python:** 3,871,341 bytes
- **Jupyter Notebook:** 2,607,506 bytes
- **Shell:** 7,322 bytes

## Topics

- `embeddings`
- `information-retrieval`
- `llm`
- `sentence-embeddings`
- `text-semantic-similarity`
- `retrieval-augmented-generation`

## Top Contributors

1. **hanhainebula** - 350 contributions
2. **545999961** - 327 contributions
3. **staoxiao** - 182 contributions
4. **ZiyiXia** - 113 contributions
5. **namespace-Pt** - 101 contributions
6. **JUNJIE99** - 78 contributions
7. **ftgreat** - 20 contributions
8. **ZhengLiu101** - 15 contributions
9. **Muennighoff** - 12 contributions
10. **shuyansy** - 10 contributions

## File Structure (Sample of 10 files)

Total files: 1,395

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.gitignore` (blob)
- `FlagEmbedding` (tree)
- `FlagEmbedding/__init__.py` (blob)
- `FlagEmbedding/abc` (tree)
- `FlagEmbedding/abc/__init__.py` (blob)
- `FlagEmbedding/abc/evaluation` (tree)
- `FlagEmbedding/abc/evaluation/__init__.py` (blob)

## Recent Issues

- 🟢 **#1480** Continued pretraining using RetroMAE (open)
- 🟢 **#1479** 微调bge-reranker-base一直卡顿，无日志输出 (open)
- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1477** 训练过程中train_loss和eval_loss都是先下降后上升再缓慢下降 (open)
- 🟢 **#1476** How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline (open)

## Recent Pull Requests

- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1475** Add inline comment to setup.py (open)
- 🟢 **#1469** Extend tutorials to include cloud deployment options (open)
- 🔴 **#1466** update reranker inference (closed)
- 🔴 **#1465** update environment (closed)

## Recent Commits

- **875fd4ff** update docs - ZiyiXia (2025-06-04T09:57:10+00:00)
- **235f967a** update tutorials - ZiyiXia (2025-06-04T09:27:36+00:00)
- **5e64baa6** Merge pull request #1466 from 545999961/master - chaofan (2025-05-28T08:15:17+00:00)
- **018b984e** update reranker inference - cfli (2025-05-28T08:14:32+00:00)
- **d04f7989** release version 1.3.5 - chaofan (2025-05-28T07:12:19+00:00)
- **8576fde7** Merge pull request #1465 from 545999961/master - chaofan (2025-05-28T06:58:53+00:00)
- **bf410d51** update environment - cfli (2025-05-28T06:55:54+00:00)
- **97ca0732** Merge pull request #1464 from 545999961/master - chaofan (2025-05-28T06:18:23+00:00)
- **b51706d6** upload coder eval script - cfli (2025-05-28T06:17:19+00:00)
- **821c3879** upload coder eval script - cfli (2025-05-28T06:12:16+00:00)

## External Links Found in README

- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation
- https://arxiv.org/pdf/2310.07554.pdf
- https://huggingface.co/BAAI/bge-reranker-base
- https://arxiv.org/pdf/2409.05591v1
- https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset
- https://img.shields.io/badge/BGE_series-🤗-yellow">
- https://huggingface.co/Shitao
- https://huggingface.co/BAAI/bge-base-en
- https://huggingface.co/BAAI/llm-embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder
- https://huggingface.co/BAAI/bge-reranker-large
- https://img.shields.io/badge/Contribution-Welcome-blue">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder
- https://huggingface.co/BAAI/bge-small-en-v1.5
- https://huggingface.co/BAAI/bge-m3
- https://github.com/VectorSpaceLab/OmniGen
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge
- https://github.com/VectorSpaceLab/MegaPairs

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673596675,
  "name": "FlagEmbedding",
  "full_name": "FlagOpen/FlagEmbedding",
  "description": "Retrieval and Retrieval-augmented LLMs",
  "html_url": "https://github.com/FlagOpen/FlagEmbedding",
  "clone_url": "https://github.com/FlagOpen/FlagEmbedding.git",
  "ssh_url": "git@github.com:FlagOpen/FlagEmbedding.git",
  "homepage": "http://www.bge-model.com/",
  "topics": [
    "embeddings",
    "information-retrieval",
    "llm",
    "sentence-embeddings",
    "text-semantic-similarity",
    "retrieval-augmented-generation"
  ],
  "default_branch": "master",
  "created_at": "2023-08-02T02:08:11+00:00",
  "updated_at": "2025-06-21T10:44:22+00:00",
  "pushed_at": "2025-06-04T10:00:26+00:00",
  "size_kb": 52310,
  "watchers_count": 9974,
  "stargazers_count": 9974,
  "forks_count": 737,
  "open_issues_count": 847,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"
  },
  "languages": {
    "Python": 3871341,
    "Jupyter Notebook": 2607506,
    "Shell": 7322
  },
  "top_contributors": [
    {
      "login": "hanhainebula",
      "contributions": 350
    },
    {
      "login": "545999961",
      "contributions": 327
    },
    {
      "login": "staoxiao",
      "contributions": 182
    },
    {
      "login": "ZiyiXia",
      "contributions": 113
    },
    {
      "login": "namespace-Pt",
      "contributions": 101
    },
    {
      "login": "JUNJIE99",
      "contributions": 78
    },
    {
      "login": "ftgreat",
      "contributions": 20
    },
    {
      "login": "ZhengLiu101",
      "contributions": 15
    },
    {
      "login": "Muennighoff",
      "contributions": 12
    },
    {
      "login": "shuyansy",
      "contributions": 10
    },
    {
      "login": "rainym00d",
      "contributions": 3
    },
    {
      "login": "chinainfant",
      "contributions": 3
    },
    {
      "login": "GeoloeG-IsT",
      "contributions": 3
    },
    {
      "login": "blue-vision0",
      "contributions": 2
    },
    {
      "login": "muazhari",
      "contributions": 2
    },
    {
      "login": "luisegarduno",
      "contributions": 2
    },
    {
      "login": "khazic",
      "contributions": 2
    },
    {
      "login": "shtdbb",
      "contributions": 2
    },
    {
      "login": "pengjunfeng11",
      "contributions": 2
    },
    {
      "login": "ycjcl868",
      "contributions": 2
    }
  ],
  "file_tree_count": 1395,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc/evaluation",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/evaluation/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 217,
  "recent_issues": [
    {
      "number": 1480,
      "title": "Continued pretraining using RetroMAE",
      "state": "open"
    },
    {
      "number": 1479,
      "title": "\u5fae\u8c03bge-reranker-base\u4e00\u76f4\u5361\u987f\uff0c\u65e0\u65e5\u5fd7\u8f93\u51fa",
      "state": "open"
    },
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1477,
      "title": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2dtrain_loss\u548ceval_loss\u90fd\u662f\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u518d\u7f13\u6162\u4e0b\u964d",
      "state": "open"
    },
    {
      "number": 1476,
      "title": "How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1475,
      "title": "Add inline comment to setup.py",
      "state": "open"
    },
    {
      "number": 1469,
      "title": "Extend tutorials to include cloud deployment options",
      "state": "open"
    },
    {
      "number": 1466,
      "title": "update reranker inference",
      "state": "closed"
    },
    {
      "number": 1465,
      "title": "update environment",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "875fd4ffcb894313a918e8bb0c9b7ead82f37e64",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:57:10+00:00",
      "message": "update docs"
    },
    {
      "sha": "235f967a0122b9d5b85104fd4e8f36dc15ee85a8",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:27:36+00:00",
      "message": "update tutorials"
    },
    {
      "sha": "5e64baa61e75df23105a66d1e9d09ad799366e2a",
      "author": "chaofan",
      "date": "2025-05-28T08:15:17+00:00",
      "message": "Merge pull request #1466 from 545999961/master"
    },
    {
      "sha": "018b984e23e12d9793e59f2a1ef07fb9153ab08e",
      "author": "cfli",
      "date": "2025-05-28T08:14:32+00:00",
      "message": "update reranker inference"
    },
    {
      "sha": "d04f7989eeec2c6db4ead7c77ddef656f9c9d3aa",
      "author": "chaofan",
      "date": "2025-05-28T07:12:19+00:00",
      "message": "release version 1.3.5"
    },
    {
      "sha": "8576fde79b84224ecbe493857115552fcfb72ed7",
      "author": "chaofan",
      "date": "2025-05-28T06:58:53+00:00",
      "message": "Merge pull request #1465 from 545999961/master"
    },
    {
      "sha": "bf410d51e9e685e7f1b2e3816f09f3dc64ad8292",
      "author": "cfli",
      "date": "2025-05-28T06:55:54+00:00",
      "message": "update environment"
    },
    {
      "sha": "97ca07325d9e786d668f9a038502ad736dded9d5",
      "author": "chaofan",
      "date": "2025-05-28T06:18:23+00:00",
      "message": "Merge pull request #1464 from 545999961/master"
    },
    {
      "sha": "b51706d63ae8a1d86443df8b0e5ff6e45eb08fdc",
      "author": "cfli",
      "date": "2025-05-28T06:17:19+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "821c387961b5597fe672a4386232110ef54d5107",
      "author": "cfli",
      "date": "2025-05-28T06:12:16+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "3332a1956e2d5af7c52628a60e5c9788bd1b5ed9",
      "author": "chaofan",
      "date": "2025-05-22T10:07:40+00:00",
      "message": "Merge pull request #1459 from 545999961/master"
    },
    {
      "sha": "d412884a4e96c45e4a9e83a43e507e9beaa04713",
      "author": "cfli",
      "date": "2025-05-22T10:06:43+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "5c11feffc104497953a333036aa56b6b543e7cee",
      "author": "chaofan",
      "date": "2025-05-22T09:56:48+00:00",
      "message": "Merge pull request #1458 from 545999961/master"
    },
    {
      "sha": "41b526441f13242b49126261c219b6b9f14e17cb",
      "author": "cfli",
      "date": "2025-05-22T09:55:44+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "52bb9fddb5def2074b277f6e9a2f02ed8aa17dee",
      "author": "chaofan",
      "date": "2025-05-22T09:52:21+00:00",
      "message": "Merge pull request #1457 from 545999961/master"
    },
    {
      "sha": "f05d4b3fbeff5df9dabeb71ab16225fe5e1e8c95",
      "author": "cfli",
      "date": "2025-05-22T09:51:20+00:00",
      "message": "upload reinforced_ir"
    },
    {
      "sha": "25ee99c228e9df63a2be79e8d795d549ab5001e9",
      "author": "chaofan",
      "date": "2025-05-21T04:46:03+00:00",
      "message": "Merge pull request #1453 from 545999961/master"
    },
    {
      "sha": "436210bbed3ca352aac82a9be66cd4e665408944",
      "author": "cfli",
      "date": "2025-05-21T04:45:10+00:00",
      "message": "upload Matroyshka_reranker"
    },
    {
      "sha": "1946f89d5a6926871861c9ed9f4acbb31665acfe",
      "author": "cfli",
      "date": "2025-05-20T08:13:30+00:00",
      "message": "upload Matroyshka Re-Ranker"
    },
    {
      "sha": "f6924351bba09620938b69a2c933c6712be829c4",
      "author": "cfli",
      "date": "2025-05-20T08:12:08+00:00",
      "message": "upload Matroyshka Re-Ranker"
    }
  ],
  "readme_text": "[<img src=\"./imgs/FlagOpen.png\">](https://flagopen.baai.ac.cn/)\n\n<h1 align=\"center\">\u26a1\ufe0fBGE: One-Stop Retrieval Toolkit For Search and RAG</h1>\n\n![bge_logo](./imgs/bge_logo.jpg)\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Contribution-Welcome-blue\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\">\n    </a>\n    <a href=\"https://huggingface.co/C-MTEB\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=#news>News</a> |\n        <a href=#installation>Installation</a> |\n        <a href=#quick-start>Quick Start</a> |\n        <a href=#community>Community</a> |\n        <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a> |\n        <a href=#model-list>Model List</a> |\n        <a href=\"#contributors\">Contributor</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\n\n## News\n\n- 3/6/2025: :fire::fire: Introduce **BGE-VL** ([HF repo](https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92)), State-Of-The-Art multimodal embedding models to support Any visual search applications (everything, including text-to-image, image-to-text, image&prompt-to-image, text-to-image&text, and more)! They are released under the MIT license and are completely free for both academic and commercial use. We also release **MegaPairs** ([repo](https://github.com/VectorSpaceLab/MegaPairs), [paper](https://arxiv.org/abs/2412.14475)), a massive synthetic dataset which empowers BGE-VL!\n- 12/5/2024: :book: We built the [BGE documentation](https://www.bge-model.com) for centralized BGE information and materials!\n- 10/29/2024: :earth_asia: We created WeChat group for BGE. Scan the [QR code](./imgs/BGE_WeChat_Group.png) to join the group chat! To get the first hand message about our updates and new release, or having any questions or ideas, join us now!\n- <img src=\"./imgs/BGE_WeChat_Group.png\" alt=\"bge_wechat_group\" class=\"center\" width=\"200\">\n\n- 10/22/2024: We release another interesting model: [OmniGen](https://github.com/VectorSpaceLab/OmniGen), which is a unified image generation model supporting various tasks. OmniGen can accomplish complex image generation tasks without the need for additional plugins like ControlNet, IP-Adapter, or auxiliary models such as pose detection and face detection.\n- 9/10/2024: Introducing **MemoRAG**, a step forward towards RAG 2.0 on top of memory-inspired knowledge discovery (repo: https://github.com/qhjqhj00/MemoRAG, paper: https://arxiv.org/pdf/2409.05591v1) \n- 9/2/2024: Start to maintain the [tutorials](./Tutorials/). The contents within will be actively updated and eariched, stay tuned! :books:\n- 7/26/2024: Release a new embedding model [bge-en-icl](https://huggingface.co/BAAI/bge-en-icl), an embedding model that incorporates in-context learning capabilities, which, by providing task-relevant query-response examples, can encode semantically richer queries, further enhancing the semantic representation ability of the embeddings.\n- 7/26/2024: Release a new embedding model [bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2), a multilingual embedding model based on gemma-2-9b, which supports multiple languages and diverse downstream tasks, achieving new SOTA on multilingual benchmarks (MIRACL, MTEB-fr, and MTEB-pl). \n- 7/26/2024: Release a new lightweight reranker [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight), a lightweight reranker based on gemma-2-9b, which supports token compression and layerwise lightweight operations, can still ensure good performance while saving a significant amount of resources. :fire:\n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n\n- 6/7/2024: Release a new benchmark [MLVU](https://github.com/JUNJIE99/MLVU), the first comprehensive benchmark specifically designed for long video understanding. MLVU features an extensive range of video durations, a diverse collection of video sources, and a set of evaluation tasks uniquely tailored for long-form video understanding. :fire:\n- 5/21/2024: Release a new benchmark [AIR-Bench](https://github.com/AIR-Bench/AIR-Bench) together with Jina AI, Zilliz, HuggingFace, and other partners. AIR-Bench focuses on a fair out-of-distribution evaluation for Neural IR & RAG. It generates the synthetic data for benchmarking w.r.t. diverse domains and languages. It is dynamic and will be updated on regular basis. [Leaderboard](https://huggingface.co/spaces/AIR-Bench/leaderboard) :fire:\n- 4/30/2024: Release [Llama-3-8B-Instruct-80K-QLoRA](https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA), extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA training on a few synthesized long-context data. The model achieves remarkable performance on various long-context benchmarks. [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora) :fire:\n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually :smiley:) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation :fire:\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data. :fire:\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://arxiv.org/pdf/2402.03216.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) \n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n\n</details>\n\n\nBGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n![projects](./imgs/projects.png)\n\n- **Inference**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker)\n- **Finetune**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)\n- **[Evaluation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation)**\n- **[Dataset](https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset)**\n- **[Tutorials](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)**\n- **[research](https://github.com/FlagOpen/FlagEmbedding/tree/master/research)**\n\n\n## Installation\n### Using pip:\nIf you do not want to finetune the models, you can install the package without the finetune dependency:\n```\npip install -U FlagEmbedding\n```\nIf you want to finetune the models, you can install the package with the finetune dependency:\n```\npip install -U FlagEmbedding[finetune]\n```\n### Install from sources:\n\nClone the repository and install\n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install  .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install  .[finetune]\n```\nFor development in editable mode:\n```\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install -e .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install -e .[finetune]\n```\n\n## Quick Start\nFirst, load one of the BGE embedding model:\n```\nfrom FlagEmbedding import FlagAutoModel\n\nmodel = FlagAutoModel.from_finetuned('BAAI/bge-base-en-v1.5',\n                                      query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n                                      use_fp16=True)\n```\nThen, feed some sentences to the model and get their embeddings:\n```\nsentences_1 = [\"I love NLP\", \"I love machine learning\"]\nsentences_2 = [\"I love BGE\", \"I love text retrieval\"]\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\n```\nOnce we get the embeddings, we can compute similarity by inner product:\n```\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\n\nFor more details, you can refer to [embedder inference](./examples/inference/embedder), [reranker inference](./examples/inference/reranker), [embedder finetune](./examples/finetune/embedder), [reranker fintune](./examples/finetune/reranker), [evaluation](./examples/evaluation).\n\nIf you're unfamiliar with any of related concepts, please check out the [tutorial](./Tutorials/). If it's not there, let us know.\n\nFor more interesting topics related to BGE, take a look at [research](./research).\n\n## Community\n\nWe are actively maintaining the community of BGE and FlagEmbedding. Let us know if you have any suggessions or ideas!\n\nCurrently we are updating the [tutorials](./Tutorials/), we aim to create a comprehensive and detailed tutorial for beginners on text retrieval and RAG. Stay tuned!\n\nThe following contents are releasing in the upcoming weeks:\n\n- Evaluation\n- BGE-EN-ICL\n\n<details>\n  <summary>The whole tutorial roadmap</summary>\n    <img src=\"./Tutorials/tutorial_map.png\"/>\n</details>\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n| Model                                                                     | Language |                                                             Description                                                             |                                query instruction for retrieval                                 |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Provide instructions and few-shot examples freely based on the given task. |\n| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |    Multilingual     | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |        Provide instructions based on the given task.         |\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n| [LM-Cocktail](https://huggingface.co/Shitao)                   |   English |                     fine-tuned models (Llama and BGE) which can be used to reproduce the results of LM-Cocktail                     |  |\n| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English |                         a unified embedding model to support diverse retrieval augmentation needs for LLMs                          | See [README](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder) |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | Multilingual | a lightweight cross-encoder model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |  |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |  |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers, compress ratio and compress layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)             |   English |                                             Embedding Model which map text into vector                                              |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)               |   English |                                    a base-scale model but with similar ability to `bge-large-en`                                    |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)             |   English |                                        a small-scale model but with competitive performance                                         |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)             |   Chinese |                                             Embedding Model which map text into vector                                              |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)               |   Chinese |                                    a base-scale model but with similar ability to `bge-large-zh`                                    |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)             |   Chinese |                                        a small-scale model but with competitive performance                                         |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n\n\n\n\n### Contributors:\nThank all our contributors for their efforts and warmly welcome new members to join in!\n\n<a href=\"https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\" />\n</a>\n\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_m3,\n  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},\n  year={2023},\n  eprint={2309.07597},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n@misc{cocktail,\n      title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Xingrun Xing},\n      year={2023},\n      eprint={2311.13534},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{llm_embedder,\n      title={Retrieve Anything To Augment Large Language Models}, \n      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\n      year={2023},\n      eprint={2310.07554},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). \n\n",
  "external_links_in_readme": [
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation",
    "https://arxiv.org/pdf/2310.07554.pdf",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://arxiv.org/pdf/2409.05591v1",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset",
    "https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">",
    "https://huggingface.co/Shitao",
    "https://huggingface.co/BAAI/bge-base-en",
    "https://huggingface.co/BAAI/llm-embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://img.shields.io/badge/Contribution-Welcome-blue\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://huggingface.co/BAAI/bge-m3",
    "https://github.com/VectorSpaceLab/OmniGen",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge",
    "https://github.com/VectorSpaceLab/MegaPairs",
    "https://github.com/FlagOpen/FlagEmbedding.git",
    "https://huggingface.co/spaces/AIR-Bench/leaderboard",
    "https://github.com/AIR-Bench/AIR-Bench",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker",
    "https://github.com/JUNJIE99/MLVU",
    "https://github.com/qhjqhj00/MemoRAG,",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail",
    "https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research",
    "https://huggingface.co/BAAI/bge-small-zh-v1.5",
    "https://arxiv.org/abs/2412.14475",
    "https://img.shields.io/badge/LICENSE-MIT-green\">",
    "https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">",
    "https://huggingface.co/BAAI/bge-reranker-v2-gemma",
    "https://huggingface.co/spaces/mteb/leaderboard",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker",
    "https://arxiv.org/pdf/2402.03216.pdf",
    "https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\"",
    "https://huggingface.co/BAAI/bge-base-zh",
    "https://arxiv.org/abs/2311.13534",
    "https://huggingface.co/C-MTEB\">",
    "https://flagopen.baai.ac.cn/",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding",
    "https://arxiv.org/abs/2401.03462",
    "https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">",
    "https://arxiv.org/pdf/2309.07597.pdf",
    "https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a>",
    "https://huggingface.co/BAAI/bge-en-icl",
    "https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight",
    "https://www.bge-model.com",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://arxiv.org/abs/2312.15503",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials",
    "https://huggingface.co/BAAI/bge-multilingual-gemma2",
    "https://github.com/FlagOpen/FlagEmbedding\">",
    "https://huggingface.co/BAAI/bge-small-zh",
    "https://huggingface.co/BAAI/bge-small-en",
    "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE",
    "https://data.baai.ac.cn/details/BAAI-MTP",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker",
    "https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise",
    "https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92",
    "https://huggingface.co/BAAI/bge-large-en",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md",
    "https://huggingface.co/BAAI/bge-large-zh",
    "https://huggingface.co/BAAI/bge-base-zh-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora",
    "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA"
  ]
}
```

</details>


---

## Repository 15: FlagOpen/FlagEmbedding

# GitHub Repository Data

**Repository:** [FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## Basic Information

- **Description:** Retrieval and Retrieval-augmented LLMs
- **Created:** 2023-08-02T02:08:11+00:00
- **Last Updated:** 2025-06-21T10:44:22+00:00
- **Last Pushed:** 2025-06-04T10:00:26+00:00
- **Default Branch:** master
- **Size:** 52310 KB

## Statistics

- **Stars:** 9,974
- **Forks:** 737
- **Watchers:** 9,974
- **Open Issues:** 847
- **Total Issues:** 0
- **Pull Requests:** 217

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE)

## Languages

- **Python:** 3,871,341 bytes
- **Jupyter Notebook:** 2,607,506 bytes
- **Shell:** 7,322 bytes

## Topics

- `embeddings`
- `information-retrieval`
- `llm`
- `sentence-embeddings`
- `text-semantic-similarity`
- `retrieval-augmented-generation`

## Top Contributors

1. **hanhainebula** - 350 contributions
2. **545999961** - 327 contributions
3. **staoxiao** - 182 contributions
4. **ZiyiXia** - 113 contributions
5. **namespace-Pt** - 101 contributions
6. **JUNJIE99** - 78 contributions
7. **ftgreat** - 20 contributions
8. **ZhengLiu101** - 15 contributions
9. **Muennighoff** - 12 contributions
10. **shuyansy** - 10 contributions

## File Structure (Sample of 10 files)

Total files: 1,395

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.gitignore` (blob)
- `FlagEmbedding` (tree)
- `FlagEmbedding/__init__.py` (blob)
- `FlagEmbedding/abc` (tree)
- `FlagEmbedding/abc/__init__.py` (blob)
- `FlagEmbedding/abc/evaluation` (tree)
- `FlagEmbedding/abc/evaluation/__init__.py` (blob)

## Recent Issues

- 🟢 **#1480** Continued pretraining using RetroMAE (open)
- 🟢 **#1479** 微调bge-reranker-base一直卡顿，无日志输出 (open)
- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1477** 训练过程中train_loss和eval_loss都是先下降后上升再缓慢下降 (open)
- 🟢 **#1476** How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline (open)

## Recent Pull Requests

- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1475** Add inline comment to setup.py (open)
- 🟢 **#1469** Extend tutorials to include cloud deployment options (open)
- 🔴 **#1466** update reranker inference (closed)
- 🔴 **#1465** update environment (closed)

## Recent Commits

- **875fd4ff** update docs - ZiyiXia (2025-06-04T09:57:10+00:00)
- **235f967a** update tutorials - ZiyiXia (2025-06-04T09:27:36+00:00)
- **5e64baa6** Merge pull request #1466 from 545999961/master - chaofan (2025-05-28T08:15:17+00:00)
- **018b984e** update reranker inference - cfli (2025-05-28T08:14:32+00:00)
- **d04f7989** release version 1.3.5 - chaofan (2025-05-28T07:12:19+00:00)
- **8576fde7** Merge pull request #1465 from 545999961/master - chaofan (2025-05-28T06:58:53+00:00)
- **bf410d51** update environment - cfli (2025-05-28T06:55:54+00:00)
- **97ca0732** Merge pull request #1464 from 545999961/master - chaofan (2025-05-28T06:18:23+00:00)
- **b51706d6** upload coder eval script - cfli (2025-05-28T06:17:19+00:00)
- **821c3879** upload coder eval script - cfli (2025-05-28T06:12:16+00:00)

## External Links Found in README

- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation
- https://arxiv.org/pdf/2310.07554.pdf
- https://huggingface.co/BAAI/bge-reranker-base
- https://arxiv.org/pdf/2409.05591v1
- https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset
- https://img.shields.io/badge/BGE_series-🤗-yellow">
- https://huggingface.co/Shitao
- https://huggingface.co/BAAI/bge-base-en
- https://huggingface.co/BAAI/llm-embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder
- https://huggingface.co/BAAI/bge-reranker-large
- https://img.shields.io/badge/Contribution-Welcome-blue">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder
- https://huggingface.co/BAAI/bge-small-en-v1.5
- https://huggingface.co/BAAI/bge-m3
- https://github.com/VectorSpaceLab/OmniGen
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge
- https://github.com/VectorSpaceLab/MegaPairs

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673596675,
  "name": "FlagEmbedding",
  "full_name": "FlagOpen/FlagEmbedding",
  "description": "Retrieval and Retrieval-augmented LLMs",
  "html_url": "https://github.com/FlagOpen/FlagEmbedding",
  "clone_url": "https://github.com/FlagOpen/FlagEmbedding.git",
  "ssh_url": "git@github.com:FlagOpen/FlagEmbedding.git",
  "homepage": "http://www.bge-model.com/",
  "topics": [
    "embeddings",
    "information-retrieval",
    "llm",
    "sentence-embeddings",
    "text-semantic-similarity",
    "retrieval-augmented-generation"
  ],
  "default_branch": "master",
  "created_at": "2023-08-02T02:08:11+00:00",
  "updated_at": "2025-06-21T10:44:22+00:00",
  "pushed_at": "2025-06-04T10:00:26+00:00",
  "size_kb": 52310,
  "watchers_count": 9974,
  "stargazers_count": 9974,
  "forks_count": 737,
  "open_issues_count": 847,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"
  },
  "languages": {
    "Python": 3871341,
    "Jupyter Notebook": 2607506,
    "Shell": 7322
  },
  "top_contributors": [
    {
      "login": "hanhainebula",
      "contributions": 350
    },
    {
      "login": "545999961",
      "contributions": 327
    },
    {
      "login": "staoxiao",
      "contributions": 182
    },
    {
      "login": "ZiyiXia",
      "contributions": 113
    },
    {
      "login": "namespace-Pt",
      "contributions": 101
    },
    {
      "login": "JUNJIE99",
      "contributions": 78
    },
    {
      "login": "ftgreat",
      "contributions": 20
    },
    {
      "login": "ZhengLiu101",
      "contributions": 15
    },
    {
      "login": "Muennighoff",
      "contributions": 12
    },
    {
      "login": "shuyansy",
      "contributions": 10
    },
    {
      "login": "rainym00d",
      "contributions": 3
    },
    {
      "login": "chinainfant",
      "contributions": 3
    },
    {
      "login": "GeoloeG-IsT",
      "contributions": 3
    },
    {
      "login": "blue-vision0",
      "contributions": 2
    },
    {
      "login": "muazhari",
      "contributions": 2
    },
    {
      "login": "luisegarduno",
      "contributions": 2
    },
    {
      "login": "khazic",
      "contributions": 2
    },
    {
      "login": "shtdbb",
      "contributions": 2
    },
    {
      "login": "pengjunfeng11",
      "contributions": 2
    },
    {
      "login": "ycjcl868",
      "contributions": 2
    }
  ],
  "file_tree_count": 1395,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc/evaluation",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/evaluation/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 217,
  "recent_issues": [
    {
      "number": 1480,
      "title": "Continued pretraining using RetroMAE",
      "state": "open"
    },
    {
      "number": 1479,
      "title": "\u5fae\u8c03bge-reranker-base\u4e00\u76f4\u5361\u987f\uff0c\u65e0\u65e5\u5fd7\u8f93\u51fa",
      "state": "open"
    },
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1477,
      "title": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2dtrain_loss\u548ceval_loss\u90fd\u662f\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u518d\u7f13\u6162\u4e0b\u964d",
      "state": "open"
    },
    {
      "number": 1476,
      "title": "How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1475,
      "title": "Add inline comment to setup.py",
      "state": "open"
    },
    {
      "number": 1469,
      "title": "Extend tutorials to include cloud deployment options",
      "state": "open"
    },
    {
      "number": 1466,
      "title": "update reranker inference",
      "state": "closed"
    },
    {
      "number": 1465,
      "title": "update environment",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "875fd4ffcb894313a918e8bb0c9b7ead82f37e64",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:57:10+00:00",
      "message": "update docs"
    },
    {
      "sha": "235f967a0122b9d5b85104fd4e8f36dc15ee85a8",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:27:36+00:00",
      "message": "update tutorials"
    },
    {
      "sha": "5e64baa61e75df23105a66d1e9d09ad799366e2a",
      "author": "chaofan",
      "date": "2025-05-28T08:15:17+00:00",
      "message": "Merge pull request #1466 from 545999961/master"
    },
    {
      "sha": "018b984e23e12d9793e59f2a1ef07fb9153ab08e",
      "author": "cfli",
      "date": "2025-05-28T08:14:32+00:00",
      "message": "update reranker inference"
    },
    {
      "sha": "d04f7989eeec2c6db4ead7c77ddef656f9c9d3aa",
      "author": "chaofan",
      "date": "2025-05-28T07:12:19+00:00",
      "message": "release version 1.3.5"
    },
    {
      "sha": "8576fde79b84224ecbe493857115552fcfb72ed7",
      "author": "chaofan",
      "date": "2025-05-28T06:58:53+00:00",
      "message": "Merge pull request #1465 from 545999961/master"
    },
    {
      "sha": "bf410d51e9e685e7f1b2e3816f09f3dc64ad8292",
      "author": "cfli",
      "date": "2025-05-28T06:55:54+00:00",
      "message": "update environment"
    },
    {
      "sha": "97ca07325d9e786d668f9a038502ad736dded9d5",
      "author": "chaofan",
      "date": "2025-05-28T06:18:23+00:00",
      "message": "Merge pull request #1464 from 545999961/master"
    },
    {
      "sha": "b51706d63ae8a1d86443df8b0e5ff6e45eb08fdc",
      "author": "cfli",
      "date": "2025-05-28T06:17:19+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "821c387961b5597fe672a4386232110ef54d5107",
      "author": "cfli",
      "date": "2025-05-28T06:12:16+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "3332a1956e2d5af7c52628a60e5c9788bd1b5ed9",
      "author": "chaofan",
      "date": "2025-05-22T10:07:40+00:00",
      "message": "Merge pull request #1459 from 545999961/master"
    },
    {
      "sha": "d412884a4e96c45e4a9e83a43e507e9beaa04713",
      "author": "cfli",
      "date": "2025-05-22T10:06:43+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "5c11feffc104497953a333036aa56b6b543e7cee",
      "author": "chaofan",
      "date": "2025-05-22T09:56:48+00:00",
      "message": "Merge pull request #1458 from 545999961/master"
    },
    {
      "sha": "41b526441f13242b49126261c219b6b9f14e17cb",
      "author": "cfli",
      "date": "2025-05-22T09:55:44+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "52bb9fddb5def2074b277f6e9a2f02ed8aa17dee",
      "author": "chaofan",
      "date": "2025-05-22T09:52:21+00:00",
      "message": "Merge pull request #1457 from 545999961/master"
    },
    {
      "sha": "f05d4b3fbeff5df9dabeb71ab16225fe5e1e8c95",
      "author": "cfli",
      "date": "2025-05-22T09:51:20+00:00",
      "message": "upload reinforced_ir"
    },
    {
      "sha": "25ee99c228e9df63a2be79e8d795d549ab5001e9",
      "author": "chaofan",
      "date": "2025-05-21T04:46:03+00:00",
      "message": "Merge pull request #1453 from 545999961/master"
    },
    {
      "sha": "436210bbed3ca352aac82a9be66cd4e665408944",
      "author": "cfli",
      "date": "2025-05-21T04:45:10+00:00",
      "message": "upload Matroyshka_reranker"
    },
    {
      "sha": "1946f89d5a6926871861c9ed9f4acbb31665acfe",
      "author": "cfli",
      "date": "2025-05-20T08:13:30+00:00",
      "message": "upload Matroyshka Re-Ranker"
    },
    {
      "sha": "f6924351bba09620938b69a2c933c6712be829c4",
      "author": "cfli",
      "date": "2025-05-20T08:12:08+00:00",
      "message": "upload Matroyshka Re-Ranker"
    }
  ],
  "readme_text": "[<img src=\"./imgs/FlagOpen.png\">](https://flagopen.baai.ac.cn/)\n\n<h1 align=\"center\">\u26a1\ufe0fBGE: One-Stop Retrieval Toolkit For Search and RAG</h1>\n\n![bge_logo](./imgs/bge_logo.jpg)\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Contribution-Welcome-blue\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\">\n    </a>\n    <a href=\"https://huggingface.co/C-MTEB\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=#news>News</a> |\n        <a href=#installation>Installation</a> |\n        <a href=#quick-start>Quick Start</a> |\n        <a href=#community>Community</a> |\n        <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a> |\n        <a href=#model-list>Model List</a> |\n        <a href=\"#contributors\">Contributor</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\n\n## News\n\n- 3/6/2025: :fire::fire: Introduce **BGE-VL** ([HF repo](https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92)), State-Of-The-Art multimodal embedding models to support Any visual search applications (everything, including text-to-image, image-to-text, image&prompt-to-image, text-to-image&text, and more)! They are released under the MIT license and are completely free for both academic and commercial use. We also release **MegaPairs** ([repo](https://github.com/VectorSpaceLab/MegaPairs), [paper](https://arxiv.org/abs/2412.14475)), a massive synthetic dataset which empowers BGE-VL!\n- 12/5/2024: :book: We built the [BGE documentation](https://www.bge-model.com) for centralized BGE information and materials!\n- 10/29/2024: :earth_asia: We created WeChat group for BGE. Scan the [QR code](./imgs/BGE_WeChat_Group.png) to join the group chat! To get the first hand message about our updates and new release, or having any questions or ideas, join us now!\n- <img src=\"./imgs/BGE_WeChat_Group.png\" alt=\"bge_wechat_group\" class=\"center\" width=\"200\">\n\n- 10/22/2024: We release another interesting model: [OmniGen](https://github.com/VectorSpaceLab/OmniGen), which is a unified image generation model supporting various tasks. OmniGen can accomplish complex image generation tasks without the need for additional plugins like ControlNet, IP-Adapter, or auxiliary models such as pose detection and face detection.\n- 9/10/2024: Introducing **MemoRAG**, a step forward towards RAG 2.0 on top of memory-inspired knowledge discovery (repo: https://github.com/qhjqhj00/MemoRAG, paper: https://arxiv.org/pdf/2409.05591v1) \n- 9/2/2024: Start to maintain the [tutorials](./Tutorials/). The contents within will be actively updated and eariched, stay tuned! :books:\n- 7/26/2024: Release a new embedding model [bge-en-icl](https://huggingface.co/BAAI/bge-en-icl), an embedding model that incorporates in-context learning capabilities, which, by providing task-relevant query-response examples, can encode semantically richer queries, further enhancing the semantic representation ability of the embeddings.\n- 7/26/2024: Release a new embedding model [bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2), a multilingual embedding model based on gemma-2-9b, which supports multiple languages and diverse downstream tasks, achieving new SOTA on multilingual benchmarks (MIRACL, MTEB-fr, and MTEB-pl). \n- 7/26/2024: Release a new lightweight reranker [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight), a lightweight reranker based on gemma-2-9b, which supports token compression and layerwise lightweight operations, can still ensure good performance while saving a significant amount of resources. :fire:\n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n\n- 6/7/2024: Release a new benchmark [MLVU](https://github.com/JUNJIE99/MLVU), the first comprehensive benchmark specifically designed for long video understanding. MLVU features an extensive range of video durations, a diverse collection of video sources, and a set of evaluation tasks uniquely tailored for long-form video understanding. :fire:\n- 5/21/2024: Release a new benchmark [AIR-Bench](https://github.com/AIR-Bench/AIR-Bench) together with Jina AI, Zilliz, HuggingFace, and other partners. AIR-Bench focuses on a fair out-of-distribution evaluation for Neural IR & RAG. It generates the synthetic data for benchmarking w.r.t. diverse domains and languages. It is dynamic and will be updated on regular basis. [Leaderboard](https://huggingface.co/spaces/AIR-Bench/leaderboard) :fire:\n- 4/30/2024: Release [Llama-3-8B-Instruct-80K-QLoRA](https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA), extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA training on a few synthesized long-context data. The model achieves remarkable performance on various long-context benchmarks. [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora) :fire:\n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually :smiley:) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation :fire:\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data. :fire:\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://arxiv.org/pdf/2402.03216.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) \n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n\n</details>\n\n\nBGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n![projects](./imgs/projects.png)\n\n- **Inference**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker)\n- **Finetune**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)\n- **[Evaluation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation)**\n- **[Dataset](https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset)**\n- **[Tutorials](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)**\n- **[research](https://github.com/FlagOpen/FlagEmbedding/tree/master/research)**\n\n\n## Installation\n### Using pip:\nIf you do not want to finetune the models, you can install the package without the finetune dependency:\n```\npip install -U FlagEmbedding\n```\nIf you want to finetune the models, you can install the package with the finetune dependency:\n```\npip install -U FlagEmbedding[finetune]\n```\n### Install from sources:\n\nClone the repository and install\n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install  .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install  .[finetune]\n```\nFor development in editable mode:\n```\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install -e .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install -e .[finetune]\n```\n\n## Quick Start\nFirst, load one of the BGE embedding model:\n```\nfrom FlagEmbedding import FlagAutoModel\n\nmodel = FlagAutoModel.from_finetuned('BAAI/bge-base-en-v1.5',\n                                      query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n                                      use_fp16=True)\n```\nThen, feed some sentences to the model and get their embeddings:\n```\nsentences_1 = [\"I love NLP\", \"I love machine learning\"]\nsentences_2 = [\"I love BGE\", \"I love text retrieval\"]\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\n```\nOnce we get the embeddings, we can compute similarity by inner product:\n```\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\n\nFor more details, you can refer to [embedder inference](./examples/inference/embedder), [reranker inference](./examples/inference/reranker), [embedder finetune](./examples/finetune/embedder), [reranker fintune](./examples/finetune/reranker), [evaluation](./examples/evaluation).\n\nIf you're unfamiliar with any of related concepts, please check out the [tutorial](./Tutorials/). If it's not there, let us know.\n\nFor more interesting topics related to BGE, take a look at [research](./research).\n\n## Community\n\nWe are actively maintaining the community of BGE and FlagEmbedding. Let us know if you have any suggessions or ideas!\n\nCurrently we are updating the [tutorials](./Tutorials/), we aim to create a comprehensive and detailed tutorial for beginners on text retrieval and RAG. Stay tuned!\n\nThe following contents are releasing in the upcoming weeks:\n\n- Evaluation\n- BGE-EN-ICL\n\n<details>\n  <summary>The whole tutorial roadmap</summary>\n    <img src=\"./Tutorials/tutorial_map.png\"/>\n</details>\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n| Model                                                                     | Language |                                                             Description                                                             |                                query instruction for retrieval                                 |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Provide instructions and few-shot examples freely based on the given task. |\n| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |    Multilingual     | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |        Provide instructions based on the given task.         |\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n| [LM-Cocktail](https://huggingface.co/Shitao)                   |   English |                     fine-tuned models (Llama and BGE) which can be used to reproduce the results of LM-Cocktail                     |  |\n| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English |                         a unified embedding model to support diverse retrieval augmentation needs for LLMs                          | See [README](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder) |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | Multilingual | a lightweight cross-encoder model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |  |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |  |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers, compress ratio and compress layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)             |   English |                                             Embedding Model which map text into vector                                              |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)               |   English |                                    a base-scale model but with similar ability to `bge-large-en`                                    |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)             |   English |                                        a small-scale model but with competitive performance                                         |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)             |   Chinese |                                             Embedding Model which map text into vector                                              |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)               |   Chinese |                                    a base-scale model but with similar ability to `bge-large-zh`                                    |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)             |   Chinese |                                        a small-scale model but with competitive performance                                         |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n\n\n\n\n### Contributors:\nThank all our contributors for their efforts and warmly welcome new members to join in!\n\n<a href=\"https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\" />\n</a>\n\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_m3,\n  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},\n  year={2023},\n  eprint={2309.07597},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n@misc{cocktail,\n      title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Xingrun Xing},\n      year={2023},\n      eprint={2311.13534},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{llm_embedder,\n      title={Retrieve Anything To Augment Large Language Models}, \n      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\n      year={2023},\n      eprint={2310.07554},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). \n\n",
  "external_links_in_readme": [
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation",
    "https://arxiv.org/pdf/2310.07554.pdf",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://arxiv.org/pdf/2409.05591v1",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset",
    "https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">",
    "https://huggingface.co/Shitao",
    "https://huggingface.co/BAAI/bge-base-en",
    "https://huggingface.co/BAAI/llm-embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://img.shields.io/badge/Contribution-Welcome-blue\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://huggingface.co/BAAI/bge-m3",
    "https://github.com/VectorSpaceLab/OmniGen",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge",
    "https://github.com/VectorSpaceLab/MegaPairs",
    "https://github.com/FlagOpen/FlagEmbedding.git",
    "https://huggingface.co/spaces/AIR-Bench/leaderboard",
    "https://github.com/AIR-Bench/AIR-Bench",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker",
    "https://github.com/JUNJIE99/MLVU",
    "https://github.com/qhjqhj00/MemoRAG,",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail",
    "https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research",
    "https://huggingface.co/BAAI/bge-small-zh-v1.5",
    "https://arxiv.org/abs/2412.14475",
    "https://img.shields.io/badge/LICENSE-MIT-green\">",
    "https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">",
    "https://huggingface.co/BAAI/bge-reranker-v2-gemma",
    "https://huggingface.co/spaces/mteb/leaderboard",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker",
    "https://arxiv.org/pdf/2402.03216.pdf",
    "https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\"",
    "https://huggingface.co/BAAI/bge-base-zh",
    "https://arxiv.org/abs/2311.13534",
    "https://huggingface.co/C-MTEB\">",
    "https://flagopen.baai.ac.cn/",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding",
    "https://arxiv.org/abs/2401.03462",
    "https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">",
    "https://arxiv.org/pdf/2309.07597.pdf",
    "https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a>",
    "https://huggingface.co/BAAI/bge-en-icl",
    "https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight",
    "https://www.bge-model.com",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://arxiv.org/abs/2312.15503",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials",
    "https://huggingface.co/BAAI/bge-multilingual-gemma2",
    "https://github.com/FlagOpen/FlagEmbedding\">",
    "https://huggingface.co/BAAI/bge-small-zh",
    "https://huggingface.co/BAAI/bge-small-en",
    "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE",
    "https://data.baai.ac.cn/details/BAAI-MTP",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker",
    "https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise",
    "https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92",
    "https://huggingface.co/BAAI/bge-large-en",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md",
    "https://huggingface.co/BAAI/bge-large-zh",
    "https://huggingface.co/BAAI/bge-base-zh-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora",
    "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA"
  ]
}
```

</details>


---

## Repository 16: FlagOpen/FlagEmbedding

# GitHub Repository Data

**Repository:** [FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## Basic Information

- **Description:** Retrieval and Retrieval-augmented LLMs
- **Created:** 2023-08-02T02:08:11+00:00
- **Last Updated:** 2025-06-21T10:44:22+00:00
- **Last Pushed:** 2025-06-04T10:00:26+00:00
- **Default Branch:** master
- **Size:** 52310 KB

## Statistics

- **Stars:** 9,974
- **Forks:** 737
- **Watchers:** 9,974
- **Open Issues:** 847
- **Total Issues:** 0
- **Pull Requests:** 217

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE)

## Languages

- **Python:** 3,871,341 bytes
- **Jupyter Notebook:** 2,607,506 bytes
- **Shell:** 7,322 bytes

## Topics

- `embeddings`
- `information-retrieval`
- `llm`
- `sentence-embeddings`
- `text-semantic-similarity`
- `retrieval-augmented-generation`

## Top Contributors

1. **hanhainebula** - 350 contributions
2. **545999961** - 327 contributions
3. **staoxiao** - 182 contributions
4. **ZiyiXia** - 113 contributions
5. **namespace-Pt** - 101 contributions
6. **JUNJIE99** - 78 contributions
7. **ftgreat** - 20 contributions
8. **ZhengLiu101** - 15 contributions
9. **Muennighoff** - 12 contributions
10. **shuyansy** - 10 contributions

## File Structure (Sample of 10 files)

Total files: 1,395

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.gitignore` (blob)
- `FlagEmbedding` (tree)
- `FlagEmbedding/__init__.py` (blob)
- `FlagEmbedding/abc` (tree)
- `FlagEmbedding/abc/__init__.py` (blob)
- `FlagEmbedding/abc/evaluation` (tree)
- `FlagEmbedding/abc/evaluation/__init__.py` (blob)

## Recent Issues

- 🟢 **#1480** Continued pretraining using RetroMAE (open)
- 🟢 **#1479** 微调bge-reranker-base一直卡顿，无日志输出 (open)
- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1477** 训练过程中train_loss和eval_loss都是先下降后上升再缓慢下降 (open)
- 🟢 **#1476** How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline (open)

## Recent Pull Requests

- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1475** Add inline comment to setup.py (open)
- 🟢 **#1469** Extend tutorials to include cloud deployment options (open)
- 🔴 **#1466** update reranker inference (closed)
- 🔴 **#1465** update environment (closed)

## Recent Commits

- **875fd4ff** update docs - ZiyiXia (2025-06-04T09:57:10+00:00)
- **235f967a** update tutorials - ZiyiXia (2025-06-04T09:27:36+00:00)
- **5e64baa6** Merge pull request #1466 from 545999961/master - chaofan (2025-05-28T08:15:17+00:00)
- **018b984e** update reranker inference - cfli (2025-05-28T08:14:32+00:00)
- **d04f7989** release version 1.3.5 - chaofan (2025-05-28T07:12:19+00:00)
- **8576fde7** Merge pull request #1465 from 545999961/master - chaofan (2025-05-28T06:58:53+00:00)
- **bf410d51** update environment - cfli (2025-05-28T06:55:54+00:00)
- **97ca0732** Merge pull request #1464 from 545999961/master - chaofan (2025-05-28T06:18:23+00:00)
- **b51706d6** upload coder eval script - cfli (2025-05-28T06:17:19+00:00)
- **821c3879** upload coder eval script - cfli (2025-05-28T06:12:16+00:00)

## External Links Found in README

- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation
- https://arxiv.org/pdf/2310.07554.pdf
- https://huggingface.co/BAAI/bge-reranker-base
- https://arxiv.org/pdf/2409.05591v1
- https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset
- https://img.shields.io/badge/BGE_series-🤗-yellow">
- https://huggingface.co/Shitao
- https://huggingface.co/BAAI/bge-base-en
- https://huggingface.co/BAAI/llm-embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder
- https://huggingface.co/BAAI/bge-reranker-large
- https://img.shields.io/badge/Contribution-Welcome-blue">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder
- https://huggingface.co/BAAI/bge-small-en-v1.5
- https://huggingface.co/BAAI/bge-m3
- https://github.com/VectorSpaceLab/OmniGen
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge
- https://github.com/VectorSpaceLab/MegaPairs

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673596675,
  "name": "FlagEmbedding",
  "full_name": "FlagOpen/FlagEmbedding",
  "description": "Retrieval and Retrieval-augmented LLMs",
  "html_url": "https://github.com/FlagOpen/FlagEmbedding",
  "clone_url": "https://github.com/FlagOpen/FlagEmbedding.git",
  "ssh_url": "git@github.com:FlagOpen/FlagEmbedding.git",
  "homepage": "http://www.bge-model.com/",
  "topics": [
    "embeddings",
    "information-retrieval",
    "llm",
    "sentence-embeddings",
    "text-semantic-similarity",
    "retrieval-augmented-generation"
  ],
  "default_branch": "master",
  "created_at": "2023-08-02T02:08:11+00:00",
  "updated_at": "2025-06-21T10:44:22+00:00",
  "pushed_at": "2025-06-04T10:00:26+00:00",
  "size_kb": 52310,
  "watchers_count": 9974,
  "stargazers_count": 9974,
  "forks_count": 737,
  "open_issues_count": 847,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"
  },
  "languages": {
    "Python": 3871341,
    "Jupyter Notebook": 2607506,
    "Shell": 7322
  },
  "top_contributors": [
    {
      "login": "hanhainebula",
      "contributions": 350
    },
    {
      "login": "545999961",
      "contributions": 327
    },
    {
      "login": "staoxiao",
      "contributions": 182
    },
    {
      "login": "ZiyiXia",
      "contributions": 113
    },
    {
      "login": "namespace-Pt",
      "contributions": 101
    },
    {
      "login": "JUNJIE99",
      "contributions": 78
    },
    {
      "login": "ftgreat",
      "contributions": 20
    },
    {
      "login": "ZhengLiu101",
      "contributions": 15
    },
    {
      "login": "Muennighoff",
      "contributions": 12
    },
    {
      "login": "shuyansy",
      "contributions": 10
    },
    {
      "login": "rainym00d",
      "contributions": 3
    },
    {
      "login": "chinainfant",
      "contributions": 3
    },
    {
      "login": "GeoloeG-IsT",
      "contributions": 3
    },
    {
      "login": "blue-vision0",
      "contributions": 2
    },
    {
      "login": "muazhari",
      "contributions": 2
    },
    {
      "login": "luisegarduno",
      "contributions": 2
    },
    {
      "login": "khazic",
      "contributions": 2
    },
    {
      "login": "shtdbb",
      "contributions": 2
    },
    {
      "login": "pengjunfeng11",
      "contributions": 2
    },
    {
      "login": "ycjcl868",
      "contributions": 2
    }
  ],
  "file_tree_count": 1395,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc/evaluation",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/evaluation/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 217,
  "recent_issues": [
    {
      "number": 1480,
      "title": "Continued pretraining using RetroMAE",
      "state": "open"
    },
    {
      "number": 1479,
      "title": "\u5fae\u8c03bge-reranker-base\u4e00\u76f4\u5361\u987f\uff0c\u65e0\u65e5\u5fd7\u8f93\u51fa",
      "state": "open"
    },
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1477,
      "title": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2dtrain_loss\u548ceval_loss\u90fd\u662f\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u518d\u7f13\u6162\u4e0b\u964d",
      "state": "open"
    },
    {
      "number": 1476,
      "title": "How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1475,
      "title": "Add inline comment to setup.py",
      "state": "open"
    },
    {
      "number": 1469,
      "title": "Extend tutorials to include cloud deployment options",
      "state": "open"
    },
    {
      "number": 1466,
      "title": "update reranker inference",
      "state": "closed"
    },
    {
      "number": 1465,
      "title": "update environment",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "875fd4ffcb894313a918e8bb0c9b7ead82f37e64",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:57:10+00:00",
      "message": "update docs"
    },
    {
      "sha": "235f967a0122b9d5b85104fd4e8f36dc15ee85a8",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:27:36+00:00",
      "message": "update tutorials"
    },
    {
      "sha": "5e64baa61e75df23105a66d1e9d09ad799366e2a",
      "author": "chaofan",
      "date": "2025-05-28T08:15:17+00:00",
      "message": "Merge pull request #1466 from 545999961/master"
    },
    {
      "sha": "018b984e23e12d9793e59f2a1ef07fb9153ab08e",
      "author": "cfli",
      "date": "2025-05-28T08:14:32+00:00",
      "message": "update reranker inference"
    },
    {
      "sha": "d04f7989eeec2c6db4ead7c77ddef656f9c9d3aa",
      "author": "chaofan",
      "date": "2025-05-28T07:12:19+00:00",
      "message": "release version 1.3.5"
    },
    {
      "sha": "8576fde79b84224ecbe493857115552fcfb72ed7",
      "author": "chaofan",
      "date": "2025-05-28T06:58:53+00:00",
      "message": "Merge pull request #1465 from 545999961/master"
    },
    {
      "sha": "bf410d51e9e685e7f1b2e3816f09f3dc64ad8292",
      "author": "cfli",
      "date": "2025-05-28T06:55:54+00:00",
      "message": "update environment"
    },
    {
      "sha": "97ca07325d9e786d668f9a038502ad736dded9d5",
      "author": "chaofan",
      "date": "2025-05-28T06:18:23+00:00",
      "message": "Merge pull request #1464 from 545999961/master"
    },
    {
      "sha": "b51706d63ae8a1d86443df8b0e5ff6e45eb08fdc",
      "author": "cfli",
      "date": "2025-05-28T06:17:19+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "821c387961b5597fe672a4386232110ef54d5107",
      "author": "cfli",
      "date": "2025-05-28T06:12:16+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "3332a1956e2d5af7c52628a60e5c9788bd1b5ed9",
      "author": "chaofan",
      "date": "2025-05-22T10:07:40+00:00",
      "message": "Merge pull request #1459 from 545999961/master"
    },
    {
      "sha": "d412884a4e96c45e4a9e83a43e507e9beaa04713",
      "author": "cfli",
      "date": "2025-05-22T10:06:43+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "5c11feffc104497953a333036aa56b6b543e7cee",
      "author": "chaofan",
      "date": "2025-05-22T09:56:48+00:00",
      "message": "Merge pull request #1458 from 545999961/master"
    },
    {
      "sha": "41b526441f13242b49126261c219b6b9f14e17cb",
      "author": "cfli",
      "date": "2025-05-22T09:55:44+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "52bb9fddb5def2074b277f6e9a2f02ed8aa17dee",
      "author": "chaofan",
      "date": "2025-05-22T09:52:21+00:00",
      "message": "Merge pull request #1457 from 545999961/master"
    },
    {
      "sha": "f05d4b3fbeff5df9dabeb71ab16225fe5e1e8c95",
      "author": "cfli",
      "date": "2025-05-22T09:51:20+00:00",
      "message": "upload reinforced_ir"
    },
    {
      "sha": "25ee99c228e9df63a2be79e8d795d549ab5001e9",
      "author": "chaofan",
      "date": "2025-05-21T04:46:03+00:00",
      "message": "Merge pull request #1453 from 545999961/master"
    },
    {
      "sha": "436210bbed3ca352aac82a9be66cd4e665408944",
      "author": "cfli",
      "date": "2025-05-21T04:45:10+00:00",
      "message": "upload Matroyshka_reranker"
    },
    {
      "sha": "1946f89d5a6926871861c9ed9f4acbb31665acfe",
      "author": "cfli",
      "date": "2025-05-20T08:13:30+00:00",
      "message": "upload Matroyshka Re-Ranker"
    },
    {
      "sha": "f6924351bba09620938b69a2c933c6712be829c4",
      "author": "cfli",
      "date": "2025-05-20T08:12:08+00:00",
      "message": "upload Matroyshka Re-Ranker"
    }
  ],
  "readme_text": "[<img src=\"./imgs/FlagOpen.png\">](https://flagopen.baai.ac.cn/)\n\n<h1 align=\"center\">\u26a1\ufe0fBGE: One-Stop Retrieval Toolkit For Search and RAG</h1>\n\n![bge_logo](./imgs/bge_logo.jpg)\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Contribution-Welcome-blue\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\">\n    </a>\n    <a href=\"https://huggingface.co/C-MTEB\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=#news>News</a> |\n        <a href=#installation>Installation</a> |\n        <a href=#quick-start>Quick Start</a> |\n        <a href=#community>Community</a> |\n        <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a> |\n        <a href=#model-list>Model List</a> |\n        <a href=\"#contributors\">Contributor</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\n\n## News\n\n- 3/6/2025: :fire::fire: Introduce **BGE-VL** ([HF repo](https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92)), State-Of-The-Art multimodal embedding models to support Any visual search applications (everything, including text-to-image, image-to-text, image&prompt-to-image, text-to-image&text, and more)! They are released under the MIT license and are completely free for both academic and commercial use. We also release **MegaPairs** ([repo](https://github.com/VectorSpaceLab/MegaPairs), [paper](https://arxiv.org/abs/2412.14475)), a massive synthetic dataset which empowers BGE-VL!\n- 12/5/2024: :book: We built the [BGE documentation](https://www.bge-model.com) for centralized BGE information and materials!\n- 10/29/2024: :earth_asia: We created WeChat group for BGE. Scan the [QR code](./imgs/BGE_WeChat_Group.png) to join the group chat! To get the first hand message about our updates and new release, or having any questions or ideas, join us now!\n- <img src=\"./imgs/BGE_WeChat_Group.png\" alt=\"bge_wechat_group\" class=\"center\" width=\"200\">\n\n- 10/22/2024: We release another interesting model: [OmniGen](https://github.com/VectorSpaceLab/OmniGen), which is a unified image generation model supporting various tasks. OmniGen can accomplish complex image generation tasks without the need for additional plugins like ControlNet, IP-Adapter, or auxiliary models such as pose detection and face detection.\n- 9/10/2024: Introducing **MemoRAG**, a step forward towards RAG 2.0 on top of memory-inspired knowledge discovery (repo: https://github.com/qhjqhj00/MemoRAG, paper: https://arxiv.org/pdf/2409.05591v1) \n- 9/2/2024: Start to maintain the [tutorials](./Tutorials/). The contents within will be actively updated and eariched, stay tuned! :books:\n- 7/26/2024: Release a new embedding model [bge-en-icl](https://huggingface.co/BAAI/bge-en-icl), an embedding model that incorporates in-context learning capabilities, which, by providing task-relevant query-response examples, can encode semantically richer queries, further enhancing the semantic representation ability of the embeddings.\n- 7/26/2024: Release a new embedding model [bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2), a multilingual embedding model based on gemma-2-9b, which supports multiple languages and diverse downstream tasks, achieving new SOTA on multilingual benchmarks (MIRACL, MTEB-fr, and MTEB-pl). \n- 7/26/2024: Release a new lightweight reranker [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight), a lightweight reranker based on gemma-2-9b, which supports token compression and layerwise lightweight operations, can still ensure good performance while saving a significant amount of resources. :fire:\n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n\n- 6/7/2024: Release a new benchmark [MLVU](https://github.com/JUNJIE99/MLVU), the first comprehensive benchmark specifically designed for long video understanding. MLVU features an extensive range of video durations, a diverse collection of video sources, and a set of evaluation tasks uniquely tailored for long-form video understanding. :fire:\n- 5/21/2024: Release a new benchmark [AIR-Bench](https://github.com/AIR-Bench/AIR-Bench) together with Jina AI, Zilliz, HuggingFace, and other partners. AIR-Bench focuses on a fair out-of-distribution evaluation for Neural IR & RAG. It generates the synthetic data for benchmarking w.r.t. diverse domains and languages. It is dynamic and will be updated on regular basis. [Leaderboard](https://huggingface.co/spaces/AIR-Bench/leaderboard) :fire:\n- 4/30/2024: Release [Llama-3-8B-Instruct-80K-QLoRA](https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA), extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA training on a few synthesized long-context data. The model achieves remarkable performance on various long-context benchmarks. [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora) :fire:\n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually :smiley:) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation :fire:\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data. :fire:\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://arxiv.org/pdf/2402.03216.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) \n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n\n</details>\n\n\nBGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n![projects](./imgs/projects.png)\n\n- **Inference**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker)\n- **Finetune**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)\n- **[Evaluation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation)**\n- **[Dataset](https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset)**\n- **[Tutorials](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)**\n- **[research](https://github.com/FlagOpen/FlagEmbedding/tree/master/research)**\n\n\n## Installation\n### Using pip:\nIf you do not want to finetune the models, you can install the package without the finetune dependency:\n```\npip install -U FlagEmbedding\n```\nIf you want to finetune the models, you can install the package with the finetune dependency:\n```\npip install -U FlagEmbedding[finetune]\n```\n### Install from sources:\n\nClone the repository and install\n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install  .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install  .[finetune]\n```\nFor development in editable mode:\n```\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install -e .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install -e .[finetune]\n```\n\n## Quick Start\nFirst, load one of the BGE embedding model:\n```\nfrom FlagEmbedding import FlagAutoModel\n\nmodel = FlagAutoModel.from_finetuned('BAAI/bge-base-en-v1.5',\n                                      query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n                                      use_fp16=True)\n```\nThen, feed some sentences to the model and get their embeddings:\n```\nsentences_1 = [\"I love NLP\", \"I love machine learning\"]\nsentences_2 = [\"I love BGE\", \"I love text retrieval\"]\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\n```\nOnce we get the embeddings, we can compute similarity by inner product:\n```\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\n\nFor more details, you can refer to [embedder inference](./examples/inference/embedder), [reranker inference](./examples/inference/reranker), [embedder finetune](./examples/finetune/embedder), [reranker fintune](./examples/finetune/reranker), [evaluation](./examples/evaluation).\n\nIf you're unfamiliar with any of related concepts, please check out the [tutorial](./Tutorials/). If it's not there, let us know.\n\nFor more interesting topics related to BGE, take a look at [research](./research).\n\n## Community\n\nWe are actively maintaining the community of BGE and FlagEmbedding. Let us know if you have any suggessions or ideas!\n\nCurrently we are updating the [tutorials](./Tutorials/), we aim to create a comprehensive and detailed tutorial for beginners on text retrieval and RAG. Stay tuned!\n\nThe following contents are releasing in the upcoming weeks:\n\n- Evaluation\n- BGE-EN-ICL\n\n<details>\n  <summary>The whole tutorial roadmap</summary>\n    <img src=\"./Tutorials/tutorial_map.png\"/>\n</details>\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n| Model                                                                     | Language |                                                             Description                                                             |                                query instruction for retrieval                                 |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Provide instructions and few-shot examples freely based on the given task. |\n| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |    Multilingual     | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |        Provide instructions based on the given task.         |\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n| [LM-Cocktail](https://huggingface.co/Shitao)                   |   English |                     fine-tuned models (Llama and BGE) which can be used to reproduce the results of LM-Cocktail                     |  |\n| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English |                         a unified embedding model to support diverse retrieval augmentation needs for LLMs                          | See [README](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder) |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | Multilingual | a lightweight cross-encoder model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |  |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |  |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers, compress ratio and compress layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)             |   English |                                             Embedding Model which map text into vector                                              |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)               |   English |                                    a base-scale model but with similar ability to `bge-large-en`                                    |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)             |   English |                                        a small-scale model but with competitive performance                                         |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)             |   Chinese |                                             Embedding Model which map text into vector                                              |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)               |   Chinese |                                    a base-scale model but with similar ability to `bge-large-zh`                                    |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)             |   Chinese |                                        a small-scale model but with competitive performance                                         |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n\n\n\n\n### Contributors:\nThank all our contributors for their efforts and warmly welcome new members to join in!\n\n<a href=\"https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\" />\n</a>\n\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_m3,\n  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},\n  year={2023},\n  eprint={2309.07597},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n@misc{cocktail,\n      title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Xingrun Xing},\n      year={2023},\n      eprint={2311.13534},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{llm_embedder,\n      title={Retrieve Anything To Augment Large Language Models}, \n      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\n      year={2023},\n      eprint={2310.07554},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). \n\n",
  "external_links_in_readme": [
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation",
    "https://arxiv.org/pdf/2310.07554.pdf",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://arxiv.org/pdf/2409.05591v1",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset",
    "https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">",
    "https://huggingface.co/Shitao",
    "https://huggingface.co/BAAI/bge-base-en",
    "https://huggingface.co/BAAI/llm-embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://img.shields.io/badge/Contribution-Welcome-blue\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://huggingface.co/BAAI/bge-m3",
    "https://github.com/VectorSpaceLab/OmniGen",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge",
    "https://github.com/VectorSpaceLab/MegaPairs",
    "https://github.com/FlagOpen/FlagEmbedding.git",
    "https://huggingface.co/spaces/AIR-Bench/leaderboard",
    "https://github.com/AIR-Bench/AIR-Bench",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker",
    "https://github.com/JUNJIE99/MLVU",
    "https://github.com/qhjqhj00/MemoRAG,",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail",
    "https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research",
    "https://huggingface.co/BAAI/bge-small-zh-v1.5",
    "https://arxiv.org/abs/2412.14475",
    "https://img.shields.io/badge/LICENSE-MIT-green\">",
    "https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">",
    "https://huggingface.co/BAAI/bge-reranker-v2-gemma",
    "https://huggingface.co/spaces/mteb/leaderboard",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker",
    "https://arxiv.org/pdf/2402.03216.pdf",
    "https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\"",
    "https://huggingface.co/BAAI/bge-base-zh",
    "https://arxiv.org/abs/2311.13534",
    "https://huggingface.co/C-MTEB\">",
    "https://flagopen.baai.ac.cn/",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding",
    "https://arxiv.org/abs/2401.03462",
    "https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">",
    "https://arxiv.org/pdf/2309.07597.pdf",
    "https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a>",
    "https://huggingface.co/BAAI/bge-en-icl",
    "https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight",
    "https://www.bge-model.com",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://arxiv.org/abs/2312.15503",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials",
    "https://huggingface.co/BAAI/bge-multilingual-gemma2",
    "https://github.com/FlagOpen/FlagEmbedding\">",
    "https://huggingface.co/BAAI/bge-small-zh",
    "https://huggingface.co/BAAI/bge-small-en",
    "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE",
    "https://data.baai.ac.cn/details/BAAI-MTP",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker",
    "https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise",
    "https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92",
    "https://huggingface.co/BAAI/bge-large-en",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md",
    "https://huggingface.co/BAAI/bge-large-zh",
    "https://huggingface.co/BAAI/bge-base-zh-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora",
    "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA"
  ]
}
```

</details>


---

## Repository 17: FlagOpen/FlagEmbedding

# GitHub Repository Data

**Repository:** [FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## Basic Information

- **Description:** Retrieval and Retrieval-augmented LLMs
- **Created:** 2023-08-02T02:08:11+00:00
- **Last Updated:** 2025-06-21T10:44:22+00:00
- **Last Pushed:** 2025-06-04T10:00:26+00:00
- **Default Branch:** master
- **Size:** 52310 KB

## Statistics

- **Stars:** 9,974
- **Forks:** 737
- **Watchers:** 9,974
- **Open Issues:** 847
- **Total Issues:** 0
- **Pull Requests:** 217

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE)

## Languages

- **Python:** 3,871,341 bytes
- **Jupyter Notebook:** 2,607,506 bytes
- **Shell:** 7,322 bytes

## Topics

- `embeddings`
- `information-retrieval`
- `llm`
- `sentence-embeddings`
- `text-semantic-similarity`
- `retrieval-augmented-generation`

## Top Contributors

1. **hanhainebula** - 350 contributions
2. **545999961** - 327 contributions
3. **staoxiao** - 182 contributions
4. **ZiyiXia** - 113 contributions
5. **namespace-Pt** - 101 contributions
6. **JUNJIE99** - 78 contributions
7. **ftgreat** - 20 contributions
8. **ZhengLiu101** - 15 contributions
9. **Muennighoff** - 12 contributions
10. **shuyansy** - 10 contributions

## File Structure (Sample of 10 files)

Total files: 1,395

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.gitignore` (blob)
- `FlagEmbedding` (tree)
- `FlagEmbedding/__init__.py` (blob)
- `FlagEmbedding/abc` (tree)
- `FlagEmbedding/abc/__init__.py` (blob)
- `FlagEmbedding/abc/evaluation` (tree)
- `FlagEmbedding/abc/evaluation/__init__.py` (blob)

## Recent Issues

- 🟢 **#1480** Continued pretraining using RetroMAE (open)
- 🟢 **#1479** 微调bge-reranker-base一直卡顿，无日志输出 (open)
- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1477** 训练过程中train_loss和eval_loss都是先下降后上升再缓慢下降 (open)
- 🟢 **#1476** How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline (open)

## Recent Pull Requests

- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1475** Add inline comment to setup.py (open)
- 🟢 **#1469** Extend tutorials to include cloud deployment options (open)
- 🔴 **#1466** update reranker inference (closed)
- 🔴 **#1465** update environment (closed)

## Recent Commits

- **875fd4ff** update docs - ZiyiXia (2025-06-04T09:57:10+00:00)
- **235f967a** update tutorials - ZiyiXia (2025-06-04T09:27:36+00:00)
- **5e64baa6** Merge pull request #1466 from 545999961/master - chaofan (2025-05-28T08:15:17+00:00)
- **018b984e** update reranker inference - cfli (2025-05-28T08:14:32+00:00)
- **d04f7989** release version 1.3.5 - chaofan (2025-05-28T07:12:19+00:00)
- **8576fde7** Merge pull request #1465 from 545999961/master - chaofan (2025-05-28T06:58:53+00:00)
- **bf410d51** update environment - cfli (2025-05-28T06:55:54+00:00)
- **97ca0732** Merge pull request #1464 from 545999961/master - chaofan (2025-05-28T06:18:23+00:00)
- **b51706d6** upload coder eval script - cfli (2025-05-28T06:17:19+00:00)
- **821c3879** upload coder eval script - cfli (2025-05-28T06:12:16+00:00)

## External Links Found in README

- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation
- https://arxiv.org/pdf/2310.07554.pdf
- https://huggingface.co/BAAI/bge-reranker-base
- https://arxiv.org/pdf/2409.05591v1
- https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset
- https://img.shields.io/badge/BGE_series-🤗-yellow">
- https://huggingface.co/Shitao
- https://huggingface.co/BAAI/bge-base-en
- https://huggingface.co/BAAI/llm-embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder
- https://huggingface.co/BAAI/bge-reranker-large
- https://img.shields.io/badge/Contribution-Welcome-blue">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder
- https://huggingface.co/BAAI/bge-small-en-v1.5
- https://huggingface.co/BAAI/bge-m3
- https://github.com/VectorSpaceLab/OmniGen
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge
- https://github.com/VectorSpaceLab/MegaPairs

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673596675,
  "name": "FlagEmbedding",
  "full_name": "FlagOpen/FlagEmbedding",
  "description": "Retrieval and Retrieval-augmented LLMs",
  "html_url": "https://github.com/FlagOpen/FlagEmbedding",
  "clone_url": "https://github.com/FlagOpen/FlagEmbedding.git",
  "ssh_url": "git@github.com:FlagOpen/FlagEmbedding.git",
  "homepage": "http://www.bge-model.com/",
  "topics": [
    "embeddings",
    "information-retrieval",
    "llm",
    "sentence-embeddings",
    "text-semantic-similarity",
    "retrieval-augmented-generation"
  ],
  "default_branch": "master",
  "created_at": "2023-08-02T02:08:11+00:00",
  "updated_at": "2025-06-21T10:44:22+00:00",
  "pushed_at": "2025-06-04T10:00:26+00:00",
  "size_kb": 52310,
  "watchers_count": 9974,
  "stargazers_count": 9974,
  "forks_count": 737,
  "open_issues_count": 847,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"
  },
  "languages": {
    "Python": 3871341,
    "Jupyter Notebook": 2607506,
    "Shell": 7322
  },
  "top_contributors": [
    {
      "login": "hanhainebula",
      "contributions": 350
    },
    {
      "login": "545999961",
      "contributions": 327
    },
    {
      "login": "staoxiao",
      "contributions": 182
    },
    {
      "login": "ZiyiXia",
      "contributions": 113
    },
    {
      "login": "namespace-Pt",
      "contributions": 101
    },
    {
      "login": "JUNJIE99",
      "contributions": 78
    },
    {
      "login": "ftgreat",
      "contributions": 20
    },
    {
      "login": "ZhengLiu101",
      "contributions": 15
    },
    {
      "login": "Muennighoff",
      "contributions": 12
    },
    {
      "login": "shuyansy",
      "contributions": 10
    },
    {
      "login": "rainym00d",
      "contributions": 3
    },
    {
      "login": "chinainfant",
      "contributions": 3
    },
    {
      "login": "GeoloeG-IsT",
      "contributions": 3
    },
    {
      "login": "blue-vision0",
      "contributions": 2
    },
    {
      "login": "muazhari",
      "contributions": 2
    },
    {
      "login": "luisegarduno",
      "contributions": 2
    },
    {
      "login": "khazic",
      "contributions": 2
    },
    {
      "login": "shtdbb",
      "contributions": 2
    },
    {
      "login": "pengjunfeng11",
      "contributions": 2
    },
    {
      "login": "ycjcl868",
      "contributions": 2
    }
  ],
  "file_tree_count": 1395,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc/evaluation",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/evaluation/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 217,
  "recent_issues": [
    {
      "number": 1480,
      "title": "Continued pretraining using RetroMAE",
      "state": "open"
    },
    {
      "number": 1479,
      "title": "\u5fae\u8c03bge-reranker-base\u4e00\u76f4\u5361\u987f\uff0c\u65e0\u65e5\u5fd7\u8f93\u51fa",
      "state": "open"
    },
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1477,
      "title": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2dtrain_loss\u548ceval_loss\u90fd\u662f\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u518d\u7f13\u6162\u4e0b\u964d",
      "state": "open"
    },
    {
      "number": 1476,
      "title": "How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1475,
      "title": "Add inline comment to setup.py",
      "state": "open"
    },
    {
      "number": 1469,
      "title": "Extend tutorials to include cloud deployment options",
      "state": "open"
    },
    {
      "number": 1466,
      "title": "update reranker inference",
      "state": "closed"
    },
    {
      "number": 1465,
      "title": "update environment",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "875fd4ffcb894313a918e8bb0c9b7ead82f37e64",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:57:10+00:00",
      "message": "update docs"
    },
    {
      "sha": "235f967a0122b9d5b85104fd4e8f36dc15ee85a8",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:27:36+00:00",
      "message": "update tutorials"
    },
    {
      "sha": "5e64baa61e75df23105a66d1e9d09ad799366e2a",
      "author": "chaofan",
      "date": "2025-05-28T08:15:17+00:00",
      "message": "Merge pull request #1466 from 545999961/master"
    },
    {
      "sha": "018b984e23e12d9793e59f2a1ef07fb9153ab08e",
      "author": "cfli",
      "date": "2025-05-28T08:14:32+00:00",
      "message": "update reranker inference"
    },
    {
      "sha": "d04f7989eeec2c6db4ead7c77ddef656f9c9d3aa",
      "author": "chaofan",
      "date": "2025-05-28T07:12:19+00:00",
      "message": "release version 1.3.5"
    },
    {
      "sha": "8576fde79b84224ecbe493857115552fcfb72ed7",
      "author": "chaofan",
      "date": "2025-05-28T06:58:53+00:00",
      "message": "Merge pull request #1465 from 545999961/master"
    },
    {
      "sha": "bf410d51e9e685e7f1b2e3816f09f3dc64ad8292",
      "author": "cfli",
      "date": "2025-05-28T06:55:54+00:00",
      "message": "update environment"
    },
    {
      "sha": "97ca07325d9e786d668f9a038502ad736dded9d5",
      "author": "chaofan",
      "date": "2025-05-28T06:18:23+00:00",
      "message": "Merge pull request #1464 from 545999961/master"
    },
    {
      "sha": "b51706d63ae8a1d86443df8b0e5ff6e45eb08fdc",
      "author": "cfli",
      "date": "2025-05-28T06:17:19+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "821c387961b5597fe672a4386232110ef54d5107",
      "author": "cfli",
      "date": "2025-05-28T06:12:16+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "3332a1956e2d5af7c52628a60e5c9788bd1b5ed9",
      "author": "chaofan",
      "date": "2025-05-22T10:07:40+00:00",
      "message": "Merge pull request #1459 from 545999961/master"
    },
    {
      "sha": "d412884a4e96c45e4a9e83a43e507e9beaa04713",
      "author": "cfli",
      "date": "2025-05-22T10:06:43+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "5c11feffc104497953a333036aa56b6b543e7cee",
      "author": "chaofan",
      "date": "2025-05-22T09:56:48+00:00",
      "message": "Merge pull request #1458 from 545999961/master"
    },
    {
      "sha": "41b526441f13242b49126261c219b6b9f14e17cb",
      "author": "cfli",
      "date": "2025-05-22T09:55:44+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "52bb9fddb5def2074b277f6e9a2f02ed8aa17dee",
      "author": "chaofan",
      "date": "2025-05-22T09:52:21+00:00",
      "message": "Merge pull request #1457 from 545999961/master"
    },
    {
      "sha": "f05d4b3fbeff5df9dabeb71ab16225fe5e1e8c95",
      "author": "cfli",
      "date": "2025-05-22T09:51:20+00:00",
      "message": "upload reinforced_ir"
    },
    {
      "sha": "25ee99c228e9df63a2be79e8d795d549ab5001e9",
      "author": "chaofan",
      "date": "2025-05-21T04:46:03+00:00",
      "message": "Merge pull request #1453 from 545999961/master"
    },
    {
      "sha": "436210bbed3ca352aac82a9be66cd4e665408944",
      "author": "cfli",
      "date": "2025-05-21T04:45:10+00:00",
      "message": "upload Matroyshka_reranker"
    },
    {
      "sha": "1946f89d5a6926871861c9ed9f4acbb31665acfe",
      "author": "cfli",
      "date": "2025-05-20T08:13:30+00:00",
      "message": "upload Matroyshka Re-Ranker"
    },
    {
      "sha": "f6924351bba09620938b69a2c933c6712be829c4",
      "author": "cfli",
      "date": "2025-05-20T08:12:08+00:00",
      "message": "upload Matroyshka Re-Ranker"
    }
  ],
  "readme_text": "[<img src=\"./imgs/FlagOpen.png\">](https://flagopen.baai.ac.cn/)\n\n<h1 align=\"center\">\u26a1\ufe0fBGE: One-Stop Retrieval Toolkit For Search and RAG</h1>\n\n![bge_logo](./imgs/bge_logo.jpg)\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Contribution-Welcome-blue\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\">\n    </a>\n    <a href=\"https://huggingface.co/C-MTEB\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=#news>News</a> |\n        <a href=#installation>Installation</a> |\n        <a href=#quick-start>Quick Start</a> |\n        <a href=#community>Community</a> |\n        <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a> |\n        <a href=#model-list>Model List</a> |\n        <a href=\"#contributors\">Contributor</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\n\n## News\n\n- 3/6/2025: :fire::fire: Introduce **BGE-VL** ([HF repo](https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92)), State-Of-The-Art multimodal embedding models to support Any visual search applications (everything, including text-to-image, image-to-text, image&prompt-to-image, text-to-image&text, and more)! They are released under the MIT license and are completely free for both academic and commercial use. We also release **MegaPairs** ([repo](https://github.com/VectorSpaceLab/MegaPairs), [paper](https://arxiv.org/abs/2412.14475)), a massive synthetic dataset which empowers BGE-VL!\n- 12/5/2024: :book: We built the [BGE documentation](https://www.bge-model.com) for centralized BGE information and materials!\n- 10/29/2024: :earth_asia: We created WeChat group for BGE. Scan the [QR code](./imgs/BGE_WeChat_Group.png) to join the group chat! To get the first hand message about our updates and new release, or having any questions or ideas, join us now!\n- <img src=\"./imgs/BGE_WeChat_Group.png\" alt=\"bge_wechat_group\" class=\"center\" width=\"200\">\n\n- 10/22/2024: We release another interesting model: [OmniGen](https://github.com/VectorSpaceLab/OmniGen), which is a unified image generation model supporting various tasks. OmniGen can accomplish complex image generation tasks without the need for additional plugins like ControlNet, IP-Adapter, or auxiliary models such as pose detection and face detection.\n- 9/10/2024: Introducing **MemoRAG**, a step forward towards RAG 2.0 on top of memory-inspired knowledge discovery (repo: https://github.com/qhjqhj00/MemoRAG, paper: https://arxiv.org/pdf/2409.05591v1) \n- 9/2/2024: Start to maintain the [tutorials](./Tutorials/). The contents within will be actively updated and eariched, stay tuned! :books:\n- 7/26/2024: Release a new embedding model [bge-en-icl](https://huggingface.co/BAAI/bge-en-icl), an embedding model that incorporates in-context learning capabilities, which, by providing task-relevant query-response examples, can encode semantically richer queries, further enhancing the semantic representation ability of the embeddings.\n- 7/26/2024: Release a new embedding model [bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2), a multilingual embedding model based on gemma-2-9b, which supports multiple languages and diverse downstream tasks, achieving new SOTA on multilingual benchmarks (MIRACL, MTEB-fr, and MTEB-pl). \n- 7/26/2024: Release a new lightweight reranker [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight), a lightweight reranker based on gemma-2-9b, which supports token compression and layerwise lightweight operations, can still ensure good performance while saving a significant amount of resources. :fire:\n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n\n- 6/7/2024: Release a new benchmark [MLVU](https://github.com/JUNJIE99/MLVU), the first comprehensive benchmark specifically designed for long video understanding. MLVU features an extensive range of video durations, a diverse collection of video sources, and a set of evaluation tasks uniquely tailored for long-form video understanding. :fire:\n- 5/21/2024: Release a new benchmark [AIR-Bench](https://github.com/AIR-Bench/AIR-Bench) together with Jina AI, Zilliz, HuggingFace, and other partners. AIR-Bench focuses on a fair out-of-distribution evaluation for Neural IR & RAG. It generates the synthetic data for benchmarking w.r.t. diverse domains and languages. It is dynamic and will be updated on regular basis. [Leaderboard](https://huggingface.co/spaces/AIR-Bench/leaderboard) :fire:\n- 4/30/2024: Release [Llama-3-8B-Instruct-80K-QLoRA](https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA), extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA training on a few synthesized long-context data. The model achieves remarkable performance on various long-context benchmarks. [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora) :fire:\n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually :smiley:) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation :fire:\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data. :fire:\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://arxiv.org/pdf/2402.03216.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) \n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n\n</details>\n\n\nBGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n![projects](./imgs/projects.png)\n\n- **Inference**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker)\n- **Finetune**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)\n- **[Evaluation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation)**\n- **[Dataset](https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset)**\n- **[Tutorials](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)**\n- **[research](https://github.com/FlagOpen/FlagEmbedding/tree/master/research)**\n\n\n## Installation\n### Using pip:\nIf you do not want to finetune the models, you can install the package without the finetune dependency:\n```\npip install -U FlagEmbedding\n```\nIf you want to finetune the models, you can install the package with the finetune dependency:\n```\npip install -U FlagEmbedding[finetune]\n```\n### Install from sources:\n\nClone the repository and install\n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install  .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install  .[finetune]\n```\nFor development in editable mode:\n```\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install -e .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install -e .[finetune]\n```\n\n## Quick Start\nFirst, load one of the BGE embedding model:\n```\nfrom FlagEmbedding import FlagAutoModel\n\nmodel = FlagAutoModel.from_finetuned('BAAI/bge-base-en-v1.5',\n                                      query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n                                      use_fp16=True)\n```\nThen, feed some sentences to the model and get their embeddings:\n```\nsentences_1 = [\"I love NLP\", \"I love machine learning\"]\nsentences_2 = [\"I love BGE\", \"I love text retrieval\"]\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\n```\nOnce we get the embeddings, we can compute similarity by inner product:\n```\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\n\nFor more details, you can refer to [embedder inference](./examples/inference/embedder), [reranker inference](./examples/inference/reranker), [embedder finetune](./examples/finetune/embedder), [reranker fintune](./examples/finetune/reranker), [evaluation](./examples/evaluation).\n\nIf you're unfamiliar with any of related concepts, please check out the [tutorial](./Tutorials/). If it's not there, let us know.\n\nFor more interesting topics related to BGE, take a look at [research](./research).\n\n## Community\n\nWe are actively maintaining the community of BGE and FlagEmbedding. Let us know if you have any suggessions or ideas!\n\nCurrently we are updating the [tutorials](./Tutorials/), we aim to create a comprehensive and detailed tutorial for beginners on text retrieval and RAG. Stay tuned!\n\nThe following contents are releasing in the upcoming weeks:\n\n- Evaluation\n- BGE-EN-ICL\n\n<details>\n  <summary>The whole tutorial roadmap</summary>\n    <img src=\"./Tutorials/tutorial_map.png\"/>\n</details>\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n| Model                                                                     | Language |                                                             Description                                                             |                                query instruction for retrieval                                 |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Provide instructions and few-shot examples freely based on the given task. |\n| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |    Multilingual     | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |        Provide instructions based on the given task.         |\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n| [LM-Cocktail](https://huggingface.co/Shitao)                   |   English |                     fine-tuned models (Llama and BGE) which can be used to reproduce the results of LM-Cocktail                     |  |\n| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English |                         a unified embedding model to support diverse retrieval augmentation needs for LLMs                          | See [README](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder) |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | Multilingual | a lightweight cross-encoder model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |  |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |  |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers, compress ratio and compress layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)             |   English |                                             Embedding Model which map text into vector                                              |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)               |   English |                                    a base-scale model but with similar ability to `bge-large-en`                                    |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)             |   English |                                        a small-scale model but with competitive performance                                         |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)             |   Chinese |                                             Embedding Model which map text into vector                                              |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)               |   Chinese |                                    a base-scale model but with similar ability to `bge-large-zh`                                    |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)             |   Chinese |                                        a small-scale model but with competitive performance                                         |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n\n\n\n\n### Contributors:\nThank all our contributors for their efforts and warmly welcome new members to join in!\n\n<a href=\"https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\" />\n</a>\n\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_m3,\n  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},\n  year={2023},\n  eprint={2309.07597},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n@misc{cocktail,\n      title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Xingrun Xing},\n      year={2023},\n      eprint={2311.13534},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{llm_embedder,\n      title={Retrieve Anything To Augment Large Language Models}, \n      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\n      year={2023},\n      eprint={2310.07554},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). \n\n",
  "external_links_in_readme": [
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation",
    "https://arxiv.org/pdf/2310.07554.pdf",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://arxiv.org/pdf/2409.05591v1",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset",
    "https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">",
    "https://huggingface.co/Shitao",
    "https://huggingface.co/BAAI/bge-base-en",
    "https://huggingface.co/BAAI/llm-embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://img.shields.io/badge/Contribution-Welcome-blue\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://huggingface.co/BAAI/bge-m3",
    "https://github.com/VectorSpaceLab/OmniGen",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge",
    "https://github.com/VectorSpaceLab/MegaPairs",
    "https://github.com/FlagOpen/FlagEmbedding.git",
    "https://huggingface.co/spaces/AIR-Bench/leaderboard",
    "https://github.com/AIR-Bench/AIR-Bench",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker",
    "https://github.com/JUNJIE99/MLVU",
    "https://github.com/qhjqhj00/MemoRAG,",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail",
    "https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research",
    "https://huggingface.co/BAAI/bge-small-zh-v1.5",
    "https://arxiv.org/abs/2412.14475",
    "https://img.shields.io/badge/LICENSE-MIT-green\">",
    "https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">",
    "https://huggingface.co/BAAI/bge-reranker-v2-gemma",
    "https://huggingface.co/spaces/mteb/leaderboard",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker",
    "https://arxiv.org/pdf/2402.03216.pdf",
    "https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\"",
    "https://huggingface.co/BAAI/bge-base-zh",
    "https://arxiv.org/abs/2311.13534",
    "https://huggingface.co/C-MTEB\">",
    "https://flagopen.baai.ac.cn/",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding",
    "https://arxiv.org/abs/2401.03462",
    "https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">",
    "https://arxiv.org/pdf/2309.07597.pdf",
    "https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a>",
    "https://huggingface.co/BAAI/bge-en-icl",
    "https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight",
    "https://www.bge-model.com",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://arxiv.org/abs/2312.15503",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials",
    "https://huggingface.co/BAAI/bge-multilingual-gemma2",
    "https://github.com/FlagOpen/FlagEmbedding\">",
    "https://huggingface.co/BAAI/bge-small-zh",
    "https://huggingface.co/BAAI/bge-small-en",
    "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE",
    "https://data.baai.ac.cn/details/BAAI-MTP",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker",
    "https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise",
    "https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92",
    "https://huggingface.co/BAAI/bge-large-en",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md",
    "https://huggingface.co/BAAI/bge-large-zh",
    "https://huggingface.co/BAAI/bge-base-zh-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora",
    "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA"
  ]
}
```

</details>


---

## Repository 18: FlagOpen/FlagEmbedding

# GitHub Repository Data

**Repository:** [FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## Basic Information

- **Description:** Retrieval and Retrieval-augmented LLMs
- **Created:** 2023-08-02T02:08:11+00:00
- **Last Updated:** 2025-06-21T10:44:22+00:00
- **Last Pushed:** 2025-06-04T10:00:26+00:00
- **Default Branch:** master
- **Size:** 52310 KB

## Statistics

- **Stars:** 9,974
- **Forks:** 737
- **Watchers:** 9,974
- **Open Issues:** 847
- **Total Issues:** 0
- **Pull Requests:** 217

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE)

## Languages

- **Python:** 3,871,341 bytes
- **Jupyter Notebook:** 2,607,506 bytes
- **Shell:** 7,322 bytes

## Topics

- `embeddings`
- `information-retrieval`
- `llm`
- `sentence-embeddings`
- `text-semantic-similarity`
- `retrieval-augmented-generation`

## Top Contributors

1. **hanhainebula** - 350 contributions
2. **545999961** - 327 contributions
3. **staoxiao** - 182 contributions
4. **ZiyiXia** - 113 contributions
5. **namespace-Pt** - 101 contributions
6. **JUNJIE99** - 78 contributions
7. **ftgreat** - 20 contributions
8. **ZhengLiu101** - 15 contributions
9. **Muennighoff** - 12 contributions
10. **shuyansy** - 10 contributions

## File Structure (Sample of 10 files)

Total files: 1,395

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.gitignore` (blob)
- `FlagEmbedding` (tree)
- `FlagEmbedding/__init__.py` (blob)
- `FlagEmbedding/abc` (tree)
- `FlagEmbedding/abc/__init__.py` (blob)
- `FlagEmbedding/abc/evaluation` (tree)
- `FlagEmbedding/abc/evaluation/__init__.py` (blob)

## Recent Issues

- 🟢 **#1480** Continued pretraining using RetroMAE (open)
- 🟢 **#1479** 微调bge-reranker-base一直卡顿，无日志输出 (open)
- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1477** 训练过程中train_loss和eval_loss都是先下降后上升再缓慢下降 (open)
- 🟢 **#1476** How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline (open)

## Recent Pull Requests

- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1475** Add inline comment to setup.py (open)
- 🟢 **#1469** Extend tutorials to include cloud deployment options (open)
- 🔴 **#1466** update reranker inference (closed)
- 🔴 **#1465** update environment (closed)

## Recent Commits

- **875fd4ff** update docs - ZiyiXia (2025-06-04T09:57:10+00:00)
- **235f967a** update tutorials - ZiyiXia (2025-06-04T09:27:36+00:00)
- **5e64baa6** Merge pull request #1466 from 545999961/master - chaofan (2025-05-28T08:15:17+00:00)
- **018b984e** update reranker inference - cfli (2025-05-28T08:14:32+00:00)
- **d04f7989** release version 1.3.5 - chaofan (2025-05-28T07:12:19+00:00)
- **8576fde7** Merge pull request #1465 from 545999961/master - chaofan (2025-05-28T06:58:53+00:00)
- **bf410d51** update environment - cfli (2025-05-28T06:55:54+00:00)
- **97ca0732** Merge pull request #1464 from 545999961/master - chaofan (2025-05-28T06:18:23+00:00)
- **b51706d6** upload coder eval script - cfli (2025-05-28T06:17:19+00:00)
- **821c3879** upload coder eval script - cfli (2025-05-28T06:12:16+00:00)

## External Links Found in README

- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation
- https://arxiv.org/pdf/2310.07554.pdf
- https://huggingface.co/BAAI/bge-reranker-base
- https://arxiv.org/pdf/2409.05591v1
- https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset
- https://img.shields.io/badge/BGE_series-🤗-yellow">
- https://huggingface.co/Shitao
- https://huggingface.co/BAAI/bge-base-en
- https://huggingface.co/BAAI/llm-embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder
- https://huggingface.co/BAAI/bge-reranker-large
- https://img.shields.io/badge/Contribution-Welcome-blue">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder
- https://huggingface.co/BAAI/bge-small-en-v1.5
- https://huggingface.co/BAAI/bge-m3
- https://github.com/VectorSpaceLab/OmniGen
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge
- https://github.com/VectorSpaceLab/MegaPairs

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673596675,
  "name": "FlagEmbedding",
  "full_name": "FlagOpen/FlagEmbedding",
  "description": "Retrieval and Retrieval-augmented LLMs",
  "html_url": "https://github.com/FlagOpen/FlagEmbedding",
  "clone_url": "https://github.com/FlagOpen/FlagEmbedding.git",
  "ssh_url": "git@github.com:FlagOpen/FlagEmbedding.git",
  "homepage": "http://www.bge-model.com/",
  "topics": [
    "embeddings",
    "information-retrieval",
    "llm",
    "sentence-embeddings",
    "text-semantic-similarity",
    "retrieval-augmented-generation"
  ],
  "default_branch": "master",
  "created_at": "2023-08-02T02:08:11+00:00",
  "updated_at": "2025-06-21T10:44:22+00:00",
  "pushed_at": "2025-06-04T10:00:26+00:00",
  "size_kb": 52310,
  "watchers_count": 9974,
  "stargazers_count": 9974,
  "forks_count": 737,
  "open_issues_count": 847,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"
  },
  "languages": {
    "Python": 3871341,
    "Jupyter Notebook": 2607506,
    "Shell": 7322
  },
  "top_contributors": [
    {
      "login": "hanhainebula",
      "contributions": 350
    },
    {
      "login": "545999961",
      "contributions": 327
    },
    {
      "login": "staoxiao",
      "contributions": 182
    },
    {
      "login": "ZiyiXia",
      "contributions": 113
    },
    {
      "login": "namespace-Pt",
      "contributions": 101
    },
    {
      "login": "JUNJIE99",
      "contributions": 78
    },
    {
      "login": "ftgreat",
      "contributions": 20
    },
    {
      "login": "ZhengLiu101",
      "contributions": 15
    },
    {
      "login": "Muennighoff",
      "contributions": 12
    },
    {
      "login": "shuyansy",
      "contributions": 10
    },
    {
      "login": "rainym00d",
      "contributions": 3
    },
    {
      "login": "chinainfant",
      "contributions": 3
    },
    {
      "login": "GeoloeG-IsT",
      "contributions": 3
    },
    {
      "login": "blue-vision0",
      "contributions": 2
    },
    {
      "login": "muazhari",
      "contributions": 2
    },
    {
      "login": "luisegarduno",
      "contributions": 2
    },
    {
      "login": "khazic",
      "contributions": 2
    },
    {
      "login": "shtdbb",
      "contributions": 2
    },
    {
      "login": "pengjunfeng11",
      "contributions": 2
    },
    {
      "login": "ycjcl868",
      "contributions": 2
    }
  ],
  "file_tree_count": 1395,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc/evaluation",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/evaluation/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 217,
  "recent_issues": [
    {
      "number": 1480,
      "title": "Continued pretraining using RetroMAE",
      "state": "open"
    },
    {
      "number": 1479,
      "title": "\u5fae\u8c03bge-reranker-base\u4e00\u76f4\u5361\u987f\uff0c\u65e0\u65e5\u5fd7\u8f93\u51fa",
      "state": "open"
    },
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1477,
      "title": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2dtrain_loss\u548ceval_loss\u90fd\u662f\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u518d\u7f13\u6162\u4e0b\u964d",
      "state": "open"
    },
    {
      "number": 1476,
      "title": "How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1475,
      "title": "Add inline comment to setup.py",
      "state": "open"
    },
    {
      "number": 1469,
      "title": "Extend tutorials to include cloud deployment options",
      "state": "open"
    },
    {
      "number": 1466,
      "title": "update reranker inference",
      "state": "closed"
    },
    {
      "number": 1465,
      "title": "update environment",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "875fd4ffcb894313a918e8bb0c9b7ead82f37e64",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:57:10+00:00",
      "message": "update docs"
    },
    {
      "sha": "235f967a0122b9d5b85104fd4e8f36dc15ee85a8",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:27:36+00:00",
      "message": "update tutorials"
    },
    {
      "sha": "5e64baa61e75df23105a66d1e9d09ad799366e2a",
      "author": "chaofan",
      "date": "2025-05-28T08:15:17+00:00",
      "message": "Merge pull request #1466 from 545999961/master"
    },
    {
      "sha": "018b984e23e12d9793e59f2a1ef07fb9153ab08e",
      "author": "cfli",
      "date": "2025-05-28T08:14:32+00:00",
      "message": "update reranker inference"
    },
    {
      "sha": "d04f7989eeec2c6db4ead7c77ddef656f9c9d3aa",
      "author": "chaofan",
      "date": "2025-05-28T07:12:19+00:00",
      "message": "release version 1.3.5"
    },
    {
      "sha": "8576fde79b84224ecbe493857115552fcfb72ed7",
      "author": "chaofan",
      "date": "2025-05-28T06:58:53+00:00",
      "message": "Merge pull request #1465 from 545999961/master"
    },
    {
      "sha": "bf410d51e9e685e7f1b2e3816f09f3dc64ad8292",
      "author": "cfli",
      "date": "2025-05-28T06:55:54+00:00",
      "message": "update environment"
    },
    {
      "sha": "97ca07325d9e786d668f9a038502ad736dded9d5",
      "author": "chaofan",
      "date": "2025-05-28T06:18:23+00:00",
      "message": "Merge pull request #1464 from 545999961/master"
    },
    {
      "sha": "b51706d63ae8a1d86443df8b0e5ff6e45eb08fdc",
      "author": "cfli",
      "date": "2025-05-28T06:17:19+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "821c387961b5597fe672a4386232110ef54d5107",
      "author": "cfli",
      "date": "2025-05-28T06:12:16+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "3332a1956e2d5af7c52628a60e5c9788bd1b5ed9",
      "author": "chaofan",
      "date": "2025-05-22T10:07:40+00:00",
      "message": "Merge pull request #1459 from 545999961/master"
    },
    {
      "sha": "d412884a4e96c45e4a9e83a43e507e9beaa04713",
      "author": "cfli",
      "date": "2025-05-22T10:06:43+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "5c11feffc104497953a333036aa56b6b543e7cee",
      "author": "chaofan",
      "date": "2025-05-22T09:56:48+00:00",
      "message": "Merge pull request #1458 from 545999961/master"
    },
    {
      "sha": "41b526441f13242b49126261c219b6b9f14e17cb",
      "author": "cfli",
      "date": "2025-05-22T09:55:44+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "52bb9fddb5def2074b277f6e9a2f02ed8aa17dee",
      "author": "chaofan",
      "date": "2025-05-22T09:52:21+00:00",
      "message": "Merge pull request #1457 from 545999961/master"
    },
    {
      "sha": "f05d4b3fbeff5df9dabeb71ab16225fe5e1e8c95",
      "author": "cfli",
      "date": "2025-05-22T09:51:20+00:00",
      "message": "upload reinforced_ir"
    },
    {
      "sha": "25ee99c228e9df63a2be79e8d795d549ab5001e9",
      "author": "chaofan",
      "date": "2025-05-21T04:46:03+00:00",
      "message": "Merge pull request #1453 from 545999961/master"
    },
    {
      "sha": "436210bbed3ca352aac82a9be66cd4e665408944",
      "author": "cfli",
      "date": "2025-05-21T04:45:10+00:00",
      "message": "upload Matroyshka_reranker"
    },
    {
      "sha": "1946f89d5a6926871861c9ed9f4acbb31665acfe",
      "author": "cfli",
      "date": "2025-05-20T08:13:30+00:00",
      "message": "upload Matroyshka Re-Ranker"
    },
    {
      "sha": "f6924351bba09620938b69a2c933c6712be829c4",
      "author": "cfli",
      "date": "2025-05-20T08:12:08+00:00",
      "message": "upload Matroyshka Re-Ranker"
    }
  ],
  "readme_text": "[<img src=\"./imgs/FlagOpen.png\">](https://flagopen.baai.ac.cn/)\n\n<h1 align=\"center\">\u26a1\ufe0fBGE: One-Stop Retrieval Toolkit For Search and RAG</h1>\n\n![bge_logo](./imgs/bge_logo.jpg)\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Contribution-Welcome-blue\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\">\n    </a>\n    <a href=\"https://huggingface.co/C-MTEB\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=#news>News</a> |\n        <a href=#installation>Installation</a> |\n        <a href=#quick-start>Quick Start</a> |\n        <a href=#community>Community</a> |\n        <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a> |\n        <a href=#model-list>Model List</a> |\n        <a href=\"#contributors\">Contributor</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\n\n## News\n\n- 3/6/2025: :fire::fire: Introduce **BGE-VL** ([HF repo](https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92)), State-Of-The-Art multimodal embedding models to support Any visual search applications (everything, including text-to-image, image-to-text, image&prompt-to-image, text-to-image&text, and more)! They are released under the MIT license and are completely free for both academic and commercial use. We also release **MegaPairs** ([repo](https://github.com/VectorSpaceLab/MegaPairs), [paper](https://arxiv.org/abs/2412.14475)), a massive synthetic dataset which empowers BGE-VL!\n- 12/5/2024: :book: We built the [BGE documentation](https://www.bge-model.com) for centralized BGE information and materials!\n- 10/29/2024: :earth_asia: We created WeChat group for BGE. Scan the [QR code](./imgs/BGE_WeChat_Group.png) to join the group chat! To get the first hand message about our updates and new release, or having any questions or ideas, join us now!\n- <img src=\"./imgs/BGE_WeChat_Group.png\" alt=\"bge_wechat_group\" class=\"center\" width=\"200\">\n\n- 10/22/2024: We release another interesting model: [OmniGen](https://github.com/VectorSpaceLab/OmniGen), which is a unified image generation model supporting various tasks. OmniGen can accomplish complex image generation tasks without the need for additional plugins like ControlNet, IP-Adapter, or auxiliary models such as pose detection and face detection.\n- 9/10/2024: Introducing **MemoRAG**, a step forward towards RAG 2.0 on top of memory-inspired knowledge discovery (repo: https://github.com/qhjqhj00/MemoRAG, paper: https://arxiv.org/pdf/2409.05591v1) \n- 9/2/2024: Start to maintain the [tutorials](./Tutorials/). The contents within will be actively updated and eariched, stay tuned! :books:\n- 7/26/2024: Release a new embedding model [bge-en-icl](https://huggingface.co/BAAI/bge-en-icl), an embedding model that incorporates in-context learning capabilities, which, by providing task-relevant query-response examples, can encode semantically richer queries, further enhancing the semantic representation ability of the embeddings.\n- 7/26/2024: Release a new embedding model [bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2), a multilingual embedding model based on gemma-2-9b, which supports multiple languages and diverse downstream tasks, achieving new SOTA on multilingual benchmarks (MIRACL, MTEB-fr, and MTEB-pl). \n- 7/26/2024: Release a new lightweight reranker [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight), a lightweight reranker based on gemma-2-9b, which supports token compression and layerwise lightweight operations, can still ensure good performance while saving a significant amount of resources. :fire:\n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n\n- 6/7/2024: Release a new benchmark [MLVU](https://github.com/JUNJIE99/MLVU), the first comprehensive benchmark specifically designed for long video understanding. MLVU features an extensive range of video durations, a diverse collection of video sources, and a set of evaluation tasks uniquely tailored for long-form video understanding. :fire:\n- 5/21/2024: Release a new benchmark [AIR-Bench](https://github.com/AIR-Bench/AIR-Bench) together with Jina AI, Zilliz, HuggingFace, and other partners. AIR-Bench focuses on a fair out-of-distribution evaluation for Neural IR & RAG. It generates the synthetic data for benchmarking w.r.t. diverse domains and languages. It is dynamic and will be updated on regular basis. [Leaderboard](https://huggingface.co/spaces/AIR-Bench/leaderboard) :fire:\n- 4/30/2024: Release [Llama-3-8B-Instruct-80K-QLoRA](https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA), extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA training on a few synthesized long-context data. The model achieves remarkable performance on various long-context benchmarks. [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora) :fire:\n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually :smiley:) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation :fire:\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data. :fire:\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://arxiv.org/pdf/2402.03216.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) \n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n\n</details>\n\n\nBGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n![projects](./imgs/projects.png)\n\n- **Inference**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker)\n- **Finetune**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)\n- **[Evaluation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation)**\n- **[Dataset](https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset)**\n- **[Tutorials](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)**\n- **[research](https://github.com/FlagOpen/FlagEmbedding/tree/master/research)**\n\n\n## Installation\n### Using pip:\nIf you do not want to finetune the models, you can install the package without the finetune dependency:\n```\npip install -U FlagEmbedding\n```\nIf you want to finetune the models, you can install the package with the finetune dependency:\n```\npip install -U FlagEmbedding[finetune]\n```\n### Install from sources:\n\nClone the repository and install\n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install  .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install  .[finetune]\n```\nFor development in editable mode:\n```\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install -e .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install -e .[finetune]\n```\n\n## Quick Start\nFirst, load one of the BGE embedding model:\n```\nfrom FlagEmbedding import FlagAutoModel\n\nmodel = FlagAutoModel.from_finetuned('BAAI/bge-base-en-v1.5',\n                                      query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n                                      use_fp16=True)\n```\nThen, feed some sentences to the model and get their embeddings:\n```\nsentences_1 = [\"I love NLP\", \"I love machine learning\"]\nsentences_2 = [\"I love BGE\", \"I love text retrieval\"]\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\n```\nOnce we get the embeddings, we can compute similarity by inner product:\n```\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\n\nFor more details, you can refer to [embedder inference](./examples/inference/embedder), [reranker inference](./examples/inference/reranker), [embedder finetune](./examples/finetune/embedder), [reranker fintune](./examples/finetune/reranker), [evaluation](./examples/evaluation).\n\nIf you're unfamiliar with any of related concepts, please check out the [tutorial](./Tutorials/). If it's not there, let us know.\n\nFor more interesting topics related to BGE, take a look at [research](./research).\n\n## Community\n\nWe are actively maintaining the community of BGE and FlagEmbedding. Let us know if you have any suggessions or ideas!\n\nCurrently we are updating the [tutorials](./Tutorials/), we aim to create a comprehensive and detailed tutorial for beginners on text retrieval and RAG. Stay tuned!\n\nThe following contents are releasing in the upcoming weeks:\n\n- Evaluation\n- BGE-EN-ICL\n\n<details>\n  <summary>The whole tutorial roadmap</summary>\n    <img src=\"./Tutorials/tutorial_map.png\"/>\n</details>\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n| Model                                                                     | Language |                                                             Description                                                             |                                query instruction for retrieval                                 |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Provide instructions and few-shot examples freely based on the given task. |\n| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |    Multilingual     | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |        Provide instructions based on the given task.         |\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n| [LM-Cocktail](https://huggingface.co/Shitao)                   |   English |                     fine-tuned models (Llama and BGE) which can be used to reproduce the results of LM-Cocktail                     |  |\n| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English |                         a unified embedding model to support diverse retrieval augmentation needs for LLMs                          | See [README](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder) |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | Multilingual | a lightweight cross-encoder model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |  |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |  |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers, compress ratio and compress layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)             |   English |                                             Embedding Model which map text into vector                                              |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)               |   English |                                    a base-scale model but with similar ability to `bge-large-en`                                    |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)             |   English |                                        a small-scale model but with competitive performance                                         |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)             |   Chinese |                                             Embedding Model which map text into vector                                              |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)               |   Chinese |                                    a base-scale model but with similar ability to `bge-large-zh`                                    |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)             |   Chinese |                                        a small-scale model but with competitive performance                                         |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n\n\n\n\n### Contributors:\nThank all our contributors for their efforts and warmly welcome new members to join in!\n\n<a href=\"https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\" />\n</a>\n\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_m3,\n  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},\n  year={2023},\n  eprint={2309.07597},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n@misc{cocktail,\n      title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Xingrun Xing},\n      year={2023},\n      eprint={2311.13534},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{llm_embedder,\n      title={Retrieve Anything To Augment Large Language Models}, \n      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\n      year={2023},\n      eprint={2310.07554},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). \n\n",
  "external_links_in_readme": [
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation",
    "https://arxiv.org/pdf/2310.07554.pdf",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://arxiv.org/pdf/2409.05591v1",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset",
    "https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">",
    "https://huggingface.co/Shitao",
    "https://huggingface.co/BAAI/bge-base-en",
    "https://huggingface.co/BAAI/llm-embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://img.shields.io/badge/Contribution-Welcome-blue\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://huggingface.co/BAAI/bge-m3",
    "https://github.com/VectorSpaceLab/OmniGen",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge",
    "https://github.com/VectorSpaceLab/MegaPairs",
    "https://github.com/FlagOpen/FlagEmbedding.git",
    "https://huggingface.co/spaces/AIR-Bench/leaderboard",
    "https://github.com/AIR-Bench/AIR-Bench",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker",
    "https://github.com/JUNJIE99/MLVU",
    "https://github.com/qhjqhj00/MemoRAG,",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail",
    "https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research",
    "https://huggingface.co/BAAI/bge-small-zh-v1.5",
    "https://arxiv.org/abs/2412.14475",
    "https://img.shields.io/badge/LICENSE-MIT-green\">",
    "https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">",
    "https://huggingface.co/BAAI/bge-reranker-v2-gemma",
    "https://huggingface.co/spaces/mteb/leaderboard",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker",
    "https://arxiv.org/pdf/2402.03216.pdf",
    "https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\"",
    "https://huggingface.co/BAAI/bge-base-zh",
    "https://arxiv.org/abs/2311.13534",
    "https://huggingface.co/C-MTEB\">",
    "https://flagopen.baai.ac.cn/",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding",
    "https://arxiv.org/abs/2401.03462",
    "https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">",
    "https://arxiv.org/pdf/2309.07597.pdf",
    "https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a>",
    "https://huggingface.co/BAAI/bge-en-icl",
    "https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight",
    "https://www.bge-model.com",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://arxiv.org/abs/2312.15503",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials",
    "https://huggingface.co/BAAI/bge-multilingual-gemma2",
    "https://github.com/FlagOpen/FlagEmbedding\">",
    "https://huggingface.co/BAAI/bge-small-zh",
    "https://huggingface.co/BAAI/bge-small-en",
    "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE",
    "https://data.baai.ac.cn/details/BAAI-MTP",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker",
    "https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise",
    "https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92",
    "https://huggingface.co/BAAI/bge-large-en",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md",
    "https://huggingface.co/BAAI/bge-large-zh",
    "https://huggingface.co/BAAI/bge-base-zh-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora",
    "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA"
  ]
}
```

</details>


---

## Repository 19: michaelfeil/infinity

# GitHub Repository Data

**Repository:** [michaelfeil/infinity](https://github.com/michaelfeil/infinity)

## Basic Information

- **Description:** Infinity is a high-throughput, low-latency serving engine for text-embeddings, reranking models, clip, clap and colpali
- **Created:** 2023-10-11T17:53:38+00:00
- **Last Updated:** 2025-06-21T15:55:32+00:00
- **Last Pushed:** 2025-06-21T17:03:19+00:00
- **Default Branch:** main
- **Size:** 12721 KB

## Statistics

- **Stars:** 2,254
- **Forks:** 152
- **Watchers:** 2,254
- **Open Issues:** 104
- **Total Issues:** 0
- **Pull Requests:** 283

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/michaelfeil/infinity/blob/main/LICENSE)

## Languages

- **Python:** 540,426 bytes
- **Makefile:** 7,982 bytes
- **Jinja:** 6,008 bytes
- **Shell:** 5,377 bytes

## Topics

- `llm`
- `text-embeddings`
- `bert-embeddings`

## Top Contributors

1. **michaelfeil** - 878 contributions
2. **wirthual** - 39 contributions
3. **chiragjn** - 9 contributions
4. **fadkeabhi** - 3 contributions
5. **charlesfrye** - 3 contributions
6. **NirantK** - 3 contributions
7. **BrianPulfer** - 2 contributions
8. **aowen14** - 2 contributions
9. **AranavMahalpure** - 1 contributions
10. **deep-diver** - 1 contributions

## File Structure (Sample of 10 files)

Total files: 300

- `.github` (tree)
- `.github/ISSUE_TEMPLATE` (tree)
- `.github/ISSUE_TEMPLATE/bug-report.yml` (blob)
- `.github/ISSUE_TEMPLATE/config.yml` (blob)
- `.github/ISSUE_TEMPLATE/feature-request.yml` (blob)
- `.github/ISSUE_TEMPLATE/new-model-addition.yml` (blob)
- `.github/actions` (tree)
- `.github/actions/disk_cleanup` (tree)
- `.github/actions/disk_cleanup/action.yml` (blob)
- `.github/actions/poetry_setup` (tree)

## Recent Issues

- 🟢 **#603** Fix client generation pipeline (open)
- 🔴 **#602** Caching huggingface models in GH actions (closed)
- 🟢 **#601** About Concurrency and Stability (open)
- 🟢 **#600** Add version check for transformers package when trying to import bettertransformers (open)
- 🟢 **#599** support new model: jinaai/jina-reranker-m0 (open)

## Recent Pull Requests

- 🟢 **#603** Fix client generation pipeline (open)
- 🔴 **#602** Caching huggingface models in GH actions (closed)
- 🟢 **#600** Add version check for transformers package when trying to import bettertransformers (open)
- 🟢 **#594** allow numpy >= 2.0 (open)
- 🔴 **#587** [Docs] Correct outdated mention of OpenAIEmbeddingInput (closed)

## Recent Commits

- **6b2dee34** Merge pull request #602 from michaelfeil/add-cache - wirthual (2025-06-21T15:55:27+00:00)
- **b0fc1891** Merge branch 'main' into add-cache - wirthual (2025-06-21T00:02:05+00:00)
- **7db7d502** split up execution to allow more efficient caching - wirthual (2025-06-20T22:42:28+00:00)
- **25a32ca7** change key - wirthual (2025-06-20T22:24:05+00:00)
- **c33464b1** remove key - wirthual (2025-06-20T20:17:58+00:00)
- **cf030a6d** add different keys for os and test type - wirthual (2025-06-20T20:17:07+00:00)
- **a7e1220c** try caching huggingface models - wirthual (2025-06-20T19:36:03+00:00)
- **644bb375** update convert script for qwen3 - michaelfeil (2025-06-06T18:43:03+00:00)
- **a269283a** [Docs] Correct outdated mention of OpenAIEmbeddingInput (#587) - ℍ𝕠𝕝𝕝𝕠𝕨 𝕄𝕒𝕟 (2025-05-16T23:29:28+00:00)
- **05b61171** add deps (#570) - Michael Feil (2025-04-16T19:57:01+00:00)

## External Links Found in README

- https://zenodo.org/badge/703686617.svg
- https://github.com/dwarvesf/llm-hosting"><img
- https://huggingface.co/BAAI/bge-reranker-base
- https://github.com/huggingface/chat-ui/blob/daf695ea4a6e2d081587d7dbcae3cacd466bf8b2/docs/source/configuration/embeddings.md#openai"><img
- https://github.com/mixedbread-ai/batched
- https://github.com/michaelfeil/infinity/stargazers
- https://huggingface.co/michaelfeil/colqwen2-v0.1
- https://zenodo.org/doi/10.5281/zenodo.11406462
- https://avatars.githubusercontent.com/u/54861414"
- https://platform.openai.com/docs/guides/embeddings
- https://michaelfeil.github.io/infinity
- https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct/discussions/20
- https://github.com/michaelfeil/infinity/actions
- https://huggingface.co/Alibaba-NLP/gte-multilingual-reranker-base
- https://huggingface.co/colbert-ir/colbertv2.0
- https://img.shields.io/github/license/michaelfeil/infinity.svg?style=for-the-badge
- https://huggingface.co/BAAI/bge-reranker-large
- https://github.com/user-attachments/assets/1b515b0f-2332-4b12-be82-933056bddee4"
- https://infinity.modal.michaelfeil.eu/docs
- https://github.com/the-seeds/imitater

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 703686617,
  "name": "infinity",
  "full_name": "michaelfeil/infinity",
  "description": "Infinity is a high-throughput, low-latency serving engine for text-embeddings, reranking models, clip, clap and colpali",
  "html_url": "https://github.com/michaelfeil/infinity",
  "clone_url": "https://github.com/michaelfeil/infinity.git",
  "ssh_url": "git@github.com:michaelfeil/infinity.git",
  "homepage": "https://michaelfeil.github.io/infinity/",
  "topics": [
    "llm",
    "text-embeddings",
    "bert-embeddings"
  ],
  "default_branch": "main",
  "created_at": "2023-10-11T17:53:38+00:00",
  "updated_at": "2025-06-21T15:55:32+00:00",
  "pushed_at": "2025-06-21T17:03:19+00:00",
  "size_kb": 12721,
  "watchers_count": 2254,
  "stargazers_count": 2254,
  "forks_count": 152,
  "open_issues_count": 104,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/michaelfeil/infinity/blob/main/LICENSE"
  },
  "languages": {
    "Python": 540426,
    "Makefile": 7982,
    "Jinja": 6008,
    "Shell": 5377
  },
  "top_contributors": [
    {
      "login": "michaelfeil",
      "contributions": 878
    },
    {
      "login": "wirthual",
      "contributions": 39
    },
    {
      "login": "chiragjn",
      "contributions": 9
    },
    {
      "login": "fadkeabhi",
      "contributions": 3
    },
    {
      "login": "charlesfrye",
      "contributions": 3
    },
    {
      "login": "NirantK",
      "contributions": 3
    },
    {
      "login": "BrianPulfer",
      "contributions": 2
    },
    {
      "login": "aowen14",
      "contributions": 2
    },
    {
      "login": "AranavMahalpure",
      "contributions": 1
    },
    {
      "login": "deep-diver",
      "contributions": 1
    },
    {
      "login": "davleop",
      "contributions": 1
    },
    {
      "login": "Gavinfornever",
      "contributions": 1
    },
    {
      "login": "kartik-ganesh",
      "contributions": 1
    },
    {
      "login": "LetsssGo55",
      "contributions": 1
    },
    {
      "login": "YadlaMani",
      "contributions": 1
    },
    {
      "login": "niranjan-kurhade",
      "contributions": 1
    },
    {
      "login": "rawsh",
      "contributions": 1
    },
    {
      "login": "bufferoverflow",
      "contributions": 1
    },
    {
      "login": "samos123",
      "contributions": 1
    },
    {
      "login": "sherwin684",
      "contributions": 1
    }
  ],
  "file_tree_count": 300,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/bug-report.yml",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/config.yml",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/feature-request.yml",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/new-model-addition.yml",
      "type": "blob"
    },
    {
      "path": ".github/actions",
      "type": "tree"
    },
    {
      "path": ".github/actions/disk_cleanup",
      "type": "tree"
    },
    {
      "path": ".github/actions/disk_cleanup/action.yml",
      "type": "blob"
    },
    {
      "path": ".github/actions/poetry_setup",
      "type": "tree"
    }
  ],
  "issues_count": 0,
  "pulls_count": 283,
  "recent_issues": [
    {
      "number": 603,
      "title": "Fix client generation pipeline",
      "state": "open"
    },
    {
      "number": 602,
      "title": "Caching huggingface models in GH actions",
      "state": "closed"
    },
    {
      "number": 601,
      "title": "About Concurrency and Stability",
      "state": "open"
    },
    {
      "number": 600,
      "title": "Add version check for transformers package when trying to import bettertransformers",
      "state": "open"
    },
    {
      "number": 599,
      "title": "support new model: jinaai/jina-reranker-m0",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 603,
      "title": "Fix client generation pipeline",
      "state": "open"
    },
    {
      "number": 602,
      "title": "Caching huggingface models in GH actions",
      "state": "closed"
    },
    {
      "number": 600,
      "title": "Add version check for transformers package when trying to import bettertransformers",
      "state": "open"
    },
    {
      "number": 594,
      "title": "allow numpy >= 2.0",
      "state": "open"
    },
    {
      "number": 587,
      "title": "[Docs] Correct outdated mention of OpenAIEmbeddingInput",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "6b2dee347bb2ddff66c97d1b3d7a6d530983e364",
      "author": "wirthual",
      "date": "2025-06-21T15:55:27+00:00",
      "message": "Merge pull request #602 from michaelfeil/add-cache"
    },
    {
      "sha": "b0fc1891442a8904db870f957e848564f46645c1",
      "author": "wirthual",
      "date": "2025-06-21T00:02:05+00:00",
      "message": "Merge branch 'main' into add-cache"
    },
    {
      "sha": "7db7d50225cec49ef3079bcfafba5d6a367d27b7",
      "author": "wirthual",
      "date": "2025-06-20T22:42:28+00:00",
      "message": "split up execution to allow more efficient caching"
    },
    {
      "sha": "25a32ca700c5af3dc07b97645f1f5f769cf98602",
      "author": "wirthual",
      "date": "2025-06-20T22:24:05+00:00",
      "message": "change key"
    },
    {
      "sha": "c33464b12c37305276d9cd16a77f81609bb08a19",
      "author": "wirthual",
      "date": "2025-06-20T20:17:58+00:00",
      "message": "remove key"
    },
    {
      "sha": "cf030a6d50543dc39dbe1b5967ffcd53264c0128",
      "author": "wirthual",
      "date": "2025-06-20T20:17:07+00:00",
      "message": "add different keys for os and test type"
    },
    {
      "sha": "a7e1220c65ca3bbe30ff883d81906f0def4f54d3",
      "author": "wirthual",
      "date": "2025-06-20T19:36:03+00:00",
      "message": "try caching huggingface models"
    },
    {
      "sha": "644bb375adaa50a1a648f0a6b5dc04459218d452",
      "author": "michaelfeil",
      "date": "2025-06-06T18:43:03+00:00",
      "message": "update convert script for qwen3"
    },
    {
      "sha": "a269283a9391b70039278c5949e29b3f7f604cf8",
      "author": "\u210d\ud835\udd60\ud835\udd5d\ud835\udd5d\ud835\udd60\ud835\udd68 \ud835\udd44\ud835\udd52\ud835\udd5f",
      "date": "2025-05-16T23:29:28+00:00",
      "message": "[Docs] Correct outdated mention of OpenAIEmbeddingInput (#587)"
    },
    {
      "sha": "05b6117164bb69f036dd8bddde82182390a88b11",
      "author": "Michael Feil",
      "date": "2025-04-16T19:57:01+00:00",
      "message": "add deps (#570)"
    },
    {
      "sha": "0bbfdbc2fa1334e04dc1dc8eec64beb48f1b87ca",
      "author": "Yasyf Mohamedali",
      "date": "2025-04-16T17:01:38+00:00",
      "message": "Add ColQwen2_5 to IMAGE_COL_MODELS (#568)"
    },
    {
      "sha": "88bad14e1f9bf03f2806848b6039f7aa61717653",
      "author": "wirthual",
      "date": "2025-03-28T06:00:23+00:00",
      "message": "Update colpali engine to latest version (#563)"
    },
    {
      "sha": "1c84442531cf88c1f8dc7bf30ce38ea2bad20aaa",
      "author": "Michael Feil",
      "date": "2025-03-16T22:50:15+00:00",
      "message": "bump to 2.6 torch (#556)"
    },
    {
      "sha": "5859c68e1a4b72fc6a08a391418a1df43348e492",
      "author": "Michael Feil",
      "date": "2025-03-14T05:07:14+00:00",
      "message": "just upload"
    },
    {
      "sha": "952fb88e6bf93fecbcc4ec89b50771ca2e758035",
      "author": "Michael Feil",
      "date": "2025-03-14T04:27:54+00:00",
      "message": "add convert lm script"
    },
    {
      "sha": "154160c8af464134103649ab1ce22406713eef5b",
      "author": "michaelfeil",
      "date": "2025-02-11T06:31:31+00:00",
      "message": "add vision client"
    },
    {
      "sha": "215f7956874832285cdb484c82855f3f617d02ff",
      "author": "wirthual",
      "date": "2025-02-11T04:36:57+00:00",
      "message": "Merge pull request #526 from michaelfeil/add-vision-client-template"
    },
    {
      "sha": "b1c43dbcc42aeee93388b67dd7ecf8289f97dd2f",
      "author": "wirthual",
      "date": "2025-02-11T03:23:20+00:00",
      "message": "README: add example for using local model wtth docker container (#528)"
    },
    {
      "sha": "8003c5b58364a23ddad1c827b3fb4febbf03fae0",
      "author": "Raphael",
      "date": "2025-02-03T16:51:52+00:00",
      "message": "add vision client template"
    },
    {
      "sha": "b17b588bf811340683de8aee6473c65f43145a87",
      "author": "wirthual",
      "date": "2025-02-03T16:39:20+00:00",
      "message": "Merge pull request #524 from michaelfeil/version-check-step"
    }
  ],
  "readme_text": "\n<!-- PROJECT SHIELDS -->\n<!--\n*** I'm using markdown \"reference style\" links for readability.\n*** Reference links are enclosed in brackets [ ] instead of parentheses ( ).\n*** See the bottom of this document for the declaration of the reference variables\n*** for contributors-url, forks-url, etc. This is an optional, concise syntax you may use.\n*** https://www.markdownguide.org/basic-syntax/#reference-style-links\n-->\n[![Contributors][contributors-shield]][contributors-url]\n[![Forks][forks-shield]][forks-url]\n[![Stargazers][stars-shield]][stars-url]\n[![Issues][issues-shield]][issues-url]\n[![MIT License][license-shield]][license-url]\n\n# Infinity \u267e\ufe0f\n[![codecov][codecov-shield]][codecov-url]\n[![ci][ci-shield]][ci-url]\n[![Downloads][pepa-shield]][pepa-url]\n[![DOI](https://zenodo.org/badge/703686617.svg)](https://zenodo.org/doi/10.5281/zenodo.11406462)\n![Docker pulls](https://img.shields.io/docker/pulls/michaelf34/infinity)\n\n\nInfinity is a high-throughput, low-latency REST API for serving text-embeddings, reranking models, clip, clap and colpali. Infinity is developed under [MIT License](https://github.com/michaelfeil/infinity/blob/main/LICENSE).\n\n## Why Infinity\n* **Deploy any model from HuggingFace**: deploy any embedding, reranking, clip and sentence-transformer model from [HuggingFace]( https://huggingface.co/models?other=text-embeddings-inference&sort=trending)\n* **Fast inference backends**: The inference server is built on top of [PyTorch](https://github.com/pytorch/pytorch), [optimum (ONNX/TensorRT)](https://huggingface.co/docs/optimum/index) and [CTranslate2](https://github.com/OpenNMT/CTranslate2), using FlashAttention to get the most out of your **NVIDIA CUDA**, **AMD ROCM**, **CPU**, **AWS INF2** or **APPLE MPS** accelerator. Infinity uses dynamic batching and tokenization dedicated in worker threads.\n* **Multi-modal and multi-model**: Mix-and-match multiple models. Infinity orchestrates them.\n* **Tested implementation**: Unit and end-to-end tested. Embeddings via infinity are correctly embedded. Lets API users create embeddings till infinity and beyond.\n* **Easy to use**: Built on [FastAPI](https://fastapi.tiangolo.com/). Infinity CLI v2 allows launching of all arguments via Environment variable or argument. OpenAPI aligned to [OpenAI's API specs](https://platform.openai.com/docs/guides/embeddings/what-are-embeddings). View the docs at [https://michaelfeil.github.io/infinity](https://michaelfeil.github.io/infinity/) on how to get started.\n\n<p align=\"center\">\n  <a href=\"https://github.com/basetenlabs/truss-examples/tree/7025918c813d08d718b8939f44f10651a0ff2c8c/custom-server/infinity-embedding-server\"><img src=\"https://avatars.githubusercontent.com/u/54861414\" alt=\"Logo Baseten.co\" width=\"50\"/></a>\n  <a href=\"https://github.com/runpod-workers/worker-infinity-embedding\"><img src=\"https://github.com/user-attachments/assets/24f1906d-31b8-4e16-a479-1382cbdea046\" alt=\"Logo Runpod\" width=\"50\"/></a>\n  <a href=\"https://www.truefoundry.com/cognita\"><img src=\"https://github.com/user-attachments/assets/1b515b0f-2332-4b12-be82-933056bddee4\" alt=\"Logo TrueFoundry\" width=\"50\"/></a>\n  <a href=\"https://vast.ai/article/serving-infinity\"><img src=\"https://github.com/user-attachments/assets/8286d620-f403-48f5-bd7f-f471b228ae7b\" alt=\"Logo Vast\" width=\"46\"/></a>\n  <a href=\"https://www.dataguard.de\"><img src=\"https://github.com/user-attachments/assets/3fde1ac6-c299-455d-9fc2-ba4012799f9c\" alt=\"Logo DataGuard\" width=\"50\"/></a>\n  <a href=\"https://community.sap.com/t5/artificial-intelligence-and-machine-learning-blogs/bring-open-source-llms-into-sap-ai-core/ba-p/13655167\"><img src=\"https://github.com/user-attachments/assets/743e932b-ed5b-4a71-84cb-f28235707a84\" alt=\"Logo SAP\" width=\"47\"/></a>\n  <a href=\"https://x.com/StuartReid1929/status/1763434100382163333\"><img src=\"https://github.com/user-attachments/assets/477a4c54-1113-434b-83bc-1985f10981d3\" alt=\"Logo Nosible\" width=\"44\"/></a>\n  <a href=\"https://github.com/freshworksinc/freddy-infinity\"><img src=\"https://github.com/user-attachments/assets/a68da78b-d958-464e-aaf6-f39132be68a0\" alt=\"Logo FreshWorks\" width=\"50\"/></a>\n  <a href=\"https://github.com/dstackai/dstack/tree/master/examples/deployment/infinity\"><img src=\"https://github.com/user-attachments/assets/9cde2d6b-dc16-4f0a-81ba-535a84321467\" alt=\"Logo Dstack\" width=\"50\"/></a>\n  <a href=\"https://embeddedllm.com/blog/\"><img src=\"https://avatars.githubusercontent.com/u/148834374\" alt=\"Logo JamAI\" width=\"50\"/></a>\n  <a href=\"https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct#infinity_emb\"><img src=\"https://avatars.githubusercontent.com/u/1961952\" alt=\"Logo Alibaba Group\" width=\"50\"/></a>\n  <a href=\"https://github.com/bentoml/BentoInfinity/\"><img src=\"https://avatars.githubusercontent.com/u/49176046\" alt=\"Logo BentoML\" width=\"50\"/></a>\n  <a href=\"https://x.com/bo_wangbo/status/1766371909086724481\"><img src=\"https://avatars.githubusercontent.com/u/60539444\" alt=\"Logo JinaAi\" width=\"50\"/></a>\n  <a href=\"https://github.com/dwarvesf/llm-hosting\"><img src=\"https://avatars.githubusercontent.com/u/10388449\" alt=\"Logo Dwarves Foundation\" width=\"50\"/></a>\n  <a href=\"https://github.com/huggingface/chat-ui/blob/daf695ea4a6e2d081587d7dbcae3cacd466bf8b2/docs/source/configuration/embeddings.md#openai\"><img src=\"https://avatars.githubusercontent.com/u/25720743\" alt=\"Logo HF\" width=\"50\"/></a>\n  <a href=\"https://www.linkedin.com/posts/markhng525_join-me-and-ekin-karabulut-at-the-ai-infra-activity-7163233344875393024-LafB?utm_source=share&utm_medium=member_desktop\"><img src=\"https://avatars.githubusercontent.com/u/86131705\" alt=\"Logo Gradient.ai\" width=\"50\"/></a>\n</p> \n\n### Latest News \ud83d\udd25\n\n- [2024/11] AMD, CPU, ONNX docker images\n- [2024/10] `pip install infinity_client`\n- [2024/07] Inference deployment example via [Modal](./infra/modal/README.md) and a [free GPU deployment](https://infinity.modal.michaelfeil.eu/)\n- [2024/06] Support for multi-modal: clip, text-classification & launch all arguments from env variables\n- [2024/05] launch multiple models using the `v2` cli, including `--api-key`\n- [2024/03] infinity supports experimental int8 (cpu/cuda) and fp8 (H100/MI300) support\n- [2024/03] Docs are online: https://michaelfeil.github.io/infinity/latest/\n- [2024/02] Community meetup at the [Run:AI Infra Club](https://discord.gg/7D4fbEgWjv)\n- [2024/01] TensorRT / ONNX inference\n- [2023/10] Initial release\n\n## Getting started\n### Launch the cli via pip install\n```bash\npip install infinity-emb[all]\n```\nAfter your pip install, with your venv active, you can run the CLI directly.\n\n```bash\ninfinity_emb v2 --model-id BAAI/bge-small-en-v1.5\n```\nCheck the `v2 --help` command to get a description for all parameters.\n```bash\ninfinity_emb v2 --help\n```\n### Launch the CLI using a pre-built docker container (recommended)\nInstead of installing the CLI via pip, you may also use docker to run `michaelf34/infinity`. \nMake sure you mount your accelerator ( i.e. install `nvidia-docker` and activate with `--gpus all`). \n\n```bash\nport=7997\nmodel1=michaelfeil/bge-small-en-v1.5\nmodel2=mixedbread-ai/mxbai-rerank-xsmall-v1\nvolume=$PWD/data\n\ndocker run -it --gpus all \\\n -v $volume:/app/.cache \\\n -p $port:$port \\\n michaelf34/infinity:latest \\\n v2 \\\n --model-id $model1 \\\n --model-id $model2 \\\n --port $port\n```\nThe cache path inside the docker container is set by the environment variable `HF_HOME`.\n\n#### Specialized docker images\n<details>\n  <summary>Docker container for CPU</summary>\n  Use the `latest-cpu` image or `x.x.x-cpu` for slimer image. \n  Run like any other cpu-only docker image. \n  Optimum/Onnx is often the prefered engine. \n\n  ```\n  docker run -it \\\n  -v $volume:/app/.cache \\\n  -p $port:$port \\\n  michaelf34/infinity:latest-cpu \\\n  v2 \\\n  --engine optimum \\\n  --model-id $model1 \\\n  --model-id $model2 \\\n  --port $port\n  ```\n</details>\n\n<details>\n  <summary>Docker Container for ROCm (MI200 Series and MI300 Series)</summary>\n  Use the `latest-rocm` image or `x.x.x-rocm` for rocm compatible inference.\n  **This image is currently not build via CI/CD (to large), consider pinning to exact version.**\n  Make sure you have ROCm is correctly installed and ready to use with Docker.\n\n  Visit [Docs](https://michaelfeil.github.io/infinity) for more info.\n</details>\n \n<details>\n  <summary>Docker Container for Onnx-GPU, Cuda Extensions, TensorRT</summary>\n  Use the `latest-trt-onnx` image or `x.x.x-trt-onnx` for nvidia compatible inference.\n  **This image is currently not build via CI/CD (to large), consider pinning to exact version.**\n\n  This image has support for:\n  - ONNX-Cuda \"CudaExecutionProvider\" \n  - ONNX-TensorRT \"TensorRTExecutionProvider\" (may not always work due to version mismatch with ORT)\n  - CudaExtensions and packages, e.g. Tri-Dao's `pip install flash-attn` package when using Pytorch.\n  - nvcc compiler support\n  \n  ```\n  docker run -it \\\n  -v $volume:/app/.cache \\\n  -p $port:$port \\\n  michaelf34/infinity:latest-trt-onnx \\\n  v2 \\\n  --engine optimum \\\n  --device cuda \\\n  --model-id $model1 \\\n  --port $port\n  ```\n</details>\n\n#### Using local models with Docker container\n\nIn order to deploy a local model with a docker container, you need to mount the model inside the container and specify the path in the container to the launch command.\n\nExample:\n```bash\ngit lfs install \ncd /tmp\nmkdir models && cd models && git clone https://huggingface.co/BAAI/bge-small-en-v1.5\ndocker run -it   -v /tmp/models:/models  -p 8081:8081  michaelf34/infinity:latest v2  --model-id \"/models/bge-small-en-v1.5\" --port 8081\n```\n\n#### Advanced CLI usage\n\n<details>\n  <summary>Launching multiple models at once</summary>\n  \n  Since `infinity_emb>=0.0.34`, you can use cli `v2` method to launch multiple models at the same time.\n  Checkout `infinity_emb v2 --help` for all args and validation.\n\n  Multiple Model CLI Playbook:                                                                                         \n   - 1. cli options can be repeated e.g. `v2 --model-id model/id1 --model-id model/id2 --batch-size 8 --batch-size 4`. This will create two models `model/id1` and `model/id2`\n   - 2. or adapt the defaults by setting ENV Variables separated by `;`: `INFINITY_MODEL_ID=\"model/id1;model/id2;\" && INFINITY_BATCH_SIZE=\"8;4;\"`\n   - 3. single items are broadcasted to `--model-id` length,  `v2 --model-id model/id1 --model-id/id2 --batch-size 8` making both models have batch-size 8.\n   - 4. Everything is broadcasted to the number of `--model-id` + API requests are routed to the `--served-model-name/--model-id`\n</details>\n\n<details>\n  <summary>Using environment variables instead of the cli</summary>\n  All CLI arguments are also launchable via environment variables.\n\n  Environment variables start with `INFINITY_{UPPER_CASE_SNAKE_CASE}` and often match the `--{lower-case-kebab-case}` cli arguments.\n  \n  The following two are equivalent:\n  - CLI `infinity_emb v2 --model-id BAAI/bge-base-en-v1.5`\n  - ENV-CLI: `export INFINITY_MODEL_ID=\"BAAI/bge-base-en-v1.5\" && infinity_emb v2`\n\n  Multiple arguments can be used via `;` syntax: `INFINITY_MODEL_ID=\"model/id1;model/id2;\"`\n</details>\n\n<details>\n  <summary>API Key</summary>\n  Supply an `--api-key secret123` via CLI or ENV INFINITY_API_KEY=\"secret123\".\n</details>\n\n<details>\n  <summary>Chosing the fastest engine</summary>\n  \n  With the command `--engine torch` the model must be compatible with https://github.com/UKPLab/sentence-transformers/ and AutoModel\n\n  With the command `--engine optimum`, there must be an onnx file. Models from https://huggingface.co/Xenova are recommended.\n  \n  With the command `--engine ctranslate2`\n    - only `BERT` models are supported.\n</details>\n\n<details>\n  <summary>Telemetry opt-out</summary>\n  \n  See which telemetry is collected: https://michaelfeil.eu/infinity/main/telemetry/\n  ```\n  # Disable\n  export INFINITY_ANONYMOUS_USAGE_STATS=\"0\"\n  ```\n</details>\n\n### Supported Tasks and Models by Infinity\n\nInfinity aims to be the inference server supporting most functionality for embeddings, reranking and related RAG tasks. The following  Infinity tests 15+ architectures and all of the below cases in the Github CI.\nClick on the sections below to find tasks and **validated example models**.\n\n<details>\n  <summary>Text Embeddings</summary>\n  \n  Text embeddings measure the relatedness of text strings. Embeddings are used for search, clustering, recommendations.\n  Think about a private deployed version of openai's text embeddings. https://platform.openai.com/docs/guides/embeddings\n\n  Tested embedding models:\n  - [mixedbread-ai/mxbai-embed-large-v1](https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1)\n  - [WhereIsAI/UAE-Large-V1](https://huggingface.co/WhereIsAI/UAE-Large-V1)\n  - [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)\n  - [Alibaba-NLP/gte-large-en-v1.5](https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5)\n  - [jinaai/jina-embeddings-v2-base-code](https://huggingface.co/jinaai/jina-embeddings-v2-base-code)\n  - [sentence-transformers/all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)\n  - [intfloat/multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct)\n  - [intfloat/multilingual-e5-small](https://huggingface.co/intfloat/multilingual-e5-small)\n  - [jinaai/jina-embeddings-v3](nomic-ai/nomic-embed-text-v1.5)\n  - [BAAI/bge-m3, no sparse](https://huggingface.co/BAAI/bge-m3)\n  - decoder-based models. Keep in mind that they are ~20-100x larger (&slower) than bert-small models:\n    - [Alibaba-NLP/gte-Qwen2-1.5B-instruct](https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct/discussions/20)\n    - [Salesforce/SFR-Embedding-2_R](https://huggingface.co/Salesforce/SFR-Embedding-2_R/discussions/6)\n    - [Alibaba-NLP/gte-Qwen2-7B-instruct](https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct/discussions/39)\n\n  Other models:\n  - Most embedding model are likely supported: https://huggingface.co/models?pipeline_tag=feature-extraction&other=text-embeddings-inference&sort=trending\n  - Check MTEB leaderboard for models https://huggingface.co/spaces/mteb/leaderboard.\n</details>\n\n<details>\n  <summary>Reranking</summary>\n  Given a query and a list of documents, Reranking indexes the documents from most to least semantically relevant to the query.\n  Think like a locally deployed version of https://docs.cohere.com/reference/rerank\n  \n  Tested reranking models:\n  - [mixedbread-ai/mxbai-rerank-xsmall-v1](https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1)\n  - [Alibaba-NLP/gte-multilingual-reranker-base](https://huggingface.co/Alibaba-NLP/gte-multilingual-reranker-base)\n  - [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)\n  - [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large)\n  - [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3)\n  - [jinaai/jina-reranker-v1-turbo-en](https://huggingface.co/jinaai/jina-reranker-v1-turbo-en)\n\n  Other reranking models:\n  - Reranking Models supported by infinity are bert-style classification Models with one category.\n  - Most reranking model are likely supported: https://huggingface.co/models?pipeline_tag=text-classification&other=text-embeddings-inference&sort=trending\n  - https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=rerank\n</details>\n\n<details>\n  <summary>Multi-modal and cross-modal - image and audio embeddings</summary>\n  Specialized embedding models that allow for image<->text or image<->audio search. \n  Typically, these models allow for text<->text, text<->other and other<->other search, with accuracy tradeoffs when going cross-modal.\n  \n  Image<->text models can be used for e.g. photo-gallery search, where users can type in keywords to find photos, or use a photo to find related images.\n  Audio<->text models are less popular, and can be e.g. used to find music songs based on a text description or related music songs.\n  \n  Tested image<->text models:\n  - [wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M](https://huggingface.co/wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M)\n  - [jinaai/jina-clip-v1](https://huggingface.co/jinaai/jina-clip-v1)\n  - [google/siglip-so400m-patch14-384](https://huggingface.co/google/siglip-so400m-patch14-384)\n  - Models of type: ClipModel / SiglipModel in `config.json`\n  \n  Tested audio<->text models:\n  - [Clap Models from LAION](https://huggingface.co/collections/laion/clap-contrastive-language-audio-pretraining-65415c0b18373b607262a490)\n  - limited number open source organizations training these models\n  - * Note: The sampling rate of the audio data needs to match the model *\n\n  Not supported:\n  - Plain vision models e.g. nomic-ai/nomic-embed-vision-v1.5\n</details>\n\n<details>\n  <summary>ColBert-style late-interaction Embeddings</summary>\n  ColBert Embeddings don't perform any special Pooling methods, but return the raw **token embeddings**.\n  The **token embeddings** are then to be scored with the MaxSim Metric in a VectorDB (Qdrant / Vespa)\n  \n  For usage via the RestAPI, late-interaction embeddings may best be transported via `base64` encoding.\n  Example notebook: https://colab.research.google.com/drive/14FqLc0N_z92_VgL_zygWV5pJZkaskyk7?usp=sharing\n  \n  Tested colbert models:\n  - [colbert-ir/colbertv2.0](https://huggingface.co/colbert-ir/colbertv2.0)\n  - [jinaai/jina-colbert-v2](https://huggingface.co/jinaai/jina-colbert-v2)\n  - [mixedbread-ai/mxbai-colbert-large-v1](https://huggingface.co/mixedbread-ai/mxbai-colbert-large-v1)\n  - [answerai-colbert-small-v1 - click link for instructions](https://huggingface.co/answerdotai/answerai-colbert-small-v1/discussions/14)\n\n</details>\n\n<details>\n  <summary>ColPali-style late-interaction Image<->Text Embeddings</summary>\n  Similar usage to ColBert, but scanning over an image<->text instead of only text.\n  \n  For usage via the RestAPI, late-interaction embeddings may best be transported via `base64` encoding.\n  Example notebook: https://colab.research.google.com/drive/14FqLc0N_z92_VgL_zygWV5pJZkaskyk7?usp=sharing\n  \n  Tested ColPali/ColQwen models:\n  - [vidore/colpali-v1.2-merged](https://huggingface.co/michaelfeil/colpali-v1.2-merged)\n  - [michaelfeil/colqwen2-v0.1](https://huggingface.co/michaelfeil/colqwen2-v0.1)\n  - No lora adapters supported, only \"merged\" models.\n</details>\n\n<details>\n  <summary>Text classification</summary>\n  A bert-style multi-label text classification. Classifies it into distinct categories. \n  \n  Tested models:\n  - [ProsusAI/finbert](https://huggingface.co/ProsusAI/finbert), financial news classification\n  - [SamLowe/roberta-base-go_emotions](https://huggingface.co/SamLowe/roberta-base-go_emotions), text to emotion categories.\n  - bert-style text-classifcation models with more than >1 label in `config.json`\n</details>\n\n### Infinity usage via the Python API\n\nInstead of the cli & RestAPI use infinity's interface via the Python API. \nThis gives you most flexibility. The Python API builds on `asyncio` with its `await/async` features, to allow concurrent processing of requests. Arguments of the CLI are also available via Python.\n\n#### Embeddings\n```python\nimport asyncio\nfrom infinity_emb import AsyncEngineArray, EngineArgs, AsyncEmbeddingEngine\n\nsentences = [\"Embed this is sentence via Infinity.\", \"Paris is in France.\"]\narray = AsyncEngineArray.from_args([\n  EngineArgs(model_name_or_path = \"BAAI/bge-small-en-v1.5\", engine=\"torch\", embedding_dtype=\"float32\", dtype=\"auto\")\n])\n\nasync def embed_text(engine: AsyncEmbeddingEngine): \n    async with engine: \n        embeddings, usage = await engine.embed(sentences=sentences)\n    # or handle the async start / stop yourself.\n    await engine.astart()\n    embeddings, usage = await engine.embed(sentences=sentences)\n    await engine.astop()\nasyncio.run(embed_text(array[0]))\n```\n\n#### Reranking\n\nReranking gives you a score for similarity between a query and multiple documents. \nUse it in conjunction with a VectorDB+Embeddings, or as standalone for small amount of documents.\nPlease select a model from huggingface that is a AutoModelForSequenceClassification compatible model with one class classification.\n\n```python\nimport asyncio\nfrom infinity_emb import AsyncEngineArray, EngineArgs, AsyncEmbeddingEngine\nquery = \"What is the python package infinity_emb?\"\ndocs = [\"This is a document not related to the python package infinity_emb, hence...\", \n    \"Paris is in France!\",\n    \"infinity_emb is a package for sentence embeddings and rerankings using transformer models in Python!\"]\narray = AsyncEmbeddingEngine.from_args(\n  [EngineArgs(model_name_or_path = \"mixedbread-ai/mxbai-rerank-xsmall-v1\", engine=\"torch\")]\n)\n\nasync def rerank(engine: AsyncEmbeddingEngine): \n    async with engine:\n        ranking, usage = await engine.rerank(query=query, docs=docs)\n        print(list(zip(ranking, docs)))\n    # or handle the async start / stop yourself.\n    await engine.astart()\n    ranking, usage = await engine.rerank(query=query, docs=docs)\n    await engine.astop()\n\nasyncio.run(rerank(array[0]))\n```\n\nWhen using the CLI, use this command to launch rerankers:\n```bash\ninfinity_emb v2 --model-id mixedbread-ai/mxbai-rerank-xsmall-v1\n```\n\n#### Image-Embeddings: CLIP models\n\nCLIP models are able to encode images and text at the same time. \n\n```python\nimport asyncio\nfrom infinity_emb import AsyncEngineArray, EngineArgs, AsyncEmbeddingEngine\n\nsentences = [\"This is awesome.\", \"I am bored.\"]\nimages = [\"http://images.cocodataset.org/val2017/000000039769.jpg\"]\nengine_args = EngineArgs(\n    model_name_or_path = \"wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M\", \n    engine=\"torch\"\n)\narray = AsyncEngineArray.from_args([engine_args])\n\nasync def embed(engine: AsyncEmbeddingEngine): \n    await engine.astart()\n    embeddings, usage = await engine.embed(sentences=sentences)\n    embeddings_image, _ = await engine.image_embed(images=images)\n    await engine.astop()\n\nasyncio.run(embed(array[\"wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M\"]))\n```\n\n#### Audio-Embeddings: CLAP models\n\nCLAP models are able to encode audio and text at the same time. \n\n```python\nimport asyncio\nfrom infinity_emb import AsyncEngineArray, EngineArgs, AsyncEmbeddingEngine\nimport requests\nimport soundfile as sf\nimport io\n\nsentences = [\"This is awesome.\", \"I am bored.\"]\n\nurl = \"https://bigsoundbank.com/UPLOAD/wav/2380.wav\"\nraw_bytes = requests.get(url, stream=True).content\n\naudios = [raw_bytes]\nengine_args = EngineArgs(\n    model_name_or_path = \"laion/clap-htsat-unfused\",\n    dtype=\"float32\", \n    engine=\"torch\"\n\n)\narray = AsyncEngineArray.from_args([engine_args])\n\nasync def embed(engine: AsyncEmbeddingEngine): \n    await engine.astart()\n    embeddings, usage = await engine.embed(sentences=sentences)\n    embedding_audios = await engine.audio_embed(audios=audios)\n    await engine.astop()\n\nasyncio.run(embed(array[\"laion/clap-htsat-unfused\"]))\n```\n\n#### Text Classification \n\nUse text classification with Infinity's `classify` feature, which allows for sentiment analysis, emotion detection, and more classification tasks.\n\n```python\nimport asyncio\nfrom infinity_emb import AsyncEngineArray, EngineArgs, AsyncEmbeddingEngine\n\nsentences = [\"This is awesome.\", \"I am bored.\"]\nengine_args = EngineArgs(\n    model_name_or_path = \"SamLowe/roberta-base-go_emotions\", \n    engine=\"torch\", model_warmup=True)\narray = AsyncEngineArray.from_args([engine_args])\n\nasync def classifier(engine: AsyncEmbeddingEngine): \n    async with engine:\n        predictions, usage = await engine.classify(sentences=sentences)\n    # or handle the async start / stop yourself.\n    await engine.astart()\n    predictions, usage = await engine.classify(sentences=sentences)\n    await engine.astop()\nasyncio.run(classifier(array[\"SamLowe/roberta-base-go_emotions\"]))\n```\n\n### Infinity usage via the Python Client\n\nInfinity has a generated client code for RestAPI client side usage.\n\nIf you want to call a remote infinity instance via RestAPI, install the following package locally:\n```bash\npip install infinity_client\n```\n\nFor more information, check out the Client Readme\nhttps://github.com/michaelfeil/infinity/tree/main/libs/client_infinity/infinity_client\n\n## Integrations:\n- [Serverless deployments at Runpod](https://github.com/runpod-workers/worker-infinity-embedding)\n- [Truefoundry Cognita](https://github.com/truefoundry/cognita)\n- [Langchain example](https://github.com/langchain-ai/langchain)\n- [imitater - A unified language model server built upon vllm and infinity.](https://github.com/the-seeds/imitater)\n- [Dwarves Foundation: Deployment examples using Modal.com](https://github.com/dwarvesf/llm-hosting)\n- [infiniflow/Ragflow](https://github.com/infiniflow/ragflow)\n- [SAP Core AI](https://github.com/SAP-samples/btp-generative-ai-hub-use-cases/tree/main/10-byom-oss-llm-ai-core)\n- [gpt_server - gpt_server is an open-source framework designed for production-level deployment of LLMs (Large Language Models) or Embeddings.](https://github.com/shell-nlp/gpt_server)\n- [KubeAI: Kubernetes AI Operator for inferencing](https://github.com/substratusai/kubeai)\n- [LangChain](https://python.langchain.com/docs/integrations/text_embedding/infinity)\n- [Batched, modification of the Batching algoritm in Infinity](https://github.com/mixedbread-ai/batched)\n\n## Documentation\nView the docs at [https:///michaelfeil.github.io/infinity](https://michaelfeil.github.io/infinity) on how to get started.\nAfter startup, the Swagger Ui will be available under `{url}:{port}/docs`, in this case `http://localhost:7997/docs`. You can also find a interactive preview here: https://infinity.modal.michaelfeil.eu/docs (and https://michaelfeil-infinity.hf.space/docs)\n\n## Contribute and Develop\n\nInstall via Poetry 1.8.1, Python3.11 on Ubuntu 22.04\n```bash\ncd libs/infinity_emb\npoetry install --extras all --with lint,test\n```\n\nTo pass the CI:\n```bash\ncd libs/infinity_emb\nmake precommit\n```\n\nAll contributions must be made in a way to be compatible with the MIT License of this repo. \n\n### Citation\n```\n@software{feil_2023_11630143,\n  author       = {Feil, Michael},\n  title        = {Infinity - To Embeddings and Beyond},\n  month        = oct,\n  year         = 2023,\n  publisher    = {Zenodo},\n  doi          = {10.5281/zenodo.11630143},\n  url          = {https://doi.org/10.5281/zenodo.11630143}\n}\n```\n\n### \ud83d\udc9a Current contributors <a name=\"Current contributors\"></a>\n\n<a href=\"https://github.com/michaelfeil/infinity=y/graphs/contributors\">\n  <img src=\"https://contributors-img.web.app/image?repo=michaelfeil/infinity\" />\n</a>\n\n<!-- MARKDOWN LINKS & IMAGES -->\n<!-- https://www.markdownguide.org/basic-syntax/#reference-style-links -->\n[contributors-shield]: https://img.shields.io/github/contributors/michaelfeil/infinity.svg?style=for-the-badge\n[contributors-url]: https://github.com/michaelfeil/infinity/graphs/contributors\n[forks-shield]: https://img.shields.io/github/forks/michaelfeil/infinity.svg?style=for-the-badge\n[forks-url]: https://github.com/michaelfeil/infinity/network/members\n[stars-shield]: https://img.shields.io/github/stars/michaelfeil/infinity.svg?style=for-the-badge\n[stars-url]: https://github.com/michaelfeil/infinity/stargazers\n[issues-shield]: https://img.shields.io/github/issues/michaelfeil/infinity.svg?style=for-the-badge\n[issues-url]: https://github.com/michaelfeil/infinity/issues\n[license-shield]: https://img.shields.io/github/license/michaelfeil/infinity.svg?style=for-the-badge\n[license-url]: https://github.com/michaelfeil/infinity/blob/main/LICENSE\n[pepa-shield]: https://static.pepy.tech/badge/infinity-emb\n[pepa-url]: https://www.pepy.tech/projects/infinity-emb\n[codecov-shield]: https://codecov.io/gh/michaelfeil/infinity/branch/main/graph/badge.svg?token=NMVQY5QOFQ\n[codecov-url]: https://codecov.io/gh/michaelfeil/infinity/branch/main\n[ci-shield]: https://github.com/michaelfeil/infinity/actions/workflows/ci.yaml/badge.svg\n[ci-url]: https://github.com/michaelfeil/infinity/actions\n",
  "external_links_in_readme": [
    "https://zenodo.org/badge/703686617.svg",
    "https://github.com/dwarvesf/llm-hosting\"><img",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://github.com/huggingface/chat-ui/blob/daf695ea4a6e2d081587d7dbcae3cacd466bf8b2/docs/source/configuration/embeddings.md#openai\"><img",
    "https://github.com/mixedbread-ai/batched",
    "https://github.com/michaelfeil/infinity/stargazers",
    "https://huggingface.co/michaelfeil/colqwen2-v0.1",
    "https://zenodo.org/doi/10.5281/zenodo.11406462",
    "https://avatars.githubusercontent.com/u/54861414\"",
    "https://platform.openai.com/docs/guides/embeddings",
    "https://michaelfeil.github.io/infinity",
    "https://huggingface.co/Alibaba-NLP/gte-Qwen2-1.5B-instruct/discussions/20",
    "https://github.com/michaelfeil/infinity/actions",
    "https://huggingface.co/Alibaba-NLP/gte-multilingual-reranker-base",
    "https://huggingface.co/colbert-ir/colbertv2.0",
    "https://img.shields.io/github/license/michaelfeil/infinity.svg?style=for-the-badge",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://github.com/user-attachments/assets/1b515b0f-2332-4b12-be82-933056bddee4\"",
    "https://infinity.modal.michaelfeil.eu/docs",
    "https://github.com/the-seeds/imitater",
    "https://community.sap.com/t5/artificial-intelligence-and-machine-learning-blogs/bring-open-source-llms-into-sap-ai-core/ba-p/13655167\"><img",
    "https://discord.gg/7D4fbEgWjv",
    "https://huggingface.co/mixedbread-ai/mxbai-embed-large-v1",
    "https://huggingface.co/mixedbread-ai/mxbai-rerank-xsmall-v1",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://x.com/bo_wangbo/status/1766371909086724481\"><img",
    "https://huggingface.co/BAAI/bge-m3",
    "https://huggingface.co/models?pipeline_tag=feature-extraction&other=text-embeddings-inference&sort=trending",
    "https://infinity.modal.michaelfeil.eu/",
    "https://huggingface.co/jinaai/jina-reranker-v1-turbo-en",
    "https://fastapi.tiangolo.com/",
    "https://www.markdownguide.org/basic-syntax/#reference-style-links",
    "https://github.com/freshworksinc/freddy-infinity\"><img",
    "https://huggingface.co/WhereIsAI/UAE-Large-V1",
    "https://vast.ai/article/serving-infinity\"><img",
    "https://x.com/StuartReid1929/status/1763434100382163333\"><img",
    "https://michaelfeil-infinity.hf.space/docs",
    "https://www.linkedin.com/posts/markhng525_join-me-and-ekin-karabulut-at-the-ai-infra-activity-7163233344875393024-LafB?utm_source=share&utm_medium=member_desktop\"><img",
    "https:///michaelfeil.github.io/infinity](https://michaelfeil.github.io/infinity",
    "https://huggingface.co/intfloat/multilingual-e5-small",
    "https://embeddedllm.com/blog/\"><img",
    "https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct/discussions/39",
    "https://huggingface.co/michaelfeil/colpali-v1.2-merged",
    "https://github.com/michaelfeil/infinity/network/members",
    "https://avatars.githubusercontent.com/u/49176046\"",
    "https://huggingface.co/jinaai/jina-embeddings-v2-base-code",
    "https://docs.cohere.com/reference/rerank",
    "https://github.com/user-attachments/assets/743e932b-ed5b-4a71-84cb-f28235707a84\"",
    "https://michaelfeil.github.io/infinity](https://michaelfeil.github.io/infinity/",
    "https://avatars.githubusercontent.com/u/1961952\"",
    "https://huggingface.co/intfloat/multilingual-e5-large-instruct",
    "https://python.langchain.com/docs/integrations/text_embedding/infinity",
    "http://localhost:7997/docs`.",
    "https://huggingface.co/Salesforce/SFR-Embedding-2_R/discussions/6",
    "https://avatars.githubusercontent.com/u/60539444\"",
    "https://huggingface.co/wkcn/TinyCLIP-ViT-8M-16-Text-3M-YFCC15M",
    "https://github.com/michaelfeil/infinity/graphs/contributors",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://img.shields.io/github/stars/michaelfeil/infinity.svg?style=for-the-badge",
    "https://github.com/user-attachments/assets/3fde1ac6-c299-455d-9fc2-ba4012799f9c\"",
    "https://github.com/SAP-samples/btp-generative-ai-hub-use-cases/tree/main/10-byom-oss-llm-ai-core",
    "https://huggingface.co/spaces/mteb/leaderboard.",
    "https://github.com/user-attachments/assets/a68da78b-d958-464e-aaf6-f39132be68a0\"",
    "https://img.shields.io/github/issues/michaelfeil/infinity.svg?style=for-the-badge",
    "https://github.com/michaelfeil/infinity=y/graphs/contributors\">",
    "https://platform.openai.com/docs/guides/embeddings/what-are-embeddings",
    "https://huggingface.co/ProsusAI/finbert",
    "https://colab.research.google.com/drive/14FqLc0N_z92_VgL_zygWV5pJZkaskyk7?usp=sharing",
    "https://avatars.githubusercontent.com/u/25720743\"",
    "https://github.com/UKPLab/sentence-transformers/",
    "https://codecov.io/gh/michaelfeil/infinity/branch/main/graph/badge.svg?token=NMVQY5QOFQ",
    "https://github.com/OpenNMT/CTranslate2",
    "https://codecov.io/gh/michaelfeil/infinity/branch/main",
    "https://github.com/truefoundry/cognita",
    "https://img.shields.io/github/contributors/michaelfeil/infinity.svg?style=for-the-badge",
    "https://github.com/michaelfeil/infinity/blob/main/LICENSE",
    "https://github.com/infiniflow/ragflow",
    "https://huggingface.co/mixedbread-ai/mxbai-colbert-large-v1",
    "https://huggingface.co/jinaai/jina-clip-v1",
    "https://huggingface.co/answerdotai/answerai-colbert-small-v1/discussions/14",
    "https://github.com/bentoml/BentoInfinity/\"><img",
    "https://michaelfeil.eu/infinity/main/telemetry/",
    "https://huggingface.co/Alibaba-NLP/gte-large-en-v1.5",
    "https://contributors-img.web.app/image?repo=michaelfeil/infinity\"",
    "https://avatars.githubusercontent.com/u/148834374\"",
    "https://doi.org/10.5281/zenodo.11630143}",
    "https://huggingface.co/jinaai/jina-colbert-v2",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://michaelfeil.github.io/infinity/latest/",
    "http://images.cocodataset.org/val2017/000000039769.jpg\"]",
    "https://www.pepy.tech/projects/infinity-emb",
    "https://huggingface.co/docs/optimum/index",
    "https://huggingface.co/collections/laion/clap-contrastive-language-audio-pretraining-65415c0b18373b607262a490",
    "https://github.com/substratusai/kubeai",
    "https://huggingface.co/google/siglip-so400m-patch14-384",
    "https://avatars.githubusercontent.com/u/86131705\"",
    "https://avatars.githubusercontent.com/u/10388449\"",
    "https://github.com/shell-nlp/gpt_server",
    "https://huggingface.co/SamLowe/roberta-base-go_emotions",
    "https://img.shields.io/github/forks/michaelfeil/infinity.svg?style=for-the-badge",
    "https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct#infinity_emb\"><img",
    "https://huggingface.co/Xenova",
    "https://github.com/michaelfeil/infinity/issues",
    "https://www.truefoundry.com/cognita\"><img",
    "https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2",
    "https://github.com/user-attachments/assets/9cde2d6b-dc16-4f0a-81ba-535a84321467\"",
    "https://github.com/user-attachments/assets/24f1906d-31b8-4e16-a479-1382cbdea046\"",
    "https://github.com/runpod-workers/worker-infinity-embedding\"><img",
    "https://github.com/langchain-ai/langchain",
    "https://bigsoundbank.com/UPLOAD/wav/2380.wav\"",
    "https://img.shields.io/docker/pulls/michaelf34/infinity",
    "https://github.com/basetenlabs/truss-examples/tree/7025918c813d08d718b8939f44f10651a0ff2c8c/custom-server/infinity-embedding-server\"><img",
    "https://huggingface.co/models?other=text-embeddings-inference&sort=trending",
    "https://github.com/dstackai/dstack/tree/master/examples/deployment/infinity\"><img",
    "https://github.com/michaelfeil/infinity/actions/workflows/ci.yaml/badge.svg",
    "https://github.com/dwarvesf/llm-hosting",
    "https://static.pepy.tech/badge/infinity-emb",
    "https://github.com/user-attachments/assets/477a4c54-1113-434b-83bc-1985f10981d3\"",
    "https://github.com/user-attachments/assets/8286d620-f403-48f5-bd7f-f471b228ae7b\"",
    "https://www.dataguard.de\"><img",
    "https://github.com/pytorch/pytorch",
    "https://github.com/runpod-workers/worker-infinity-embedding",
    "https://huggingface.co/models?pipeline_tag=text-classification&sort=trending&search=rerank",
    "https://huggingface.co/models?pipeline_tag=text-classification&other=text-embeddings-inference&sort=trending",
    "https://github.com/michaelfeil/infinity/tree/main/libs/client_infinity/infinity_client"
  ]
}
```

</details>


---

## Repository 20: FlagOpen/FlagEmbedding

# GitHub Repository Data

**Repository:** [FlagOpen/FlagEmbedding](https://github.com/FlagOpen/FlagEmbedding)

## Basic Information

- **Description:** Retrieval and Retrieval-augmented LLMs
- **Created:** 2023-08-02T02:08:11+00:00
- **Last Updated:** 2025-06-21T10:44:22+00:00
- **Last Pushed:** 2025-06-04T10:00:26+00:00
- **Default Branch:** master
- **Size:** 52310 KB

## Statistics

- **Stars:** 9,974
- **Forks:** 737
- **Watchers:** 9,974
- **Open Issues:** 847
- **Total Issues:** 0
- **Pull Requests:** 217

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE)

## Languages

- **Python:** 3,871,341 bytes
- **Jupyter Notebook:** 2,607,506 bytes
- **Shell:** 7,322 bytes

## Topics

- `embeddings`
- `information-retrieval`
- `llm`
- `sentence-embeddings`
- `text-semantic-similarity`
- `retrieval-augmented-generation`

## Top Contributors

1. **hanhainebula** - 350 contributions
2. **545999961** - 327 contributions
3. **staoxiao** - 182 contributions
4. **ZiyiXia** - 113 contributions
5. **namespace-Pt** - 101 contributions
6. **JUNJIE99** - 78 contributions
7. **ftgreat** - 20 contributions
8. **ZhengLiu101** - 15 contributions
9. **Muennighoff** - 12 contributions
10. **shuyansy** - 10 contributions

## File Structure (Sample of 10 files)

Total files: 1,395

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.gitignore` (blob)
- `FlagEmbedding` (tree)
- `FlagEmbedding/__init__.py` (blob)
- `FlagEmbedding/abc` (tree)
- `FlagEmbedding/abc/__init__.py` (blob)
- `FlagEmbedding/abc/evaluation` (tree)
- `FlagEmbedding/abc/evaluation/__init__.py` (blob)

## Recent Issues

- 🟢 **#1480** Continued pretraining using RetroMAE (open)
- 🟢 **#1479** 微调bge-reranker-base一直卡顿，无日志输出 (open)
- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1477** 训练过程中train_loss和eval_loss都是先下降后上升再缓慢下降 (open)
- 🟢 **#1476** How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline (open)

## Recent Pull Requests

- 🟢 **#1478** Make fine-tuning code runs properly in non-distributed mode. (open)
- 🟢 **#1475** Add inline comment to setup.py (open)
- 🟢 **#1469** Extend tutorials to include cloud deployment options (open)
- 🔴 **#1466** update reranker inference (closed)
- 🔴 **#1465** update environment (closed)

## Recent Commits

- **875fd4ff** update docs - ZiyiXia (2025-06-04T09:57:10+00:00)
- **235f967a** update tutorials - ZiyiXia (2025-06-04T09:27:36+00:00)
- **5e64baa6** Merge pull request #1466 from 545999961/master - chaofan (2025-05-28T08:15:17+00:00)
- **018b984e** update reranker inference - cfli (2025-05-28T08:14:32+00:00)
- **d04f7989** release version 1.3.5 - chaofan (2025-05-28T07:12:19+00:00)
- **8576fde7** Merge pull request #1465 from 545999961/master - chaofan (2025-05-28T06:58:53+00:00)
- **bf410d51** update environment - cfli (2025-05-28T06:55:54+00:00)
- **97ca0732** Merge pull request #1464 from 545999961/master - chaofan (2025-05-28T06:18:23+00:00)
- **b51706d6** upload coder eval script - cfli (2025-05-28T06:17:19+00:00)
- **821c3879** upload coder eval script - cfli (2025-05-28T06:12:16+00:00)

## External Links Found in README

- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation
- https://arxiv.org/pdf/2310.07554.pdf
- https://huggingface.co/BAAI/bge-reranker-base
- https://arxiv.org/pdf/2409.05591v1
- https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset
- https://img.shields.io/badge/BGE_series-🤗-yellow">
- https://huggingface.co/Shitao
- https://huggingface.co/BAAI/bge-base-en
- https://huggingface.co/BAAI/llm-embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder
- https://huggingface.co/BAAI/bge-reranker-large
- https://img.shields.io/badge/Contribution-Welcome-blue">
- https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder
- https://huggingface.co/BAAI/bge-small-en-v1.5
- https://huggingface.co/BAAI/bge-m3
- https://github.com/VectorSpaceLab/OmniGen
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder
- https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge
- https://github.com/VectorSpaceLab/MegaPairs

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673596675,
  "name": "FlagEmbedding",
  "full_name": "FlagOpen/FlagEmbedding",
  "description": "Retrieval and Retrieval-augmented LLMs",
  "html_url": "https://github.com/FlagOpen/FlagEmbedding",
  "clone_url": "https://github.com/FlagOpen/FlagEmbedding.git",
  "ssh_url": "git@github.com:FlagOpen/FlagEmbedding.git",
  "homepage": "http://www.bge-model.com/",
  "topics": [
    "embeddings",
    "information-retrieval",
    "llm",
    "sentence-embeddings",
    "text-semantic-similarity",
    "retrieval-augmented-generation"
  ],
  "default_branch": "master",
  "created_at": "2023-08-02T02:08:11+00:00",
  "updated_at": "2025-06-21T10:44:22+00:00",
  "pushed_at": "2025-06-04T10:00:26+00:00",
  "size_kb": 52310,
  "watchers_count": 9974,
  "stargazers_count": 9974,
  "forks_count": 737,
  "open_issues_count": 847,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE"
  },
  "languages": {
    "Python": 3871341,
    "Jupyter Notebook": 2607506,
    "Shell": 7322
  },
  "top_contributors": [
    {
      "login": "hanhainebula",
      "contributions": 350
    },
    {
      "login": "545999961",
      "contributions": 327
    },
    {
      "login": "staoxiao",
      "contributions": 182
    },
    {
      "login": "ZiyiXia",
      "contributions": 113
    },
    {
      "login": "namespace-Pt",
      "contributions": 101
    },
    {
      "login": "JUNJIE99",
      "contributions": 78
    },
    {
      "login": "ftgreat",
      "contributions": 20
    },
    {
      "login": "ZhengLiu101",
      "contributions": 15
    },
    {
      "login": "Muennighoff",
      "contributions": 12
    },
    {
      "login": "shuyansy",
      "contributions": 10
    },
    {
      "login": "rainym00d",
      "contributions": 3
    },
    {
      "login": "chinainfant",
      "contributions": 3
    },
    {
      "login": "GeoloeG-IsT",
      "contributions": 3
    },
    {
      "login": "blue-vision0",
      "contributions": 2
    },
    {
      "login": "muazhari",
      "contributions": 2
    },
    {
      "login": "luisegarduno",
      "contributions": 2
    },
    {
      "login": "khazic",
      "contributions": 2
    },
    {
      "login": "shtdbb",
      "contributions": 2
    },
    {
      "login": "pengjunfeng11",
      "contributions": 2
    },
    {
      "login": "ycjcl868",
      "contributions": 2
    }
  ],
  "file_tree_count": 1395,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/__init__.py",
      "type": "blob"
    },
    {
      "path": "FlagEmbedding/abc/evaluation",
      "type": "tree"
    },
    {
      "path": "FlagEmbedding/abc/evaluation/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 217,
  "recent_issues": [
    {
      "number": 1480,
      "title": "Continued pretraining using RetroMAE",
      "state": "open"
    },
    {
      "number": 1479,
      "title": "\u5fae\u8c03bge-reranker-base\u4e00\u76f4\u5361\u987f\uff0c\u65e0\u65e5\u5fd7\u8f93\u51fa",
      "state": "open"
    },
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1477,
      "title": "\u8bad\u7ec3\u8fc7\u7a0b\u4e2dtrain_loss\u548ceval_loss\u90fd\u662f\u5148\u4e0b\u964d\u540e\u4e0a\u5347\u518d\u7f13\u6162\u4e0b\u964d",
      "state": "open"
    },
    {
      "number": 1476,
      "title": "How can I conduct evaluations using the mteb and cmteb datasets that I manually downloaded offline",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1478,
      "title": "Make fine-tuning code runs properly in non-distributed mode.",
      "state": "open"
    },
    {
      "number": 1475,
      "title": "Add inline comment to setup.py",
      "state": "open"
    },
    {
      "number": 1469,
      "title": "Extend tutorials to include cloud deployment options",
      "state": "open"
    },
    {
      "number": 1466,
      "title": "update reranker inference",
      "state": "closed"
    },
    {
      "number": 1465,
      "title": "update environment",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "875fd4ffcb894313a918e8bb0c9b7ead82f37e64",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:57:10+00:00",
      "message": "update docs"
    },
    {
      "sha": "235f967a0122b9d5b85104fd4e8f36dc15ee85a8",
      "author": "ZiyiXia",
      "date": "2025-06-04T09:27:36+00:00",
      "message": "update tutorials"
    },
    {
      "sha": "5e64baa61e75df23105a66d1e9d09ad799366e2a",
      "author": "chaofan",
      "date": "2025-05-28T08:15:17+00:00",
      "message": "Merge pull request #1466 from 545999961/master"
    },
    {
      "sha": "018b984e23e12d9793e59f2a1ef07fb9153ab08e",
      "author": "cfli",
      "date": "2025-05-28T08:14:32+00:00",
      "message": "update reranker inference"
    },
    {
      "sha": "d04f7989eeec2c6db4ead7c77ddef656f9c9d3aa",
      "author": "chaofan",
      "date": "2025-05-28T07:12:19+00:00",
      "message": "release version 1.3.5"
    },
    {
      "sha": "8576fde79b84224ecbe493857115552fcfb72ed7",
      "author": "chaofan",
      "date": "2025-05-28T06:58:53+00:00",
      "message": "Merge pull request #1465 from 545999961/master"
    },
    {
      "sha": "bf410d51e9e685e7f1b2e3816f09f3dc64ad8292",
      "author": "cfli",
      "date": "2025-05-28T06:55:54+00:00",
      "message": "update environment"
    },
    {
      "sha": "97ca07325d9e786d668f9a038502ad736dded9d5",
      "author": "chaofan",
      "date": "2025-05-28T06:18:23+00:00",
      "message": "Merge pull request #1464 from 545999961/master"
    },
    {
      "sha": "b51706d63ae8a1d86443df8b0e5ff6e45eb08fdc",
      "author": "cfli",
      "date": "2025-05-28T06:17:19+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "821c387961b5597fe672a4386232110ef54d5107",
      "author": "cfli",
      "date": "2025-05-28T06:12:16+00:00",
      "message": "upload coder eval script"
    },
    {
      "sha": "3332a1956e2d5af7c52628a60e5c9788bd1b5ed9",
      "author": "chaofan",
      "date": "2025-05-22T10:07:40+00:00",
      "message": "Merge pull request #1459 from 545999961/master"
    },
    {
      "sha": "d412884a4e96c45e4a9e83a43e507e9beaa04713",
      "author": "cfli",
      "date": "2025-05-22T10:06:43+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "5c11feffc104497953a333036aa56b6b543e7cee",
      "author": "chaofan",
      "date": "2025-05-22T09:56:48+00:00",
      "message": "Merge pull request #1458 from 545999961/master"
    },
    {
      "sha": "41b526441f13242b49126261c219b6b9f14e17cb",
      "author": "cfli",
      "date": "2025-05-22T09:55:44+00:00",
      "message": "update reinforced_ir"
    },
    {
      "sha": "52bb9fddb5def2074b277f6e9a2f02ed8aa17dee",
      "author": "chaofan",
      "date": "2025-05-22T09:52:21+00:00",
      "message": "Merge pull request #1457 from 545999961/master"
    },
    {
      "sha": "f05d4b3fbeff5df9dabeb71ab16225fe5e1e8c95",
      "author": "cfli",
      "date": "2025-05-22T09:51:20+00:00",
      "message": "upload reinforced_ir"
    },
    {
      "sha": "25ee99c228e9df63a2be79e8d795d549ab5001e9",
      "author": "chaofan",
      "date": "2025-05-21T04:46:03+00:00",
      "message": "Merge pull request #1453 from 545999961/master"
    },
    {
      "sha": "436210bbed3ca352aac82a9be66cd4e665408944",
      "author": "cfli",
      "date": "2025-05-21T04:45:10+00:00",
      "message": "upload Matroyshka_reranker"
    },
    {
      "sha": "1946f89d5a6926871861c9ed9f4acbb31665acfe",
      "author": "cfli",
      "date": "2025-05-20T08:13:30+00:00",
      "message": "upload Matroyshka Re-Ranker"
    },
    {
      "sha": "f6924351bba09620938b69a2c933c6712be829c4",
      "author": "cfli",
      "date": "2025-05-20T08:12:08+00:00",
      "message": "upload Matroyshka Re-Ranker"
    }
  ],
  "readme_text": "[<img src=\"./imgs/FlagOpen.png\">](https://flagopen.baai.ac.cn/)\n\n<h1 align=\"center\">\u26a1\ufe0fBGE: One-Stop Retrieval Toolkit For Search and RAG</h1>\n\n![bge_logo](./imgs/bge_logo.jpg)\n\n<p align=\"center\">\n    <a href=\"https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding\">\n            <img alt=\"Build\" src=\"https://img.shields.io/badge/Contribution-Welcome-blue\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">\n        <img alt=\"License\" src=\"https://img.shields.io/badge/LICENSE-MIT-green\">\n    </a>\n    <a href=\"https://huggingface.co/C-MTEB\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">\n    </a>\n    <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">\n        <img alt=\"Build\" src=\"https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">\n    </a>\n</p>\n<h4 align=\"center\">\n    <p>\n        <a href=#news>News</a> |\n        <a href=#installation>Installation</a> |\n        <a href=#quick-start>Quick Start</a> |\n        <a href=#community>Community</a> |\n        <a href=\"https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a> |\n        <a href=#model-list>Model List</a> |\n        <a href=\"#contributors\">Contributor</a> |\n        <a href=\"#citation\">Citation</a> |\n        <a href=\"#license\">License</a> \n    <p>\n</h4>\n\n\n[English](README.md) | [\u4e2d\u6587](https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md)\n\n\n\n## News\n\n- 3/6/2025: :fire::fire: Introduce **BGE-VL** ([HF repo](https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92)), State-Of-The-Art multimodal embedding models to support Any visual search applications (everything, including text-to-image, image-to-text, image&prompt-to-image, text-to-image&text, and more)! They are released under the MIT license and are completely free for both academic and commercial use. We also release **MegaPairs** ([repo](https://github.com/VectorSpaceLab/MegaPairs), [paper](https://arxiv.org/abs/2412.14475)), a massive synthetic dataset which empowers BGE-VL!\n- 12/5/2024: :book: We built the [BGE documentation](https://www.bge-model.com) for centralized BGE information and materials!\n- 10/29/2024: :earth_asia: We created WeChat group for BGE. Scan the [QR code](./imgs/BGE_WeChat_Group.png) to join the group chat! To get the first hand message about our updates and new release, or having any questions or ideas, join us now!\n- <img src=\"./imgs/BGE_WeChat_Group.png\" alt=\"bge_wechat_group\" class=\"center\" width=\"200\">\n\n- 10/22/2024: We release another interesting model: [OmniGen](https://github.com/VectorSpaceLab/OmniGen), which is a unified image generation model supporting various tasks. OmniGen can accomplish complex image generation tasks without the need for additional plugins like ControlNet, IP-Adapter, or auxiliary models such as pose detection and face detection.\n- 9/10/2024: Introducing **MemoRAG**, a step forward towards RAG 2.0 on top of memory-inspired knowledge discovery (repo: https://github.com/qhjqhj00/MemoRAG, paper: https://arxiv.org/pdf/2409.05591v1) \n- 9/2/2024: Start to maintain the [tutorials](./Tutorials/). The contents within will be actively updated and eariched, stay tuned! :books:\n- 7/26/2024: Release a new embedding model [bge-en-icl](https://huggingface.co/BAAI/bge-en-icl), an embedding model that incorporates in-context learning capabilities, which, by providing task-relevant query-response examples, can encode semantically richer queries, further enhancing the semantic representation ability of the embeddings.\n- 7/26/2024: Release a new embedding model [bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2), a multilingual embedding model based on gemma-2-9b, which supports multiple languages and diverse downstream tasks, achieving new SOTA on multilingual benchmarks (MIRACL, MTEB-fr, and MTEB-pl). \n- 7/26/2024: Release a new lightweight reranker [bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight), a lightweight reranker based on gemma-2-9b, which supports token compression and layerwise lightweight operations, can still ensure good performance while saving a significant amount of resources. :fire:\n\n<details>\n  <summary>More</summary>\n<!-- ### More -->\n\n- 6/7/2024: Release a new benchmark [MLVU](https://github.com/JUNJIE99/MLVU), the first comprehensive benchmark specifically designed for long video understanding. MLVU features an extensive range of video durations, a diverse collection of video sources, and a set of evaluation tasks uniquely tailored for long-form video understanding. :fire:\n- 5/21/2024: Release a new benchmark [AIR-Bench](https://github.com/AIR-Bench/AIR-Bench) together with Jina AI, Zilliz, HuggingFace, and other partners. AIR-Bench focuses on a fair out-of-distribution evaluation for Neural IR & RAG. It generates the synthetic data for benchmarking w.r.t. diverse domains and languages. It is dynamic and will be updated on regular basis. [Leaderboard](https://huggingface.co/spaces/AIR-Bench/leaderboard) :fire:\n- 4/30/2024: Release [Llama-3-8B-Instruct-80K-QLoRA](https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA), extending the context length of Llama-3-8B-Instruct from 8K to 80K via QLoRA training on a few synthesized long-context data. The model achieves remarkable performance on various long-context benchmarks. [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora) :fire:\n- 3/18/2024: Release new [rerankers](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker), built upon powerful M3 and LLM (GEMMA and MiniCPM, not so large actually :smiley:) backbones, supporitng multi-lingual processing and larger inputs, massive improvements of ranking performances on BEIR, C-MTEB/Retrieval, MIRACL, LlamaIndex Evaluation :fire:\n- 3/18/2024: Release [Visualized-BGE](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge), equipping BGE with visual capabilities. Visualized-BGE can be utilized to generate embeddings for hybrid image-text data. :fire:\n- 1/30/2024: Release **BGE-M3**, a new member to BGE model series! M3 stands for **M**ulti-linguality (100+ languages), **M**ulti-granularities (input length up to 8192), **M**ulti-Functionality (unification of dense, lexical, multi-vec/colbert retrieval). \nIt is the first embedding model which supports all three retrieval methods, achieving new SOTA on multi-lingual (MIRACL) and cross-lingual (MKQA) benchmarks.\n[Technical Report](https://arxiv.org/pdf/2402.03216.pdf) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3). :fire:\n- 1/9/2024: Release [Activation-Beacon](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon), an effective, efficient, compatible, and low-cost (training) method to extend the context length of LLM. [Technical Report](https://arxiv.org/abs/2401.03462) \n- 12/24/2023: Release **LLaRA**, a LLaMA-7B based dense retriever, leading to state-of-the-art performances on MS MARCO and BEIR. Model and code will be open-sourced. Please stay tuned. [Technical Report](https://arxiv.org/abs/2312.15503) and [Code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA)\n- 11/23/2023: Release [LM-Cocktail](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail), a method to maintain general capabilities during fine-tuning by merging multiple language models. [Technical Report](https://arxiv.org/abs/2311.13534) \n- 10/12/2023: Release [LLM-Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder), a unified embedding model to support diverse retrieval augmentation needs for LLMs. [Technical Report](https://arxiv.org/pdf/2310.07554.pdf)\n- 09/15/2023: The [technical report](https://arxiv.org/pdf/2309.07597.pdf) of BGE has been released \n- 09/15/2023: The [massive training data](https://data.baai.ac.cn/details/BAAI-MTP) of BGE has been released \n- 09/12/2023: New models: \n    - **New reranker model**: release cross-encoder models `BAAI/bge-reranker-base` and `BAAI/bge-reranker-large`, which are more powerful than embedding model. We recommend to use/fine-tune them to re-rank top-k documents returned by embedding models. \n    - **update embedding model**: release `bge-*-v1.5` embedding model to alleviate the issue of the similarity distribution, and enhance its retrieval ability without instruction.\n- 09/07/2023: Update [fine-tune code](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding): Add script to mine hard negatives and support adding instruction during fine-tuning. \n- 08/09/2023: BGE Models are integrated into **Langchain**, you can use it like [this](#using-langchain); C-MTEB **leaderboard** is [available](https://huggingface.co/spaces/mteb/leaderboard).  \n- 08/05/2023: Release base-scale and small-scale models, **best performance among the models of the same size \ud83e\udd17**  \n- 08/02/2023: Release `bge-large-*`(short for BAAI General Embedding) Models, **rank 1st on MTEB and C-MTEB benchmark!** :tada: :tada:   \n- 08/01/2023: We release the [Chinese Massive Text Embedding Benchmark](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB) (**C-MTEB**), consisting of 31 test dataset.  \n  \n\n</details>\n\n\nBGE (BAAI General Embedding) focuses on retrieval-augmented LLMs, consisting of the following projects currently:\n\n![projects](./imgs/projects.png)\n\n- **Inference**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker)\n- **Finetune**: [Embedder](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder), [Reranker](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker)\n- **[Evaluation](https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation)**\n- **[Dataset](https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset)**\n- **[Tutorials](https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials)**\n- **[research](https://github.com/FlagOpen/FlagEmbedding/tree/master/research)**\n\n\n## Installation\n### Using pip:\nIf you do not want to finetune the models, you can install the package without the finetune dependency:\n```\npip install -U FlagEmbedding\n```\nIf you want to finetune the models, you can install the package with the finetune dependency:\n```\npip install -U FlagEmbedding[finetune]\n```\n### Install from sources:\n\nClone the repository and install\n```\ngit clone https://github.com/FlagOpen/FlagEmbedding.git\ncd FlagEmbedding\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install  .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install  .[finetune]\n```\nFor development in editable mode:\n```\n# If you do not need to finetune the models, you can install the package without the finetune dependency:\npip install -e .\n# If you want to finetune the models, install the package with the finetune dependency:\n# pip install -e .[finetune]\n```\n\n## Quick Start\nFirst, load one of the BGE embedding model:\n```\nfrom FlagEmbedding import FlagAutoModel\n\nmodel = FlagAutoModel.from_finetuned('BAAI/bge-base-en-v1.5',\n                                      query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n                                      use_fp16=True)\n```\nThen, feed some sentences to the model and get their embeddings:\n```\nsentences_1 = [\"I love NLP\", \"I love machine learning\"]\nsentences_2 = [\"I love BGE\", \"I love text retrieval\"]\nembeddings_1 = model.encode(sentences_1)\nembeddings_2 = model.encode(sentences_2)\n```\nOnce we get the embeddings, we can compute similarity by inner product:\n```\nsimilarity = embeddings_1 @ embeddings_2.T\nprint(similarity)\n```\n\nFor more details, you can refer to [embedder inference](./examples/inference/embedder), [reranker inference](./examples/inference/reranker), [embedder finetune](./examples/finetune/embedder), [reranker fintune](./examples/finetune/reranker), [evaluation](./examples/evaluation).\n\nIf you're unfamiliar with any of related concepts, please check out the [tutorial](./Tutorials/). If it's not there, let us know.\n\nFor more interesting topics related to BGE, take a look at [research](./research).\n\n## Community\n\nWe are actively maintaining the community of BGE and FlagEmbedding. Let us know if you have any suggessions or ideas!\n\nCurrently we are updating the [tutorials](./Tutorials/), we aim to create a comprehensive and detailed tutorial for beginners on text retrieval and RAG. Stay tuned!\n\nThe following contents are releasing in the upcoming weeks:\n\n- Evaluation\n- BGE-EN-ICL\n\n<details>\n  <summary>The whole tutorial roadmap</summary>\n    <img src=\"./Tutorials/tutorial_map.png\"/>\n</details>\n\n## Model List\n\n`bge` is short for `BAAI general embedding`.\n\n| Model                                                                     | Language |                                                             Description                                                             |                                query instruction for retrieval                                 |\n|:--------------------------------------------------------------------------|:--------:|:-----------------------------------------------------------------------------------------------------------------------------------:|:----------------------------------------------------------------------------------------------:|\n| [BAAI/bge-en-icl](https://huggingface.co/BAAI/bge-en-icl) | English | A LLM-based embedding model with in-context learning capabilities, which can fully leverage the model's potential based on a few shot examples | Provide instructions and few-shot examples freely based on the given task. |\n| [BAAI/bge-multilingual-gemma2](https://huggingface.co/BAAI/bge-multilingual-gemma2) |    Multilingual     | A LLM-based multilingual embedding model, trained on a diverse range of languages and tasks. |        Provide instructions based on the given task.         |\n| [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)                   |    Multilingual     | Multi-Functionality(dense retrieval, sparse retrieval, multi-vector(colbert)), Multi-Linguality, and Multi-Granularity(8192 tokens) |  |\n| [LM-Cocktail](https://huggingface.co/Shitao)                   |   English |                     fine-tuned models (Llama and BGE) which can be used to reproduce the results of LM-Cocktail                     |  |\n| [BAAI/llm-embedder](https://huggingface.co/BAAI/llm-embedder)             |   English |                         a unified embedding model to support diverse retrieval augmentation needs for LLMs                          | See [README](https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder) |\n| [BAAI/bge-reranker-v2-m3](https://huggingface.co/BAAI/bge-reranker-v2-m3) | Multilingual | a lightweight cross-encoder model, possesses strong multilingual capabilities, easy to deploy, with fast inference. |  |\n| [BAAI/bge-reranker-v2-gemma](https://huggingface.co/BAAI/bge-reranker-v2-gemma) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English proficiency and multilingual capabilities. |  |\n| [BAAI/bge-reranker-v2-minicpm-layerwise](https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-v2.5-gemma2-lightweight](https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight) | Multilingual | a cross-encoder model which is suitable for multilingual contexts, performs well in both English and Chinese proficiency, allows freedom to select layers, compress ratio and compress layers for output, facilitating accelerated inference. |  |\n| [BAAI/bge-reranker-large](https://huggingface.co/BAAI/bge-reranker-large) |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-reranker-base](https://huggingface.co/BAAI/bge-reranker-base)   |   Chinese and English |                                   a cross-encoder model which is more accurate but less efficient                                   |                                                                                                |\n| [BAAI/bge-large-en-v1.5](https://huggingface.co/BAAI/bge-large-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en-v1.5](https://huggingface.co/BAAI/bge-base-en-v1.5)     |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en-v1.5](https://huggingface.co/BAAI/bge-small-en-v1.5)   |   English |                                      version 1.5 with more reasonable similarity distribution                                       |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh-v1.5](https://huggingface.co/BAAI/bge-large-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh-v1.5](https://huggingface.co/BAAI/bge-base-zh-v1.5)     |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh-v1.5](https://huggingface.co/BAAI/bge-small-zh-v1.5)   |   Chinese |                                      version 1.5 with more reasonable similarity distribution                                       |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-large-en](https://huggingface.co/BAAI/bge-large-en)             |   English |                                             Embedding Model which map text into vector                                              |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-base-en](https://huggingface.co/BAAI/bge-base-en)               |   English |                                    a base-scale model but with similar ability to `bge-large-en`                                    |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-small-en](https://huggingface.co/BAAI/bge-small-en)             |   English |                                        a small-scale model but with competitive performance                                         |                  `Represent this sentence for searching relevant passages: `                   |\n| [BAAI/bge-large-zh](https://huggingface.co/BAAI/bge-large-zh)             |   Chinese |                                             Embedding Model which map text into vector                                              |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-base-zh](https://huggingface.co/BAAI/bge-base-zh)               |   Chinese |                                    a base-scale model but with similar ability to `bge-large-zh`                                    |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n| [BAAI/bge-small-zh](https://huggingface.co/BAAI/bge-small-zh)             |   Chinese |                                        a small-scale model but with competitive performance                                         |                                     `\u4e3a\u8fd9\u4e2a\u53e5\u5b50\u751f\u6210\u8868\u793a\u4ee5\u7528\u4e8e\u68c0\u7d22\u76f8\u5173\u6587\u7ae0\uff1a`                                      |\n\n\n\n\n### Contributors:\nThank all our contributors for their efforts and warmly welcome new members to join in!\n\n<a href=\"https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\" />\n</a>\n\n\n\n\n## Citation\n\nIf you find this repository useful, please consider giving a star :star: and citation\n\n```\n@misc{bge_m3,\n  title={BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation},\n  author={Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},\n  year={2023},\n  eprint={2309.07597},\n  archivePrefix={arXiv},\n  primaryClass={cs.CL}\n}\n\n@misc{cocktail,\n      title={LM-Cocktail: Resilient Tuning of Language Models via Model Merging}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Xingrun Xing},\n      year={2023},\n      eprint={2311.13534},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n\n@misc{llm_embedder,\n      title={Retrieve Anything To Augment Large Language Models}, \n      author={Peitian Zhang and Shitao Xiao and Zheng Liu and Zhicheng Dou and Jian-Yun Nie},\n      year={2023},\n      eprint={2310.07554},\n      archivePrefix={arXiv},\n      primaryClass={cs.IR}\n}\n\n@misc{bge_embedding,\n      title={C-Pack: Packaged Resources To Advance General Chinese Embedding}, \n      author={Shitao Xiao and Zheng Liu and Peitian Zhang and Niklas Muennighoff},\n      year={2023},\n      eprint={2309.07597},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL}\n}\n```\n\n## License\nFlagEmbedding is licensed under the [MIT License](https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE). \n\n",
  "external_links_in_readme": [
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/evaluation",
    "https://arxiv.org/pdf/2310.07554.pdf",
    "https://huggingface.co/BAAI/bge-reranker-base",
    "https://arxiv.org/pdf/2409.05591v1",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset",
    "https://img.shields.io/badge/BGE_series-\ud83e\udd17-yellow\">",
    "https://huggingface.co/Shitao",
    "https://huggingface.co/BAAI/bge-base-en",
    "https://huggingface.co/BAAI/llm-embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/embedder",
    "https://huggingface.co/BAAI/bge-reranker-large",
    "https://img.shields.io/badge/Contribution-Welcome-blue\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/embedder",
    "https://huggingface.co/BAAI/bge-small-en-v1.5",
    "https://huggingface.co/BAAI/bge-m3",
    "https://github.com/VectorSpaceLab/OmniGen",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_embedder",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/visual_bge",
    "https://github.com/VectorSpaceLab/MegaPairs",
    "https://github.com/FlagOpen/FlagEmbedding.git",
    "https://huggingface.co/spaces/AIR-Bench/leaderboard",
    "https://github.com/AIR-Bench/AIR-Bench",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/inference/reranker",
    "https://github.com/JUNJIE99/MLVU",
    "https://github.com/qhjqhj00/MemoRAG,",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LM_Cocktail",
    "https://img.shields.io/badge/FlagEmbedding-1.3.0-red\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research",
    "https://huggingface.co/BAAI/bge-small-zh-v1.5",
    "https://arxiv.org/abs/2412.14475",
    "https://img.shields.io/badge/LICENSE-MIT-green\">",
    "https://github.com/FlagOpen/FlagEmbedding/graphs/contributors\">",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE\">",
    "https://huggingface.co/BAAI/bge-reranker-v2-gemma",
    "https://huggingface.co/spaces/mteb/leaderboard",
    "https://huggingface.co/BAAI/bge-reranker-v2-m3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/C_MTEB",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/llm_reranker",
    "https://arxiv.org/pdf/2402.03216.pdf",
    "https://contrib.rocks/image?repo=FlagOpen/FlagEmbedding\"",
    "https://huggingface.co/BAAI/bge-base-zh",
    "https://arxiv.org/abs/2311.13534",
    "https://huggingface.co/C-MTEB\">",
    "https://flagopen.baai.ac.cn/",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/baai_general_embedding",
    "https://arxiv.org/abs/2401.03462",
    "https://img.shields.io/badge/C_MTEB-\ud83e\udd17-yellow\">",
    "https://arxiv.org/pdf/2309.07597.pdf",
    "https://huggingface.co/collections/BAAI/bge-66797a74476eb1f085c7446d\">",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research\">Projects</a>",
    "https://huggingface.co/BAAI/bge-en-icl",
    "https://huggingface.co/BAAI/bge-reranker-v2.5-gemma2-lightweight",
    "https://www.bge-model.com",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/BGE_M3",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/activation_beacon",
    "https://huggingface.co/BAAI/bge-base-en-v1.5",
    "https://arxiv.org/abs/2312.15503",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/Tutorials",
    "https://huggingface.co/BAAI/bge-multilingual-gemma2",
    "https://github.com/FlagOpen/FlagEmbedding\">",
    "https://huggingface.co/BAAI/bge-small-zh",
    "https://huggingface.co/BAAI/bge-small-en",
    "https://huggingface.co/BAAI/bge-large-en-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/LICENSE",
    "https://data.baai.ac.cn/details/BAAI-MTP",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/examples/finetune/reranker",
    "https://huggingface.co/BAAI/bge-reranker-v2-minicpm-layerwise",
    "https://huggingface.co/collections/BAAI/megapairs-67c6bbe49c15a9e7a7c69d92",
    "https://huggingface.co/BAAI/bge-large-en",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/LLARA",
    "https://github.com/FlagOpen/FlagEmbedding/blob/master/README_zh.md",
    "https://huggingface.co/BAAI/bge-large-zh",
    "https://huggingface.co/BAAI/bge-base-zh-v1.5",
    "https://github.com/FlagOpen/FlagEmbedding/tree/master/research/Long_LLM/longllm_qlora",
    "https://huggingface.co/BAAI/bge-large-zh-v1.5",
    "https://huggingface.co/namespace-Pt/Llama-3-8B-Instruct-80K-QLoRA"
  ]
}
```

</details>


---

