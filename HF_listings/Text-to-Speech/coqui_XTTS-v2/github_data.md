# GitHub Data for coqui_XTTS-v2

**Task Category:** Text-to-Speech

## Repository 1: coqui-ai/TTS

# GitHub Repository Data

**Repository:** [coqui-ai/TTS](https://github.com/coqui-ai/TTS)

## Basic Information

- **Description:** 游냦游눫 - a deep learning toolkit for Text-to-Speech, battle-tested in research and production
- **Created:** 2020-05-20T15:45:28+00:00
- **Last Updated:** 2025-06-22T02:34:07+00:00
- **Last Pushed:** 2024-08-16T12:07:14+00:00
- **Default Branch:** dev
- **Size:** 170196 KB

## Statistics

- **Stars:** 40,876
- **Forks:** 5,285
- **Watchers:** 40,876
- **Open Issues:** 17
- **Total Issues:** 0
- **Pull Requests:** 759

## License

- **Type:** Mozilla Public License 2.0
- **SPDX ID:** MPL-2.0
- **URL:** [License](https://github.com/coqui-ai/TTS/blob/dev/LICENSE.txt)

## Languages

- **Python:** 2,992,534 bytes
- **Jupyter Notebook:** 244,647 bytes
- **HTML:** 8,448 bytes
- **Shell:** 4,288 bytes
- **Makefile:** 2,246 bytes
- **Cython:** 1,236 bytes
- **Dockerfile:** 1,015 bytes

## Topics

- `python`
- `text-to-speech`
- `deep-learning`
- `speech`
- `pytorch`
- `tts`
- `vocoder`
- `tacotron`
- `glow-tts`
- `melgan`
- `speaker-encoder`
- `hifigan`
- `speaker-encodings`
- `multi-speaker-tts`
- `tts-model`
- `speech-synthesis`
- `voice-cloning`
- `voice-synthesis`
- `voice-conversion`

## Top Contributors

1. **erogol** - 2463 contributions
2. **Edresson** - 304 contributions
3. **WeberJulian** - 171 contributions
4. **lexkoro** - 48 contributions
5. **twerkmeister** - 41 contributions
6. **reuben** - 39 contributions
7. **thorstenMueller** - 32 contributions
8. **kirianguiller** - 26 contributions
9. **gerazov** - 22 contributions
10. **thllwg** - 18 contributions

## File Structure (Sample of 10 files)

Total files: 870

- `.cardboardlint.yml` (blob)
- `.dockerignore` (blob)
- `.github` (tree)
- `.github/ISSUE_TEMPLATE` (tree)
- `.github/ISSUE_TEMPLATE/bug_report.yaml` (blob)
- `.github/ISSUE_TEMPLATE/config.yml` (blob)
- `.github/ISSUE_TEMPLATE/feature_request.md` (blob)
- `.github/PR_TEMPLATE.md` (blob)
- `.github/stale.yml` (blob)
- `.github/workflows` (tree)

## Recent Issues

- 游릭 **#4306** Add support for Malagasy language (open)
- 游릭 **#4303** git clone https://github.com/coqui-ai/TTS.git cd TTS pip install -r requirements.txt pip install . (open)
- 游릭 **#4300** README.md (open)
- 游릭 **#4299** [Feature request] (open)
- 游릭 **#4298** Hebrew diacritics normalization + support direct phoneme input via [[...]] blocks (open)

## Recent Pull Requests

- 游릭 **#4306** Add support for Malagasy language (open)
- 游릭 **#4300** README.md (open)
- 游릭 **#4298** Hebrew diacritics normalization + support direct phoneme input via [[...]] blocks (open)
- 游댮 **#4214** Fix loading fairseq model. (closed)
- 游댮 **#4194** Create PAQUI.tts (closed)

## Recent Commits

- **dbf1a08a** Update generic_utils.py (#3561) - Nick Potafiy (2024-02-10T14:20:58+00:00)
- **5dcc16d1** Bug fix in MP3 and FLAC compute length on TTSDataset (#3092) - Edresson Casanova (2023-12-27T16:23:43+00:00)
- **55c70637** Merge pull request #3423 from idiap/fix-aux-tests - Eren G칬lge (2023-12-14T17:00:30+00:00)
- **99fee6f5** build: use Trainer>=0.0.36 - Enno Hermann (2023-12-08T07:37:53+00:00)
- **186cafb3** Merge pull request #3412 from coqui-ai/reuben/docs-studio-refs - Eren G칬lge (2023-12-13T07:54:57+00:00)
- **3991d83b** Merge branch 'dev' into reuben/docs-studio-refs - Eren G칬lge (2023-12-13T07:53:43+00:00)
- **fa28f99f** Update to v0.22.0 - Eren G칬lge (2023-12-12T15:10:46+00:00)
- **8c1a8b52** Merge pull request #3405 from coqui-ai/studio_speakers - Eren G칬lge (2023-12-12T15:10:09+00:00)
- **0859e9f2** Remove Coqui Studio references - Reuben Morais (2023-12-12T15:09:57+00:00)
- **9f325b1f** fixup! Fix aux unit tests - Enno Hermann (2023-12-08T07:37:28+00:00)

## External Links Found in README

- https://arxiv.org/abs/2106.07889
- https://arxiv.org/abs/2005.05106
- https://zenodo.org/badge/latestdoi/265612440
- https://arxiv.org/abs/1910.11480
- http://[::1]:5002/
- https://readthedocs.org/projects/tts/badge/?version=latest&style=plastic>
- https://github.com/coqui-ai/TTS/releases
- https://github.com/coqui-ai/TTS/actions/workflows/inference_tests.yml/badge.svg
- https://github.com/coqui-ai/TTS/actions/workflows/docker.yaml/badge.svg
- https://arxiv.org/abs/1907.09006
- https://github.com/coqui-ai/TTS/actions/workflows/data_tests.yml/badge.svg
- https://arxiv.org/abs/2005.11129
- https://tts.readthedocs.io/en/latest/inference.html
- https://badge.fury.io/py/TTS
- https://arxiv.org/abs/1906.03402
- https://arxiv.org/abs/2010.05646
- https://github.com/coqui-ai/TTS/blob/master/CODE_OF_CONDUCT.md
- https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png"
- https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html
- https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png"

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 265612440,
  "name": "TTS",
  "full_name": "coqui-ai/TTS",
  "description": "\ud83d\udc38\ud83d\udcac - a deep learning toolkit for Text-to-Speech, battle-tested in research and production",
  "html_url": "https://github.com/coqui-ai/TTS",
  "clone_url": "https://github.com/coqui-ai/TTS.git",
  "ssh_url": "git@github.com:coqui-ai/TTS.git",
  "homepage": "http://coqui.ai",
  "topics": [
    "python",
    "text-to-speech",
    "deep-learning",
    "speech",
    "pytorch",
    "tts",
    "vocoder",
    "tacotron",
    "glow-tts",
    "melgan",
    "speaker-encoder",
    "hifigan",
    "speaker-encodings",
    "multi-speaker-tts",
    "tts-model",
    "speech-synthesis",
    "voice-cloning",
    "voice-synthesis",
    "voice-conversion"
  ],
  "default_branch": "dev",
  "created_at": "2020-05-20T15:45:28+00:00",
  "updated_at": "2025-06-22T02:34:07+00:00",
  "pushed_at": "2024-08-16T12:07:14+00:00",
  "size_kb": 170196,
  "watchers_count": 40876,
  "stargazers_count": 40876,
  "forks_count": 5285,
  "open_issues_count": 17,
  "license": {
    "key": "mpl-2.0",
    "name": "Mozilla Public License 2.0",
    "spdx_id": "MPL-2.0",
    "url": "https://github.com/coqui-ai/TTS/blob/dev/LICENSE.txt"
  },
  "languages": {
    "Python": 2992534,
    "Jupyter Notebook": 244647,
    "HTML": 8448,
    "Shell": 4288,
    "Makefile": 2246,
    "Cython": 1236,
    "Dockerfile": 1015
  },
  "top_contributors": [
    {
      "login": "erogol",
      "contributions": 2463
    },
    {
      "login": "Edresson",
      "contributions": 304
    },
    {
      "login": "WeberJulian",
      "contributions": 171
    },
    {
      "login": "lexkoro",
      "contributions": 48
    },
    {
      "login": "twerkmeister",
      "contributions": 41
    },
    {
      "login": "reuben",
      "contributions": 39
    },
    {
      "login": "thorstenMueller",
      "contributions": 32
    },
    {
      "login": "kirianguiller",
      "contributions": 26
    },
    {
      "login": "gerazov",
      "contributions": 22
    },
    {
      "login": "thllwg",
      "contributions": 18
    },
    {
      "login": "eginhard",
      "contributions": 14
    },
    {
      "login": "Mic92",
      "contributions": 13
    },
    {
      "login": "akx",
      "contributions": 13
    },
    {
      "login": "AyushExel",
      "contributions": 12
    },
    {
      "login": "kaiidams",
      "contributions": 12
    },
    {
      "login": "nmstoker",
      "contributions": 12
    },
    {
      "login": "sanjaesc",
      "contributions": 11
    },
    {
      "login": "p0p4k",
      "contributions": 10
    },
    {
      "login": "rishikksh20",
      "contributions": 8
    },
    {
      "login": "synesthesiam",
      "contributions": 8
    }
  ],
  "file_tree_count": 870,
  "file_tree_sample": [
    {
      "path": ".cardboardlint.yml",
      "type": "blob"
    },
    {
      "path": ".dockerignore",
      "type": "blob"
    },
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/bug_report.yaml",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/config.yml",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/feature_request.md",
      "type": "blob"
    },
    {
      "path": ".github/PR_TEMPLATE.md",
      "type": "blob"
    },
    {
      "path": ".github/stale.yml",
      "type": "blob"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    }
  ],
  "issues_count": 0,
  "pulls_count": 759,
  "recent_issues": [
    {
      "number": 4306,
      "title": "Add support for Malagasy language",
      "state": "open"
    },
    {
      "number": 4303,
      "title": "git clone https://github.com/coqui-ai/TTS.git cd TTS pip install -r requirements.txt pip install .",
      "state": "open"
    },
    {
      "number": 4300,
      "title": "README.md",
      "state": "open"
    },
    {
      "number": 4299,
      "title": "[Feature request]",
      "state": "open"
    },
    {
      "number": 4298,
      "title": "Hebrew diacritics normalization + support direct phoneme input via [[...]] blocks",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 4306,
      "title": "Add support for Malagasy language",
      "state": "open"
    },
    {
      "number": 4300,
      "title": "README.md",
      "state": "open"
    },
    {
      "number": 4298,
      "title": "Hebrew diacritics normalization + support direct phoneme input via [[...]] blocks",
      "state": "open"
    },
    {
      "number": 4214,
      "title": "Fix loading fairseq model.",
      "state": "closed"
    },
    {
      "number": 4194,
      "title": "Create PAQUI.tts",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "dbf1a08a0d4e47fdad6172e433eeb34bc6b13b4e",
      "author": "Nick Potafiy",
      "date": "2024-02-10T14:20:58+00:00",
      "message": "Update generic_utils.py (#3561)"
    },
    {
      "sha": "5dcc16d1931538e5bce7cb20c1986df371ee8cd6",
      "author": "Edresson Casanova",
      "date": "2023-12-27T16:23:43+00:00",
      "message": "Bug fix in MP3 and FLAC compute length on TTSDataset (#3092)"
    },
    {
      "sha": "55c7063724ec28085f9a61210e3dd3e34753c4d0",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-14T17:00:30+00:00",
      "message": "Merge pull request #3423 from idiap/fix-aux-tests"
    },
    {
      "sha": "99fee6f5ad9cdb990a6dab864662800ddd7838a3",
      "author": "Enno Hermann",
      "date": "2023-12-08T07:37:53+00:00",
      "message": "build: use Trainer>=0.0.36"
    },
    {
      "sha": "186cafb34cc2d533bc102e520bf5c7cebfc7e7c7",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-13T07:54:57+00:00",
      "message": "Merge pull request #3412 from coqui-ai/reuben/docs-studio-refs"
    },
    {
      "sha": "3991d83b2cf1b73eb872216179b9c0fb1d326c73",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-13T07:53:43+00:00",
      "message": "Merge branch 'dev' into reuben/docs-studio-refs"
    },
    {
      "sha": "fa28f99f1508b5b5366539b2149963edcb80ba62",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-12T15:10:46+00:00",
      "message": "Update to v0.22.0"
    },
    {
      "sha": "8c1a8b522b164470daf385820ecd1fb7c1522eb4",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-12T15:10:09+00:00",
      "message": "Merge pull request #3405 from coqui-ai/studio_speakers"
    },
    {
      "sha": "0859e9f25284d2d07096763b0783cda5b3f23b36",
      "author": "Reuben Morais",
      "date": "2023-12-12T15:09:57+00:00",
      "message": "Remove Coqui Studio references"
    },
    {
      "sha": "9f325b1f6c6dee00cbaab1bfaaea22efdbde25ce",
      "author": "Enno Hermann",
      "date": "2023-12-08T07:37:28+00:00",
      "message": "fixup! Fix aux unit tests"
    },
    {
      "sha": "fc099218df2f325490e5779b3b60b1126427afd8",
      "author": "Edresson Casanova",
      "date": "2023-12-06T20:47:33+00:00",
      "message": "Fix aux unit tests"
    },
    {
      "sha": "934b87bbd1d041cc4f7fcbb35144f47376d67414",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-12T12:51:26+00:00",
      "message": "Merge pull request #3391 from aaron-lii/multi-gpu"
    },
    {
      "sha": "b0fe0e678d7d4afe0c8d906b8b70e24c5669848f",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-12T12:50:59+00:00",
      "message": "Merge pull request #3392 from joelhoward0/fix_contributing_typo"
    },
    {
      "sha": "936084be7ee54ff7c25fae479f1565b01c8c11ee",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-12T12:50:27+00:00",
      "message": "Merge pull request #3404 from freds0/dev"
    },
    {
      "sha": "8e6a7cbfbf66aaab0b05971d7a2a7a9113a9448e",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-12T12:50:01+00:00",
      "message": "Update .models.json"
    },
    {
      "sha": "8999780aff330a6d4f5b0709268a647505e47762",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-12T12:30:21+00:00",
      "message": "Update test_models.py"
    },
    {
      "sha": "4dc0722bbc138d2183c47b6548214df0fabd66c9",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-12T12:28:16+00:00",
      "message": "Update .models.json"
    },
    {
      "sha": "4b33699b415752748a77de45ae4fa93e7aaf0399",
      "author": "Edresson Casanova",
      "date": "2023-12-12T12:22:07+00:00",
      "message": "Update docs"
    },
    {
      "sha": "b6e1ac66d94958a00c8f116b25128cf273c44de8",
      "author": "Edresson Casanova",
      "date": "2023-12-12T12:19:56+00:00",
      "message": "Add docs"
    },
    {
      "sha": "61b67ef16ff2bfdc7533f380226c33d7c55105f9",
      "author": "WeberJulian",
      "date": "2023-12-11T22:58:52+00:00",
      "message": "Fix read_json_with_comments"
    }
  ],
  "readme_text": "\n## \ud83d\udc38Coqui.ai News\n- \ud83d\udce3 \u24cdTTSv2 is here with 16 languages and better performance across the board.\n- \ud83d\udce3 \u24cdTTS fine-tuning code is out. Check the [example recipes](https://github.com/coqui-ai/TTS/tree/dev/recipes/ljspeech).\n- \ud83d\udce3 \u24cdTTS can now stream with <200ms latency.\n- \ud83d\udce3 \u24cdTTS, our production TTS model that can speak 13 languages, is released [Blog Post](https://coqui.ai/blog/tts/open_xtts), [Demo](https://huggingface.co/spaces/coqui/xtts), [Docs](https://tts.readthedocs.io/en/dev/models/xtts.html)\n- \ud83d\udce3 [\ud83d\udc36Bark](https://github.com/suno-ai/bark) is now available for inference with unconstrained voice cloning. [Docs](https://tts.readthedocs.io/en/dev/models/bark.html)\n- \ud83d\udce3 You can use [~1100 Fairseq models](https://github.com/facebookresearch/fairseq/tree/main/examples/mms) with \ud83d\udc38TTS.\n- \ud83d\udce3 \ud83d\udc38TTS now supports \ud83d\udc22Tortoise with faster inference. [Docs](https://tts.readthedocs.io/en/dev/models/tortoise.html)\n\n<div align=\"center\">\n<img src=\"https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2\" />\n\n## <img src=\"https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png\" height=\"56\"/>\n\n\n**\ud83d\udc38TTS is a library for advanced Text-to-Speech generation.**\n\n\ud83d\ude80 Pretrained models in +1100 languages.\n\n\ud83d\udee0\ufe0f Tools for training new models and fine-tuning existing models in any language.\n\n\ud83d\udcda Utilities for dataset analysis and curation.\n______________________________________________________________________\n\n[![Discord](https://img.shields.io/discord/1037326658807533628?color=%239B59B6&label=chat%20on%20discord)](https://discord.gg/5eXr5seRrv)\n[![License](<https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg>)](https://opensource.org/licenses/MPL-2.0)\n[![PyPI version](https://badge.fury.io/py/TTS.svg)](https://badge.fury.io/py/TTS)\n[![Covenant](https://camo.githubusercontent.com/7d620efaa3eac1c5b060ece5d6aacfcc8b81a74a04d05cd0398689c01c4463bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e7472696275746f72253230436f76656e616e742d76322e3025323061646f707465642d6666363962342e737667)](https://github.com/coqui-ai/TTS/blob/master/CODE_OF_CONDUCT.md)\n[![Downloads](https://pepy.tech/badge/tts)](https://pepy.tech/project/tts)\n[![DOI](https://zenodo.org/badge/265612440.svg)](https://zenodo.org/badge/latestdoi/265612440)\n\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/aux_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/data_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/docker.yaml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/inference_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/style_check.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/text_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/tts_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/vocoder_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests0.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests1.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests2.yml/badge.svg)\n[![Docs](<https://readthedocs.org/projects/tts/badge/?version=latest&style=plastic>)](https://tts.readthedocs.io/en/latest/)\n\n</div>\n\n______________________________________________________________________\n\n## \ud83d\udcac Where to ask questions\nPlease use our dedicated channels for questions and discussion. Help is much more valuable if it's shared publicly so that more people can benefit from it.\n\n| Type                            | Platforms                               |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udea8 **Bug Reports**              | [GitHub Issue Tracker]                  |\n| \ud83c\udf81 **Feature Requests & Ideas** | [GitHub Issue Tracker]                  |\n| \ud83d\udc69\u200d\ud83d\udcbb **Usage Questions**          | [GitHub Discussions]                    |\n| \ud83d\uddef **General Discussion**       | [GitHub Discussions] or [Discord]   |\n\n[github issue tracker]: https://github.com/coqui-ai/tts/issues\n[github discussions]: https://github.com/coqui-ai/TTS/discussions\n[discord]: https://discord.gg/5eXr5seRrv\n[Tutorials and Examples]: https://github.com/coqui-ai/TTS/wiki/TTS-Notebooks-and-Tutorials\n\n\n## \ud83d\udd17 Links and Resources\n| Type                            | Links                               |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udcbc **Documentation**              | [ReadTheDocs](https://tts.readthedocs.io/en/latest/)\n| \ud83d\udcbe **Installation**               | [TTS/README.md](https://github.com/coqui-ai/TTS/tree/dev#installation)|\n| \ud83d\udc69\u200d\ud83d\udcbb **Contributing**               | [CONTRIBUTING.md](https://github.com/coqui-ai/TTS/blob/main/CONTRIBUTING.md)|\n| \ud83d\udccc **Road Map**                   | [Main Development Plans](https://github.com/coqui-ai/TTS/issues/378)\n| \ud83d\ude80 **Released Models**            | [TTS Releases](https://github.com/coqui-ai/TTS/releases) and [Experimental Models](https://github.com/coqui-ai/TTS/wiki/Experimental-Released-Models)|\n| \ud83d\udcf0 **Papers**                    | [TTS Papers](https://github.com/erogol/TTS-papers)|\n\n\n## \ud83e\udd47 TTS Performance\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png\" width=\"800\" /></p>\n\nUnderlined \"TTS*\" and \"Judy*\" are **internal** \ud83d\udc38TTS models that are not released open-source. They are here to show the potential. Models prefixed with a dot (.Jofish .Abe and .Janice) are real human voices.\n\n## Features\n- High-performance Deep Learning models for Text2Speech tasks.\n    - Text2Spec models (Tacotron, Tacotron2, Glow-TTS, SpeedySpeech).\n    - Speaker Encoder to compute speaker embeddings efficiently.\n    - Vocoder models (MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN)\n- Fast and efficient model training.\n- Detailed training logs on the terminal and Tensorboard.\n- Support for Multi-speaker TTS.\n- Efficient, flexible, lightweight but feature complete `Trainer API`.\n- Released and ready-to-use models.\n- Tools to curate Text2Speech datasets under```dataset_analysis```.\n- Utilities to use and test your models.\n- Modular (but not too much) code base enabling easy implementation of new ideas.\n\n## Model Implementations\n### Spectrogram models\n- Tacotron: [paper](https://arxiv.org/abs/1703.10135)\n- Tacotron2: [paper](https://arxiv.org/abs/1712.05884)\n- Glow-TTS: [paper](https://arxiv.org/abs/2005.11129)\n- Speedy-Speech: [paper](https://arxiv.org/abs/2008.03802)\n- Align-TTS: [paper](https://arxiv.org/abs/2003.01950)\n- FastPitch: [paper](https://arxiv.org/pdf/2006.06873.pdf)\n- FastSpeech: [paper](https://arxiv.org/abs/1905.09263)\n- FastSpeech2: [paper](https://arxiv.org/abs/2006.04558)\n- SC-GlowTTS: [paper](https://arxiv.org/abs/2104.05557)\n- Capacitron: [paper](https://arxiv.org/abs/1906.03402)\n- OverFlow: [paper](https://arxiv.org/abs/2211.06892)\n- Neural HMM TTS: [paper](https://arxiv.org/abs/2108.13320)\n- Delightful TTS: [paper](https://arxiv.org/abs/2110.12612)\n\n### End-to-End Models\n- \u24cdTTS: [blog](https://coqui.ai/blog/tts/open_xtts)\n- VITS: [paper](https://arxiv.org/pdf/2106.06103)\n- \ud83d\udc38 YourTTS: [paper](https://arxiv.org/abs/2112.02418)\n- \ud83d\udc22 Tortoise: [orig. repo](https://github.com/neonbjb/tortoise-tts)\n- \ud83d\udc36 Bark: [orig. repo](https://github.com/suno-ai/bark)\n\n### Attention Methods\n- Guided Attention: [paper](https://arxiv.org/abs/1710.08969)\n- Forward Backward Decoding: [paper](https://arxiv.org/abs/1907.09006)\n- Graves Attention: [paper](https://arxiv.org/abs/1910.10288)\n- Double Decoder Consistency: [blog](https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/)\n- Dynamic Convolutional Attention: [paper](https://arxiv.org/pdf/1910.10288.pdf)\n- Alignment Network: [paper](https://arxiv.org/abs/2108.10447)\n\n### Speaker Encoder\n- GE2E: [paper](https://arxiv.org/abs/1710.10467)\n- Angular Loss: [paper](https://arxiv.org/pdf/2003.11982.pdf)\n\n### Vocoders\n- MelGAN: [paper](https://arxiv.org/abs/1910.06711)\n- MultiBandMelGAN: [paper](https://arxiv.org/abs/2005.05106)\n- ParallelWaveGAN: [paper](https://arxiv.org/abs/1910.11480)\n- GAN-TTS discriminators: [paper](https://arxiv.org/abs/1909.11646)\n- WaveRNN: [origin](https://github.com/fatchord/WaveRNN/)\n- WaveGrad: [paper](https://arxiv.org/abs/2009.00713)\n- HiFiGAN: [paper](https://arxiv.org/abs/2010.05646)\n- UnivNet: [paper](https://arxiv.org/abs/2106.07889)\n\n### Voice Conversion\n- FreeVC: [paper](https://arxiv.org/abs/2210.15418)\n\nYou can also help us implement more models.\n\n## Installation\n\ud83d\udc38TTS is tested on Ubuntu 18.04 with **python >= 3.9, < 3.12.**.\n\nIf you are only interested in [synthesizing speech](https://tts.readthedocs.io/en/latest/inference.html) with the released \ud83d\udc38TTS models, installing from PyPI is the easiest option.\n\n```bash\npip install TTS\n```\n\nIf you plan to code or train models, clone \ud83d\udc38TTS and install it locally.\n\n```bash\ngit clone https://github.com/coqui-ai/TTS\npip install -e .[all,dev,notebooks]  # Select the relevant extras\n```\n\nIf you are on Ubuntu (Debian), you can also run following commands for installation.\n\n```bash\n$ make system-deps  # intended to be used on Ubuntu (Debian). Let us know if you have a different OS.\n$ make install\n```\n\nIf you are on Windows, \ud83d\udc51@GuyPaddock wrote installation instructions [here](https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system).\n\n\n## Docker Image\nYou can also try TTS without install with the docker image.\nSimply run the following command and you will be able to run TTS without installing it.\n\n```bash\ndocker run --rm -it -p 5002:5002 --entrypoint /bin/bash ghcr.io/coqui-ai/tts-cpu\npython3 TTS/server/server.py --list_models #To get the list of available models\npython3 TTS/server/server.py --model_name tts_models/en/vctk/vits # To start a server\n```\n\nYou can then enjoy the TTS server [here](http://[::1]:5002/)\nMore details about the docker images (like GPU support) can be found [here](https://tts.readthedocs.io/en/latest/docker_images.html)\n\n\n## Synthesizing speech by \ud83d\udc38TTS\n\n### \ud83d\udc0d Python API\n\n#### Running a multi-speaker and multi-lingual model\n\n```python\nimport torch\nfrom TTS.api import TTS\n\n# Get device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# List available \ud83d\udc38TTS models\nprint(TTS().list_models())\n\n# Init TTS\ntts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n\n# Run TTS\n# \u2757 Since this model is multi-lingual voice cloning model, we must set the target speaker_wav and language\n# Text to speech list of amplitude values as output\nwav = tts.tts(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\")\n# Text to speech to a file\ntts.tts_to_file(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")\n```\n\n#### Running a single speaker model\n\n```python\n# Init TTS with the target model name\ntts = TTS(model_name=\"tts_models/de/thorsten/tacotron2-DDC\", progress_bar=False).to(device)\n\n# Run TTS\ntts.tts_to_file(text=\"Ich bin eine Testnachricht.\", file_path=OUTPUT_PATH)\n\n# Example voice cloning with YourTTS in English, French and Portuguese\ntts = TTS(model_name=\"tts_models/multilingual/multi-dataset/your_tts\", progress_bar=False).to(device)\ntts.tts_to_file(\"This is voice cloning.\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")\ntts.tts_to_file(\"C'est le clonage de la voix.\", speaker_wav=\"my/cloning/audio.wav\", language=\"fr-fr\", file_path=\"output.wav\")\ntts.tts_to_file(\"Isso \u00e9 clonagem de voz.\", speaker_wav=\"my/cloning/audio.wav\", language=\"pt-br\", file_path=\"output.wav\")\n```\n\n#### Example voice conversion\n\nConverting the voice in `source_wav` to the voice of `target_wav`\n\n```python\ntts = TTS(model_name=\"voice_conversion_models/multilingual/vctk/freevc24\", progress_bar=False).to(\"cuda\")\ntts.voice_conversion_to_file(source_wav=\"my/source.wav\", target_wav=\"my/target.wav\", file_path=\"output.wav\")\n```\n\n#### Example voice cloning together with the voice conversion model.\nThis way, you can clone voices by using any model in \ud83d\udc38TTS.\n\n```python\n\ntts = TTS(\"tts_models/de/thorsten/tacotron2-DDC\")\ntts.tts_with_vc_to_file(\n    \"Wie sage ich auf Italienisch, dass ich dich liebe?\",\n    speaker_wav=\"target/speaker.wav\",\n    file_path=\"output.wav\"\n)\n```\n\n#### Example text to speech using **Fairseq models in ~1100 languages** \ud83e\udd2f.\nFor Fairseq models, use the following name format: `tts_models/<lang-iso_code>/fairseq/vits`.\nYou can find the language ISO codes [here](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)\nand learn about the Fairseq models [here](https://github.com/facebookresearch/fairseq/tree/main/examples/mms).\n\n```python\n# TTS with on the fly voice conversion\napi = TTS(\"tts_models/deu/fairseq/vits\")\napi.tts_with_vc_to_file(\n    \"Wie sage ich auf Italienisch, dass ich dich liebe?\",\n    speaker_wav=\"target/speaker.wav\",\n    file_path=\"output.wav\"\n)\n```\n\n### Command-line `tts`\n\n<!-- begin-tts-readme -->\n\nSynthesize speech on command line.\n\nYou can either use your trained model or choose a model from the provided list.\n\nIf you don't specify any models, then it uses LJSpeech based English model.\n\n#### Single Speaker Models\n\n- List provided models:\n\n  ```\n  $ tts --list_models\n  ```\n\n- Get model info (for both tts_models and vocoder_models):\n\n  - Query by type/name:\n    The model_info_by_name uses the name as it from the --list_models.\n    ```\n    $ tts --model_info_by_name \"<model_type>/<language>/<dataset>/<model_name>\"\n    ```\n    For example:\n    ```\n    $ tts --model_info_by_name tts_models/tr/common-voice/glow-tts\n    $ tts --model_info_by_name vocoder_models/en/ljspeech/hifigan_v2\n    ```\n  - Query by type/idx:\n    The model_query_idx uses the corresponding idx from --list_models.\n\n    ```\n    $ tts --model_info_by_idx \"<model_type>/<model_query_idx>\"\n    ```\n\n    For example:\n\n    ```\n    $ tts --model_info_by_idx tts_models/3\n    ```\n\n  - Query info for model info by full name:\n    ```\n    $ tts --model_info_by_name \"<model_type>/<language>/<dataset>/<model_name>\"\n    ```\n\n- Run TTS with default models:\n\n  ```\n  $ tts --text \"Text for TTS\" --out_path output/path/speech.wav\n  ```\n\n- Run TTS and pipe out the generated TTS wav file data:\n\n  ```\n  $ tts --text \"Text for TTS\" --pipe_out --out_path output/path/speech.wav | aplay\n  ```\n\n- Run a TTS model with its default vocoder model:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"<model_type>/<language>/<dataset>/<model_name>\" --out_path output/path/speech.wav\n  ```\n\n  For example:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"tts_models/en/ljspeech/glow-tts\" --out_path output/path/speech.wav\n  ```\n\n- Run with specific TTS and vocoder models from the list:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"<model_type>/<language>/<dataset>/<model_name>\" --vocoder_name \"<model_type>/<language>/<dataset>/<model_name>\" --out_path output/path/speech.wav\n  ```\n\n  For example:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"tts_models/en/ljspeech/glow-tts\" --vocoder_name \"vocoder_models/en/ljspeech/univnet\" --out_path output/path/speech.wav\n  ```\n\n- Run your own TTS model (Using Griffin-Lim Vocoder):\n\n  ```\n  $ tts --text \"Text for TTS\" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n  ```\n\n- Run your own TTS and Vocoder models:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n      --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json\n  ```\n\n#### Multi-speaker Models\n\n- List the available speakers and choose a <speaker_id> among them:\n\n  ```\n  $ tts --model_name \"<language>/<dataset>/<model_name>\"  --list_speaker_idxs\n  ```\n\n- Run the multi-speaker TTS model with the target speaker ID:\n\n  ```\n  $ tts --text \"Text for TTS.\" --out_path output/path/speech.wav --model_name \"<language>/<dataset>/<model_name>\"  --speaker_idx <speaker_id>\n  ```\n\n- Run your own multi-speaker TTS model:\n\n  ```\n  $ tts --text \"Text for TTS\" --out_path output/path/speech.wav --model_path path/to/model.pth --config_path path/to/config.json --speakers_file_path path/to/speaker.json --speaker_idx <speaker_id>\n  ```\n\n### Voice Conversion Models\n\n```\n$ tts --out_path output/path/speech.wav --model_name \"<language>/<dataset>/<model_name>\" --source_wav <path/to/speaker/wav> --target_wav <path/to/reference/wav>\n```\n\n<!-- end-tts-readme -->\n\n## Directory Structure\n```\n|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)\n|- utils/           (common utilities.)\n|- TTS\n    |- bin/             (folder for all the executables.)\n      |- train*.py                  (train your target model.)\n      |- ...\n    |- tts/             (text to speech models)\n        |- layers/          (model layer definitions)\n        |- models/          (model definitions)\n        |- utils/           (model specific utilities.)\n    |- speaker_encoder/ (Speaker Encoder models.)\n        |- (same)\n    |- vocoder/         (Vocoder models.)\n        |- (same)\n```\n",
  "external_links_in_readme": [
    "https://arxiv.org/abs/2106.07889",
    "https://arxiv.org/abs/2005.05106",
    "https://zenodo.org/badge/latestdoi/265612440",
    "https://arxiv.org/abs/1910.11480",
    "http://[::1]:5002/",
    "https://readthedocs.org/projects/tts/badge/?version=latest&style=plastic>",
    "https://github.com/coqui-ai/TTS/releases",
    "https://github.com/coqui-ai/TTS/actions/workflows/inference_tests.yml/badge.svg",
    "https://github.com/coqui-ai/TTS/actions/workflows/docker.yaml/badge.svg",
    "https://arxiv.org/abs/1907.09006",
    "https://github.com/coqui-ai/TTS/actions/workflows/data_tests.yml/badge.svg",
    "https://arxiv.org/abs/2005.11129",
    "https://tts.readthedocs.io/en/latest/inference.html",
    "https://badge.fury.io/py/TTS",
    "https://arxiv.org/abs/1906.03402",
    "https://arxiv.org/abs/2010.05646",
    "https://github.com/coqui-ai/TTS/blob/master/CODE_OF_CONDUCT.md",
    "https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png\"",
    "https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html",
    "https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png\"",
    "https://github.com/coqui-ai/TTS/actions/workflows/aux_tests.yml/badge.svg",
    "https://arxiv.org/abs/2108.10447",
    "https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg>",
    "https://github.com/coqui-ai/TTS/wiki/Experimental-Released-Models",
    "https://arxiv.org/abs/2211.06892",
    "https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests1.yml/badge.svg",
    "https://github.com/coqui-ai/TTS/actions/workflows/style_check.yml/badge.svg",
    "https://arxiv.org/pdf/2106.06103",
    "https://arxiv.org/abs/2104.05557",
    "https://arxiv.org/abs/1703.10135",
    "https://tts.readthedocs.io/en/dev/models/bark.html",
    "https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests2.yml/badge.svg",
    "https://github.com/coqui-ai/TTS/discussions",
    "https://arxiv.org/abs/2003.01950",
    "https://github.com/coqui-ai/TTS",
    "https://github.com/facebookresearch/fairseq/tree/main/examples/mms",
    "https://arxiv.org/abs/1710.10467",
    "https://arxiv.org/abs/1905.09263",
    "https://github.com/fatchord/WaveRNN/",
    "https://tts.readthedocs.io/en/latest/",
    "https://arxiv.org/abs/2009.00713",
    "https://arxiv.org/abs/1712.05884",
    "https://badge.fury.io/py/TTS.svg",
    "https://arxiv.org/abs/2110.12612",
    "https://github.com/coqui-ai/TTS/actions/workflows/vocoder_tests.yml/badge.svg",
    "https://tts.readthedocs.io/en/latest/docker_images.html",
    "https://arxiv.org/abs/2008.03802",
    "https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system",
    "https://github.com/suno-ai/bark",
    "https://github.com/erogol/TTS-papers",
    "https://arxiv.org/abs/1909.11646",
    "https://github.com/coqui-ai/TTS/blob/main/CONTRIBUTING.md",
    "https://arxiv.org/pdf/2006.06873.pdf",
    "https://arxiv.org/abs/1710.08969",
    "https://github.com/neonbjb/tortoise-tts",
    "https://arxiv.org/abs/2112.02418",
    "https://arxiv.org/abs/2210.15418",
    "https://huggingface.co/spaces/coqui/xtts",
    "https://arxiv.org/abs/2108.13320",
    "https://tts.readthedocs.io/en/dev/models/tortoise.html",
    "https://github.com/coqui-ai/TTS/issues/378",
    "https://discord.gg/5eXr5seRrv",
    "https://camo.githubusercontent.com/7d620efaa3eac1c5b060ece5d6aacfcc8b81a74a04d05cd0398689c01c4463bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e7472696275746f72253230436f76656e616e742d76322e3025323061646f707465642d6666363962342e737667",
    "https://arxiv.org/abs/1910.06711",
    "https://pepy.tech/badge/tts",
    "https://img.shields.io/discord/1037326658807533628?color=%239B59B6&label=chat%20on%20discord",
    "https://coqui.ai/blog/tts/open_xtts",
    "https://github.com/coqui-ai/TTS/actions/workflows/tts_tests.yml/badge.svg",
    "https://github.com/coqui-ai/TTS/actions/workflows/text_tests.yml/badge.svg",
    "https://arxiv.org/abs/1910.10288",
    "https://github.com/coqui-ai/TTS/wiki/TTS-Notebooks-and-Tutorials",
    "https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests0.yml/badge.svg",
    "https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/",
    "https://arxiv.org/pdf/1910.10288.pdf",
    "https://github.com/coqui-ai/tts/issues",
    "https://github.com/coqui-ai/TTS/tree/dev/recipes/ljspeech",
    "https://zenodo.org/badge/265612440.svg",
    "https://github.com/coqui-ai/TTS/tree/dev#installation",
    "https://tts.readthedocs.io/en/dev/models/xtts.html",
    "https://arxiv.org/pdf/2003.11982.pdf",
    "https://pepy.tech/project/tts",
    "https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2\"",
    "https://arxiv.org/abs/2006.04558",
    "https://opensource.org/licenses/MPL-2.0"
  ]
}
```

</details>


---

## Repository 2: coqui-ai/TTS

# GitHub Repository Data

**Repository:** [coqui-ai/TTS](https://github.com/coqui-ai/TTS)

## Basic Information

- **Description:** 游냦游눫 - a deep learning toolkit for Text-to-Speech, battle-tested in research and production
- **Created:** 2020-05-20T15:45:28+00:00
- **Last Updated:** 2025-06-22T02:34:07+00:00
- **Last Pushed:** 2024-08-16T12:07:14+00:00
- **Default Branch:** dev
- **Size:** 170196 KB

## Statistics

- **Stars:** 40,876
- **Forks:** 5,285
- **Watchers:** 40,876
- **Open Issues:** 17
- **Total Issues:** 0
- **Pull Requests:** 759

## License

- **Type:** Mozilla Public License 2.0
- **SPDX ID:** MPL-2.0
- **URL:** [License](https://github.com/coqui-ai/TTS/blob/dev/LICENSE.txt)

## Languages

- **Python:** 2,992,534 bytes
- **Jupyter Notebook:** 244,647 bytes
- **HTML:** 8,448 bytes
- **Shell:** 4,288 bytes
- **Makefile:** 2,246 bytes
- **Cython:** 1,236 bytes
- **Dockerfile:** 1,015 bytes

## Topics

- `python`
- `text-to-speech`
- `deep-learning`
- `speech`
- `pytorch`
- `tts`
- `vocoder`
- `tacotron`
- `glow-tts`
- `melgan`
- `speaker-encoder`
- `hifigan`
- `speaker-encodings`
- `multi-speaker-tts`
- `tts-model`
- `speech-synthesis`
- `voice-cloning`
- `voice-synthesis`
- `voice-conversion`

## Top Contributors

1. **erogol** - 2463 contributions
2. **Edresson** - 304 contributions
3. **WeberJulian** - 171 contributions
4. **lexkoro** - 48 contributions
5. **twerkmeister** - 41 contributions
6. **reuben** - 39 contributions
7. **thorstenMueller** - 32 contributions
8. **kirianguiller** - 26 contributions
9. **gerazov** - 22 contributions
10. **thllwg** - 18 contributions

## File Structure (Sample of 10 files)

Total files: 870

- `.cardboardlint.yml` (blob)
- `.dockerignore` (blob)
- `.github` (tree)
- `.github/ISSUE_TEMPLATE` (tree)
- `.github/ISSUE_TEMPLATE/bug_report.yaml` (blob)
- `.github/ISSUE_TEMPLATE/config.yml` (blob)
- `.github/ISSUE_TEMPLATE/feature_request.md` (blob)
- `.github/PR_TEMPLATE.md` (blob)
- `.github/stale.yml` (blob)
- `.github/workflows` (tree)

## Recent Issues

- 游릭 **#4306** Add support for Malagasy language (open)
- 游릭 **#4303** git clone https://github.com/coqui-ai/TTS.git cd TTS pip install -r requirements.txt pip install . (open)
- 游릭 **#4300** README.md (open)
- 游릭 **#4299** [Feature request] (open)
- 游릭 **#4298** Hebrew diacritics normalization + support direct phoneme input via [[...]] blocks (open)

## Recent Pull Requests

- 游릭 **#4306** Add support for Malagasy language (open)
- 游릭 **#4300** README.md (open)
- 游릭 **#4298** Hebrew diacritics normalization + support direct phoneme input via [[...]] blocks (open)
- 游댮 **#4214** Fix loading fairseq model. (closed)
- 游댮 **#4194** Create PAQUI.tts (closed)

## Recent Commits

- **dbf1a08a** Update generic_utils.py (#3561) - Nick Potafiy (2024-02-10T14:20:58+00:00)
- **5dcc16d1** Bug fix in MP3 and FLAC compute length on TTSDataset (#3092) - Edresson Casanova (2023-12-27T16:23:43+00:00)
- **55c70637** Merge pull request #3423 from idiap/fix-aux-tests - Eren G칬lge (2023-12-14T17:00:30+00:00)
- **99fee6f5** build: use Trainer>=0.0.36 - Enno Hermann (2023-12-08T07:37:53+00:00)
- **186cafb3** Merge pull request #3412 from coqui-ai/reuben/docs-studio-refs - Eren G칬lge (2023-12-13T07:54:57+00:00)
- **3991d83b** Merge branch 'dev' into reuben/docs-studio-refs - Eren G칬lge (2023-12-13T07:53:43+00:00)
- **fa28f99f** Update to v0.22.0 - Eren G칬lge (2023-12-12T15:10:46+00:00)
- **8c1a8b52** Merge pull request #3405 from coqui-ai/studio_speakers - Eren G칬lge (2023-12-12T15:10:09+00:00)
- **0859e9f2** Remove Coqui Studio references - Reuben Morais (2023-12-12T15:09:57+00:00)
- **9f325b1f** fixup! Fix aux unit tests - Enno Hermann (2023-12-08T07:37:28+00:00)

## External Links Found in README

- https://arxiv.org/abs/2106.07889
- https://arxiv.org/abs/2005.05106
- https://zenodo.org/badge/latestdoi/265612440
- https://arxiv.org/abs/1910.11480
- http://[::1]:5002/
- https://readthedocs.org/projects/tts/badge/?version=latest&style=plastic>
- https://github.com/coqui-ai/TTS/releases
- https://github.com/coqui-ai/TTS/actions/workflows/inference_tests.yml/badge.svg
- https://github.com/coqui-ai/TTS/actions/workflows/docker.yaml/badge.svg
- https://arxiv.org/abs/1907.09006
- https://github.com/coqui-ai/TTS/actions/workflows/data_tests.yml/badge.svg
- https://arxiv.org/abs/2005.11129
- https://tts.readthedocs.io/en/latest/inference.html
- https://badge.fury.io/py/TTS
- https://arxiv.org/abs/1906.03402
- https://arxiv.org/abs/2010.05646
- https://github.com/coqui-ai/TTS/blob/master/CODE_OF_CONDUCT.md
- https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png"
- https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html
- https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png"

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 265612440,
  "name": "TTS",
  "full_name": "coqui-ai/TTS",
  "description": "\ud83d\udc38\ud83d\udcac - a deep learning toolkit for Text-to-Speech, battle-tested in research and production",
  "html_url": "https://github.com/coqui-ai/TTS",
  "clone_url": "https://github.com/coqui-ai/TTS.git",
  "ssh_url": "git@github.com:coqui-ai/TTS.git",
  "homepage": "http://coqui.ai",
  "topics": [
    "python",
    "text-to-speech",
    "deep-learning",
    "speech",
    "pytorch",
    "tts",
    "vocoder",
    "tacotron",
    "glow-tts",
    "melgan",
    "speaker-encoder",
    "hifigan",
    "speaker-encodings",
    "multi-speaker-tts",
    "tts-model",
    "speech-synthesis",
    "voice-cloning",
    "voice-synthesis",
    "voice-conversion"
  ],
  "default_branch": "dev",
  "created_at": "2020-05-20T15:45:28+00:00",
  "updated_at": "2025-06-22T02:34:07+00:00",
  "pushed_at": "2024-08-16T12:07:14+00:00",
  "size_kb": 170196,
  "watchers_count": 40876,
  "stargazers_count": 40876,
  "forks_count": 5285,
  "open_issues_count": 17,
  "license": {
    "key": "mpl-2.0",
    "name": "Mozilla Public License 2.0",
    "spdx_id": "MPL-2.0",
    "url": "https://github.com/coqui-ai/TTS/blob/dev/LICENSE.txt"
  },
  "languages": {
    "Python": 2992534,
    "Jupyter Notebook": 244647,
    "HTML": 8448,
    "Shell": 4288,
    "Makefile": 2246,
    "Cython": 1236,
    "Dockerfile": 1015
  },
  "top_contributors": [
    {
      "login": "erogol",
      "contributions": 2463
    },
    {
      "login": "Edresson",
      "contributions": 304
    },
    {
      "login": "WeberJulian",
      "contributions": 171
    },
    {
      "login": "lexkoro",
      "contributions": 48
    },
    {
      "login": "twerkmeister",
      "contributions": 41
    },
    {
      "login": "reuben",
      "contributions": 39
    },
    {
      "login": "thorstenMueller",
      "contributions": 32
    },
    {
      "login": "kirianguiller",
      "contributions": 26
    },
    {
      "login": "gerazov",
      "contributions": 22
    },
    {
      "login": "thllwg",
      "contributions": 18
    },
    {
      "login": "eginhard",
      "contributions": 14
    },
    {
      "login": "Mic92",
      "contributions": 13
    },
    {
      "login": "akx",
      "contributions": 13
    },
    {
      "login": "AyushExel",
      "contributions": 12
    },
    {
      "login": "kaiidams",
      "contributions": 12
    },
    {
      "login": "nmstoker",
      "contributions": 12
    },
    {
      "login": "sanjaesc",
      "contributions": 11
    },
    {
      "login": "p0p4k",
      "contributions": 10
    },
    {
      "login": "rishikksh20",
      "contributions": 8
    },
    {
      "login": "synesthesiam",
      "contributions": 8
    }
  ],
  "file_tree_count": 870,
  "file_tree_sample": [
    {
      "path": ".cardboardlint.yml",
      "type": "blob"
    },
    {
      "path": ".dockerignore",
      "type": "blob"
    },
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/bug_report.yaml",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/config.yml",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/feature_request.md",
      "type": "blob"
    },
    {
      "path": ".github/PR_TEMPLATE.md",
      "type": "blob"
    },
    {
      "path": ".github/stale.yml",
      "type": "blob"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    }
  ],
  "issues_count": 0,
  "pulls_count": 759,
  "recent_issues": [
    {
      "number": 4306,
      "title": "Add support for Malagasy language",
      "state": "open"
    },
    {
      "number": 4303,
      "title": "git clone https://github.com/coqui-ai/TTS.git cd TTS pip install -r requirements.txt pip install .",
      "state": "open"
    },
    {
      "number": 4300,
      "title": "README.md",
      "state": "open"
    },
    {
      "number": 4299,
      "title": "[Feature request]",
      "state": "open"
    },
    {
      "number": 4298,
      "title": "Hebrew diacritics normalization + support direct phoneme input via [[...]] blocks",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 4306,
      "title": "Add support for Malagasy language",
      "state": "open"
    },
    {
      "number": 4300,
      "title": "README.md",
      "state": "open"
    },
    {
      "number": 4298,
      "title": "Hebrew diacritics normalization + support direct phoneme input via [[...]] blocks",
      "state": "open"
    },
    {
      "number": 4214,
      "title": "Fix loading fairseq model.",
      "state": "closed"
    },
    {
      "number": 4194,
      "title": "Create PAQUI.tts",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "dbf1a08a0d4e47fdad6172e433eeb34bc6b13b4e",
      "author": "Nick Potafiy",
      "date": "2024-02-10T14:20:58+00:00",
      "message": "Update generic_utils.py (#3561)"
    },
    {
      "sha": "5dcc16d1931538e5bce7cb20c1986df371ee8cd6",
      "author": "Edresson Casanova",
      "date": "2023-12-27T16:23:43+00:00",
      "message": "Bug fix in MP3 and FLAC compute length on TTSDataset (#3092)"
    },
    {
      "sha": "55c7063724ec28085f9a61210e3dd3e34753c4d0",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-14T17:00:30+00:00",
      "message": "Merge pull request #3423 from idiap/fix-aux-tests"
    },
    {
      "sha": "99fee6f5ad9cdb990a6dab864662800ddd7838a3",
      "author": "Enno Hermann",
      "date": "2023-12-08T07:37:53+00:00",
      "message": "build: use Trainer>=0.0.36"
    },
    {
      "sha": "186cafb34cc2d533bc102e520bf5c7cebfc7e7c7",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-13T07:54:57+00:00",
      "message": "Merge pull request #3412 from coqui-ai/reuben/docs-studio-refs"
    },
    {
      "sha": "3991d83b2cf1b73eb872216179b9c0fb1d326c73",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-13T07:53:43+00:00",
      "message": "Merge branch 'dev' into reuben/docs-studio-refs"
    },
    {
      "sha": "fa28f99f1508b5b5366539b2149963edcb80ba62",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-12T15:10:46+00:00",
      "message": "Update to v0.22.0"
    },
    {
      "sha": "8c1a8b522b164470daf385820ecd1fb7c1522eb4",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-12T15:10:09+00:00",
      "message": "Merge pull request #3405 from coqui-ai/studio_speakers"
    },
    {
      "sha": "0859e9f25284d2d07096763b0783cda5b3f23b36",
      "author": "Reuben Morais",
      "date": "2023-12-12T15:09:57+00:00",
      "message": "Remove Coqui Studio references"
    },
    {
      "sha": "9f325b1f6c6dee00cbaab1bfaaea22efdbde25ce",
      "author": "Enno Hermann",
      "date": "2023-12-08T07:37:28+00:00",
      "message": "fixup! Fix aux unit tests"
    },
    {
      "sha": "fc099218df2f325490e5779b3b60b1126427afd8",
      "author": "Edresson Casanova",
      "date": "2023-12-06T20:47:33+00:00",
      "message": "Fix aux unit tests"
    },
    {
      "sha": "934b87bbd1d041cc4f7fcbb35144f47376d67414",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-12T12:51:26+00:00",
      "message": "Merge pull request #3391 from aaron-lii/multi-gpu"
    },
    {
      "sha": "b0fe0e678d7d4afe0c8d906b8b70e24c5669848f",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-12T12:50:59+00:00",
      "message": "Merge pull request #3392 from joelhoward0/fix_contributing_typo"
    },
    {
      "sha": "936084be7ee54ff7c25fae479f1565b01c8c11ee",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-12T12:50:27+00:00",
      "message": "Merge pull request #3404 from freds0/dev"
    },
    {
      "sha": "8e6a7cbfbf66aaab0b05971d7a2a7a9113a9448e",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-12T12:50:01+00:00",
      "message": "Update .models.json"
    },
    {
      "sha": "8999780aff330a6d4f5b0709268a647505e47762",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-12T12:30:21+00:00",
      "message": "Update test_models.py"
    },
    {
      "sha": "4dc0722bbc138d2183c47b6548214df0fabd66c9",
      "author": "Eren G\u00f6lge",
      "date": "2023-12-12T12:28:16+00:00",
      "message": "Update .models.json"
    },
    {
      "sha": "4b33699b415752748a77de45ae4fa93e7aaf0399",
      "author": "Edresson Casanova",
      "date": "2023-12-12T12:22:07+00:00",
      "message": "Update docs"
    },
    {
      "sha": "b6e1ac66d94958a00c8f116b25128cf273c44de8",
      "author": "Edresson Casanova",
      "date": "2023-12-12T12:19:56+00:00",
      "message": "Add docs"
    },
    {
      "sha": "61b67ef16ff2bfdc7533f380226c33d7c55105f9",
      "author": "WeberJulian",
      "date": "2023-12-11T22:58:52+00:00",
      "message": "Fix read_json_with_comments"
    }
  ],
  "readme_text": "\n## \ud83d\udc38Coqui.ai News\n- \ud83d\udce3 \u24cdTTSv2 is here with 16 languages and better performance across the board.\n- \ud83d\udce3 \u24cdTTS fine-tuning code is out. Check the [example recipes](https://github.com/coqui-ai/TTS/tree/dev/recipes/ljspeech).\n- \ud83d\udce3 \u24cdTTS can now stream with <200ms latency.\n- \ud83d\udce3 \u24cdTTS, our production TTS model that can speak 13 languages, is released [Blog Post](https://coqui.ai/blog/tts/open_xtts), [Demo](https://huggingface.co/spaces/coqui/xtts), [Docs](https://tts.readthedocs.io/en/dev/models/xtts.html)\n- \ud83d\udce3 [\ud83d\udc36Bark](https://github.com/suno-ai/bark) is now available for inference with unconstrained voice cloning. [Docs](https://tts.readthedocs.io/en/dev/models/bark.html)\n- \ud83d\udce3 You can use [~1100 Fairseq models](https://github.com/facebookresearch/fairseq/tree/main/examples/mms) with \ud83d\udc38TTS.\n- \ud83d\udce3 \ud83d\udc38TTS now supports \ud83d\udc22Tortoise with faster inference. [Docs](https://tts.readthedocs.io/en/dev/models/tortoise.html)\n\n<div align=\"center\">\n<img src=\"https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2\" />\n\n## <img src=\"https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png\" height=\"56\"/>\n\n\n**\ud83d\udc38TTS is a library for advanced Text-to-Speech generation.**\n\n\ud83d\ude80 Pretrained models in +1100 languages.\n\n\ud83d\udee0\ufe0f Tools for training new models and fine-tuning existing models in any language.\n\n\ud83d\udcda Utilities for dataset analysis and curation.\n______________________________________________________________________\n\n[![Discord](https://img.shields.io/discord/1037326658807533628?color=%239B59B6&label=chat%20on%20discord)](https://discord.gg/5eXr5seRrv)\n[![License](<https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg>)](https://opensource.org/licenses/MPL-2.0)\n[![PyPI version](https://badge.fury.io/py/TTS.svg)](https://badge.fury.io/py/TTS)\n[![Covenant](https://camo.githubusercontent.com/7d620efaa3eac1c5b060ece5d6aacfcc8b81a74a04d05cd0398689c01c4463bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e7472696275746f72253230436f76656e616e742d76322e3025323061646f707465642d6666363962342e737667)](https://github.com/coqui-ai/TTS/blob/master/CODE_OF_CONDUCT.md)\n[![Downloads](https://pepy.tech/badge/tts)](https://pepy.tech/project/tts)\n[![DOI](https://zenodo.org/badge/265612440.svg)](https://zenodo.org/badge/latestdoi/265612440)\n\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/aux_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/data_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/docker.yaml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/inference_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/style_check.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/text_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/tts_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/vocoder_tests.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests0.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests1.yml/badge.svg)\n![GithubActions](https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests2.yml/badge.svg)\n[![Docs](<https://readthedocs.org/projects/tts/badge/?version=latest&style=plastic>)](https://tts.readthedocs.io/en/latest/)\n\n</div>\n\n______________________________________________________________________\n\n## \ud83d\udcac Where to ask questions\nPlease use our dedicated channels for questions and discussion. Help is much more valuable if it's shared publicly so that more people can benefit from it.\n\n| Type                            | Platforms                               |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udea8 **Bug Reports**              | [GitHub Issue Tracker]                  |\n| \ud83c\udf81 **Feature Requests & Ideas** | [GitHub Issue Tracker]                  |\n| \ud83d\udc69\u200d\ud83d\udcbb **Usage Questions**          | [GitHub Discussions]                    |\n| \ud83d\uddef **General Discussion**       | [GitHub Discussions] or [Discord]   |\n\n[github issue tracker]: https://github.com/coqui-ai/tts/issues\n[github discussions]: https://github.com/coqui-ai/TTS/discussions\n[discord]: https://discord.gg/5eXr5seRrv\n[Tutorials and Examples]: https://github.com/coqui-ai/TTS/wiki/TTS-Notebooks-and-Tutorials\n\n\n## \ud83d\udd17 Links and Resources\n| Type                            | Links                               |\n| ------------------------------- | --------------------------------------- |\n| \ud83d\udcbc **Documentation**              | [ReadTheDocs](https://tts.readthedocs.io/en/latest/)\n| \ud83d\udcbe **Installation**               | [TTS/README.md](https://github.com/coqui-ai/TTS/tree/dev#installation)|\n| \ud83d\udc69\u200d\ud83d\udcbb **Contributing**               | [CONTRIBUTING.md](https://github.com/coqui-ai/TTS/blob/main/CONTRIBUTING.md)|\n| \ud83d\udccc **Road Map**                   | [Main Development Plans](https://github.com/coqui-ai/TTS/issues/378)\n| \ud83d\ude80 **Released Models**            | [TTS Releases](https://github.com/coqui-ai/TTS/releases) and [Experimental Models](https://github.com/coqui-ai/TTS/wiki/Experimental-Released-Models)|\n| \ud83d\udcf0 **Papers**                    | [TTS Papers](https://github.com/erogol/TTS-papers)|\n\n\n## \ud83e\udd47 TTS Performance\n<p align=\"center\"><img src=\"https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png\" width=\"800\" /></p>\n\nUnderlined \"TTS*\" and \"Judy*\" are **internal** \ud83d\udc38TTS models that are not released open-source. They are here to show the potential. Models prefixed with a dot (.Jofish .Abe and .Janice) are real human voices.\n\n## Features\n- High-performance Deep Learning models for Text2Speech tasks.\n    - Text2Spec models (Tacotron, Tacotron2, Glow-TTS, SpeedySpeech).\n    - Speaker Encoder to compute speaker embeddings efficiently.\n    - Vocoder models (MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN)\n- Fast and efficient model training.\n- Detailed training logs on the terminal and Tensorboard.\n- Support for Multi-speaker TTS.\n- Efficient, flexible, lightweight but feature complete `Trainer API`.\n- Released and ready-to-use models.\n- Tools to curate Text2Speech datasets under```dataset_analysis```.\n- Utilities to use and test your models.\n- Modular (but not too much) code base enabling easy implementation of new ideas.\n\n## Model Implementations\n### Spectrogram models\n- Tacotron: [paper](https://arxiv.org/abs/1703.10135)\n- Tacotron2: [paper](https://arxiv.org/abs/1712.05884)\n- Glow-TTS: [paper](https://arxiv.org/abs/2005.11129)\n- Speedy-Speech: [paper](https://arxiv.org/abs/2008.03802)\n- Align-TTS: [paper](https://arxiv.org/abs/2003.01950)\n- FastPitch: [paper](https://arxiv.org/pdf/2006.06873.pdf)\n- FastSpeech: [paper](https://arxiv.org/abs/1905.09263)\n- FastSpeech2: [paper](https://arxiv.org/abs/2006.04558)\n- SC-GlowTTS: [paper](https://arxiv.org/abs/2104.05557)\n- Capacitron: [paper](https://arxiv.org/abs/1906.03402)\n- OverFlow: [paper](https://arxiv.org/abs/2211.06892)\n- Neural HMM TTS: [paper](https://arxiv.org/abs/2108.13320)\n- Delightful TTS: [paper](https://arxiv.org/abs/2110.12612)\n\n### End-to-End Models\n- \u24cdTTS: [blog](https://coqui.ai/blog/tts/open_xtts)\n- VITS: [paper](https://arxiv.org/pdf/2106.06103)\n- \ud83d\udc38 YourTTS: [paper](https://arxiv.org/abs/2112.02418)\n- \ud83d\udc22 Tortoise: [orig. repo](https://github.com/neonbjb/tortoise-tts)\n- \ud83d\udc36 Bark: [orig. repo](https://github.com/suno-ai/bark)\n\n### Attention Methods\n- Guided Attention: [paper](https://arxiv.org/abs/1710.08969)\n- Forward Backward Decoding: [paper](https://arxiv.org/abs/1907.09006)\n- Graves Attention: [paper](https://arxiv.org/abs/1910.10288)\n- Double Decoder Consistency: [blog](https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/)\n- Dynamic Convolutional Attention: [paper](https://arxiv.org/pdf/1910.10288.pdf)\n- Alignment Network: [paper](https://arxiv.org/abs/2108.10447)\n\n### Speaker Encoder\n- GE2E: [paper](https://arxiv.org/abs/1710.10467)\n- Angular Loss: [paper](https://arxiv.org/pdf/2003.11982.pdf)\n\n### Vocoders\n- MelGAN: [paper](https://arxiv.org/abs/1910.06711)\n- MultiBandMelGAN: [paper](https://arxiv.org/abs/2005.05106)\n- ParallelWaveGAN: [paper](https://arxiv.org/abs/1910.11480)\n- GAN-TTS discriminators: [paper](https://arxiv.org/abs/1909.11646)\n- WaveRNN: [origin](https://github.com/fatchord/WaveRNN/)\n- WaveGrad: [paper](https://arxiv.org/abs/2009.00713)\n- HiFiGAN: [paper](https://arxiv.org/abs/2010.05646)\n- UnivNet: [paper](https://arxiv.org/abs/2106.07889)\n\n### Voice Conversion\n- FreeVC: [paper](https://arxiv.org/abs/2210.15418)\n\nYou can also help us implement more models.\n\n## Installation\n\ud83d\udc38TTS is tested on Ubuntu 18.04 with **python >= 3.9, < 3.12.**.\n\nIf you are only interested in [synthesizing speech](https://tts.readthedocs.io/en/latest/inference.html) with the released \ud83d\udc38TTS models, installing from PyPI is the easiest option.\n\n```bash\npip install TTS\n```\n\nIf you plan to code or train models, clone \ud83d\udc38TTS and install it locally.\n\n```bash\ngit clone https://github.com/coqui-ai/TTS\npip install -e .[all,dev,notebooks]  # Select the relevant extras\n```\n\nIf you are on Ubuntu (Debian), you can also run following commands for installation.\n\n```bash\n$ make system-deps  # intended to be used on Ubuntu (Debian). Let us know if you have a different OS.\n$ make install\n```\n\nIf you are on Windows, \ud83d\udc51@GuyPaddock wrote installation instructions [here](https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system).\n\n\n## Docker Image\nYou can also try TTS without install with the docker image.\nSimply run the following command and you will be able to run TTS without installing it.\n\n```bash\ndocker run --rm -it -p 5002:5002 --entrypoint /bin/bash ghcr.io/coqui-ai/tts-cpu\npython3 TTS/server/server.py --list_models #To get the list of available models\npython3 TTS/server/server.py --model_name tts_models/en/vctk/vits # To start a server\n```\n\nYou can then enjoy the TTS server [here](http://[::1]:5002/)\nMore details about the docker images (like GPU support) can be found [here](https://tts.readthedocs.io/en/latest/docker_images.html)\n\n\n## Synthesizing speech by \ud83d\udc38TTS\n\n### \ud83d\udc0d Python API\n\n#### Running a multi-speaker and multi-lingual model\n\n```python\nimport torch\nfrom TTS.api import TTS\n\n# Get device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# List available \ud83d\udc38TTS models\nprint(TTS().list_models())\n\n# Init TTS\ntts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(device)\n\n# Run TTS\n# \u2757 Since this model is multi-lingual voice cloning model, we must set the target speaker_wav and language\n# Text to speech list of amplitude values as output\nwav = tts.tts(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\")\n# Text to speech to a file\ntts.tts_to_file(text=\"Hello world!\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")\n```\n\n#### Running a single speaker model\n\n```python\n# Init TTS with the target model name\ntts = TTS(model_name=\"tts_models/de/thorsten/tacotron2-DDC\", progress_bar=False).to(device)\n\n# Run TTS\ntts.tts_to_file(text=\"Ich bin eine Testnachricht.\", file_path=OUTPUT_PATH)\n\n# Example voice cloning with YourTTS in English, French and Portuguese\ntts = TTS(model_name=\"tts_models/multilingual/multi-dataset/your_tts\", progress_bar=False).to(device)\ntts.tts_to_file(\"This is voice cloning.\", speaker_wav=\"my/cloning/audio.wav\", language=\"en\", file_path=\"output.wav\")\ntts.tts_to_file(\"C'est le clonage de la voix.\", speaker_wav=\"my/cloning/audio.wav\", language=\"fr-fr\", file_path=\"output.wav\")\ntts.tts_to_file(\"Isso \u00e9 clonagem de voz.\", speaker_wav=\"my/cloning/audio.wav\", language=\"pt-br\", file_path=\"output.wav\")\n```\n\n#### Example voice conversion\n\nConverting the voice in `source_wav` to the voice of `target_wav`\n\n```python\ntts = TTS(model_name=\"voice_conversion_models/multilingual/vctk/freevc24\", progress_bar=False).to(\"cuda\")\ntts.voice_conversion_to_file(source_wav=\"my/source.wav\", target_wav=\"my/target.wav\", file_path=\"output.wav\")\n```\n\n#### Example voice cloning together with the voice conversion model.\nThis way, you can clone voices by using any model in \ud83d\udc38TTS.\n\n```python\n\ntts = TTS(\"tts_models/de/thorsten/tacotron2-DDC\")\ntts.tts_with_vc_to_file(\n    \"Wie sage ich auf Italienisch, dass ich dich liebe?\",\n    speaker_wav=\"target/speaker.wav\",\n    file_path=\"output.wav\"\n)\n```\n\n#### Example text to speech using **Fairseq models in ~1100 languages** \ud83e\udd2f.\nFor Fairseq models, use the following name format: `tts_models/<lang-iso_code>/fairseq/vits`.\nYou can find the language ISO codes [here](https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html)\nand learn about the Fairseq models [here](https://github.com/facebookresearch/fairseq/tree/main/examples/mms).\n\n```python\n# TTS with on the fly voice conversion\napi = TTS(\"tts_models/deu/fairseq/vits\")\napi.tts_with_vc_to_file(\n    \"Wie sage ich auf Italienisch, dass ich dich liebe?\",\n    speaker_wav=\"target/speaker.wav\",\n    file_path=\"output.wav\"\n)\n```\n\n### Command-line `tts`\n\n<!-- begin-tts-readme -->\n\nSynthesize speech on command line.\n\nYou can either use your trained model or choose a model from the provided list.\n\nIf you don't specify any models, then it uses LJSpeech based English model.\n\n#### Single Speaker Models\n\n- List provided models:\n\n  ```\n  $ tts --list_models\n  ```\n\n- Get model info (for both tts_models and vocoder_models):\n\n  - Query by type/name:\n    The model_info_by_name uses the name as it from the --list_models.\n    ```\n    $ tts --model_info_by_name \"<model_type>/<language>/<dataset>/<model_name>\"\n    ```\n    For example:\n    ```\n    $ tts --model_info_by_name tts_models/tr/common-voice/glow-tts\n    $ tts --model_info_by_name vocoder_models/en/ljspeech/hifigan_v2\n    ```\n  - Query by type/idx:\n    The model_query_idx uses the corresponding idx from --list_models.\n\n    ```\n    $ tts --model_info_by_idx \"<model_type>/<model_query_idx>\"\n    ```\n\n    For example:\n\n    ```\n    $ tts --model_info_by_idx tts_models/3\n    ```\n\n  - Query info for model info by full name:\n    ```\n    $ tts --model_info_by_name \"<model_type>/<language>/<dataset>/<model_name>\"\n    ```\n\n- Run TTS with default models:\n\n  ```\n  $ tts --text \"Text for TTS\" --out_path output/path/speech.wav\n  ```\n\n- Run TTS and pipe out the generated TTS wav file data:\n\n  ```\n  $ tts --text \"Text for TTS\" --pipe_out --out_path output/path/speech.wav | aplay\n  ```\n\n- Run a TTS model with its default vocoder model:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"<model_type>/<language>/<dataset>/<model_name>\" --out_path output/path/speech.wav\n  ```\n\n  For example:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"tts_models/en/ljspeech/glow-tts\" --out_path output/path/speech.wav\n  ```\n\n- Run with specific TTS and vocoder models from the list:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"<model_type>/<language>/<dataset>/<model_name>\" --vocoder_name \"<model_type>/<language>/<dataset>/<model_name>\" --out_path output/path/speech.wav\n  ```\n\n  For example:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_name \"tts_models/en/ljspeech/glow-tts\" --vocoder_name \"vocoder_models/en/ljspeech/univnet\" --out_path output/path/speech.wav\n  ```\n\n- Run your own TTS model (Using Griffin-Lim Vocoder):\n\n  ```\n  $ tts --text \"Text for TTS\" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n  ```\n\n- Run your own TTS and Vocoder models:\n\n  ```\n  $ tts --text \"Text for TTS\" --model_path path/to/model.pth --config_path path/to/config.json --out_path output/path/speech.wav\n      --vocoder_path path/to/vocoder.pth --vocoder_config_path path/to/vocoder_config.json\n  ```\n\n#### Multi-speaker Models\n\n- List the available speakers and choose a <speaker_id> among them:\n\n  ```\n  $ tts --model_name \"<language>/<dataset>/<model_name>\"  --list_speaker_idxs\n  ```\n\n- Run the multi-speaker TTS model with the target speaker ID:\n\n  ```\n  $ tts --text \"Text for TTS.\" --out_path output/path/speech.wav --model_name \"<language>/<dataset>/<model_name>\"  --speaker_idx <speaker_id>\n  ```\n\n- Run your own multi-speaker TTS model:\n\n  ```\n  $ tts --text \"Text for TTS\" --out_path output/path/speech.wav --model_path path/to/model.pth --config_path path/to/config.json --speakers_file_path path/to/speaker.json --speaker_idx <speaker_id>\n  ```\n\n### Voice Conversion Models\n\n```\n$ tts --out_path output/path/speech.wav --model_name \"<language>/<dataset>/<model_name>\" --source_wav <path/to/speaker/wav> --target_wav <path/to/reference/wav>\n```\n\n<!-- end-tts-readme -->\n\n## Directory Structure\n```\n|- notebooks/       (Jupyter Notebooks for model evaluation, parameter selection and data analysis.)\n|- utils/           (common utilities.)\n|- TTS\n    |- bin/             (folder for all the executables.)\n      |- train*.py                  (train your target model.)\n      |- ...\n    |- tts/             (text to speech models)\n        |- layers/          (model layer definitions)\n        |- models/          (model definitions)\n        |- utils/           (model specific utilities.)\n    |- speaker_encoder/ (Speaker Encoder models.)\n        |- (same)\n    |- vocoder/         (Vocoder models.)\n        |- (same)\n```\n",
  "external_links_in_readme": [
    "https://arxiv.org/abs/2106.07889",
    "https://arxiv.org/abs/2005.05106",
    "https://zenodo.org/badge/latestdoi/265612440",
    "https://arxiv.org/abs/1910.11480",
    "http://[::1]:5002/",
    "https://readthedocs.org/projects/tts/badge/?version=latest&style=plastic>",
    "https://github.com/coqui-ai/TTS/releases",
    "https://github.com/coqui-ai/TTS/actions/workflows/inference_tests.yml/badge.svg",
    "https://github.com/coqui-ai/TTS/actions/workflows/docker.yaml/badge.svg",
    "https://arxiv.org/abs/1907.09006",
    "https://github.com/coqui-ai/TTS/actions/workflows/data_tests.yml/badge.svg",
    "https://arxiv.org/abs/2005.11129",
    "https://tts.readthedocs.io/en/latest/inference.html",
    "https://badge.fury.io/py/TTS",
    "https://arxiv.org/abs/1906.03402",
    "https://arxiv.org/abs/2010.05646",
    "https://github.com/coqui-ai/TTS/blob/master/CODE_OF_CONDUCT.md",
    "https://raw.githubusercontent.com/coqui-ai/TTS/main/images/coqui-log-green-TTS.png\"",
    "https://dl.fbaipublicfiles.com/mms/tts/all-tts-languages.html",
    "https://raw.githubusercontent.com/coqui-ai/TTS/main/images/TTS-performance.png\"",
    "https://github.com/coqui-ai/TTS/actions/workflows/aux_tests.yml/badge.svg",
    "https://arxiv.org/abs/2108.10447",
    "https://img.shields.io/badge/License-MPL%202.0-brightgreen.svg>",
    "https://github.com/coqui-ai/TTS/wiki/Experimental-Released-Models",
    "https://arxiv.org/abs/2211.06892",
    "https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests1.yml/badge.svg",
    "https://github.com/coqui-ai/TTS/actions/workflows/style_check.yml/badge.svg",
    "https://arxiv.org/pdf/2106.06103",
    "https://arxiv.org/abs/2104.05557",
    "https://arxiv.org/abs/1703.10135",
    "https://tts.readthedocs.io/en/dev/models/bark.html",
    "https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests2.yml/badge.svg",
    "https://github.com/coqui-ai/TTS/discussions",
    "https://arxiv.org/abs/2003.01950",
    "https://github.com/coqui-ai/TTS",
    "https://github.com/facebookresearch/fairseq/tree/main/examples/mms",
    "https://arxiv.org/abs/1710.10467",
    "https://arxiv.org/abs/1905.09263",
    "https://github.com/fatchord/WaveRNN/",
    "https://tts.readthedocs.io/en/latest/",
    "https://arxiv.org/abs/2009.00713",
    "https://arxiv.org/abs/1712.05884",
    "https://badge.fury.io/py/TTS.svg",
    "https://arxiv.org/abs/2110.12612",
    "https://github.com/coqui-ai/TTS/actions/workflows/vocoder_tests.yml/badge.svg",
    "https://tts.readthedocs.io/en/latest/docker_images.html",
    "https://arxiv.org/abs/2008.03802",
    "https://stackoverflow.com/questions/66726331/how-can-i-run-mozilla-tts-coqui-tts-training-with-cuda-on-a-windows-system",
    "https://github.com/suno-ai/bark",
    "https://github.com/erogol/TTS-papers",
    "https://arxiv.org/abs/1909.11646",
    "https://github.com/coqui-ai/TTS/blob/main/CONTRIBUTING.md",
    "https://arxiv.org/pdf/2006.06873.pdf",
    "https://arxiv.org/abs/1710.08969",
    "https://github.com/neonbjb/tortoise-tts",
    "https://arxiv.org/abs/2112.02418",
    "https://arxiv.org/abs/2210.15418",
    "https://huggingface.co/spaces/coqui/xtts",
    "https://arxiv.org/abs/2108.13320",
    "https://tts.readthedocs.io/en/dev/models/tortoise.html",
    "https://github.com/coqui-ai/TTS/issues/378",
    "https://discord.gg/5eXr5seRrv",
    "https://camo.githubusercontent.com/7d620efaa3eac1c5b060ece5d6aacfcc8b81a74a04d05cd0398689c01c4463bb/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f436f6e7472696275746f72253230436f76656e616e742d76322e3025323061646f707465642d6666363962342e737667",
    "https://arxiv.org/abs/1910.06711",
    "https://pepy.tech/badge/tts",
    "https://img.shields.io/discord/1037326658807533628?color=%239B59B6&label=chat%20on%20discord",
    "https://coqui.ai/blog/tts/open_xtts",
    "https://github.com/coqui-ai/TTS/actions/workflows/tts_tests.yml/badge.svg",
    "https://github.com/coqui-ai/TTS/actions/workflows/text_tests.yml/badge.svg",
    "https://arxiv.org/abs/1910.10288",
    "https://github.com/coqui-ai/TTS/wiki/TTS-Notebooks-and-Tutorials",
    "https://github.com/coqui-ai/TTS/actions/workflows/zoo_tests0.yml/badge.svg",
    "https://erogol.com/solving-attention-problems-of-tts-models-with-double-decoder-consistency/",
    "https://arxiv.org/pdf/1910.10288.pdf",
    "https://github.com/coqui-ai/tts/issues",
    "https://github.com/coqui-ai/TTS/tree/dev/recipes/ljspeech",
    "https://zenodo.org/badge/265612440.svg",
    "https://github.com/coqui-ai/TTS/tree/dev#installation",
    "https://tts.readthedocs.io/en/dev/models/xtts.html",
    "https://arxiv.org/pdf/2003.11982.pdf",
    "https://pepy.tech/project/tts",
    "https://static.scarf.sh/a.png?x-pxid=cf317fe7-2188-4721-bc01-124bb5d5dbb2\"",
    "https://arxiv.org/abs/2006.04558",
    "https://opensource.org/licenses/MPL-2.0"
  ]
}
```

</details>


---

