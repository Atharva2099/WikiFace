# GitHub Data for Gensyn_Qwen2.5-1.5B-Instruct

**Task Category:** Text Generation

## Repository 1: gensyn-ai/rl-swarm

# GitHub Repository Data

**Repository:** [gensyn-ai/rl-swarm](https://github.com/gensyn-ai/rl-swarm)

## Basic Information

- **Description:** A fully open source framework for creating RL training swarms over the internet.
- **Created:** 2025-02-25T04:21:53+00:00
- **Last Updated:** 2025-06-22T02:25:58+00:00
- **Last Pushed:** 2025-06-10T21:12:52+00:00
- **Default Branch:** main
- **Size:** 5855 KB

## Statistics

- **Stars:** 759
- **Forks:** 392
- **Watchers:** 759
- **Open Issues:** 39
- **Total Issues:** 0
- **Pull Requests:** 83

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/gensyn-ai/rl-swarm/blob/main/LICENSE.TXT)

## Languages

- **Python:** 317,063 bytes
- **TypeScript:** 108,915 bytes
- **Shell:** 10,304 bytes
- **CSS:** 3,459 bytes
- **HTML:** 2,783 bytes
- **JavaScript:** 1,512 bytes

## Top Contributors

1. **yihualou** - 30 contributions
2. **h-grieve** - 30 contributions
3. **tforbus** - 20 contributions
4. **bjw-0** - 17 contributions
5. **archaengel** - 17 contributions
6. **cniessigma** - 9 contributions
7. **jcd496** - 8 contributions
8. **cleanunicorn** - 4 contributions
9. **BenFielding** - 3 contributions
10. **jamico** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 196

- `.gitignore` (blob)
- `CONTRIBUTING.md` (blob)
- `Dockerfile.webserver` (blob)
- `LICENSE.TXT` (blob)
- `README.md` (blob)
- `docker-compose.yaml` (blob)
- `hivemind_exp` (tree)
- `hivemind_exp/__init__.py` (blob)
- `hivemind_exp/chain_utils.py` (blob)
- `hivemind_exp/configs` (tree)

## Recent Issues

- 🟢 **#348** NameError: name 'is_torch_version' is not defined. Did you mean: 'torch_version'? (open)
- 🔴 **#347** fix: Missing accelerate package many users need (closed)
- 🟢 **#346** Why am i getting error when i try to run quantized models? (open)
- 🟢 **#345** Go-based Supervisor for RL Swarm: Robust Process Management, Dependency Handling, and Logging (open)
- 🔴 **#344** Docker image? (closed)

## Recent Pull Requests

- 🔴 **#347** fix: Missing accelerate package many users need (closed)
- 🟢 **#338** fixed a typo (open)
- 🔴 **#329** Fix TypeError in layout.tsx caused by headers().get (closed)
- 🟢 **#326** Fix multiple issues in RL Swarm framework (open)
- 🟢 **#324** Add RTX 5000 series (5090/5080/5070) support (open)

## Recent Commits

- **2f779450** Mention logs if error is detected - Eddie Nuno (2025-05-31T02:08:08+00:00)
- **447c087a** small QoL change so the users do not get prompted by wandb until they ctrl+c - Johnny (2025-05-30T14:24:55+00:00)
- **15d199d3** Handle peer already registered in hivemind - Eddie Nuno (2025-05-30T01:40:11+00:00)
- **9b24b701** Refactor modal login - Tristin Forbus (2025-05-28T17:25:07+00:00)
- **3e652efe** Unconditionally return 500 on errors - Eddie Nuno (2025-05-28T17:12:57+00:00)
- **b45dd3a1** Commit yarn lock file instead of package-lock - Eddie Nuno (2025-05-28T16:27:38+00:00)
- **ddec4e6f** Add auto-open auth modal when user is not logged in - Dio Ortega (2025-05-28T15:09:57+00:00)
- **5c264ae7** Change mac config to use float16 instead of MPS-unsupported bfloat16 - Ben Fielding (2025-05-28T15:08:08+00:00)
- **ffdff46a** Update dev setup for may changes - Eddie Nuno (2025-05-27T17:31:04+00:00)
- **5c49026a** Build modal-login server before starting - Eddie Nuno (2025-05-23T18:50:01+00:00)

## External Links Found in README

- https://learn.microsoft.com/en-us/windows/wsl/install
- https://huggingface.co/docs/hub/en/security-tokens
- https://dashboard-math-hard.gensyn.ai
- http://localhost:3000/
- https://docs.wandb.ai/ref/cli/wandb-sync
- https://wandb.ai/
- https://gensyn-testnet.explorer.alchemy.com/address/0x2fC68a233EF9E9509f034DD551FF90A79a0B8F82?tab=logs
- https://discord.gg/AdnyWNzXh5
- https://dashboard-math.gensyn.ai

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 938510252,
  "name": "rl-swarm",
  "full_name": "gensyn-ai/rl-swarm",
  "description": "A fully open source framework for creating RL training swarms over the internet.",
  "html_url": "https://github.com/gensyn-ai/rl-swarm",
  "clone_url": "https://github.com/gensyn-ai/rl-swarm.git",
  "ssh_url": "git@github.com:gensyn-ai/rl-swarm.git",
  "homepage": "https://www.gensyn.ai",
  "topics": [],
  "default_branch": "main",
  "created_at": "2025-02-25T04:21:53+00:00",
  "updated_at": "2025-06-22T02:25:58+00:00",
  "pushed_at": "2025-06-10T21:12:52+00:00",
  "size_kb": 5855,
  "watchers_count": 759,
  "stargazers_count": 759,
  "forks_count": 392,
  "open_issues_count": 39,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/gensyn-ai/rl-swarm/blob/main/LICENSE.TXT"
  },
  "languages": {
    "Python": 317063,
    "TypeScript": 108915,
    "Shell": 10304,
    "CSS": 3459,
    "HTML": 2783,
    "JavaScript": 1512
  },
  "top_contributors": [
    {
      "login": "yihualou",
      "contributions": 30
    },
    {
      "login": "h-grieve",
      "contributions": 30
    },
    {
      "login": "tforbus",
      "contributions": 20
    },
    {
      "login": "bjw-0",
      "contributions": 17
    },
    {
      "login": "archaengel",
      "contributions": 17
    },
    {
      "login": "cniessigma",
      "contributions": 9
    },
    {
      "login": "jcd496",
      "contributions": 8
    },
    {
      "login": "cleanunicorn",
      "contributions": 4
    },
    {
      "login": "BenFielding",
      "contributions": 3
    },
    {
      "login": "jamico",
      "contributions": 3
    },
    {
      "login": "ajstarna",
      "contributions": 2
    },
    {
      "login": "diogoortega",
      "contributions": 2
    },
    {
      "login": "zh1p4ng",
      "contributions": 2
    },
    {
      "login": "rosaroterpanter",
      "contributions": 1
    },
    {
      "login": "sukrucildirr",
      "contributions": 1
    },
    {
      "login": "klbrvik",
      "contributions": 1
    }
  ],
  "file_tree_count": 196,
  "file_tree_sample": [
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "CONTRIBUTING.md",
      "type": "blob"
    },
    {
      "path": "Dockerfile.webserver",
      "type": "blob"
    },
    {
      "path": "LICENSE.TXT",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "docker-compose.yaml",
      "type": "blob"
    },
    {
      "path": "hivemind_exp",
      "type": "tree"
    },
    {
      "path": "hivemind_exp/__init__.py",
      "type": "blob"
    },
    {
      "path": "hivemind_exp/chain_utils.py",
      "type": "blob"
    },
    {
      "path": "hivemind_exp/configs",
      "type": "tree"
    }
  ],
  "issues_count": 0,
  "pulls_count": 83,
  "recent_issues": [
    {
      "number": 348,
      "title": "NameError: name 'is_torch_version' is not defined. Did you mean: 'torch_version'?",
      "state": "open"
    },
    {
      "number": 347,
      "title": "fix: Missing accelerate package many users need",
      "state": "closed"
    },
    {
      "number": 346,
      "title": "Why am i getting error when i try to run quantized models?",
      "state": "open"
    },
    {
      "number": 345,
      "title": "Go-based Supervisor for RL Swarm: Robust Process Management, Dependency Handling, and Logging",
      "state": "open"
    },
    {
      "number": 344,
      "title": "Docker image?",
      "state": "closed"
    }
  ],
  "recent_pulls": [
    {
      "number": 347,
      "title": "fix: Missing accelerate package many users need",
      "state": "closed"
    },
    {
      "number": 338,
      "title": "fixed a typo",
      "state": "open"
    },
    {
      "number": 329,
      "title": "Fix TypeError in layout.tsx caused by headers().get",
      "state": "closed"
    },
    {
      "number": 326,
      "title": "Fix multiple issues in RL Swarm framework",
      "state": "open"
    },
    {
      "number": 324,
      "title": "Add RTX 5000 series (5090/5080/5070) support",
      "state": "open"
    }
  ],
  "recent_commits": [
    {
      "sha": "2f779450a49bcd3458fb6a382314691548a42297",
      "author": "Eddie Nuno",
      "date": "2025-05-31T02:08:08+00:00",
      "message": "Mention logs if error is detected"
    },
    {
      "sha": "447c087aa15986c7da6651939755674a1910b092",
      "author": "Johnny",
      "date": "2025-05-30T14:24:55+00:00",
      "message": "small QoL change so the users do not get prompted by wandb until they ctrl+c"
    },
    {
      "sha": "15d199d36a94624bd5d03413017b4d2e0f582d6a",
      "author": "Eddie Nuno",
      "date": "2025-05-30T01:40:11+00:00",
      "message": "Handle peer already registered in hivemind"
    },
    {
      "sha": "9b24b7012ad1dcab3e53aed5d5ac08be84c3d773",
      "author": "Tristin Forbus",
      "date": "2025-05-28T17:25:07+00:00",
      "message": "Refactor modal login"
    },
    {
      "sha": "3e652efe797fa0bb74256b20238770bb96704147",
      "author": "Eddie Nuno",
      "date": "2025-05-28T17:12:57+00:00",
      "message": "Unconditionally return 500 on errors"
    },
    {
      "sha": "b45dd3a116600d788dd2290044154b8df311719f",
      "author": "Eddie Nuno",
      "date": "2025-05-28T16:27:38+00:00",
      "message": "Commit yarn lock file instead of package-lock"
    },
    {
      "sha": "ddec4e6ff1d25ce492ead6c80961c596c9f9b861",
      "author": "Dio Ortega",
      "date": "2025-05-28T15:09:57+00:00",
      "message": "Add auto-open auth modal when user is not logged in"
    },
    {
      "sha": "5c264ae79ba6e824c1ab37fb3bade9eb0a51d113",
      "author": "Ben Fielding",
      "date": "2025-05-28T15:08:08+00:00",
      "message": "Change mac config to use float16 instead of MPS-unsupported bfloat16"
    },
    {
      "sha": "ffdff46a7e5ac40947e5a70e2e5e282c8e292c2d",
      "author": "Eddie Nuno",
      "date": "2025-05-27T17:31:04+00:00",
      "message": "Update dev setup for may changes"
    },
    {
      "sha": "5c49026a8a9f136ca931b9e616dda34cfd6cd389",
      "author": "Eddie Nuno",
      "date": "2025-05-23T18:50:01+00:00",
      "message": "Build modal-login server before starting"
    },
    {
      "sha": "3de48420b838b9ebd6c228ef5d994f9272c363ba",
      "author": "Yihua Lou",
      "date": "2025-05-22T18:17:44+00:00",
      "message": "Pin transformers version"
    },
    {
      "sha": "73f1c94e20399ed5a41fe696a12d62fcc78409dc",
      "author": "Dio Ortega",
      "date": "2025-05-22T13:44:55+00:00",
      "message": "Update README.md for May release"
    },
    {
      "sha": "d1a900e4b042f4434b264f89a58a80af53baf13f",
      "author": "Johnny",
      "date": "2025-05-19T19:55:17+00:00",
      "message": "System Logging, Metric Logging, Process Logging"
    },
    {
      "sha": "116b01be166485b0723b3fd7fc0418b22cdb7704",
      "author": "Christopher Nies",
      "date": "2025-05-16T19:03:25+00:00",
      "message": "Increase Gas Fee (I Only Kinda Know What I'm Doing)"
    },
    {
      "sha": "a1a11b0ce7b874e94e6082d3365564dd0b35d5d2",
      "author": "Eddie Nuno",
      "date": "2025-05-15T15:37:45+00:00",
      "message": "Decode errors on modal register peer"
    },
    {
      "sha": "b72e949a08d154f93aeb13eee1e2d75cb0c68e9d",
      "author": "Eddie Nuno",
      "date": "2025-05-06T19:43:16+00:00",
      "message": "Decode known errors"
    },
    {
      "sha": "385e0b345aaa7a0a580cbec24aa4dbdb9dbd4642",
      "author": "Yihua Lou",
      "date": "2025-05-06T18:06:39+00:00",
      "message": "Remove unused flag (superceded by env var). (#227)"
    },
    {
      "sha": "87dbe865f247e3349a9dfb081cf2266007fe8763",
      "author": "Yihua Lou",
      "date": "2025-05-06T17:57:20+00:00",
      "message": "Add ability to disable unsloth (#226)"
    },
    {
      "sha": "beac33ba96412a365a375aa6143917c59468e113",
      "author": "Tristin Forbus",
      "date": "2025-05-05T15:29:16+00:00",
      "message": "Add dashboard links to readme (#223)"
    },
    {
      "sha": "40600aff4d8d07d08d14f0c0b7f0509229cc2af6",
      "author": "Eddie Nuno",
      "date": "2025-05-02T18:05:44+00:00",
      "message": "Use right property name in submit winners call (#224)"
    }
  ],
  "readme_text": "# RL Swarm\n\nRL Swarm is a peer-to-peer system for reinforcement learning. It allows you to train a model collaboratively with other models in the swarm, leveraging their collective intelligence. It is open source and permissionless, meaning you can run it on a consumer laptop at home or on a powerful GPU in the cloud. You can also connect your model to the Gensyn Testnet, to receive an on-chain identity that tracks your progress over time.\n\nThere are currently multiple swarms running on the Testnet, each training on a different data set. The current list of available models and swarms include:\n\nModels:\n   - Qwen 2.5 0.5B\n   - Qwen 2.5 1.5B\n   - Qwen 2.5 7B\n   - Qwen 2.5 32B (4 bit)\n   - Qwen 2.5 72B (4 bit)\n\nSwarms:\n   - Math (GSM8K dataset)\n   - Math Hard (DAPO-Math 17K dataset)\n\nSoon you will be able to create your own swarms with unique data sets, and eventually connect multiple swarms together to train powerful models across domains.\n\n## Requirements\n\nYour hardware requirements will vary depending on which swarm and model you choose.  Users with less powerful hardware should select a smaller model (e.g. Qwen 0.5B or 1.5B) and smaller dataset (GSM8K). Users with more powerful hardware can select a larger model (e.g. Qwen 7B, 32B or 72B) and larger dataset (DAPO-Math 17K).  The requirements for each are listed below:     \n\n**Small model (0.5B or 1.5B) + Math (GSM8K dataset)**\n\n- arm64 or x86 CPU with minimum 16gb ram (note that if you run other applications during training it might crash training).\n\n\nOR\n\n- CUDA devices (officially supported):\n    - RTX 3090\n    - RTX 4090\n    - A100\n    - H100\n\n**Big model (7B, 32B or 72B) + Math Hard (DAPO-Math 17K dataset)**\n\n- Recommended:\n    - A100 (80GB) \n    - H100 (80GB)\n\n\n\n***\n\nWith either configuration, you will need Python >=3.10 (for Mac, you will likely need to upgrade).\n\n## \u26a0\ufe0f Please read before continuing \u26a0\ufe0f\n\nThis software is **experimental** and provided as-is for users who are interested in using (or helping to develop) an early version of the Gensyn Protocol for training models.\n\nIf you care about on-chain participation, you **must** read the [Identity Management](#identity-management) section below.\n\nIf you encounter issues, please first check [Troubleshooting](#troubleshooting). If you cannot find a solution there, please check if there is an open (or closed) [Issue](../../issues). If there is no relevant issue, please file one and include 1) all relevant [logs](#troubleshooting), 2) information about your device (e.g. which GPU, if relevant), and 3) your operating system information.\n\n## Instructions\n\n### Run the swarm\n\n```sh\npython3 -m venv .venv\nsource .venv/bin/activate\n./run_rl_swarm.sh\n```\n\n### Testnet participation\n\nPlease answer 'Y' (or just press enter), N is provided as an alternative flow but isn't currently maintained.\n\n### Select your Swarm\n\nTo select your swarm, answer 'B' to join the Math Hard (DAPO-Math 17K dataset) or 'S' to join the Math (GSM8K dataset). \n\n### Select your Model\n\nTo select your model, answer '0.5', '1.5', '7', '32', or '72' to pick the parameter count. \n\n### Login\n\n1. A browser window will pop open (you'll need to manually navigate to http://localhost:3000/ if you're on a VM).\n2. Click 'login'.\n3. Login with your preferred method.\n\n### Huggingface\n\nIf you would like to upload your model to Hugging Face, enter your Hugging Face access token when prompted. You can generate one from your Hugging Face account, under [Access Tokens](https://huggingface.co/docs/hub/en/security-tokens).\n\n### Initial peering and training\n\nFrom this stage onward your device will begin training. You should see your peer register and vote on-chain [here](https://gensyn-testnet.explorer.alchemy.com/address/0x2fC68a233EF9E9509f034DD551FF90A79a0B8F82?tab=logs).\n\nYou can also track your training progress in real time:\n- For the Math swarm (GSM8K dataset): [dashboard-math.gensyn.ai](https://dashboard-math.gensyn.ai)\n- For the Math Hard swarm (DAPO-Math 17K dataset): [dashboard-math-hard.gensyn.ai](https://dashboard-math-hard.gensyn.ai)\n\n### Uploading training stats to Weights & Biases (wandb.ai)\n\nOnce you stop the `rl_swarm.sh` process in your console (e.g., by pressing Ctrl+C), you will see a message similar to this:\n\n```sh\nwandb: You can sync this run to the cloud by running:\nwandb: wandb sync logs/wandb/offline-run-xxxxxxxx_xxxxxx-xxxxxxxxxx\n```\n\nTo upload your training statistics:\n\n1. Make sure you have created an account on [wandb.ai](https://wandb.ai/).\n2. Copy the wandb sync command provided in your terminal (the part that looks like `wandb sync logs/wandb/offline-run-xxxxxxxx_xxxxxx-xxxxxxxxxx`).\n3. Run that command in your terminal.\n\nThis will upload your local training run data to the Weights & Biases cloud, allowing you to visualize and track your experiments. For more details on this command, you can refer to the [official documentation](https://docs.wandb.ai/ref/cli/wandb-sync).\n\n## Identity management\n\n### Introduction\n\nOn-chain identity is managed via an Alchemy modal sign-in screen. You need to supply an email address or login via a supported method (e.g. Google). This creates an EOA public/private key (which are stored by Alchemy). You will also receive local session keys in the `userApiKey`. Note that these aren't your EOA public/private keys. \n\nDuring the initial set-up process, you will also create a `swarm.pem` file which maintains the identity of your peer. This is then registered on chain using the EOA wallet hosted in Alchemy, triggered using your local api keys. This links the `swarm.pem` to the `email address` (and corresponding EOA in Alchemy).\n\n**If you want to link multiple nodes to a single EOA**, simply sign up each node using the same email address. You will get a new peer ID for each node, however they will all be linked to the same EOA that your email is linked to.\n\n**Please note**: if you are using a fork of this repo, or a service organised by someone else (e.g. a 'one click deployment' provider) the identity management flow below is not guaranteed.\n\n### What this means\nIn the following two scenarios, everything will work (i.e. you will have an on-chain identity linked with your RL Swarm peer training):\n\n- The very first time you run the node from scratch with a new email address. The smart account will be created fresh and linked with the swarm.pem that is also fresh.\n- If you run it again with a `swarm.pem` AND login the original `email address` used with that `swarm.pem`. Note: this will throw an error into the log on registration but will still be able to sign transactions.\n\nIn the following two scenarios, it will not work (i.e. you won't have an on-chain identity linked with your RL Swarm peer training):\n\n- If you keep your `swarm.pem` and try to link it to an `email address` distinct from the one with which it was first registered.\n\nTherefore, you should do these actions in the following scenarios\n\n- **Signed up with `email address`, generated `swarm.pem`, BUT lost `swarm.pem`** OR **You want to run multiple nodes at once**: run from scratch with the same email address and generate a new `swarm.pem`. \n- **Signed up with `email address`, generated `swarm.pem`, kept `swarm.pem`** -> you can re-run a single node using this pair if you've still got them both.\n\n## Troubleshooting\n\n- **How do I find my logs?** You can find them inside the `/logs` directory:\n    - `yarn.log`: This file contains logs for the modal login server.\n    - `swarm.log`: This is the main log file for the RL Swarm application.\n    - `wandb/`: This directory contains various logs related to your training runs, including a `debug.log` file. These can be updated to Weights & Biases (see [above](#uploading-training-stats-to-weights--biases-wandbai)).\n\n- **My peer 'skipped a round'**: this occurs when your device isn't fast enough to keep up with the pace of the swarm. For example, if you start training at round 100 and by the time you finish training the rest of the swarm reaches round 102, you will skip round 101 and go straight to 102. This is because your peer is more valuable if it is participating in the active round.\n- **My model doesn't seem to be training?**\n\n    - If you're using a consumer device (e.g. a MacBook), it is likely just running slowly - check back in 20 minutes.\n\n- **Logging in with a new account after previous login?**\n    \n    - Make sure you click 'Logout' on the login screen before you leave your previous session\n    - Make sure you delete `swarm.pem` from the root directory (try `sudo rm swarm.pem`). If you don't do this, and you previously registered with the peer-id stored in this file, it will disrupt the training process.\n\n- **Issues with the Login screen**\n\n    - **Upgrade viem**: some users report issues with the `viem` package. There are two fixes:\n        - in the `modal-login/package.json` update: `\"viem\": \"2.25.0\"`\n        - in the terminal `cd /root/rl-swarm/modal-login/ && yarn upgrade && yarn add next@latest && yarn add viem@latest`\n\n- **I'm getting lots of warnings**\n    - This is expected behaviour and usually the output of the package managers or other dependencies. The most common is the below Protobuf warning - which can be ignored\n        ```\n        WARNING: The candidate selected for download or install is a yanked version: 'protobuf' candidate...\n        ```\n\n- **Issues on VMs/VPSs?**\n\n    - **How do I access the login screen if I'm running in a VM?**: port forwarding. Add this SSH flag: `-L 3000:localhost:3000` when connecting to your VM. E.g. `gcloud compute ssh --zone \"us-central1-a\" [your-vm] --project [your-project] -- -L 3000:localhost:3000`. Note, some VPSs may not work with `rl-swarm`. Check the Gensyn [discord](https://discord.gg/AdnyWNzXh5) for up-to-date information on this.\n    \n    - **Disconnection/general issues**: If you are tunneling to a VM and suffer a broken pipe, you will likely encounter OOM or unexepected behaviour the first time you relaunch the script. If you `control + c` and kill the script it should spin down all background processes. Restart the script and everything should work normally.\n\n- **Issues with npm/general installation?**\n\n    - Try  `npm install -g node@latest`\n\n- **OOM errors on MacBook?**\n    - Try this (experimental) fix to increase memory:\n        ```\n        export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0\n        ```\n- **I have a Windows machine, can I still train a model on the swarm?**: Yes - but this is not very well tested and may require you to do some debugging to get it set up properly. Install WSL and Linux on your Windows machine using the following instructions: https://learn.microsoft.com/en-us/windows/wsl/install\n\n- **I want to move my to a different machine and/or restart with a fresh build of the repo, but I want my animal name/peer id to persist.**: To achieve this simply backup the `swarm.pem` file on your current machine and then put it in the corresponding location on your new machine/build of the repo.\n\n- **I have multiple GPUs on one machine, can I run multiple peers?**: Yes - but you'll need to manually change things. You'll need to isolate each GPU, install this repo for each GPU, and expose each peer under a different port to pass the modal onboard.\n\n- **My round/stage is behind the smart contract/other peers?**: This is expected behaviour given the different speeds of machines in the network. Once your machine completes it's current round, it will move to the the current round.\n\n- **I want to use a bigger and/or different model in the RL swarm, can I do that?**: Yes - but we only recommend doing so if you are comfortable manually changing files and appropriately configuring the model(s) you wish to run for your device(s). You'll simply need to edit the config file in `./hivemind_exp/configs/<directory_relevant_to_your_device>/grpo-qwen-2.5-0.5b-deepseek-r1.yaml` to reflect the model_name_or_path and training arguments corresponding to what you want in the swarm. Note that, although any pre-trained LLM compatible with Hugging Face's `AutoModelForCausalLM` class should work in theory, we have only tested with a handful of Qwen 2.5 instruction-tuned models.\n\n- **I am running a model in the swarm on my CPU, have received a python `RuntimeError`, and my training progress seems to have stopped.**: There are several possible causes for this, but before trying anything please wait long enough to be sure your training actually is frozen and not just slow (e.g., wait longer than a single training iteration has previously taken on your machine). If you're sure training is actually frozen, then some things to try are:\n    - Set this (experimental) fix: `export PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 && ./run_rl_swarm.sh`\n    - In the config for your device (`./hivemind_exp/configs/<directory_relevant_to_your_device>/grpo-qwen-2.5-0.5b-deepseek-r1.yaml`) add the following training argument: `max_grad_norm=0.5`\n    - Use floating point 32 instead of bfloat16 to train your model. This can be changed in the config for your device, i.e. `./hivemind_exp/configs/<directory_relevant_to_your_device>/grpo-qwen-2.5-0.5b-deepseek-r1.yaml`.\n\n- **How can I optimsie `rl-swarm` for my device**? open the `hivemind_exp/configs/gpu/grpo-qwen-2.5-0.5b-deepseek-r1.yaml`. Note that this is for the gpu and not cpu configuration. You can then edit parameters that optimsie the training run. For example, try adjusting the `vllm_gpu_memory_utilization`. Note that optimal settings will vary by device.\n",
  "external_links_in_readme": [
    "https://learn.microsoft.com/en-us/windows/wsl/install",
    "https://huggingface.co/docs/hub/en/security-tokens",
    "https://dashboard-math-hard.gensyn.ai",
    "http://localhost:3000/",
    "https://docs.wandb.ai/ref/cli/wandb-sync",
    "https://wandb.ai/",
    "https://gensyn-testnet.explorer.alchemy.com/address/0x2fC68a233EF9E9509f034DD551FF90A79a0B8F82?tab=logs",
    "https://discord.gg/AdnyWNzXh5",
    "https://dashboard-math.gensyn.ai"
  ]
}
```

</details>


---

## Repository 2: gensyn-ai/paper-rl-swarm

# GitHub Repository Data

**Repository:** [gensyn-ai/paper-rl-swarm](https://github.com/gensyn-ai/paper-rl-swarm)

## Basic Information

- **Description:** Technical report for the RL Swarm framework - a fully open source framework for creating RL training swarms over the internet.
- **Created:** 2025-02-19T03:26:35+00:00
- **Last Updated:** 2025-05-31T17:13:02+00:00
- **Last Pushed:** 2025-02-26T21:48:08+00:00
- **Default Branch:** main
- **Size:** 149 KB

## Statistics

- **Stars:** 4
- **Forks:** 2
- **Watchers:** 4
- **Open Issues:** 1
- **Total Issues:** 1
- **Pull Requests:** 0

## License

- **Type:** No license specified

## File Structure (Sample of 2 files)

Total files: 2

- `README.md` (blob)
- `latest.pdf` (blob)

## Recent Issues

- 🟢 **#1** Typo in the PointwiseRelativeRegret definition (open)

## Recent Commits

- **72a30589** add README.md - “h-grieve” (2025-02-26T21:48:04+00:00)
- **6d8dd814** upload paper - “h-grieve” (2025-02-26T21:29:32+00:00)

## External Links Found in README

- https://www.gensyn.ai/articles/rl-swarm
- https://github.com/gensyn-ai/rl-swarm

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 935185960,
  "name": "paper-rl-swarm",
  "full_name": "gensyn-ai/paper-rl-swarm",
  "description": "Technical report for the RL Swarm framework - a fully open source framework for creating RL training swarms over the internet.",
  "html_url": "https://github.com/gensyn-ai/paper-rl-swarm",
  "clone_url": "https://github.com/gensyn-ai/paper-rl-swarm.git",
  "ssh_url": "git@github.com:gensyn-ai/paper-rl-swarm.git",
  "homepage": "https://github.com/gensyn-ai/rl-swarm",
  "topics": [],
  "default_branch": "main",
  "created_at": "2025-02-19T03:26:35+00:00",
  "updated_at": "2025-05-31T17:13:02+00:00",
  "pushed_at": "2025-02-26T21:48:08+00:00",
  "size_kb": 149,
  "watchers_count": 4,
  "stargazers_count": 4,
  "forks_count": 2,
  "open_issues_count": 1,
  "license": null,
  "languages": {},
  "top_contributors": [],
  "file_tree_count": 2,
  "file_tree_sample": [
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "latest.pdf",
      "type": "blob"
    }
  ],
  "issues_count": 1,
  "pulls_count": 0,
  "recent_issues": [
    {
      "number": 1,
      "title": "Typo in the PointwiseRelativeRegret definition",
      "state": "open"
    }
  ],
  "recent_pulls": [],
  "recent_commits": [
    {
      "sha": "72a30589fb3e630a0eac30e22d1dee5e2cc5c403",
      "author": "\u201ch-grieve\u201d",
      "date": "2025-02-26T21:48:04+00:00",
      "message": "add README.md"
    },
    {
      "sha": "6d8dd8145e3d7cd571be146b62b5d1fa279fd441",
      "author": "\u201ch-grieve\u201d",
      "date": "2025-02-26T21:29:32+00:00",
      "message": "upload paper"
    }
  ],
  "readme_text": "# RL Swarm Paper\n\nThe recent success of DeepSeek-R1 highlights the potential of reinforcement learning (RL) in post-training LLMs, particularly for reasoning-intensive tasks such as mathematics where correctness can be algorithmically verified. In particular, by leveraging the GRPO algorithm introduced by Shao et al. [2024], the DeepSeek-R1-zero model [DeepSeek-AI, 2025] highlighted that models can self-evolve and develop reasoning capabilities without supervised fine-tuning (SFT) or even a critic model. These capabilities are achieved by leveraging a strong pre-trained LLM as a \u201cbase\u201d model that serves as the starting point for post-training, and then allowing the model to learn through a repeated self-assessment process in GRPO. Intuitively, each self-assessment involves a three step process where the model: i) generates multiple responses for a given prompt, ii) computes re-wards individually for each of its generated responses using a rule-based system, and iii) compares the rewards for each individual response against the average reward across all responses it generated for said prompt.\n\nA natural next step is to ask: what if we allow models to not just learn alone but together with multiple peer models on the same tasks? Can this new dimension allow each model to train more efficiently, and unlock new learning patterns for the models? This report presents RL Swarm, an open and collaborative network where models perform post-training together with an algorithm based on GRPO.\n\nWe compare models post-training on RL Swarm with the baseline of a model post-training by itself using GRPO. Our preliminary results show that models trained on the swarm produce better answers on unseen test data and generally provide more human-readable responses. These results open the path to many exciting research questions in the area of collaborative post-training using RL Swarm\u2019s infrastructure.\n\nCode [here](https://github.com/gensyn-ai/rl-swarm).\n\nBlog post [here](https://www.gensyn.ai/articles/rl-swarm).",
  "external_links_in_readme": [
    "https://www.gensyn.ai/articles/rl-swarm",
    "https://github.com/gensyn-ai/rl-swarm"
  ]
}
```

</details>


---

