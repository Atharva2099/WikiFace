# GitHub Data for meta-llama_Llama-3.1-8B-Instruct

**Task Category:** Text Generation

## Repository 1: huggingface/huggingface-llama-recipes

# GitHub Repository Data

**Repository:** [huggingface/huggingface-llama-recipes](https://github.com/huggingface/huggingface-llama-recipes)

## Basic Information

- **Description:** None
- **Created:** 2024-07-22T16:32:07+00:00
- **Last Updated:** 2025-06-19T10:03:28+00:00
- **Last Pushed:** 2025-04-30T08:53:41+00:00
- **Default Branch:** main
- **Size:** 15446 KB

## Statistics

- **Stars:** 668
- **Forks:** 77
- **Watchers:** 668
- **Open Issues:** 23
- **Total Issues:** 0
- **Pull Requests:** 42

## License

- **Type:** No license specified

## Languages

- **Jupyter Notebook:** 11,345,874 bytes
- **Python:** 20,291 bytes

## Top Contributors

1. **SunMarc** - 15 contributions
2. **sergiopaniego** - 15 contributions
3. **LysandreJik** - 14 contributions
4. **Vaibhavs10** - 13 contributions
5. **ArthurZucker** - 12 contributions
6. **osanseviero** - 10 contributions
7. **alvarobartt** - 8 contributions
8. **pcuenca** - 8 contributions
9. **xenova** - 7 contributions
10. **merveenoyan** - 7 contributions

## File Structure (Sample of 10 files)

Total files: 53

- `.github` (tree)
- `.github/CONTRIBUTING.md` (blob)
- `.github/README.md` (blob)
- `.gitignore` (blob)
- `api_inference` (tree)
- `api_inference/inference-api.ipynb` (blob)
- `assets` (tree)
- `assets/Fork.png` (blob)
- `assets/PR.png` (blob)
- `assets/gradio_chatbot_demo.png` (blob)

## Recent Issues

- 游릭 **#87** No performance gain on compile+quantization with 4090s (open)
- 游릭 **#86** Tool calling tutorial has bad import (open)
- 游댮 **#85** Update README to include Llama 3.3 (closed)
- 游댮 **#84** Tool calling recipe (closed)
- 游릭 **#83** Llama2-7b output text has irrelevant \n # characters for QA (open)

## Recent Pull Requests

- 游댮 **#85** Update README to include Llama 3.3 (closed)
- 游댮 **#84** Tool calling recipe (closed)
- 游댮 **#81** Updates CONTRIBUTING.md (closed)
- 游댮 **#80** Corrected errors in Readme.md (closed)
- 游릭 **#79** Fix: Discard KV cache for last token before reusing prompt cache for prompt + suffix (open)

## Recent Commits

- **7f5ab801** Created using Colab - Merve Noyan (2025-04-30T08:53:41+00:00)
- **a261779c** Add Llama Guard 4 - Merve Noyan (2025-04-29T18:21:43+00:00)
- **abbcb4c1** Merge pull request #85 from ariG23498/aritra/readme - vb (2024-12-10T12:51:31+00:00)
- **b46435c3** add 3.3 - ariG23498 (2024-12-10T09:14:16+00:00)
- **fdc20edf** Tool calling recipe (#84) - Sergio Paniego Blanco (2024-11-29T06:33:39+00:00)
- **a78d1093** Corrected errors in Readme.md (#80) - Parth Dwivedi (2024-10-28T04:57:50+00:00)
- **1902354e** Issue #44 Add Gradio Demos for Llama Models - Initial Contribution (#67) - Shreyas S (2024-10-21T09:20:01+00:00)
- **5994901a** Merge pull request #74 from Sakalya100/main - vb (2024-10-16T12:41:33+00:00)
- **8fa337ff** Merge pull request #69 from sergiopaniego/tgi_notebook - vb (2024-10-15T18:27:21+00:00)
- **909ef5a6** Merge branch 'tgi_notebook' of https://github.com/sergiopaniego/huggingface-llama-recipes into tgi_notebook - sergiopaniego (2024-10-15T18:04:20+00:00)

## External Links Found in README

- https://huggingface.co/meta-llama
- https://huggingface.co/learn/cookbook/index
- https://huggingface.co/blog/llama32
- https://huggingface.co/blog/llama31

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 832257345,
  "name": "huggingface-llama-recipes",
  "full_name": "huggingface/huggingface-llama-recipes",
  "description": null,
  "html_url": "https://github.com/huggingface/huggingface-llama-recipes",
  "clone_url": "https://github.com/huggingface/huggingface-llama-recipes.git",
  "ssh_url": "git@github.com:huggingface/huggingface-llama-recipes.git",
  "homepage": null,
  "topics": [],
  "default_branch": "main",
  "created_at": "2024-07-22T16:32:07+00:00",
  "updated_at": "2025-06-19T10:03:28+00:00",
  "pushed_at": "2025-04-30T08:53:41+00:00",
  "size_kb": 15446,
  "watchers_count": 668,
  "stargazers_count": 668,
  "forks_count": 77,
  "open_issues_count": 23,
  "license": null,
  "languages": {
    "Jupyter Notebook": 11345874,
    "Python": 20291
  },
  "top_contributors": [
    {
      "login": "SunMarc",
      "contributions": 15
    },
    {
      "login": "sergiopaniego",
      "contributions": 15
    },
    {
      "login": "LysandreJik",
      "contributions": 14
    },
    {
      "login": "Vaibhavs10",
      "contributions": 13
    },
    {
      "login": "ArthurZucker",
      "contributions": 12
    },
    {
      "login": "osanseviero",
      "contributions": 10
    },
    {
      "login": "alvarobartt",
      "contributions": 8
    },
    {
      "login": "pcuenca",
      "contributions": 8
    },
    {
      "login": "xenova",
      "contributions": 7
    },
    {
      "login": "merveenoyan",
      "contributions": 7
    },
    {
      "login": "ariG23498",
      "contributions": 5
    },
    {
      "login": "Sakalya100",
      "contributions": 5
    },
    {
      "login": "ianporada",
      "contributions": 3
    },
    {
      "login": "eliebak",
      "contributions": 3
    },
    {
      "login": "lewtun",
      "contributions": 2
    },
    {
      "login": "AnirudhJM24",
      "contributions": 1
    },
    {
      "login": "hanouticelina",
      "contributions": 1
    },
    {
      "login": "gabrielmbmb",
      "contributions": 1
    },
    {
      "login": "Solobrad",
      "contributions": 1
    },
    {
      "login": "Parth4git",
      "contributions": 1
    }
  ],
  "file_tree_count": 53,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/CONTRIBUTING.md",
      "type": "blob"
    },
    {
      "path": ".github/README.md",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "api_inference",
      "type": "tree"
    },
    {
      "path": "api_inference/inference-api.ipynb",
      "type": "blob"
    },
    {
      "path": "assets",
      "type": "tree"
    },
    {
      "path": "assets/Fork.png",
      "type": "blob"
    },
    {
      "path": "assets/PR.png",
      "type": "blob"
    },
    {
      "path": "assets/gradio_chatbot_demo.png",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 42,
  "recent_issues": [
    {
      "number": 87,
      "title": "No performance gain on compile+quantization with 4090s",
      "state": "open"
    },
    {
      "number": 86,
      "title": "Tool calling tutorial has bad import",
      "state": "open"
    },
    {
      "number": 85,
      "title": "Update README to include Llama 3.3",
      "state": "closed"
    },
    {
      "number": 84,
      "title": "Tool calling recipe",
      "state": "closed"
    },
    {
      "number": 83,
      "title": "Llama2-7b output text has irrelevant \\n # characters for QA",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 85,
      "title": "Update README to include Llama 3.3",
      "state": "closed"
    },
    {
      "number": 84,
      "title": "Tool calling recipe",
      "state": "closed"
    },
    {
      "number": 81,
      "title": "Updates CONTRIBUTING.md",
      "state": "closed"
    },
    {
      "number": 80,
      "title": "Corrected errors in Readme.md",
      "state": "closed"
    },
    {
      "number": 79,
      "title": "Fix: Discard KV cache for last token before reusing prompt cache for prompt + suffix",
      "state": "open"
    }
  ],
  "recent_commits": [
    {
      "sha": "7f5ab8017967914e76583eedd02b1c94f408f3ca",
      "author": "Merve Noyan",
      "date": "2025-04-30T08:53:41+00:00",
      "message": "Created using Colab"
    },
    {
      "sha": "a261779c04ac0875b4eafd3a9acc235c0ee73286",
      "author": "Merve Noyan",
      "date": "2025-04-29T18:21:43+00:00",
      "message": "Add Llama Guard 4"
    },
    {
      "sha": "abbcb4c113e9b31545a9a33ac51ccfdeae274980",
      "author": "vb",
      "date": "2024-12-10T12:51:31+00:00",
      "message": "Merge pull request #85 from ariG23498/aritra/readme"
    },
    {
      "sha": "b46435c35f3279612548b572a27d35d1c1957c97",
      "author": "ariG23498",
      "date": "2024-12-10T09:14:16+00:00",
      "message": "add 3.3"
    },
    {
      "sha": "fdc20edfd87e17d98ea1a2a6faa9bfe0d72f9a91",
      "author": "Sergio Paniego Blanco",
      "date": "2024-11-29T06:33:39+00:00",
      "message": "Tool calling recipe (#84)"
    },
    {
      "sha": "a78d1093b90b4449f2eeeedbbd6c97ddc9c7da5c",
      "author": "Parth Dwivedi",
      "date": "2024-10-28T04:57:50+00:00",
      "message": "Corrected errors in Readme.md (#80)"
    },
    {
      "sha": "1902354ef76c364f4e1094e36f65068d47556d95",
      "author": "Shreyas S",
      "date": "2024-10-21T09:20:01+00:00",
      "message": "Issue #44 Add Gradio Demos for Llama Models - Initial Contribution (#67)"
    },
    {
      "sha": "5994901a017cfaa88b26cbe736c68c8e6c09e125",
      "author": "vb",
      "date": "2024-10-16T12:41:33+00:00",
      "message": "Merge pull request #74 from Sakalya100/main"
    },
    {
      "sha": "8fa337ff2bb5b8d44dd28d3d4c17dbcc0f595e36",
      "author": "vb",
      "date": "2024-10-15T18:27:21+00:00",
      "message": "Merge pull request #69 from sergiopaniego/tgi_notebook"
    },
    {
      "sha": "909ef5a68099c9d40c54944712aa735eaf842b7d",
      "author": "sergiopaniego",
      "date": "2024-10-15T18:04:20+00:00",
      "message": "Merge branch 'tgi_notebook' of https://github.com/sergiopaniego/huggingface-llama-recipes into tgi_notebook"
    },
    {
      "sha": "1ff4a53ff9ea3d07050b0f4bc616b1ee3b2785f9",
      "author": "sergiopaniego",
      "date": "2024-10-15T18:02:52+00:00",
      "message": "Improved notebook based on suggestions"
    },
    {
      "sha": "7f69a9afdcc325d1f57fbdbca568bce2eb602354",
      "author": "Sergio Paniego Blanco",
      "date": "2024-10-15T17:56:06+00:00",
      "message": "Update .github/README.md"
    },
    {
      "sha": "360b0fcbf9db191429fb89a506423aee8dacd97e",
      "author": "Sakalya",
      "date": "2024-10-15T16:12:14+00:00",
      "message": "Added licencse for Prompt Guard and Llama Guard"
    },
    {
      "sha": "3396f38c318582464fd88e4eb6377256e7541f78",
      "author": "Sakalya",
      "date": "2024-10-14T10:21:31+00:00",
      "message": "Added explanation texts across sections and removed unused code blocks"
    },
    {
      "sha": "429d8fec982f7155ba84b4ed1db2c53f7d41aaca",
      "author": "Sergio Paniego Blanco",
      "date": "2024-10-09T09:14:45+00:00",
      "message": "Update .github/README.md"
    },
    {
      "sha": "d2c415edd5bb067e42188e88dc699fe65e268f99",
      "author": "Sergio Paniego Blanco",
      "date": "2024-10-09T09:14:15+00:00",
      "message": "Update .github/README.md"
    },
    {
      "sha": "f9a9e26eb2e3cba35e334ab0aea68570ed1364ea",
      "author": "sergiopaniego",
      "date": "2024-10-08T12:18:06+00:00",
      "message": "Added comment and renamed file/folder"
    },
    {
      "sha": "46e8016d433b21560a20c99e06044c3ad38f3a73",
      "author": "Sakalya",
      "date": "2024-10-08T08:50:20+00:00",
      "message": "Added context safety check and category returned for unsafe prompt/context"
    },
    {
      "sha": "750626abd4c6af49eb674d70dfca743c86e3edeb",
      "author": "sergiopaniego",
      "date": "2024-10-07T21:21:48+00:00",
      "message": "Updated README"
    },
    {
      "sha": "3921d1dab2bc386158af6ee46f5a95ac74a48058",
      "author": "sergiopaniego",
      "date": "2024-10-07T17:51:53+00:00",
      "message": "moved to folder"
    }
  ],
  "readme_text": "# Hugging Face Llama Recipes\n\n![thumbnail for repository](../assets/hf-llama-recepies.png)\n\n\ud83e\udd17\ud83e\udd99Welcome! This repository contains *minimal* recipes to get started quickly\nwith **Llama 3.x** models, including **Llama 3.1**, **Llama 3.2**, and **Llama 3.3**.\n\n* To get an overview of Llama 3.1, please visit the [Hugging Face announcement blog post (3.1)](https://huggingface.co/blog/llama31).\n* To get an overview of Llama 3.2, please visit the [Hugging Face announcement blog post (3.2)](https://huggingface.co/blog/llama32).\n* For more advanced end-to-end use cases with open ML, please visit the [Open Source AI Cookbook](https://huggingface.co/learn/cookbook/index).\n\nThis repository is WIP so that you might see considerable changes in the coming days.\n\n> [!NOTE]\n> To use Llama 3.x, you need to accept the license and request permission\nto access the models. Please visit [the Hugging Face repos](https://huggingface.co/meta-llama)\nand submit your request. You only need to do this once per collection; you'll get access to\nall the repos in the collection if your request is approved.\n\n## Getting Started\n\nThe easiest way to quickly run a Llama \ud83e\udd99 on your machine would be with the\n\ud83e\udd17 `transformers` repository. Make sure you have the latest release installed.\n\n```shell\n$ pip install -U transformers\n```\n\nLet us conversate with an instruction tuned model.\n\n```python\nimport torch\nfrom transformers import pipeline\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nllama_31 = \"meta-llama/Llama-3.1-8B-Instruct\" # <-- llama 3.1\nllama_32 = \"meta-llama/Llama-3.2-3B-Instruct\" # <-- llama 3.2\n\nprompt = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant, that responds as a pirate.\"},\n    {\"role\": \"user\", \"content\": \"What's Deep Learning?\"},\n]\n\ngenerator = pipeline(model=llama_32, device=device, torch_dtype=torch.bfloat16)\ngeneration = generator(\n    prompt,\n    do_sample=False,\n    temperature=1.0,\n    top_p=1,\n    max_new_tokens=50\n)\n\nprint(f\"Generation: {generation[0]['generated_text']}\")\n\n# Generation:\n# [\n#   {'role': 'system', 'content': 'You are a helpful assistant, that responds as a pirate.'},\n#   {'role': 'user', 'content': \"What's Deep Learning?\"},\n#   {'role': 'assistant', 'content': \"Yer lookin' fer a treasure trove o'\n#             knowledge on Deep Learnin', eh? Alright then, listen close and\n#             I'll tell ye about it.\\n\\nDeep Learnin' be a type o' machine\n#             learnin' that uses neural networks\"}\n# ]\n```\n\n## Local Inference\n\nWould you like to run inference of the Llama models locally?\nSo do we! The memory requirements depend on the model size and the\nprecision of the weights. Here's a table showing the approximate\nmemory needed for different configurations:\n\n| Model Size | Llama Variant | BF16/FP16 | FP8 | INT4(AWQ/GPTQ/bnb) |\n| :--: | :--: | :--: | :--: | :--: |\n| 1B | 3.2 | 2.5 GB | 1.25GB | 0.75GB |\n| 3B | 3.2 |6.5 GB | 3.2GB | 1.75GB |\n| 8B | 3.1 |16 GB | 8GB | 4GB |\n| 70B | 3.1 and 3.3 | 140 GB | 70GB | 35GB |\n|405B | 3.1 |810 GB | 405GB | 204GB |\n\n\n> [!NOTE]\n> These are estimated values and may vary based on specific\nimplementation details and optimizations.\n\nWorking with the capable Llama 3.1 8B models:\n\n* [Run Llama 3.1 8B in 4-bits with bitsandbytes](../local_inference/4bit_bnb.ipynb)\n* [Run Llama 3.1 8B in 8-bits with bitsandbytes](../local_inference/8bit_bnb.ipynb)\n* [Run Llama 3.1 8B with AWQ & fused ops](../local_inference/awq.ipynb)\n\nWorking on the \ud83d\udc18 big Llama 3.1 405B model:\n\n* [Run Llama 3.1 405B FP8](../local_inference/fp8-405B.ipynb)\n* [Run Llama 3.1 405B quantized to INT4 with AWQ](../local_inference/awq_generation.py)\n* [Run Llama 3.1 405B quantized to INT4 with GPTQ](../local_inference/gptq_generation.py)\n\n## Model Fine Tuning:\n\nIt is often not enough to run inference on the model.\nMany times, you need to fine-tune the model on some \ncustom dataset. Here are some scripts showing \nhow to fine-tune the models.\n\nFine tune models on your custom dataset:\n* [Fine tune Llama 3.2 Vision on a custom dataset](../fine_tune/Llama-Vision%20FT.ipynb)\n* [Supervised Fine Tuning on Llama 3.2 Vision with TRL](../fine_tune/sft_vlm.py)\n* [How to fine-tune Llama 3.1 8B on consumer GPU with PEFT and QLoRA with bitsandbytes](../fine_tune/peft_finetuning.py)\n* [Execute a distributed fine tuning job for the Llama 3.1 405B model on a SLURM-managed computing cluster](../fine_tune/qlora_405B.slurm)\n\n## Assisted Decoding Techniques\n\nDo you want to use the smaller Llama 3.2 models to speed up text generation\nfor bigger models? These notebooks showcase assisted decoding (speculative decoding), which gives you upto 2x speedups for text generation on Llama 3.1 70B (with greedy decoding).\n\n* [Run assisted decoding with \ud83d\udc18 Llama 3.1 70B and \ud83e\udd0f Llama 3.2 3B](../assisted_decoding/assisted_decoding_70B_3B.ipynb)\n* [Run assisted decoding with Llama 3.1 8B and Llama 3.2 1B](../assisted_decoding/assisted_decoding_8B_1B.ipynb)\n* [Assisted Decoding with 405B model](../assisted_decoding/assisted_decoding.py)\n\n## Performance Optimization\n\nLet us optimize performace shall we?\n\n* [Accelerate your inference using torch.compile](../performance_optimization/torch_compile.py)\n* [Accelerate your inference using torch.compile and 4-bit quantization with torchao](../performance_optimization/torch_compile_with_torchao.ipynb)\n* [Quantize KV Cache to lower memory requirements](../performance_optimization/quantized_cache.py)\n* [How to reuse prompts with dynamic caching](../performance_optimization/prompt_reuse.py)\n* [How to setup distributed training utilizing DeepSpeed with mixed-precision and Zero-3 optimization](../performance_optimization/deepspeed_zero3.yaml)\n\n## API inference\n\nAre these models too large for you to run at home? Would you like to experiment with Llama 70B? Try out the following examples!\n\n* [Use the Inference API for PRO users](../api_inference/inference-api.ipynb)\n\n## Llama Guard and Prompt Guard\n\nIn addition to the generative models, Meta released two new models: Llama Guard 3 and Prompt Guard. Prompt Guard is a small classifier that detects jailbreaks and prompt injections. Llama Guard 3 is a safeguard model that can classify LLM inputs and generations. Learn how to use them as done in the following notebooks:\n\n* [Detecting jailbreaks and prompt injection with Prompt Guard](../llama_guard/prompt_guard.ipynb)\n* [Integrating Llama Guard in LLM Workflows for detecting prompt safety](../llama_guard/llama_guard_3_1B.ipynb)\n\n## Synthetic Data Generation\nWith the ever hungry models, the need for synthetic data generation is\non the rise. Here we show you how to build your very own synthetic dataset.\n\n* [Generate synthetic data with `distilabel`](../synthetic_data_gen/synthetic-data-with-llama.ipynb)\n\n\n## Llama RAG \nSeeking an entry-level RAG pipeline? This notebook guides you through building a very simple streamlined RAG experiment using Llama and Hugging Face.\n\n* [Simple RAG Pipeline](../llama_rag/llama_rag_pipeline.ipynb)\n\n\n## Text Generation Inference (TGI) & API Inference with Llama Models\nText Generation Inference (TGI) framework enables efficient and  scalable deployment of Llama models. In this notebook we'll learn how to integrate TGI for fast text generation and to consume already deployed Llama models via  the Inference API:\n\n* [Text Generation Inference (TGI) with Llama Models](../llama_tgi_api_inference/tgi_api_inference_recipe.ipynb) \n\n## Chatbot Demo with Llama Models \nWould you like to build a chatbot with Llama models? Here's a simple example to get you started.\n\n* [Chatbot with Llama Models](../gradio_demos/chatbot_demo.ipynb)\n\n## Tool Calling\n\nIn this notebook, we explore how to leverage the **tool-calling capabilities** of Llama models, also using and integrating the `chat_template` functionality for tool interactions.\n\n* [Tool calling with Llama Models](../tool_calling/tool_calling.ipynb)\n",
  "external_links_in_readme": [
    "https://huggingface.co/meta-llama",
    "https://huggingface.co/learn/cookbook/index",
    "https://huggingface.co/blog/llama32",
    "https://huggingface.co/blog/llama31"
  ]
}
```

</details>


---

## Repository 2: meta-llama/llama3

# GitHub Repository Data

**Repository:** [meta-llama/llama3](https://github.com/meta-llama/llama3)

## Basic Information

- **Description:** The official Meta Llama 3 GitHub site
- **Created:** 2024-03-15T17:57:00+00:00
- **Last Updated:** 2025-06-21T19:01:33+00:00
- **Last Pushed:** 2025-01-26T21:39:06+00:00
- **Default Branch:** main
- **Size:** 569 KB

## Statistics

- **Stars:** 28,789
- **Forks:** 3,401
- **Watchers:** 28,789
- **Open Issues:** 211
- **Total Issues:** 0
- **Pull Requests:** 99

## License

- **Type:** Other
- **SPDX ID:** NOASSERTION
- **URL:** [License](https://github.com/meta-llama/llama3/blob/main/LICENSE)

## Languages

- **Python:** 42,332 bytes
- **Shell:** 2,686 bytes

## Top Contributors

1. **jspisak** - 50 contributions
2. **ruanslv** - 12 contributions
3. **astonzhang** - 8 contributions
4. **rohit-ptl** - 8 contributions
5. **aakashapoorv** - 7 contributions
6. **xingjia01** - 6 contributions
7. **HamidShojanazeri** - 5 contributions
8. **subramen** - 5 contributions
9. **fbnav** - 5 contributions
10. **jelmervdl** - 4 contributions

## File Structure (Sample of 10 files)

Total files: 23

- `.github` (tree)
- `.github/ISSUE_TEMPLATE` (tree)
- `.github/ISSUE_TEMPLATE/bug_report.md` (blob)
- `.gitignore` (blob)
- `CODE_OF_CONDUCT.md` (blob)
- `CONTRIBUTING.md` (blob)
- `LICENSE` (blob)
- `Llama3_Repo.jpeg` (blob)
- `MODEL_CARD.md` (blob)
- `README.md` (blob)

## Recent Issues

- 游릭 **#401** Groq API error (open)
- 游릭 **#400** Is there any code change guide for running 8b on CPU laptop? (open)
- 游릭 **#399** Update model.py (open)
- 游댮 **#398** 250425_model.py training mode update (closed)
- 游릭 **#397** A TEST file (open)

## Recent Pull Requests

- 游릭 **#399** Update model.py (open)
- 游댮 **#398** 250425_model.py training mode update (closed)
- 游릭 **#397** A TEST file (open)
- 游댮 **#394** Codespace musical space umbrella (closed)
- 游댮 **#391** updating visual code With llama (closed)

## Recent Commits

- **a0940f9c** Update README.md - amitsangani (2025-01-26T21:39:06+00:00)
- **11817d47** Add note about 3.1 download - Suraj Subramanian (2024-07-31T17:18:29+00:00)
- **9b60364e** Merge pull request #292 from Vasanthrs-dev/patch-2 - Joseph Spisak (2024-07-26T16:55:16+00:00)
- **c7c6c2ef** Update eval_details.md - Vasanth RS (2024-07-26T15:38:49+00:00)
- **18f515a3** Update README.md - Joseph Spisak (2024-07-23T14:50:32+00:00)
- **e14519ce** Update MODEL_CARD.md - Joseph Spisak (2024-07-11T13:26:33+00:00)
- **d3eca219** Merge pull request #141 from pchng/main - Suraj Subramanian (2024-07-03T16:59:46+00:00)
- **bf8d18cd** Merge pull request #135 from aakashapoorv/update-examples - Suraj Subramanian (2024-06-05T16:59:07+00:00)
- **f2bb4c5b** add newline at end of file - Aakash Apoorv (2024-05-21T16:18:33+00:00)
- **bed1106a** Merge remote-tracking branch 'refs/remotes/origin/update-examples' into update-examples - Aakash Apoorv (2024-05-21T16:14:02+00:00)

## External Links Found in README

- https://llama.meta.com/">Website</a>&nbsp
- https://ai.meta.com/static-resource/responsible-use-guide/
- https://llama.meta.com/get-started/">Get
- https://ai.meta.com/blog/">
- https://github.com/facebookresearch/llama-recipes
- https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
- https://github.com/meta-llama/llama-models
- https://github.com/meta-llama/llama-agentic-system
- https://github.com/meta-llama/llama3/issues](https://github.com/meta-llama/llama3/issues
- https://huggingface.co/meta-llama
- https://github.com/meta-llama/llama-recipes
- https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py#L202
- http://facebook.com/whitehat/info
- https://huggingface.co/docs/transformers/en/main_classes/pipelines
- https://github.com/meta-llama/PurpleLlama
- http://developers.facebook.com/llama_output_feedback
- https://github.com/meta-llama/llama3/blob/main/Llama3_Repo.jpeg"
- https://huggingface.co/meta-Llama">
- https://llama.meta.com/llama-downloads/
- https://github.com/facebookresearch/llama-recipes/

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 772699441,
  "name": "llama3",
  "full_name": "meta-llama/llama3",
  "description": "The official Meta Llama 3 GitHub site",
  "html_url": "https://github.com/meta-llama/llama3",
  "clone_url": "https://github.com/meta-llama/llama3.git",
  "ssh_url": "git@github.com:meta-llama/llama3.git",
  "homepage": "",
  "topics": [],
  "default_branch": "main",
  "created_at": "2024-03-15T17:57:00+00:00",
  "updated_at": "2025-06-21T19:01:33+00:00",
  "pushed_at": "2025-01-26T21:39:06+00:00",
  "size_kb": 569,
  "watchers_count": 28789,
  "stargazers_count": 28789,
  "forks_count": 3401,
  "open_issues_count": 211,
  "license": {
    "key": "other",
    "name": "Other",
    "spdx_id": "NOASSERTION",
    "url": "https://github.com/meta-llama/llama3/blob/main/LICENSE"
  },
  "languages": {
    "Python": 42332,
    "Shell": 2686
  },
  "top_contributors": [
    {
      "login": "jspisak",
      "contributions": 50
    },
    {
      "login": "ruanslv",
      "contributions": 12
    },
    {
      "login": "astonzhang",
      "contributions": 8
    },
    {
      "login": "rohit-ptl",
      "contributions": 8
    },
    {
      "login": "aakashapoorv",
      "contributions": 7
    },
    {
      "login": "xingjia01",
      "contributions": 6
    },
    {
      "login": "HamidShojanazeri",
      "contributions": 5
    },
    {
      "login": "subramen",
      "contributions": 5
    },
    {
      "login": "fbnav",
      "contributions": 5
    },
    {
      "login": "jelmervdl",
      "contributions": 4
    },
    {
      "login": "pcuenca",
      "contributions": 3
    },
    {
      "login": "pchng",
      "contributions": 2
    },
    {
      "login": "hyungupark",
      "contributions": 1
    },
    {
      "login": "hinskip92",
      "contributions": 1
    },
    {
      "login": "amitsangani",
      "contributions": 1
    },
    {
      "login": "albertodepaola",
      "contributions": 1
    },
    {
      "login": "Vasanthrs-dev",
      "contributions": 1
    },
    {
      "login": "zsmeditation",
      "contributions": 1
    },
    {
      "login": "lukedeo",
      "contributions": 1
    },
    {
      "login": "lovishmadaan",
      "contributions": 1
    }
  ],
  "file_tree_count": 23,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/bug_report.md",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "CODE_OF_CONDUCT.md",
      "type": "blob"
    },
    {
      "path": "CONTRIBUTING.md",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "Llama3_Repo.jpeg",
      "type": "blob"
    },
    {
      "path": "MODEL_CARD.md",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 99,
  "recent_issues": [
    {
      "number": 401,
      "title": "Groq API error",
      "state": "open"
    },
    {
      "number": 400,
      "title": "Is there any code change guide for running 8b on CPU laptop?",
      "state": "open"
    },
    {
      "number": 399,
      "title": "Update model.py",
      "state": "open"
    },
    {
      "number": 398,
      "title": "250425_model.py training mode update",
      "state": "closed"
    },
    {
      "number": 397,
      "title": "A TEST file",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 399,
      "title": "Update model.py",
      "state": "open"
    },
    {
      "number": 398,
      "title": "250425_model.py training mode update",
      "state": "closed"
    },
    {
      "number": 397,
      "title": "A TEST file",
      "state": "open"
    },
    {
      "number": 394,
      "title": "Codespace musical space umbrella",
      "state": "closed"
    },
    {
      "number": 391,
      "title": "updating visual code With llama",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "a0940f9cf7065d45bb6675660f80d305c041a754",
      "author": "amitsangani",
      "date": "2025-01-26T21:39:06+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "11817d47e1ba7a4959b025eb1ca308572e0e3963",
      "author": "Suraj Subramanian",
      "date": "2024-07-31T17:18:29+00:00",
      "message": "Add note about 3.1 download"
    },
    {
      "sha": "9b60364e4f4abb06e2fa184eda225e1db71d7192",
      "author": "Joseph Spisak",
      "date": "2024-07-26T16:55:16+00:00",
      "message": "Merge pull request #292 from Vasanthrs-dev/patch-2"
    },
    {
      "sha": "c7c6c2ef0c0109a281db65dd6ecbc96f9f9fa2fe",
      "author": "Vasanth RS",
      "date": "2024-07-26T15:38:49+00:00",
      "message": "Update eval_details.md"
    },
    {
      "sha": "18f515a3c3c5f02cf45c6ac56cc5d039488e867a",
      "author": "Joseph Spisak",
      "date": "2024-07-23T14:50:32+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "e14519ce6a4296858940c68a37bded0a929481d1",
      "author": "Joseph Spisak",
      "date": "2024-07-11T13:26:33+00:00",
      "message": "Update MODEL_CARD.md"
    },
    {
      "sha": "d3eca219e453eca77ce1e180f48954c7d67467a9",
      "author": "Suraj Subramanian",
      "date": "2024-07-03T16:59:46+00:00",
      "message": "Merge pull request #141 from pchng/main"
    },
    {
      "sha": "bf8d18cd087a4a0b3f61075b7de0b86cf6c70697",
      "author": "Suraj Subramanian",
      "date": "2024-06-05T16:59:07+00:00",
      "message": "Merge pull request #135 from aakashapoorv/update-examples"
    },
    {
      "sha": "f2bb4c5b1d1a11152740267e0827eb087d7fef64",
      "author": "Aakash Apoorv",
      "date": "2024-05-21T16:18:33+00:00",
      "message": "add newline at end of file"
    },
    {
      "sha": "bed1106abc33d01309de5a4091332cef56cc0688",
      "author": "Aakash Apoorv",
      "date": "2024-05-21T16:14:02+00:00",
      "message": "Merge remote-tracking branch 'refs/remotes/origin/update-examples' into update-examples"
    },
    {
      "sha": "23e448f154bc07e0f01db341586b3e0e2e2cc9e4",
      "author": "Aakash Apoorv",
      "date": "2024-05-21T16:10:36+00:00",
      "message": "add assertion to build function of Llama"
    },
    {
      "sha": "c4b2f5d8b2019324bfe710fdce8bda08c861a5e7",
      "author": "Aakash Apoorv",
      "date": "2024-05-21T13:47:43+00:00",
      "message": "add add assertion to build function of Llama"
    },
    {
      "sha": "95d36c2ee122c0a84081d95e663c92c6a60bedee",
      "author": "Aakash Apoorv",
      "date": "2024-05-21T13:41:38+00:00",
      "message": "revert changes in examples"
    },
    {
      "sha": "e2c027f06880f2287939264bcaffa8cb5d5e9cc0",
      "author": "Aakash Apoorv",
      "date": "2024-05-21T13:38:52+00:00",
      "message": "Merge remote-tracking branch 'origin/main' into update-examples"
    },
    {
      "sha": "14aab0428d3ec3a9596f1dea06d9c564f9c0e35f",
      "author": "Suraj Subramanian",
      "date": "2024-05-14T17:39:39+00:00",
      "message": "Merge pull request #191 from antonioramos1/fix_end_of_header_token_names"
    },
    {
      "sha": "b20cad1e58f86e324c5df30c74e9dabc51696ddd",
      "author": "antoniora",
      "date": "2024-05-14T17:27:57+00:00",
      "message": "update token name"
    },
    {
      "sha": "f37d7d3e023b69b6fdb86c1885370bc7362d8dcf",
      "author": "xingjia01",
      "date": "2024-05-14T16:32:32+00:00",
      "message": "Merge pull request #207 from hyungupark/patch-1"
    },
    {
      "sha": "d587ae0ab96a7c50c439025a7b2c1023f403e7f3",
      "author": "hyungupark",
      "date": "2024-05-14T06:18:01+00:00",
      "message": "Update download.sh"
    },
    {
      "sha": "cc44ca2e1c269f0e56e6926d7f4837c983c060dc",
      "author": "Joseph Spisak",
      "date": "2024-05-14T04:09:13+00:00",
      "message": "Merge pull request #134 from RealWorga/patch-1"
    },
    {
      "sha": "07c61c8e03030cea03e4a6b3c69b4802f03e6aa9",
      "author": "antoniora",
      "date": "2024-05-07T07:26:29+00:00",
      "message": "Fixes end_of_header token name in comments"
    }
  ],
  "readme_text": "<p align=\"center\">\n  <img src=\"https://github.com/meta-llama/llama3/blob/main/Llama3_Repo.jpeg\" width=\"400\"/>\n</p>\n\n<p align=\"center\">\n        \ud83e\udd17 <a href=\"https://huggingface.co/meta-Llama\"> Models on Hugging Face</a>&nbsp | <a href=\"https://ai.meta.com/blog/\"> Blog</a>&nbsp |  <a href=\"https://llama.meta.com/\">Website</a>&nbsp | <a href=\"https://llama.meta.com/get-started/\">Get Started</a>&nbsp\n<br>\n\n---\n\n## **Note of deprecation**\n\nThank you for developing with Llama models. As part of the Llama 3.1 release, we\u2019ve consolidated GitHub repos and added some additional repos as we\u2019ve expanded Llama\u2019s functionality into being an e2e Llama Stack. Please use the following repos going forward:\n- [llama-models](https://github.com/meta-llama/llama-models) - Central repo for the foundation models including basic utilities, model cards, license and use policies\n- [PurpleLlama](https://github.com/meta-llama/PurpleLlama) - Key component of Llama Stack focusing on safety risks and inference time mitigations \n- [llama-toolchain](https://github.com/meta-llama/llama-toolchain) - Model development (inference/fine-tuning/safety shields/synthetic data generation) interfaces and canonical implementations\n- [llama-agentic-system](https://github.com/meta-llama/llama-agentic-system) - E2E standalone Llama Stack system, along with opinionated underlying interface, that enables creation of agentic applications\n- [llama-cookbook](https://github.com/meta-llama/llama-recipes) - Community driven scripts and integrations\n\nIf you have any questions, please feel free to file an issue on any of the above repos and we will do our best to respond in a timely manner. \n\nThank you!\n\n\n# (Deprecated) Meta Llama 3\n\nWe are unlocking the power of large language models. Our latest version of Llama is now accessible to individuals, creators, researchers, and businesses of all sizes so that they can experiment, innovate, and scale their ideas responsibly.\n\nThis release includes model weights and starting code for pre-trained and instruction-tuned Llama 3 language models \u2014 including sizes of 8B to 70B parameters.\n\nThis repository is a minimal example of loading Llama 3 models and running inference. For more detailed examples, see [llama-cookbook](https://github.com/facebookresearch/llama-recipes/).\n\n## Download\n\nTo download the model weights and tokenizer, please visit the [Meta Llama website](https://llama.meta.com/llama-downloads/) and accept our License.\n\nOnce your request is approved, you will receive a signed URL over email. Then, run the download.sh script, passing the URL provided when prompted to start the download.\n\nPre-requisites: Ensure you have `wget` and `md5sum` installed. Then run the script: `./download.sh`.\n\nRemember that the links expire after 24 hours and a certain amount of downloads. You can always re-request a link if you start seeing errors such as `403: Forbidden`.\n\n### Access to Hugging Face\n\nWe also provide downloads on [Hugging Face](https://huggingface.co/meta-llama), in both transformers and native `llama3` formats. To download the weights from Hugging Face, please follow these steps:\n\n- Visit one of the repos, for example [meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct).\n- Read and accept the license. Once your request is approved, you'll be granted access to all the Llama 3 models. Note that requests used to take up to one hour to get processed.\n- To download the original native weights to use with this repo, click on the \"Files and versions\" tab and download the contents of the `original` folder. You can also download them from the command line if you `pip install huggingface-hub`:\n\n```bash\nhuggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --include \"original/*\" --local-dir meta-llama/Meta-Llama-3-8B-Instruct\n```\n\n- To use with transformers, the following [pipeline](https://huggingface.co/docs/transformers/en/main_classes/pipelines) snippet will download and cache the weights:\n\n  ```python\n  import transformers\n  import torch\n\n  model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\n  pipeline = transformers.pipeline(\n    \"text-generation\",\n    model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n    model_kwargs={\"torch_dtype\": torch.bfloat16},\n    device=\"cuda\",\n  )\n  ```\n\n## Quick Start\n\nYou can follow the steps below to get up and running with Llama 3 models quickly. These steps will let you run quick inference locally. For more examples, see the [Llama Cookbook repository](https://github.com/facebookresearch/llama-recipes).\n\n1. Clone and download this repository in a conda env with PyTorch / CUDA.\n\n2. In the top-level directory run:\n    ```bash\n    pip install -e .\n    ```\n3. Visit the [Meta Llama website](https://llama.meta.com/llama-downloads/) and register to download the model/s.\n\n4. Once registered, you will get an email with a URL to download the models. You will need this URL when you run the download.sh script.\n\n5. Once you get the email, navigate to your downloaded llama repository and run the download.sh script.\n    - Make sure to grant execution permissions to the download.sh script\n    - During this process, you will be prompted to enter the URL from the email.\n    - Do not use the \u201cCopy Link\u201d option; copy the link from the email manually.\n\n6. Once the model/s you want have been downloaded, you can run the model locally using the command below:\n```bash\ntorchrun --nproc_per_node 1 example_chat_completion.py \\\n    --ckpt_dir Meta-Llama-3-8B-Instruct/ \\\n    --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model \\\n    --max_seq_len 512 --max_batch_size 6\n```\n**Note**\n- Replace  `Meta-Llama-3-8B-Instruct/` with the path to your checkpoint directory and `Meta-Llama-3-8B-Instruct/tokenizer.model` with the path to your tokenizer model.\n- The `\u2013nproc_per_node` should be set to the [MP](#inference) value for the model you are using.\n- Adjust the `max_seq_len` and `max_batch_size` parameters as needed.\n- This example runs the [example_chat_completion.py](example_chat_completion.py) found in this repository, but you can change that to a different .py file.\n\n## Inference\n\nDifferent models require different model-parallel (MP) values:\n\n|  Model | MP |\n|--------|----|\n| 8B     | 1  |\n| 70B    | 8  |\n\nAll models support sequence length up to 8192 tokens, but we pre-allocate the cache according to `max_seq_len` and `max_batch_size` values. So set those according to your hardware.\n\n### Pretrained Models\n\nThese models are not finetuned for chat or Q&A. They should be prompted so that the expected answer is the natural continuation of the prompt.\n\nSee `example_text_completion.py` for some examples. To illustrate, see the command below to run it with the llama-3-8b model (`nproc_per_node` needs to be set to the `MP` value):\n\n```\ntorchrun --nproc_per_node 1 example_text_completion.py \\\n    --ckpt_dir Meta-Llama-3-8B/ \\\n    --tokenizer_path Meta-Llama-3-8B/tokenizer.model \\\n    --max_seq_len 128 --max_batch_size 4\n```\n\n### Instruction-tuned Models\n\nThe fine-tuned models were trained for dialogue applications. To get the expected features and performance for them, specific formatting defined in [`ChatFormat`](https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py#L202)\nneeds to be followed: The prompt begins with a `<|begin_of_text|>` special token, after which one or more messages follow. Each message starts with the `<|start_header_id|>` tag, the role `system`, `user` or `assistant`, and the `<|end_header_id|>` tag. After a double newline `\\n\\n`, the message's contents follow. The end of each message is marked by the `<|eot_id|>` token.\n\nYou can also deploy additional classifiers to filter out inputs and outputs that are deemed unsafe. See the llama-cookbook repo for [an example](https://github.com/meta-llama/llama-recipes/blob/main/recipes/inference/local_inference/inference.py) of how to add a safety checker to the inputs and outputs of your inference code.\n\nExamples using llama-3-8b-chat:\n\n```\ntorchrun --nproc_per_node 1 example_chat_completion.py \\\n    --ckpt_dir Meta-Llama-3-8B-Instruct/ \\\n    --tokenizer_path Meta-Llama-3-8B-Instruct/tokenizer.model \\\n    --max_seq_len 512 --max_batch_size 6\n```\n\nLlama 3 is a new technology that carries potential risks with use. Testing conducted to date has not \u2014 and could not \u2014 cover all scenarios.\nTo help developers address these risks, we have created the [Responsible Use Guide](https://ai.meta.com/static-resource/responsible-use-guide/).\n\n## Issues\n\nPlease report any software \u201cbug\u201d or other problems with the models through one of the following means:\n- Reporting issues with the model: [https://github.com/meta-llama/llama3/issues](https://github.com/meta-llama/llama3/issues)\n- Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n## Model Card\nSee [MODEL_CARD.md](MODEL_CARD.md).\n\n## License\n\nOur model and weights are licensed for researchers and commercial entities, upholding the principles of openness. Our mission is to empower individuals and industry through this opportunity while fostering an environment of discovery and ethical AI advancements.\n\nSee the [LICENSE](LICENSE) file, as well as our accompanying [Acceptable Use Policy](USE_POLICY.md)\n\n## Questions\n\nFor common questions, the FAQ can be found [here](https://llama.meta.com/faq), which will be updated over time as new questions arise.\n",
  "external_links_in_readme": [
    "https://llama.meta.com/\">Website</a>&nbsp",
    "https://ai.meta.com/static-resource/responsible-use-guide/",
    "https://llama.meta.com/get-started/\">Get",
    "https://ai.meta.com/blog/\">",
    "https://github.com/facebookresearch/llama-recipes",
    "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct",
    "https://github.com/meta-llama/llama-models",
    "https://github.com/meta-llama/llama-agentic-system",
    "https://github.com/meta-llama/llama3/issues](https://github.com/meta-llama/llama3/issues",
    "https://huggingface.co/meta-llama",
    "https://github.com/meta-llama/llama-recipes",
    "https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py#L202",
    "http://facebook.com/whitehat/info",
    "https://huggingface.co/docs/transformers/en/main_classes/pipelines",
    "https://github.com/meta-llama/PurpleLlama",
    "http://developers.facebook.com/llama_output_feedback",
    "https://github.com/meta-llama/llama3/blob/main/Llama3_Repo.jpeg\"",
    "https://huggingface.co/meta-Llama\">",
    "https://llama.meta.com/llama-downloads/",
    "https://github.com/facebookresearch/llama-recipes/",
    "https://github.com/meta-llama/llama-recipes/blob/main/recipes/inference/local_inference/inference.py",
    "https://llama.meta.com/faq",
    "https://github.com/meta-llama/llama-toolchain"
  ]
}
```

</details>


---

## Repository 3: meta-llama/llama-agentic-system

# GitHub Repository Data

**Repository:** [meta-llama/llama-stack-apps](https://github.com/meta-llama/llama-stack-apps)

## Basic Information

- **Description:** Agentic components of the Llama Stack APIs
- **Created:** 2024-06-25T22:29:07+00:00
- **Last Updated:** 2025-06-19T08:00:05+00:00
- **Last Pushed:** 2025-04-30T18:01:33+00:00
- **Default Branch:** main
- **Size:** 103737 KB

## Statistics

- **Stars:** 4,262
- **Forks:** 627
- **Watchers:** 4,262
- **Open Issues:** 26
- **Total Issues:** 0
- **Pull Requests:** 135

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/meta-llama/llama-stack-apps/blob/main/LICENSE)

## Top Contributors

1. **yanxi0830** - 66 contributions
2. **ashwinb** - 50 contributions
3. **dltn** - 23 contributions
4. **hardikjshah** - 12 contributions
5. **jeffxtang** - 11 contributions
6. **cmodi-meta** - 9 contributions
7. **ehhuang** - 7 contributions
8. **Riandy** - 7 contributions
9. **raghotham** - 4 contributions
10. **WuhanMonkey** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 119

- `.flake8` (blob)
- `.github` (tree)
- `.github/CODEOWNERS` (blob)
- `.github/ISSUE_TEMPLATE` (tree)
- `.github/ISSUE_TEMPLATE/bug.yml` (blob)
- `.github/ISSUE_TEMPLATE/feature-request.yml` (blob)
- `.github/PULL_REQUEST_TEMPLATE.md` (blob)
- `.gitignore` (blob)
- `.gitmodules` (blob)
- `.pre-commit-config.yaml` (blob)

## Recent Issues

- 游릭 **#214** Tool call not working (open)
- 游댮 **#213** Update README.md (closed)
- 游댮 **#211** feat: add an example with image input (closed)
- 游댮 **#210** feat: Updates to examples/agents  (closed)
- 游댮 **#209** update rag as attachment script (closed)

## Recent Pull Requests

- 游댮 **#213** Update README.md (closed)
- 游댮 **#211** feat: add an example with image input (closed)
- 游댮 **#210** feat: Updates to examples/agents  (closed)
- 游댮 **#209** update rag as attachment script (closed)
- 游댮 **#208** chore: simplify imports (closed)

## Recent Commits

- **a1b3d89a** fix: small cleanup for README - Ashwin Bharambe (2025-04-22T18:05:25+00:00)
- **5ef19db5** Update README.md (#213) - ttharuntej (2025-04-22T18:00:17+00:00)
- **3153a841** Update README.md - Hardik Shah (2025-04-03T01:20:53+00:00)
- **8e20887c** feat: add an example with image input (#211) - ehhuang (2025-04-02T23:07:20+00:00)
- **5f59693b** docs: update readme (#203) - Reid (2025-04-01T06:38:58+00:00)
- **c6b8be26** feat: Updates to examples/agents  (#210) - Hardik Shah (2025-04-01T04:17:45+00:00)
- **592a101c** update rag as attachment script (#209) - Xi Yan (2025-03-23T23:57:56+00:00)
- **d53823eb** fix: update scripts so you are explicitly asked for model IDs and we dont pick random models which dont work - Ashwin Bharambe (2025-03-23T22:30:13+00:00)
- **355abf49** chore: simplify imports (#208) - ehhuang (2025-03-20T17:16:24+00:00)
- **21c526f9** Move demo apps from Apps to SDK (#207) - Chirag Modi (2025-03-15T00:06:39+00:00)

## External Links Found in README

- https://developer.wolframalpha.com/
- https://github.com/meta-llama/llama-stack-client-node
- https://github.com/meta-llama/llama-stack-client-python
- https://www.nba.com/playoffs/2024",
- http://localhost:8321`
- https://brave.com/search/api/
- https://img.shields.io/discord/1257833999603335178
- https://en.wikipedia.org/wiki/2024_NBA_playoffs",
- https://www.gradio.app/
- https://github.com/meta-llama/llama-stack
- https://www.basketball-reference.com/playoffs/2024-nba-western-conference-semifinals-mavericks-vs-thunder.html",
- https://github.com/meta-llama/llama-stack-client-kotlin
- http://[::]:8321
- https://github.com/meta-llama/llama-stack-apps/tree/main/examples/agent_store
- https://discord.gg/llama-stack
- https://llama-stack.readthedocs.io/en/latest/getting_started/index.html
- https://github.com/meta-llama/llama-stack-client-swift

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 820144632,
  "name": "llama-stack-apps",
  "full_name": "meta-llama/llama-stack-apps",
  "description": "Agentic components of the Llama Stack APIs",
  "html_url": "https://github.com/meta-llama/llama-stack-apps",
  "clone_url": "https://github.com/meta-llama/llama-stack-apps.git",
  "ssh_url": "git@github.com:meta-llama/llama-stack-apps.git",
  "homepage": null,
  "topics": [],
  "default_branch": "main",
  "created_at": "2024-06-25T22:29:07+00:00",
  "updated_at": "2025-06-19T08:00:05+00:00",
  "pushed_at": "2025-04-30T18:01:33+00:00",
  "size_kb": 103737,
  "watchers_count": 4262,
  "stargazers_count": 4262,
  "forks_count": 627,
  "open_issues_count": 26,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/meta-llama/llama-stack-apps/blob/main/LICENSE"
  },
  "languages": {},
  "top_contributors": [
    {
      "login": "yanxi0830",
      "contributions": 66
    },
    {
      "login": "ashwinb",
      "contributions": 50
    },
    {
      "login": "dltn",
      "contributions": 23
    },
    {
      "login": "hardikjshah",
      "contributions": 12
    },
    {
      "login": "jeffxtang",
      "contributions": 11
    },
    {
      "login": "cmodi-meta",
      "contributions": 9
    },
    {
      "login": "ehhuang",
      "contributions": 7
    },
    {
      "login": "Riandy",
      "contributions": 7
    },
    {
      "login": "raghotham",
      "contributions": 4
    },
    {
      "login": "WuhanMonkey",
      "contributions": 3
    },
    {
      "login": "dineshyv",
      "contributions": 3
    },
    {
      "login": "heyjustinai",
      "contributions": 3
    },
    {
      "login": "terrytangyuan",
      "contributions": 2
    },
    {
      "login": "cheesecake100201",
      "contributions": 2
    },
    {
      "login": "varunfb",
      "contributions": 1
    },
    {
      "login": "ttharuntej",
      "contributions": 1
    },
    {
      "login": "architSrivastav",
      "contributions": 1
    },
    {
      "login": "benjibc",
      "contributions": 1
    },
    {
      "login": "Yuan-ManX",
      "contributions": 1
    },
    {
      "login": "vladimirivic",
      "contributions": 1
    }
  ],
  "file_tree_count": 119,
  "file_tree_sample": [
    {
      "path": ".flake8",
      "type": "blob"
    },
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/CODEOWNERS",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/bug.yml",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/feature-request.yml",
      "type": "blob"
    },
    {
      "path": ".github/PULL_REQUEST_TEMPLATE.md",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": ".gitmodules",
      "type": "blob"
    },
    {
      "path": ".pre-commit-config.yaml",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 135,
  "recent_issues": [
    {
      "number": 214,
      "title": "Tool call not working",
      "state": "open"
    },
    {
      "number": 213,
      "title": "Update README.md",
      "state": "closed"
    },
    {
      "number": 211,
      "title": "feat: add an example with image input",
      "state": "closed"
    },
    {
      "number": 210,
      "title": "feat: Updates to examples/agents ",
      "state": "closed"
    },
    {
      "number": 209,
      "title": "update rag as attachment script",
      "state": "closed"
    }
  ],
  "recent_pulls": [
    {
      "number": 213,
      "title": "Update README.md",
      "state": "closed"
    },
    {
      "number": 211,
      "title": "feat: add an example with image input",
      "state": "closed"
    },
    {
      "number": 210,
      "title": "feat: Updates to examples/agents ",
      "state": "closed"
    },
    {
      "number": 209,
      "title": "update rag as attachment script",
      "state": "closed"
    },
    {
      "number": 208,
      "title": "chore: simplify imports",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "a1b3d89ad7f47f4d8e5cb6510bb6ba0e3bbabb72",
      "author": "Ashwin Bharambe",
      "date": "2025-04-22T18:05:25+00:00",
      "message": "fix: small cleanup for README"
    },
    {
      "sha": "5ef19db54b82d71507c7de6f8a9ab2f72d03c879",
      "author": "ttharuntej",
      "date": "2025-04-22T18:00:17+00:00",
      "message": "Update README.md (#213)"
    },
    {
      "sha": "3153a8418c23eef200c87f019b07f98d9f8af1e7",
      "author": "Hardik Shah",
      "date": "2025-04-03T01:20:53+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "8e20887cf61aa3f03a76cbf6c1eef3e1294e41ec",
      "author": "ehhuang",
      "date": "2025-04-02T23:07:20+00:00",
      "message": "feat: add an example with image input (#211)"
    },
    {
      "sha": "5f59693b907fc45c9dc059f77450123972d23f0b",
      "author": "Reid",
      "date": "2025-04-01T06:38:58+00:00",
      "message": "docs: update readme (#203)"
    },
    {
      "sha": "c6b8be2624870692b5ee9027255cffd676192f3b",
      "author": "Hardik Shah",
      "date": "2025-04-01T04:17:45+00:00",
      "message": "feat: Updates to examples/agents  (#210)"
    },
    {
      "sha": "592a101c155082125d8f1170cf4824860a0fd921",
      "author": "Xi Yan",
      "date": "2025-03-23T23:57:56+00:00",
      "message": "update rag as attachment script (#209)"
    },
    {
      "sha": "d53823ebfcc40f3fe24a62db58547f3fc9613a55",
      "author": "Ashwin Bharambe",
      "date": "2025-03-23T22:30:13+00:00",
      "message": "fix: update scripts so you are explicitly asked for model IDs and we dont pick random models which dont work"
    },
    {
      "sha": "355abf4994186be2fbf27e10fcce6995af51b8bc",
      "author": "ehhuang",
      "date": "2025-03-20T17:16:24+00:00",
      "message": "chore: simplify imports (#208)"
    },
    {
      "sha": "21c526f9691011255dfb856ba4f9cdd90d51878c",
      "author": "Chirag Modi",
      "date": "2025-03-15T00:06:39+00:00",
      "message": "Move demo apps from Apps to SDK (#207)"
    },
    {
      "sha": "622e27bf710db2226cd4fa89c8a9fe08af1028c6",
      "author": "cmodi-meta",
      "date": "2025-03-14T19:38:39+00:00",
      "message": "Readme Cleanup and add LS-Swift Project Dependecy in iOSQuickDemo"
    },
    {
      "sha": "50d1e5e7768ef92164555479239a19433d7edc19",
      "author": "cmodi-meta",
      "date": "2025-03-14T01:10:47+00:00",
      "message": "Including LS-Swift Project Dependency and Rolling back to LS v0.1.4"
    },
    {
      "sha": "3ec1309c62756d9d415692b6ead6d27c5e905e0e",
      "author": "Jeff Tang",
      "date": "2025-03-11T23:27:26+00:00",
      "message": "ios demo README update"
    },
    {
      "sha": "0adb6049eb265d8448be8c204d85fecc3b1f4c77",
      "author": "Jeff Tang",
      "date": "2025-03-11T23:20:56+00:00",
      "message": "RemoteAgents passing API key"
    },
    {
      "sha": "f3383f454e19e0c677468f6dc030e0a1d4567fea",
      "author": "Jeff Tang",
      "date": "2025-03-11T22:51:04+00:00",
      "message": "passing together api key in remote inference; readme update"
    },
    {
      "sha": "b1e6d0d46bf69ce0b09de8c516fe185d1a966d39",
      "author": "Chirag Modi",
      "date": "2025-03-13T01:21:22+00:00",
      "message": "Clean-up iOSCalendarAssitantWithLocalInf Project Dependencies (#206)"
    },
    {
      "sha": "c52943f2a8020bf70cf98b6dcf4aa38845249b9c",
      "author": "Xi Yan",
      "date": "2025-03-10T18:14:49+00:00",
      "message": "chore: update react example (#204)"
    },
    {
      "sha": "d5ba6918e4aed93e29ab914ecdbcbc280f995e08",
      "author": "ehhuang",
      "date": "2025-03-07T01:02:34+00:00",
      "message": "chore: update example to use new Agent API (#200)"
    },
    {
      "sha": "806bdb191d27742def4d1e21dd2482b905749d85",
      "author": "ehhuang",
      "date": "2025-03-07T00:32:12+00:00",
      "message": "fix: rag_as_attachments, rag_with_vector_db example (#198)"
    },
    {
      "sha": "471155ba0a38b5bfefab782ea34e4fa63b2867d4",
      "author": "Chirag Modi",
      "date": "2025-03-07T00:21:30+00:00",
      "message": "Update README.md (#202)"
    }
  ],
  "readme_text": "# llama-stack-apps\n\n[![Discord](https://img.shields.io/discord/1257833999603335178)](https://discord.gg/llama-stack)\n\nThis repo shows examples of applications built on top of [Llama Stack](https://github.com/meta-llama/llama-stack). Starting Llama 3.1 you can build agentic applications capable of:\n\n- breaking a task down and performing multi-step reasoning.\n- using tools to perform some actions\n  - built-in: the model has built-in knowledge of tools like search or code interpreter\n  - zero-shot: the model can learn to call tools using previously unseen, in-context tool definitions\n- providing system level safety protections using models like Llama Guard.\n\n\n\nAn agentic app requires a few components:\n- ability to run inference on the underlying Llama series of models\n- ability to run safety checks using the Llama Guard series of models\n- ability to execute tools, including a code execution environment, and loop using the model's multi-step reasoning process\n\nAll of these components are now offered by a single Llama Stack Distribution. The [Llama Stack](https://github.com/meta-llama/llama-stack) defines and standardizes these components and many others that are needed to make building Generative AI applications smoother. Various implementations of these APIs are then assembled together via a **Llama Stack Distribution**.\n\n## Getting started with the Llama Stack Apps\n\nTo get started with Llama Stack Apps, you'll need to:\n\n1. Install prerequisites\n3. Start a Llama Stack server\n4. Connect your client agentic app to Llama Stack server\n\nOnce started, you can then just point your agentic app to the URL for this server (e.g. `http://localhost:8321`).\n\n### 1. Install Prerequisites\n\n**Python Packages**\n\nWe recommend creating an isolated conda Python environment.\n\n```bash\n# Create and activate a virtual environment\nENV=stack\nconda create -n $ENV python=3.10\ncd <path-to-llama-stack-apps-repo>\nconda activate $ENV\n\n# Install dependencies\npip install -r requirements.txt\n```\n\nThis will install all dependencies required to (1) Build and start a Llama Stack server (2) Connect your client app to Llama Stack server.\n\n\n### 2. Starting a Llama Stack Server\n- Please see our [llama-stack](https://github.com/meta-llama/llama-stack) repo's [Getting Started Guide](https://llama-stack.readthedocs.io/en/latest/getting_started/index.html) for setting up a Llama Stack distribution and running server to serve API endpoints. You should have a server endpoint for building your client apps.\n\nOnce your server is started, you should have seen outputs --\n```\n...\nListening on :::8321\nINFO:     Started server process [587053]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\nINFO:     Uvicorn running on http://[::]:8321 (Press CTRL+C to quit)\n```\n\n### 3. Test agents demo script\n\nWe have built sample demo scripts for interacting with the Stack server.\n\nWith the server running, you may run to test out an simple Agent\n\nThis example will require the API key from Tavily Search. You need to set it to the environment variable `TAVILY_SEARCH_API_KEY`\n\nLinux/Mac\n```\nexport TAVILY_SEARCH_API_KEY=[KEY]\n```\nTo run:\n```\npython -m examples.agents.hello localhost 8321\n```\n\nYou will see outputs of the form --\n```\n> created agents with agent_id=d050201b-0ca1-4abd-8eee-3cba2b8c0fbc\nUser> Hello\nshield_call> No Violation\ninference> How can I assist you today?\nshield_call> No Violation\nUser> Which players played in the winning team of the NBA western conference semifinals of 2024, please use tools\nshield_call> No Violation\ninference> brave_search.call(query=\"NBA Western Conference Semifinals 2024 winning team players\")\ntool_execution> Tool:brave_search Args:{'query': 'NBA Western Conference Semifinals 2024 winning team players'}\ntool_execution> Tool:brave_search Response:{\"query\": \"NBA Western Conference Semifinals 2024 winning team players\", \"top_k\": [{\"title\": \"2024 NBA Western Conference Semifinals - Mavericks vs. Thunder | Basketball-Reference.com\", \"url\": \"https://www.basketball-reference.com/playoffs/2024-nba-western-conference-semifinals-mavericks-vs-thunder.html\", \"description\": \"Summary and statistics for the <strong>2024</strong> <strong>NBA</strong> <strong>Western</strong> <strong>Conference</strong> <strong>Semifinals</strong> - Mavericks vs. Thunder\", \"type\": \"search_result\"}, {\"title\": \"2024 NBA playoffs - Wikipedia\", \"url\": \"https://en.wikipedia.org/wiki/2024_NBA_playoffs\", \"description\": \"Aged 20 years and 96 days old, ... youngest <strong>player</strong> <strong>in</strong> <strong>NBA</strong> history to record 10+ points and 15+ rebounds in a playoff game, coming during game 6 of the Maverick&#x27;s <strong>Western</strong> <strong>Conference</strong> <strong>Semifinal</strong> <strong>win</strong> against the Thunder on May 18. The Timberwolves overcame a 20\\u2013point deficit to <strong>win</strong> game 7 against the Nuggets, the largest game 7 comeback in <strong>NBA</strong> playoffs history. With the defending champion Nuggets losing to the Minnesota Timberwolves, the <strong>2024</strong> playoffs marked ...\", \"type\": \"search_result\"}, {\"title\": \"2024 NBA Playoffs | Official Bracket, Schedule and Series Matchups\", \"url\": \"https://www.nba.com/playoffs/2024\", \"description\": \"The official site of the <strong>2024</strong> <strong>NBA</strong> Playoffs. Latest news, schedules, matchups, highlights, bracket and more.\", \"type\": \"search_result\"}]}\nshield_call> No Violation\ninference> The players who played in the winning team of the NBA Western Conference Semifinals of 2024 are not specified in the search results provided. However, the search results suggest that the Mavericks played against the Thunder in the Western Conference Semifinals, and the Mavericks won the series.\nshield_call> No Violation\n```\n\n## Start an App and Interact with the Server\n\nNow that the Stack server is setup, the next thing would be to run an agentic app using Agents APIs.\n\nWe have built sample scripts, notebooks and a UI chat interface ( using [Gradio]([url](https://www.gradio.app/)) ! ) to help you get started.\n\nStart an app (local) and interact with it by running the following command:\n```bash\nPYTHONPATH=. python examples/agent_store/app.py localhost 8321\n```\n\n<img src=\"demo.png\" alt=\"Chat App\" width=\"600\"/>\n\nOptionally, you can setup API keys for custom tools:\n- [WolframAlpha](https://developer.wolframalpha.com/): store in `WOLFRAM_ALPHA_API_KEY` environment variable\n- [Brave Search](https://brave.com/search/api/): store in `BRAVE_SEARCH_API_KEY` environment variable\n\nYou may see other ways of interacting in [Agent Store README.md](https://github.com/meta-llama/llama-stack-apps/tree/main/examples/agent_store)\n\n## Create agentic systems and interact with the Stack server\n\nNOTE: Ensure that Stack server is still running.\n\n```bash\ncd <path-to-llama-stack-apps-repo>\nconda activate $ENV\nllama stack run <name> # If not already started\n\npython -m examples.agents.rag_with_vector_db localhost 8321\n```\n\nYou should see outputs to stdout of the form --\n```bash\nAvailable shields found: ['meta-llama/Llama-Guard-3-8B']\nUsing model: meta-llama/Llama-3.1-405B-Instruct\nCreated session_id=cdb8a978-0085-4f3d-a976-939ba2b19de9 for Agent(0cfe05a8-cb97-430f-bec6-b1c7d42c712a)\nshield_call> No Violation\nmemory_retrieval> Retrieved context from vector dbs: ['test_vector_db'].\n====\nHere are the retrieved documents for relevant context:\n=== START-RETRIEVED-CONTEXT ===\n id:num-1; content:_\nthe template from Llama2 to better support multiturn conversations. The same text\nin the Lla...\n>\ninference> Based on the provided documentation, here are the top 5 topics explained:\n\n* Fine-tuning Llama3 with chat data\n* Template changes from Llama2 to Llama3\n* Tokenizing prompt templates and special tokens\n* Fine-tuning on a custom chat dataset\n* Using prompt templates for specific tasks\nshield_call> No Violation\n\n...\n```\n\n\nFeel free to reach out if you have questions.\n\n\n## The Llama Stack Client SDK\n- Check out our client SDKs for connecting to Llama Stack server, you can choose from [python](https://github.com/meta-llama/llama-stack-client-python), [node](https://github.com/meta-llama/llama-stack-client-node), [swift](https://github.com/meta-llama/llama-stack-client-swift), and [kotlin](https://github.com/meta-llama/llama-stack-client-kotlin) programming languages to quickly build your applications.\n\n\n## Using VirtualEnv instead of Conda\n\n> [!NOTE]\n> While you can run the apps using `venv`, installation of a distribution requires conda.\n\n#### In Linux\n\n```bash\n# Create and activate a virtual environment\npython3 -m venv venv\nsource venv/bin/activate\n```\n\n#### For Windows\n\n```bash\n# Create and activate a virtual environment\npython -m venv venv\nvenv\\Scripts\\activate  # For Command Prompt\n# or\n.\\venv\\Scripts\\Activate.ps1  # For PowerShell\n# or\nsource venv/Scripts/activate  # For Git Bash\n```\n\nThe instructions thereafter (including `pip install -r requirements.txt` for installing the dependencies) remain the same.\n",
  "external_links_in_readme": [
    "https://developer.wolframalpha.com/",
    "https://github.com/meta-llama/llama-stack-client-node",
    "https://github.com/meta-llama/llama-stack-client-python",
    "https://www.nba.com/playoffs/2024\",",
    "http://localhost:8321`",
    "https://brave.com/search/api/",
    "https://img.shields.io/discord/1257833999603335178",
    "https://en.wikipedia.org/wiki/2024_NBA_playoffs\",",
    "https://www.gradio.app/",
    "https://github.com/meta-llama/llama-stack",
    "https://www.basketball-reference.com/playoffs/2024-nba-western-conference-semifinals-mavericks-vs-thunder.html\",",
    "https://github.com/meta-llama/llama-stack-client-kotlin",
    "http://[::]:8321",
    "https://github.com/meta-llama/llama-stack-apps/tree/main/examples/agent_store",
    "https://discord.gg/llama-stack",
    "https://llama-stack.readthedocs.io/en/latest/getting_started/index.html",
    "https://github.com/meta-llama/llama-stack-client-swift"
  ]
}
```

</details>


---

## Repository 4: meta-llama/llama-models

# GitHub Repository Data

**Repository:** [meta-llama/llama-models](https://github.com/meta-llama/llama-models)

## Basic Information

- **Description:** Utilities intended for use with Llama models.
- **Created:** 2024-06-27T22:14:09+00:00
- **Last Updated:** 2025-06-21T19:26:30+00:00
- **Last Pushed:** 2025-06-02T14:30:54+00:00
- **Default Branch:** main
- **Size:** 4442 KB

## Statistics

- **Stars:** 7,085
- **Forks:** 1,188
- **Watchers:** 7,085
- **Open Issues:** 136
- **Total Issues:** 0
- **Pull Requests:** 143

## License

- **Type:** Other
- **SPDX ID:** NOASSERTION
- **URL:** [License](https://github.com/meta-llama/llama-models/blob/main/LICENSE)

## Languages

- **Python:** 308,457 bytes

## Top Contributors

1. **ashwinb** - 90 contributions
2. **yanxi0830** - 11 contributions
3. **dltn** - 11 contributions
4. **jspisak** - 10 contributions
5. **ehhuang** - 6 contributions
6. **wukaixingxp** - 6 contributions
7. **raghotham** - 6 contributions
8. **dvrogozh** - 5 contributions
9. **github-actions[bot]** - 5 contributions
10. **machina-source** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 110

- `.github` (tree)
- `.github/CODEOWNERS` (blob)
- `.github/workflows` (tree)
- `.github/workflows/publish-to-test-pypi.yml` (blob)
- `.gitignore` (blob)
- `.pre-commit-config.yaml` (blob)
- `.ruff.toml` (blob)
- `CODE_OF_CONDUCT.md` (blob)
- `CONTRIBUTING.md` (blob)
- `LICENSE` (blob)

## Recent Issues

- 游릭 **#370** Super slow inference using eager attention with Llama-4-Scout-17B-16E-Instruct (open)
- 游릭 **#369** Webi3 Llama (open)
- 游릭 **#368** Issue downloading Llama Models (open)
- 游릭 **#367** Create devcontainer.json (open)
- 游릭 **#366** 403 error downloading models (open)

## Recent Pull Requests

- 游릭 **#369** Webi3 Llama (open)
- 游릭 **#367** Create devcontainer.json (open)
- 游댮 **#357** chore: remove usage of load_tiktoken_bpe (closed)
- 游릭 **#353** Andriod Darvin Monteras (open)
- 游댮 **#345** sync prompt_format.md (closed)

## Recent Commits

- **01dc8ce4** chore: remove usage of load_tiktoken_bpe (#357) - S칠bastien Han (2025-06-02T14:30:54+00:00)
- **f3d16d73** sync prompt_format.md (#345) - ehhuang (2025-05-07T18:02:54+00:00)
- **02b654c9** Fix Llama 3.3 model size in README (#339) - Marek Jeli켻ski (2025-05-02T21:15:43+00:00)
- **9921d278** update llama4 prompt_format.md (#332) - ehhuang (2025-04-25T22:54:25+00:00)
- **823cd862** fix: re-add python_start, python_end tokens (#327) - Ashwin Bharambe (2025-04-12T03:29:27+00:00)
- **28fa4e3b** fix: on-the-fly int4 quantize parameter (#324) - Jiawen Liu (2025-04-09T20:28:43+00:00)
- **408c5771** fix: update rope scaling for Llama-4-Scout (#322) - Ashwin Bharambe (2025-04-09T06:25:11+00:00)
- **78eb422f** fix: Kill L2Norm since it was a misnomer, this is just an unscaled rmsnorm, name it as such (#320) - Ashwin Bharambe (2025-04-09T05:49:12+00:00)
- **cb246012** feat: support xccl distributed backend in llama3 (#319) - Dmitry Rogozhkin (2025-04-09T05:49:00+00:00)
- **5c4377d2** feat: add xpu support to llama3 completion scripts (#318) - Dmitry Rogozhkin (2025-04-09T05:48:12+00:00)

## External Links Found in README

- https://huggingface.co/meta-Llama">
- https://llama.meta.com/">Website</a>&nbsp
- https://img.shields.io/pypi/dm/llama-models
- https://llama.meta.com/llama-downloads/
- https://llama.meta.com/get-started/">Get
- https://huggingface.co/meta-llama
- https://ai.meta.com/static-resource/responsible-use-guide/
- https://github.com/meta-llama/llama-cookbook">Llama
- https://ai.meta.com/blog/">
- https://img.shields.io/discord/1257833999603335178
- https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E
- http://facebook.com/whitehat/info
- https://github.com/meta-llama/llama-stack
- https://discord.gg/TZAAYNVtrU
- https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues
- https://llama.meta.com/faq
- https://pypi.org/project/llama-models/
- http://developers.facebook.com/llama_output_feedback

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 821134374,
  "name": "llama-models",
  "full_name": "meta-llama/llama-models",
  "description": "Utilities intended for use with Llama models.",
  "html_url": "https://github.com/meta-llama/llama-models",
  "clone_url": "https://github.com/meta-llama/llama-models.git",
  "ssh_url": "git@github.com:meta-llama/llama-models.git",
  "homepage": null,
  "topics": [],
  "default_branch": "main",
  "created_at": "2024-06-27T22:14:09+00:00",
  "updated_at": "2025-06-21T19:26:30+00:00",
  "pushed_at": "2025-06-02T14:30:54+00:00",
  "size_kb": 4442,
  "watchers_count": 7085,
  "stargazers_count": 7085,
  "forks_count": 1188,
  "open_issues_count": 136,
  "license": {
    "key": "other",
    "name": "Other",
    "spdx_id": "NOASSERTION",
    "url": "https://github.com/meta-llama/llama-models/blob/main/LICENSE"
  },
  "languages": {
    "Python": 308457
  },
  "top_contributors": [
    {
      "login": "ashwinb",
      "contributions": 90
    },
    {
      "login": "yanxi0830",
      "contributions": 11
    },
    {
      "login": "dltn",
      "contributions": 11
    },
    {
      "login": "jspisak",
      "contributions": 10
    },
    {
      "login": "ehhuang",
      "contributions": 6
    },
    {
      "login": "wukaixingxp",
      "contributions": 6
    },
    {
      "login": "raghotham",
      "contributions": 6
    },
    {
      "login": "dvrogozh",
      "contributions": 5
    },
    {
      "login": "github-actions[bot]",
      "contributions": 5
    },
    {
      "login": "machina-source",
      "contributions": 3
    },
    {
      "login": "ConnorHack",
      "contributions": 3
    },
    {
      "login": "rohit-ptl",
      "contributions": 3
    },
    {
      "login": "hardikjshah",
      "contributions": 3
    },
    {
      "login": "samuelselvan",
      "contributions": 2
    },
    {
      "login": "wysuperfly",
      "contributions": 1
    },
    {
      "login": "varunfb",
      "contributions": 1
    },
    {
      "login": "pmysl",
      "contributions": 1
    },
    {
      "login": "noah23olsen",
      "contributions": 1
    },
    {
      "login": "minimalic",
      "contributions": 1
    },
    {
      "login": "liyunlu0618",
      "contributions": 1
    }
  ],
  "file_tree_count": 110,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/CODEOWNERS",
      "type": "blob"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/publish-to-test-pypi.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": ".pre-commit-config.yaml",
      "type": "blob"
    },
    {
      "path": ".ruff.toml",
      "type": "blob"
    },
    {
      "path": "CODE_OF_CONDUCT.md",
      "type": "blob"
    },
    {
      "path": "CONTRIBUTING.md",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 143,
  "recent_issues": [
    {
      "number": 370,
      "title": "Super slow inference using eager attention with Llama-4-Scout-17B-16E-Instruct",
      "state": "open"
    },
    {
      "number": 369,
      "title": "Webi3 Llama",
      "state": "open"
    },
    {
      "number": 368,
      "title": "Issue downloading Llama Models",
      "state": "open"
    },
    {
      "number": 367,
      "title": "Create devcontainer.json",
      "state": "open"
    },
    {
      "number": 366,
      "title": "403 error downloading models",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 369,
      "title": "Webi3 Llama",
      "state": "open"
    },
    {
      "number": 367,
      "title": "Create devcontainer.json",
      "state": "open"
    },
    {
      "number": 357,
      "title": "chore: remove usage of load_tiktoken_bpe",
      "state": "closed"
    },
    {
      "number": 353,
      "title": "Andriod Darvin Monteras",
      "state": "open"
    },
    {
      "number": 345,
      "title": "sync prompt_format.md",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "01dc8ce46fecf06b639598f715efbb4ab981fb4c",
      "author": "S\u00e9bastien Han",
      "date": "2025-06-02T14:30:54+00:00",
      "message": "chore: remove usage of load_tiktoken_bpe (#357)"
    },
    {
      "sha": "f3d16d734f4de7d5bb7427705399e350da5e200f",
      "author": "ehhuang",
      "date": "2025-05-07T18:02:54+00:00",
      "message": "sync prompt_format.md (#345)"
    },
    {
      "sha": "02b654c9d4923d0bede37d5f1a9a8c8c56022e4d",
      "author": "Marek Jeli\u0144ski",
      "date": "2025-05-02T21:15:43+00:00",
      "message": "Fix Llama 3.3 model size in README (#339)"
    },
    {
      "sha": "9921d278bf26cf4f3385e7bea399c2b6e7847f8b",
      "author": "ehhuang",
      "date": "2025-04-25T22:54:25+00:00",
      "message": "update llama4 prompt_format.md (#332)"
    },
    {
      "sha": "823cd8622e44d90b8e989e9f41ca364c06f5701d",
      "author": "Ashwin Bharambe",
      "date": "2025-04-12T03:29:27+00:00",
      "message": "fix: re-add python_start, python_end tokens (#327)"
    },
    {
      "sha": "28fa4e3b287e84f6a6a92aab3c931f7479c827c1",
      "author": "Jiawen Liu",
      "date": "2025-04-09T20:28:43+00:00",
      "message": "fix: on-the-fly int4 quantize parameter (#324)"
    },
    {
      "sha": "408c577168a187941f9359c916004fa3b21fbc3b",
      "author": "Ashwin Bharambe",
      "date": "2025-04-09T06:25:11+00:00",
      "message": "fix: update rope scaling for Llama-4-Scout (#322)"
    },
    {
      "sha": "78eb422f717d4de4707cebf520f0f8224537e4d3",
      "author": "Ashwin Bharambe",
      "date": "2025-04-09T05:49:12+00:00",
      "message": "fix: Kill L2Norm since it was a misnomer, this is just an unscaled rmsnorm, name it as such (#320)"
    },
    {
      "sha": "cb246012bc7177ac32ae17ed9611d3327b890cba",
      "author": "Dmitry Rogozhkin",
      "date": "2025-04-09T05:49:00+00:00",
      "message": "feat: support xccl distributed backend in llama3 (#319)"
    },
    {
      "sha": "5c4377d2a9be016b137cd20c765cdf52589f4b72",
      "author": "Dmitry Rogozhkin",
      "date": "2025-04-09T05:48:12+00:00",
      "message": "feat: add xpu support to llama3 completion scripts (#318)"
    },
    {
      "sha": "0ed6a1f4ce962bdf9c97b44d5032dcc3bc8c8394",
      "author": "Dmitry Rogozhkin",
      "date": "2025-04-09T05:47:35+00:00",
      "message": "fix: fix llama3 generation test (#317)"
    },
    {
      "sha": "f62cac6d2d145033df6016901aca53548193ead3",
      "author": "Ashwin Bharambe",
      "date": "2025-04-08T14:19:49+00:00",
      "message": "fix: ensure finetune_right_pad stays at the same position before the hiding of python_start, python_end"
    },
    {
      "sha": "63172b319ee6412d165095cc5a8f0fde12288ec0",
      "author": "Ashwin Bharambe",
      "date": "2025-04-07T22:06:03+00:00",
      "message": "fix: a couple minor bugs"
    },
    {
      "sha": "88e9f6aadc0067cb7a4724cc5e46eca384c11a91",
      "author": "Ashwin Bharambe",
      "date": "2025-04-07T18:00:41+00:00",
      "message": "fix: iron out differences between llama-models and llama-stack"
    },
    {
      "sha": "b12e46273bf002ab1318064bc0e14c49ecbe63a0",
      "author": "Ashwin Bharambe",
      "date": "2025-04-07T05:40:17+00:00",
      "message": "refactor: make llama3 generation closer to llama4 (#309)"
    },
    {
      "sha": "699a02993512fb36936b1b0741e13c06790bcf98",
      "author": "Ashwin Bharambe",
      "date": "2025-04-06T20:23:46+00:00",
      "message": "fix: quantization script had regressed (#308)"
    },
    {
      "sha": "4f45ca9d0522581aedcb7b2e2231e87b084bbadf",
      "author": "Ashwin Bharambe",
      "date": "2025-04-06T17:53:23+00:00",
      "message": "fix: make sure fp8 quantization only quantizes MoE layers"
    },
    {
      "sha": "2b2e5b2645c962f92dc004aa868696ec0e53b05c",
      "author": "Ashwin Bharambe",
      "date": "2025-04-06T02:05:57+00:00",
      "message": "fix: cleanup MoE docs"
    },
    {
      "sha": "ebe3527e5104647c0976af25ddbe88784778ed18",
      "author": "raghotham",
      "date": "2025-04-06T00:43:29+00:00",
      "message": "Create llama4 prompt_format.md (#303)"
    },
    {
      "sha": "eececc27d275a0f7d4d14ec83648d51c10a76560",
      "author": "Hamid Shojanazeri",
      "date": "2025-04-05T19:28:34+00:00",
      "message": "Update README.md (#300)"
    }
  ],
  "readme_text": "<p align=\"center\">\n  <img src=\"/Llama_Repo.jpeg\" width=\"400\"/>\n</p>\n\n<p align=\"center\">\n        \ud83e\udd17 <a href=\"https://huggingface.co/meta-Llama\"> Models on Hugging Face</a>&nbsp | <a href=\"https://ai.meta.com/blog/\"> Blog</a>&nbsp |  <a href=\"https://llama.meta.com/\">Website</a>&nbsp | <a href=\"https://llama.meta.com/get-started/\">Get Started</a>&nbsp | <a href=\"https://github.com/meta-llama/llama-cookbook\">Llama Cookbook</a>&nbsp\n<br>\n\n---\n\n# Llama Models\n\nLlama is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. A few key aspects:\n1. **Open access**: Easy accessibility to cutting-edge large language models, fostering collaboration and advancements among developers, researchers, and organizations\n2. **Broad ecosystem**: Llama models have been downloaded hundreds of millions of times, there are thousands of community projects built on Llama and platform support is broad from cloud providers to startups - the world is building with Llama!\n3. **Trust & safety**: Llama models are part of a comprehensive approach to trust and safety, releasing models and tools that are designed to enable community collaboration and encourage the standardization of the development and usage of trust and safety tools for generative AI\n\nOur mission is to empower individuals and industry through this opportunity while fostering an environment of discovery and ethical AI advancements. The model weights are licensed for researchers and commercial entities, upholding the principles of openness.\n\n## Llama Models\n\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-models)](https://pypi.org/project/llama-models/)\n[![Discord](https://img.shields.io/discord/1257833999603335178)](https://discord.gg/TZAAYNVtrU)\n\n|  **Model** | **Launch date** | **Model sizes** | **Context Length** | **Tokenizer** | **Acceptable use policy**  |  **License** | **Model Card** |\n| :----: | :----: | :----: | :----:|:----:|:----:|:----:|:----:|\n| Llama 2 | 7/18/2023 | 7B, 13B, 70B | 4K | Sentencepiece | [Use Policy](models/llama2/USE_POLICY.md) | [License](models/llama2/LICENSE) | [Model Card](models/llama2/MODEL_CARD.md) |\n| Llama 3 | 4/18/2024 | 8B, 70B | 8K | TikToken-based | [Use Policy](models/llama3/USE_POLICY.md) | [License](models/llama3/LICENSE) | [Model Card](models/llama3/MODEL_CARD.md) |\n| Llama 3.1 | 7/23/2024 | 8B, 70B, 405B | 128K | TikToken-based | [Use Policy](models/llama3_1/USE_POLICY.md) | [License](models/llama3_1/LICENSE) | [Model Card](models/llama3_1/MODEL_CARD.md) |\n| Llama 3.2 | 9/25/2024 | 1B, 3B | 128K | TikToken-based | [Use Policy](models/llama3_2/USE_POLICY.md) | [License](models/llama3_2/LICENSE) | [Model Card](models/llama3_2/MODEL_CARD.md) |\n| Llama 3.2-Vision | 9/25/2024 | 11B, 90B | 128K | TikToken-based | [Use Policy](models/llama3_2/USE_POLICY.md) | [License](models/llama3_2/LICENSE) | [Model Card](models/llama3_2/MODEL_CARD_VISION.md) |\n| Llama 3.3 | 12/04/2024 | 70B | 128K | TikToken-based | [Use Policy](models/llama3_3/USE_POLICY.md) | [License](models/llama3_3/LICENSE) | [Model Card](models/llama3_3/MODEL_CARD.md) |\n| Llama 4 | 4/5/2025 | Scout-17B-16E, Maverick-17B-128E | 10M, 1M | TikToken-based | [Use Policy](models/llama4/USE_POLICY.md) | [License](models/llama4/LICENSE) | [Model Card](models/llama4/MODEL_CARD.md) |\n\n## Download\n\nTo download the model weights and tokenizer:\n\n1. Visit the [Meta Llama website](https://llama.meta.com/llama-downloads/).\n2. Read and accept the license.\n3. Once your request is approved you will receive a signed URL via email.\n4. Install the [Llama CLI](https://github.com/meta-llama/llama-stack): `pip install llama-stack`. (**<-- Start Here if you have received an email already.**)\n5. Run `llama model list` to show the latest available models and determine the model ID you wish to download. **NOTE**:\nIf you want older versions of models, run `llama model list --show-all` to show all the available Llama models.\n\n6. Run: `llama download --source meta --model-id CHOSEN_MODEL_ID`\n7. Pass the URL provided when prompted to start the download.\n\nRemember that the links expire after 24 hours and a certain amount of downloads. You can always re-request a link if you start seeing errors such as `403: Forbidden`.\n\n## Running the models\n\nIn order to run the models, you will need to install dependencies after checking out the repository.\n\n```bash\n# Run this within a suitable Python environment (uv, conda, or virtualenv)\npip install .[torch]\n```\n\nExample scripts are available in `models/{ llama3, llama4 }/scripts/` sub-directory. Note that the Llama4 series of models require at least 4 GPUs to run inference at full (bf16) precision.\n\n```bash\n#!/bin/bash\n\nNGPUS=4\nCHECKPOINT_DIR=~/.llama/checkpoints/Llama-4-Scout-17B-16E-Instruct\nPYTHONPATH=$(git rev-parse --show-toplevel) \\\n  torchrun --nproc_per_node=$NGPUS \\\n  -m models.llama4.scripts.chat_completion $CHECKPOINT_DIR \\\n  --world_size $NGPUS\n```\n\nThe above script should be used with an Instruct (Chat) model. For a Base model, update the `CHECKPOINT_DIR` path and use the script `models.llama4.scripts.completion`.\n\n\n## Running inference with FP8 and Int4 Quantization\n\nYou can reduce the memory footprint of the models at the cost of minimal loss in accuracy by running inference with FP8 or Int4 quantization. Use the `--quantization-mode` flag to specify the quantization mode. There are two modes:\n- `fp8_mixed`: Mixed precision inference with FP8 for some weights and bfloat16 for activations.\n- `int4_mixed`: Mixed precision inference with Int4 for some weights and bfloat16 for activations.\n\nUsing FP8, running Llama-4-Scout-17B-16E-Instruct requires 2 GPUs with 80GB of memory. Using Int4, you need a single GPU with 80GB of memory.\n\n```bash\nMODE=fp8_mixed  # or int4_mixed\nif [ $MODE == \"fp8_mixed\" ]; then\n  NGPUS=2\nelse\n  NGPUS=1\nfi\nCHECKPOINT_DIR=~/.llama/checkpoints/Llama-4-Scout-17B-16E-Instruct\nPYTHONPATH=$(git rev-parse --show-toplevel) \\\n  torchrun --nproc_per_node=$NGPUS \\\n  -m models.llama4.scripts.chat_completion $CHECKPOINT_DIR \\\n  --world_size $NGPUS \\\n  --quantization-mode $MODE\n```\n\n\nFor more flexibility in running inference (including using other providers), please see the [`Llama Stack`](https://github.com/meta-llama/llama-stack) toolset.\n\n\n## Access to Hugging Face\n\nWe also provide downloads on [Hugging Face](https://huggingface.co/meta-llama), in both transformers and native `llama4` formats. To download the weights from Hugging Face, please follow these steps:\n\n- Visit one of the repos, for example [meta-llama/Llama-4-Scout-17B-16E](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E).\n- Read and accept the license. Once your request is approved, you'll be granted access to all Llama 3.1 models as well as previous versions. Note that requests used to take up to one hour to get processed.\n- To download the original native weights to use with this repo, click on the \"Files and versions\" tab and download the contents of the `original` folder. You can also download them from the command line if you `pip install huggingface-hub`:\n\n```bash\nhuggingface-cli download meta-llama/Llama-4-Scout-17B-16E-Instruct-Original --local-dir meta-llama/Llama-4-Scout-17B-16E-Instruct-Original\n```\n\n- To use with transformers, the following snippet will download and cache the weights:\n\n  ```python\n  # inference.py\n  from transformers import AutoTokenizer, Llama4ForConditionalGeneration\n  import torch\n\n  model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n\n  tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n  messages = [\n      {\"role\": \"user\", \"content\": \"Who are you?\"},\n  ]\n  inputs = tokenizer.apply_chat_template(\n      messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True\n  )\n\n  model = Llama4ForConditionalGeneration.from_pretrained(\n      model_id, device_map=\"auto\", torch_dtype=torch.bfloat16\n  )\n\n  outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n  outputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1] :])\n  print(outputs[0])\n  ```\n  ```bash\n   torchrun --nnodes=1 --nproc_per_node=8 inference.py\n   ```\n\n## Installations\n\nYou can install this repository as a [package](https://pypi.org/project/llama-models/) by just doing `pip install llama-models`\n\n## Responsible Use\n\nLlama models are a new technology that carries potential risks with use. Testing conducted to date has not \u2014 and could not \u2014 cover all scenarios.\nTo help developers address these risks, we have created the [Responsible Use Guide](https://ai.meta.com/static-resource/responsible-use-guide/).\n\n## Issues\n\nPlease report any software \u201cbug\u201d or other problems with the models through one of the following means:\n- Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\n- Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n\n## Questions\n\nFor common questions, the FAQ can be found [here](https://llama.meta.com/faq), which will be updated over time as new questions arise.\n",
  "external_links_in_readme": [
    "https://huggingface.co/meta-Llama\">",
    "https://llama.meta.com/\">Website</a>&nbsp",
    "https://img.shields.io/pypi/dm/llama-models",
    "https://llama.meta.com/llama-downloads/",
    "https://llama.meta.com/get-started/\">Get",
    "https://huggingface.co/meta-llama",
    "https://ai.meta.com/static-resource/responsible-use-guide/",
    "https://github.com/meta-llama/llama-cookbook\">Llama",
    "https://ai.meta.com/blog/\">",
    "https://img.shields.io/discord/1257833999603335178",
    "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E",
    "http://facebook.com/whitehat/info",
    "https://github.com/meta-llama/llama-stack",
    "https://discord.gg/TZAAYNVtrU",
    "https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues",
    "https://llama.meta.com/faq",
    "https://pypi.org/project/llama-models/",
    "http://developers.facebook.com/llama_output_feedback"
  ]
}
```

</details>


---

## Repository 5: meta-llama/llama-recipes

# GitHub Repository Data

**Repository:** [meta-llama/llama-cookbook](https://github.com/meta-llama/llama-cookbook)

## Basic Information

- **Description:** Welcome to the Llama Cookbook! This is your go to guide for Building with Llama: Getting started with Inference, Fine-Tuning, RAG. We also show you how to solve end to end problems using Llama model family and using them on various provider services  
- **Created:** 2023-07-17T07:33:48+00:00
- **Last Updated:** 2025-06-21T23:53:04+00:00
- **Last Pushed:** 2025-06-18T22:28:43+00:00
- **Default Branch:** main
- **Size:** 232307 KB

## Statistics

- **Stars:** 17,511
- **Forks:** 2,528
- **Watchers:** 17,511
- **Open Issues:** 53
- **Total Issues:** 0
- **Pull Requests:** 513

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/meta-llama/llama-cookbook/blob/main/LICENSE)

## Languages

- **Jupyter Notebook:** 12,919,729 bytes
- **Python:** 706,839 bytes
- **Java:** 55,936 bytes
- **JavaScript:** 35,266 bytes
- **Kotlin:** 10,860 bytes
- **HTML:** 1,721 bytes
- **Shell:** 1,566 bytes
- **HCL:** 845 bytes
- **CSS:** 623 bytes

## Topics

- `ai`
- `finetuning`
- `langchain`
- `llama`
- `llama2`
- `llm`
- `machine-learning`
- `python`
- `pytorch`
- `vllm`

## Top Contributors

1. **HamidShojanazeri** - 246 contributions
2. **mreso** - 218 contributions
3. **jeffxtang** - 199 contributions
4. **init27** - 197 contributions
5. **wukaixingxp** - 164 contributions
6. **albertodepaola** - 117 contributions
7. **Kyriection** - 112 contributions
8. **subramen** - 81 contributions
9. **IgorKasianenko** - 76 contributions
10. **tmoreau89** - 65 contributions

## File Structure (Sample of 10 files)

Total files: 800

- `.github` (tree)
- `.github/ISSUE_TEMPLATE` (tree)
- `.github/ISSUE_TEMPLATE/bug.yml` (blob)
- `.github/ISSUE_TEMPLATE/feature-request.yml` (blob)
- `.github/PULL_REQUEST_TEMPLATE.md` (blob)
- `.github/scripts` (tree)
- `.github/scripts/check_copyright_header.py` (blob)
- `.github/scripts/markdown_link_check_config.json` (blob)
- `.github/scripts/spellcheck.sh` (blob)
- `.github/scripts/spellcheck_conf` (tree)

## Recent Issues

- 游댮 **#965** fixing typo (closed)
- 游댮 **#964** Edited out as spam (closed)
- 游릭 **#963** Guided grammar api signature inquiry (open)
- 游댮 **#962** Fix README links for use cases (closed)
- 游릭 **#961** Update/deprecate Llama 3 recipe (open)

## Recent Pull Requests

- 游댮 **#965** fixing typo (closed)
- 游댮 **#962** Fix README links for use cases (closed)
- 游댮 **#959** Created an api inference script with its supporting documentation (closed)
- 游릭 **#958** add vertex notebooks for gcp (open)
- 游릭 **#957** Updated notebook to use Llama API (open)

## Recent Commits

- **d7c57eea** fixing typo (#965) - Sanyam Bhutani (2025-06-18T22:28:43+00:00)
- **666604c8** fixing typo - albertodepaola (2025-06-18T22:27:55+00:00)
- **0f4e671f** Fix README links for use cases (#962) - Navyata Bawa (2025-06-11T17:31:54+00:00)
- **6477af07** Fix README links for use cases - Navyata Bawa (2025-06-11T17:20:41+00:00)
- **ddd7429c** Created an api inference script with its supporting documentation (#959) - Omar Abdelwahab (2025-06-06T23:14:28+00:00)
- **3c74c5b6** Fix deprecated langchain warning for llama3 (#953) - Igor Kasianenko (2025-06-03T16:10:32+00:00)
- **319b9543** add beautifulsoup4 to dependency - Igor Kasianenko (2025-05-29T10:55:07+00:00)
- **571c2476** nit - cleanup - Srinidhi Viswanathan (2025-05-28T15:54:27+00:00)
- **ad91da40** nit - fixing last cell - Srinidhi Viswanathan (2025-05-28T15:40:46+00:00)
- **f251c3bc** Update: addressing comments + fixing hugging face import - Srinidhi Viswanathan (2025-05-28T15:39:03+00:00)

## External Links Found in README

- https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/USE_POLICY.md
- https://img.shields.io/badge/Llama_Tools-llama--prompt--ops-orange?logo=meta"
- https://github.com/meta-llama/llama-cookbook/tree/main/end-to-end-use-cases
- https://github.com/meta-llama/llama-cookbook/tree/archive-main
- https://github.com/meta-llama/synthetic-data-kit"><img
- https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE
- https://github.com/meta-llama/llama-models/blob/main/models/llama2/USE_POLICY.md
- https://img.shields.io/badge/Hugging_Face-meta--llama-yellow?logo=huggingface"
- https://huggingface.co/meta-llama"><img
- https://github.com/meta-llama/llama-models/blob/main/models/llama3/USE_POLICY.md
- https://img.shields.io/badge/Llama_OSS-Documentation-4BA9FE?logo=meta"
- https://bit.ly/llama-api-main
- https://www.llama.com/docs/overview/?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main"><img
- https://github.com/meta-llama/llama-cookbook/tree/main/3p-integrations
- https://img.shields.io/badge/Llama_API-Documentation-4BA9FE?logo=meta"
- https://img.shields.io/badge/Llama_API-Join_Waitlist-brightgreen?logo=meta"
- https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE
- https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE
- https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/USE_POLICY.md
- https://github.com/meta-llama/llama-prompt-ops"><img

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 667297720,
  "name": "llama-cookbook",
  "full_name": "meta-llama/llama-cookbook",
  "description": "Welcome to the Llama Cookbook! This is your go to guide for Building with Llama: Getting started with Inference, Fine-Tuning, RAG. We also show you how to solve end to end problems using Llama model family and using them on various provider services  ",
  "html_url": "https://github.com/meta-llama/llama-cookbook",
  "clone_url": "https://github.com/meta-llama/llama-cookbook.git",
  "ssh_url": "git@github.com:meta-llama/llama-cookbook.git",
  "homepage": "https://www.llama.com/",
  "topics": [
    "ai",
    "finetuning",
    "langchain",
    "llama",
    "llama2",
    "llm",
    "machine-learning",
    "python",
    "pytorch",
    "vllm"
  ],
  "default_branch": "main",
  "created_at": "2023-07-17T07:33:48+00:00",
  "updated_at": "2025-06-21T23:53:04+00:00",
  "pushed_at": "2025-06-18T22:28:43+00:00",
  "size_kb": 232307,
  "watchers_count": 17511,
  "stargazers_count": 17511,
  "forks_count": 2528,
  "open_issues_count": 53,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/meta-llama/llama-cookbook/blob/main/LICENSE"
  },
  "languages": {
    "Jupyter Notebook": 12919729,
    "Python": 706839,
    "Java": 55936,
    "JavaScript": 35266,
    "Kotlin": 10860,
    "HTML": 1721,
    "Shell": 1566,
    "HCL": 845,
    "CSS": 623
  },
  "top_contributors": [
    {
      "login": "HamidShojanazeri",
      "contributions": 246
    },
    {
      "login": "mreso",
      "contributions": 218
    },
    {
      "login": "jeffxtang",
      "contributions": 199
    },
    {
      "login": "init27",
      "contributions": 197
    },
    {
      "login": "wukaixingxp",
      "contributions": 164
    },
    {
      "login": "albertodepaola",
      "contributions": 117
    },
    {
      "login": "Kyriection",
      "contributions": 112
    },
    {
      "login": "subramen",
      "contributions": 81
    },
    {
      "login": "IgorKasianenko",
      "contributions": 76
    },
    {
      "login": "tmoreau89",
      "contributions": 65
    },
    {
      "login": "WuhanMonkey",
      "contributions": 61
    },
    {
      "login": "chauhang",
      "contributions": 51
    },
    {
      "login": "seyeong-han",
      "contributions": 48
    },
    {
      "login": "sekyondaMeta",
      "contributions": 45
    },
    {
      "login": "himanshushukla12",
      "contributions": 36
    },
    {
      "login": "varunfb",
      "contributions": 23
    },
    {
      "login": "abhilash1910",
      "contributions": 20
    },
    {
      "login": "Monireh2",
      "contributions": 18
    },
    {
      "login": "agunapal",
      "contributions": 16
    },
    {
      "login": "rlancemartin",
      "contributions": 15
    }
  ],
  "file_tree_count": 800,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/bug.yml",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/feature-request.yml",
      "type": "blob"
    },
    {
      "path": ".github/PULL_REQUEST_TEMPLATE.md",
      "type": "blob"
    },
    {
      "path": ".github/scripts",
      "type": "tree"
    },
    {
      "path": ".github/scripts/check_copyright_header.py",
      "type": "blob"
    },
    {
      "path": ".github/scripts/markdown_link_check_config.json",
      "type": "blob"
    },
    {
      "path": ".github/scripts/spellcheck.sh",
      "type": "blob"
    },
    {
      "path": ".github/scripts/spellcheck_conf",
      "type": "tree"
    }
  ],
  "issues_count": 0,
  "pulls_count": 513,
  "recent_issues": [
    {
      "number": 965,
      "title": "fixing typo",
      "state": "closed"
    },
    {
      "number": 964,
      "title": "Edited out as spam",
      "state": "closed"
    },
    {
      "number": 963,
      "title": "Guided grammar api signature inquiry",
      "state": "open"
    },
    {
      "number": 962,
      "title": "Fix README links for use cases",
      "state": "closed"
    },
    {
      "number": 961,
      "title": "Update/deprecate Llama 3 recipe",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 965,
      "title": "fixing typo",
      "state": "closed"
    },
    {
      "number": 962,
      "title": "Fix README links for use cases",
      "state": "closed"
    },
    {
      "number": 959,
      "title": "Created an api inference script with its supporting documentation",
      "state": "closed"
    },
    {
      "number": 958,
      "title": "add vertex notebooks for gcp",
      "state": "open"
    },
    {
      "number": 957,
      "title": "Updated notebook to use Llama API",
      "state": "open"
    }
  ],
  "recent_commits": [
    {
      "sha": "d7c57eea23eadaa7f9bf4294c70731110d367509",
      "author": "Sanyam Bhutani",
      "date": "2025-06-18T22:28:43+00:00",
      "message": "fixing typo (#965)"
    },
    {
      "sha": "666604c8a806f40f573b33ad286ecfec5953be61",
      "author": "albertodepaola",
      "date": "2025-06-18T22:27:55+00:00",
      "message": "fixing typo"
    },
    {
      "sha": "0f4e671f2ccb794b6017391d10a87f0f2af8a2c9",
      "author": "Navyata Bawa",
      "date": "2025-06-11T17:31:54+00:00",
      "message": "Fix README links for use cases (#962)"
    },
    {
      "sha": "6477af074bb1724bd42d14831f341d7525e63b4c",
      "author": "Navyata Bawa",
      "date": "2025-06-11T17:20:41+00:00",
      "message": "Fix README links for use cases"
    },
    {
      "sha": "ddd7429c6191feec6a5b1c35d59b4d843c05b761",
      "author": "Omar Abdelwahab",
      "date": "2025-06-06T23:14:28+00:00",
      "message": "Created an api inference script with its supporting documentation (#959)"
    },
    {
      "sha": "3c74c5b6af465ed51e1e7c7f1be2a5d2d3755632",
      "author": "Igor Kasianenko",
      "date": "2025-06-03T16:10:32+00:00",
      "message": "Fix deprecated langchain warning for llama3 (#953)"
    },
    {
      "sha": "319b9543258fbf6fb8fb6ec8e118af708ebdb980",
      "author": "Igor Kasianenko",
      "date": "2025-05-29T10:55:07+00:00",
      "message": "add beautifulsoup4 to dependency"
    },
    {
      "sha": "571c247616b2ca431c7e58d2b9eabcaf918ef521",
      "author": "Srinidhi Viswanathan",
      "date": "2025-05-28T15:54:27+00:00",
      "message": "nit - cleanup"
    },
    {
      "sha": "ad91da40a8441a925e4ca076c37b2c182883730a",
      "author": "Srinidhi Viswanathan",
      "date": "2025-05-28T15:40:46+00:00",
      "message": "nit - fixing last cell"
    },
    {
      "sha": "f251c3bc231571222b0fcfb85f992ff0004c4a19",
      "author": "Srinidhi Viswanathan",
      "date": "2025-05-28T15:39:03+00:00",
      "message": "Update: addressing comments + fixing hugging face import"
    },
    {
      "sha": "fa38b3742ced5c635cf8a3a61dbf5aca9a5de5fe",
      "author": "Srinidhi Viswanathan",
      "date": "2025-05-25T01:58:57+00:00",
      "message": "Fix deprecated langchain warning for llama3"
    },
    {
      "sha": "04b6c8cb46a2e3f797d4aba3626a2fb1df88a1c5",
      "author": "Igor Kasianenko",
      "date": "2025-05-22T08:48:23+00:00",
      "message": "Fix/update samsum (#950)"
    },
    {
      "sha": "d819ec8ad92255918a67b8cbaebb1baf801ac865",
      "author": "Igor Kasianenko",
      "date": "2025-05-21T18:06:58+00:00",
      "message": "update HELM link (#902)"
    },
    {
      "sha": "4c32e76ca9b023334e0bf992e059439d4fce2c1a",
      "author": "Igor Kasianenko",
      "date": "2025-05-21T15:01:10+00:00",
      "message": "Notebook for generating evals using synthetic data (#937)"
    },
    {
      "sha": "4ecdbf9639746d3d776156007adc9faf055a6b1d",
      "author": "Igor Kasianenko",
      "date": "2025-05-21T14:54:29+00:00",
      "message": "typo fixes"
    },
    {
      "sha": "e7c093fc3610c66a0af9ed4ed97719e76b1442ca",
      "author": "Igor Kasianenko",
      "date": "2025-05-21T09:50:10+00:00",
      "message": "Fix model string name for Llama-Prompt-Guard-2-86M in inference.py (#951)"
    },
    {
      "sha": "3c106f3e6ee79d6df51ae706bc7ef2d734ec3ded",
      "author": "Nicholas Carlini",
      "date": "2025-05-20T15:56:25+00:00",
      "message": "Fix model string name for Llama-Prompt-Guard-2-86M in inference.py"
    },
    {
      "sha": "8941ec49d05bf1e7c392d71f364745d123e546f6",
      "author": "nabidam",
      "date": "2025-05-20T12:37:23+00:00",
      "message": "not formatting file"
    },
    {
      "sha": "b1213aea2175fa5761fe72b15dba81b8807f3408",
      "author": "nabidam",
      "date": "2025-05-20T12:35:15+00:00",
      "message": "remove back tick"
    },
    {
      "sha": "186f8f2a5f54b38645df86f9be5531c010a7da36",
      "author": "nabidam",
      "date": "2025-05-20T12:29:25+00:00",
      "message": "change samsum repo"
    }
  ],
  "readme_text": "<h1 align=\"center\"> Llama Cookbook </h1>\n<p align=\"center\">\n\t<a href=\"https://llama.developer.meta.com/join_waitlist?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main\"><img src=\"https://img.shields.io/badge/Llama_API-Join_Waitlist-brightgreen?logo=meta\" /></a>\n\t<a href=\"https://llama.developer.meta.com/docs?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main\"><img src=\"https://img.shields.io/badge/Llama_API-Documentation-4BA9FE?logo=meta\" /></a>\n\n</p>\n<p align=\"center\">\n\t<a href=\"https://github.com/meta-llama/llama-models/blob/main/models/?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main\"><img alt=\"Llama Model cards\" src=\"https://img.shields.io/badge/Llama_OSS-Model_cards-green?logo=meta\" /></a>\n\t<a href=\"https://www.llama.com/docs/overview/?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main\"><img alt=\"Llama Documentation\" src=\"https://img.shields.io/badge/Llama_OSS-Documentation-4BA9FE?logo=meta\" /></a>\n\t<a href=\"https://huggingface.co/meta-llama\"><img alt=\"Hugging Face meta-llama\" src=\"https://img.shields.io/badge/Hugging_Face-meta--llama-yellow?logo=huggingface\" /></a>\n\n</p>\n<p align=\"center\">\n\t<a href=\"https://github.com/meta-llama/synthetic-data-kit\"><img alt=\"Llama Tools Syntethic Data Kit\" src=\"https://img.shields.io/badge/Llama_Tools-synthetic--data--kit-orange?logo=meta\" /></a>\n\t<a href=\"https://github.com/meta-llama/llama-prompt-ops\"><img alt=\"Llama Tools Syntethic Data Kit\" src=\"https://img.shields.io/badge/Llama_Tools-llama--prompt--ops-orange?logo=meta\" /></a>\n</p>\n<h2> Official Guide to building with Llama </h2>\n\n\n\nWelcome to the official repository for helping you get started with [inference](https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/inference/), [fine-tuning](https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/finetuning) and [end-to-end use-cases](https://github.com/meta-llama/llama-cookbook/tree/main/end-to-end-use-cases) of building with the Llama Model family.\n\nThis repository covers the most popular community approaches, use-cases and the latest recipes for Llama Text and Vision models.\n\n## Latest Llama 4 recipes\n\n* [Get started](./getting-started/build_with_llama_api.ipynb) with [Llama API](https://bit.ly/llama-api-main)\n* Integrate [Llama API](https://bit.ly/llama-api-main) with [WhatsApp](./end-to-end-use-cases/whatsapp_llama_4_bot/README.md)\n* 5M long context using [Llama 4 Scout](./getting-started/build_with_llama_4.ipynb)\n* Analyze research papers with [Llama 4 Maverick](./end-to-end-use-cases/research_paper_analyzer/README.md)\n* Create a character mind map from a book using [Llama 4 Maverick](./end-to-end-use-cases/book-character-mindmap/README.md)\n\n## Repository Structure:\n\n- [3P Integrations](https://github.com/meta-llama/llama-cookbook/tree/main/3p-integrations): Getting Started Recipes and End to End Use-Cases from various Llama providers\n- [End to End Use Cases](https://github.com/meta-llama/llama-cookbook/tree/main/end-to-end-use-cases): As the name suggests, spanning various domains and applications\n- [Getting Started](https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/): Reference for inferencing, fine-tuning and RAG examples\n- [src](https://github.com/meta-llama/llama-cookbook/tree/main/src/): Contains the src for the original llama-recipes library along with some FAQs for fine-tuning.\n\n> Note: We recently did a refactor of the repo, [archive-main](https://github.com/meta-llama/llama-cookbook/tree/archive-main) is a snapshot branch from before the refactor\n\n## FAQ:\n\n- **Q:** What happened to llama-recipes?\n  **A:** We recently renamed llama-recipes to llama-cookbook.\n\n- **Q:** I have some questions for Fine-Tuning, is there a section to address these?\n  **A:** Check out the Fine-Tuning FAQ [here](https://github.com/meta-llama/llama-cookbook/tree/main/src/docs/).\n\n- **Q:** Some links are broken/folders are missing:\n  **A:** We recently did a refactor of the repo, [archive-main](https://github.com/meta-llama/llama-cookbook/tree/archive-main) is a snapshot branch from before the refactor.\n\n- **Q:** Where can we find details about the latest models?\n  **A:** Official [Llama models website](https://www.llama.com).\n\n## Contributing\n\nPlease read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct, and the process for submitting pull requests to us.\n\n## License\n<!-- markdown-link-check-disable -->\nSee the License file for Meta Llama 4 [here](https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE) and Acceptable Use Policy [here](https://github.com/meta-llama/llama-models/blob/main/models/llama4/USE_POLICY.md)\n\nSee the License file for Meta Llama 3.3 [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE) and Acceptable Use Policy [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/USE_POLICY.md)\n\nSee the License file for Meta Llama 3.2 [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE) and Acceptable Use Policy [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/USE_POLICY.md)\n\nSee the License file for Meta Llama 3.1 [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE) and Acceptable Use Policy [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/USE_POLICY.md)\n\nSee the License file for Meta Llama 3 [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3/LICENSE) and Acceptable Use Policy [here](https://github.com/meta-llama/llama-models/blob/main/models/llama3/USE_POLICY.md)\n\nSee the License file for Meta Llama 2 [here](https://github.com/meta-llama/llama-models/blob/main/models/llama2/LICENSE) and Acceptable Use Policy [here](https://github.com/meta-llama/llama-models/blob/main/models/llama2/USE_POLICY.md)\n<!-- markdown-link-check-enable -->\n",
  "external_links_in_readme": [
    "https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/USE_POLICY.md",
    "https://img.shields.io/badge/Llama_Tools-llama--prompt--ops-orange?logo=meta\"",
    "https://github.com/meta-llama/llama-cookbook/tree/main/end-to-end-use-cases",
    "https://github.com/meta-llama/llama-cookbook/tree/archive-main",
    "https://github.com/meta-llama/synthetic-data-kit\"><img",
    "https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/LICENSE",
    "https://github.com/meta-llama/llama-models/blob/main/models/llama2/USE_POLICY.md",
    "https://img.shields.io/badge/Hugging_Face-meta--llama-yellow?logo=huggingface\"",
    "https://huggingface.co/meta-llama\"><img",
    "https://github.com/meta-llama/llama-models/blob/main/models/llama3/USE_POLICY.md",
    "https://img.shields.io/badge/Llama_OSS-Documentation-4BA9FE?logo=meta\"",
    "https://bit.ly/llama-api-main",
    "https://www.llama.com/docs/overview/?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main\"><img",
    "https://github.com/meta-llama/llama-cookbook/tree/main/3p-integrations",
    "https://img.shields.io/badge/Llama_API-Documentation-4BA9FE?logo=meta\"",
    "https://img.shields.io/badge/Llama_API-Join_Waitlist-brightgreen?logo=meta\"",
    "https://github.com/meta-llama/llama-models/blob/main/models/llama4/LICENSE",
    "https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/LICENSE",
    "https://github.com/meta-llama/llama-models/blob/main/models/llama3_2/USE_POLICY.md",
    "https://github.com/meta-llama/llama-prompt-ops\"><img",
    "https://github.com/meta-llama/llama-models/blob/main/models/llama2/LICENSE",
    "https://github.com/meta-llama/llama-models/blob/main/models/llama3/LICENSE",
    "https://github.com/meta-llama/llama-cookbook/tree/main/src/",
    "https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/LICENSE",
    "https://img.shields.io/badge/Llama_OSS-Model_cards-green?logo=meta\"",
    "https://github.com/meta-llama/llama-models/blob/main/models/llama4/USE_POLICY.md",
    "https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/finetuning",
    "https://llama.developer.meta.com/join_waitlist?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main\"><img",
    "https://github.com/meta-llama/llama-cookbook/tree/main/src/docs/",
    "https://llama.developer.meta.com/docs?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main\"><img",
    "https://github.com/meta-llama/llama-models/blob/main/models/?utm_source=llama-cookbook&utm_medium=readme&utm_campaign=main\"><img",
    "https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/",
    "https://img.shields.io/badge/Llama_Tools-synthetic--data--kit-orange?logo=meta\"",
    "https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/USE_POLICY.md",
    "https://www.llama.com",
    "https://github.com/meta-llama/llama-cookbook/tree/main/getting-started/inference/"
  ]
}
```

</details>


---

## Repository 6: meta-llama/llama

# GitHub Repository Data

**Repository:** [meta-llama/llama](https://github.com/meta-llama/llama)

## Basic Information

- **Description:** Inference code for Llama models
- **Created:** 2023-02-14T09:29:12+00:00
- **Last Updated:** 2025-06-21T21:43:55+00:00
- **Last Pushed:** 2025-01-26T21:42:26+00:00
- **Default Branch:** main
- **Size:** 1150 KB

## Statistics

- **Stars:** 58,405
- **Forks:** 9,778
- **Watchers:** 58,405
- **Open Issues:** 490
- **Total Issues:** 0
- **Pull Requests:** 199

## License

- **Type:** Other
- **SPDX ID:** NOASSERTION
- **URL:** [License](https://github.com/meta-llama/llama/blob/main/LICENSE)

## Languages

- **Python:** 45,017 bytes
- **Shell:** 2,544 bytes

## Top Contributors

1. **jspisak** - 42 contributions
2. **ruanslv** - 14 contributions
3. **sekyondaMeta** - 12 contributions
4. **samuelselvan** - 7 contributions
5. **subramen** - 5 contributions
6. **paksha** - 4 contributions
7. **flu0r1ne** - 3 contributions
8. **rajveer43** - 3 contributions
9. **vubui** - 3 contributions
10. **sagindykovsl** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 22

- `.github` (tree)
- `.github/ISSUE_TEMPLATE` (tree)
- `.github/ISSUE_TEMPLATE/bug_report.md` (blob)
- `.gitignore` (blob)
- `CODE_OF_CONDUCT.md` (blob)
- `CONTRIBUTING.md` (blob)
- `LICENSE` (blob)
- `MODEL_CARD.md` (blob)
- `README.md` (blob)
- `Responsible-Use-Guide.pdf` (blob)

## Recent Issues

- 游릭 **#1376** SSL: CERTIFICATE_VERIFY_FAILED (open)
- 游릭 **#1375** Cannot download SSL Verification issue (open)
- 游릭 **#1374** Hash mismatch (open)
- 游릭 **#1373** Bsia8wbbwwje (open)
- 游릭 **#1372** AGI Blueprint Proposal by EpyQ Group (open)

## Recent Pull Requests

- 游릭 **#1372** AGI Blueprint Proposal by EpyQ Group (open)
- 游릭 **#1339** Add new bug report template and VSCode settings (open)
- 游릭 **#1335** Monday  (open)
- 游릭 **#1326** Add LlamaSafetyOptimizer for Runtime Safety Checks and Performance Optimization (open)
- 游릭 **#1325** [Edited] Refactor code to optimize performance (open)

## Recent Commits

- **689c7f26** Update README.md - amitsangani (2025-01-26T21:42:26+00:00)
- **8fac8bef** Update README.md - Joseph Spisak (2024-07-23T14:50:27+00:00)
- **227d378a** Merge pull request #1125 from hyungupark/patch-1 - Samuel Selvan (2024-07-23T03:59:46+00:00)
- **66bc7307** Update download.sh - Samuel Selvan (2024-07-23T01:20:54+00:00)
- **12b676b9** Update download.sh - Samuel Selvan (2024-07-23T01:15:37+00:00)
- **c0098be8** Update download.sh - hyungupark (2024-05-15T03:49:24+00:00)
- **be327c42** Merge pull request #1124 from dandv/patch-1 - Joseph Spisak (2024-05-14T22:31:19+00:00)
- **893ff972** README: LLama 2 is no longer the latest version - Dan Dascalescu (2024-05-14T21:53:25+00:00)
- **b8348da3** Merge pull request #1079 from MattGurney/fix-model-card - Samuel Selvan (2024-04-09T16:17:49+00:00)
- **04b200c5** Merge pull request #1091 from osanseviero/patch-1 - Samuel Selvan (2024-04-09T16:16:48+00:00)

## External Links Found in README

- https://github.com/facebookresearch/llama/tree/llama_v1
- https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212
- http://github.com/facebookresearch/llama
- https://github.com/facebookresearch/llama-recipes
- https://ai.meta.com/resources/models-and-libraries/llama-downloads/
- https://github.com/meta-llama/llama-models
- https://github.com/meta-llama/llama-agentic-system
- https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
- https://huggingface.co/meta-llama
- https://github.com/meta-llama/llama-recipes
- https://github.com/facebookresearch/llama-recipes/blob/main/examples/inference.py
- https://ai.meta.com/llama/faq/
- https://ai.meta.com/resources/models-and-libraries/llama
- http://facebook.com/whitehat/info
- http://developers.facebook.com/llama_output_feedback
- https://github.com/meta-llama/PurpleLlama
- https://ai.meta.com/llama/open-innovation-ai-research-community/
- https://github.com/facebookresearch/llama-recipes/
- https://github.com/meta-llama/llama-toolchain

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 601538369,
  "name": "llama",
  "full_name": "meta-llama/llama",
  "description": "Inference code for Llama models",
  "html_url": "https://github.com/meta-llama/llama",
  "clone_url": "https://github.com/meta-llama/llama.git",
  "ssh_url": "git@github.com:meta-llama/llama.git",
  "homepage": "",
  "topics": [],
  "default_branch": "main",
  "created_at": "2023-02-14T09:29:12+00:00",
  "updated_at": "2025-06-21T21:43:55+00:00",
  "pushed_at": "2025-01-26T21:42:26+00:00",
  "size_kb": 1150,
  "watchers_count": 58405,
  "stargazers_count": 58405,
  "forks_count": 9778,
  "open_issues_count": 490,
  "license": {
    "key": "other",
    "name": "Other",
    "spdx_id": "NOASSERTION",
    "url": "https://github.com/meta-llama/llama/blob/main/LICENSE"
  },
  "languages": {
    "Python": 45017,
    "Shell": 2544
  },
  "top_contributors": [
    {
      "login": "jspisak",
      "contributions": 42
    },
    {
      "login": "ruanslv",
      "contributions": 14
    },
    {
      "login": "sekyondaMeta",
      "contributions": 12
    },
    {
      "login": "samuelselvan",
      "contributions": 7
    },
    {
      "login": "subramen",
      "contributions": 5
    },
    {
      "login": "paksha",
      "contributions": 4
    },
    {
      "login": "flu0r1ne",
      "contributions": 3
    },
    {
      "login": "rajveer43",
      "contributions": 3
    },
    {
      "login": "vubui",
      "contributions": 3
    },
    {
      "login": "sagindykovsl",
      "contributions": 3
    },
    {
      "login": "ShorthillsAI",
      "contributions": 2
    },
    {
      "login": "timlacroix",
      "contributions": 2
    },
    {
      "login": "huy-ha",
      "contributions": 2
    },
    {
      "login": "bashnick",
      "contributions": 1
    },
    {
      "login": "NinoRisteski",
      "contributions": 1
    },
    {
      "login": "osanseviero",
      "contributions": 1
    },
    {
      "login": "kit1980",
      "contributions": 1
    },
    {
      "login": "vinnymeller",
      "contributions": 1
    },
    {
      "login": "amitsangani",
      "contributions": 1
    },
    {
      "login": "godpeny",
      "contributions": 1
    }
  ],
  "file_tree_count": 22,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/bug_report.md",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "CODE_OF_CONDUCT.md",
      "type": "blob"
    },
    {
      "path": "CONTRIBUTING.md",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "MODEL_CARD.md",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "Responsible-Use-Guide.pdf",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 199,
  "recent_issues": [
    {
      "number": 1376,
      "title": "SSL: CERTIFICATE_VERIFY_FAILED",
      "state": "open"
    },
    {
      "number": 1375,
      "title": "Cannot download SSL Verification issue",
      "state": "open"
    },
    {
      "number": 1374,
      "title": "Hash mismatch",
      "state": "open"
    },
    {
      "number": 1373,
      "title": "Bsia8wbbwwje",
      "state": "open"
    },
    {
      "number": 1372,
      "title": "AGI Blueprint Proposal by EpyQ Group",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1372,
      "title": "AGI Blueprint Proposal by EpyQ Group",
      "state": "open"
    },
    {
      "number": 1339,
      "title": "Add new bug report template and VSCode settings",
      "state": "open"
    },
    {
      "number": 1335,
      "title": "Monday ",
      "state": "open"
    },
    {
      "number": 1326,
      "title": "Add LlamaSafetyOptimizer for Runtime Safety Checks and Performance Optimization",
      "state": "open"
    },
    {
      "number": 1325,
      "title": "[Edited] Refactor code to optimize performance",
      "state": "open"
    }
  ],
  "recent_commits": [
    {
      "sha": "689c7f261b9c5514636ecc3c5fefefcbb3e6eed7",
      "author": "amitsangani",
      "date": "2025-01-26T21:42:26+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "8fac8befd776bc03242fe7bc2236cdb41b6c609c",
      "author": "Joseph Spisak",
      "date": "2024-07-23T14:50:27+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "227d378a775c1e05d1434e41a1baca1c11d08d2a",
      "author": "Samuel Selvan",
      "date": "2024-07-23T03:59:46+00:00",
      "message": "Merge pull request #1125 from hyungupark/patch-1"
    },
    {
      "sha": "66bc7307da52efc6327307407386e26b17afa09c",
      "author": "Samuel Selvan",
      "date": "2024-07-23T01:20:54+00:00",
      "message": "Update download.sh"
    },
    {
      "sha": "12b676b909368581d39cebafae57226688d5676a",
      "author": "Samuel Selvan",
      "date": "2024-07-23T01:15:37+00:00",
      "message": "Update download.sh"
    },
    {
      "sha": "c0098be87adea7fedeb1b149b4c272a8384395b1",
      "author": "hyungupark",
      "date": "2024-05-15T03:49:24+00:00",
      "message": "Update download.sh"
    },
    {
      "sha": "be327c427cc5e89cc1d3ab3d3fec4484df771245",
      "author": "Joseph Spisak",
      "date": "2024-05-14T22:31:19+00:00",
      "message": "Merge pull request #1124 from dandv/patch-1"
    },
    {
      "sha": "893ff972e1355f43d4a22c6aeeaaa015b73f25d3",
      "author": "Dan Dascalescu",
      "date": "2024-05-14T21:53:25+00:00",
      "message": "README: LLama 2 is no longer the latest version"
    },
    {
      "sha": "b8348da38fde8644ef00a56596efb376f86838d1",
      "author": "Samuel Selvan",
      "date": "2024-04-09T16:17:49+00:00",
      "message": "Merge pull request #1079 from MattGurney/fix-model-card"
    },
    {
      "sha": "04b200c5dce04184a56892b1c71ea8ffad7b47c6",
      "author": "Samuel Selvan",
      "date": "2024-04-09T16:16:48+00:00",
      "message": "Merge pull request #1091 from osanseviero/patch-1"
    },
    {
      "sha": "fd7308965bf7e92c7a860584198705112cd87b9f",
      "author": "Omar Sanseviero",
      "date": "2024-04-08T14:12:21+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "1f9a8d774a10fbe41321e530428e91f7eb7eb822",
      "author": "MattGurney",
      "date": "2024-03-23T08:03:21+00:00",
      "message": "Update MODEL_CARD.md"
    },
    {
      "sha": "54c22c0d63a3f3c9e77f43a6a3041c00018f4964",
      "author": "Suraj Subramanian",
      "date": "2024-03-21T15:50:25+00:00",
      "message": "Merge pull request #1077 from mst272/main"
    },
    {
      "sha": "1e8375848d3a3ebaccab83fd670b880864cf9409",
      "author": "wangzhihong",
      "date": "2024-03-21T02:09:34+00:00",
      "message": "update the code to use the module's __call__"
    },
    {
      "sha": "52afd48b0683ba219a9cc283317b80a7837eff46",
      "author": "Joseph Spisak",
      "date": "2024-03-20T17:55:51+00:00",
      "message": "Merge pull request #1076 from meta-llama/jspisak-patch-7"
    },
    {
      "sha": "826ad1198c02f93fe14fef5dbf4a499fcb02e3b1",
      "author": "Joseph Spisak",
      "date": "2024-03-20T17:50:59+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "2f58b8d7b6ac64810b3b832a8da65671d0361ba7",
      "author": "Joseph Spisak",
      "date": "2024-03-13T17:23:21+00:00",
      "message": "Merge pull request #1063 from jeffxtang/LLaMA_lowercase"
    },
    {
      "sha": "0b466166eee0b57ec876f83ded533c78ff9ff7d2",
      "author": "Jeff Tang",
      "date": "2024-03-13T17:18:18+00:00",
      "message": "change LLaMA to Llama in README"
    },
    {
      "sha": "9a001c7a0987afd7b8de94e538916eff8950a73a",
      "author": "Joseph Spisak",
      "date": "2024-03-06T03:28:07+00:00",
      "message": "Merge pull request #1058 from shorthills-ai/main"
    },
    {
      "sha": "11ebe80305e3bcc8a8a8d75ec0e4b39955b916eb",
      "author": "Shorthills AI",
      "date": "2024-03-06T02:36:47+00:00",
      "message": "Update README.md"
    }
  ],
  "readme_text": "## **Note of deprecation**\n\nThank you for developing with Llama models. As part of the Llama 3.1 release, we\u2019ve consolidated GitHub repos and added some additional repos as we\u2019ve expanded Llama\u2019s functionality into being an e2e Llama Stack. Please use the following repos going forward:\n- [llama-models](https://github.com/meta-llama/llama-models) - Central repo for the foundation models including basic utilities, model cards, license and use policies\n- [PurpleLlama](https://github.com/meta-llama/PurpleLlama) - Key component of Llama Stack focusing on safety risks and inference time mitigations \n- [llama-toolchain](https://github.com/meta-llama/llama-toolchain) - Model development (inference/fine-tuning/safety shields/synthetic data generation) interfaces and canonical implementations\n- [llama-agentic-system](https://github.com/meta-llama/llama-agentic-system) - E2E standalone Llama Stack system, along with opinionated underlying interface, that enables creation of agentic applications\n- [llama-cookbook](https://github.com/meta-llama/llama-recipes) - Community driven scripts and integrations\n\nIf you have any questions, please feel free to file an issue on any of the above repos and we will do our best to respond in a timely manner. \n\nThank you!\n\n\n# (Deprecated) Llama 2\n\nWe are unlocking the power of large language models. Llama 2 is now accessible to individuals, creators, researchers, and businesses of all sizes so that they can experiment, innovate, and scale their ideas responsibly. \n\nThis release includes model weights and starting code for pre-trained and fine-tuned Llama language models \u2014 ranging from 7B to 70B parameters.\n\nThis repository is intended as a minimal example to load [Llama 2](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/) models and run inference. For more detailed examples leveraging Hugging Face, see [llama-cookbook](https://github.com/facebookresearch/llama-recipes/).\n\n## Updates post-launch\n\nSee [UPDATES.md](UPDATES.md). Also for a running list of frequently asked questions, see [here](https://ai.meta.com/llama/faq/).\n\n## Download\n\nIn order to download the model weights and tokenizer, please visit the [Meta website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and accept our License.\n\nOnce your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download.\n\nPre-requisites: Make sure you have `wget` and `md5sum` installed. Then run the script: `./download.sh`.\n\nKeep in mind that the links expire after 24 hours and a certain amount of downloads. If you start seeing errors such as `403: Forbidden`, you can always re-request a link.\n\n### Access to Hugging Face\n\nWe are also providing downloads on [Hugging Face](https://huggingface.co/meta-llama). You can request access to the models by acknowledging the license and filling the form in the model card of a repo. After doing so, you should get access to all the Llama models of a version (Code Llama, Llama 2, or Llama Guard) within 1 hour.\n\n## Quick Start\n\nYou can follow the steps below to quickly get up and running with Llama 2 models. These steps will let you run quick inference locally. For more examples, see the [Llama 2 cookbook repository](https://github.com/facebookresearch/llama-recipes). \n\n1. In a conda env with PyTorch / CUDA available clone and download this repository.\n\n2. In the top-level directory run:\n    ```bash\n    pip install -e .\n    ```\n3. Visit the [Meta website](https://ai.meta.com/resources/models-and-libraries/llama-downloads/) and register to download the model/s.\n\n4. Once registered, you will get an email with a URL to download the models. You will need this URL when you run the download.sh script.\n\n5. Once you get the email, navigate to your downloaded llama repository and run the download.sh script. \n    - Make sure to grant execution permissions to the download.sh script\n    - During this process, you will be prompted to enter the URL from the email. \n    - Do not use the \u201cCopy Link\u201d option but rather make sure to manually copy the link from the email.\n\n6. Once the model/s you want have been downloaded, you can run the model locally using the command below:\n```bash\ntorchrun --nproc_per_node 1 example_chat_completion.py \\\n    --ckpt_dir llama-2-7b-chat/ \\\n    --tokenizer_path tokenizer.model \\\n    --max_seq_len 512 --max_batch_size 6\n```\n**Note**\n- Replace  `llama-2-7b-chat/` with the path to your checkpoint directory and `tokenizer.model` with the path to your tokenizer model.\n- The `\u2013nproc_per_node` should be set to the [MP](#inference) value for the model you are using.\n- Adjust the `max_seq_len` and `max_batch_size` parameters as needed.\n- This example runs the [example_chat_completion.py](example_chat_completion.py) found in this repository but you can change that to a different .py file.\n\n## Inference\n\nDifferent models require different model-parallel (MP) values:\n\n|  Model | MP |\n|--------|----|\n| 7B     | 1  |\n| 13B    | 2  |\n| 70B    | 8  |\n\nAll models support sequence length up to 4096 tokens, but we pre-allocate the cache according to `max_seq_len` and `max_batch_size` values. So set those according to your hardware.\n\n### Pretrained Models\n\nThese models are not finetuned for chat or Q&A. They should be prompted so that the expected answer is the natural continuation of the prompt.\n\nSee `example_text_completion.py` for some examples. To illustrate, see the command below to run it with the llama-2-7b model (`nproc_per_node` needs to be set to the `MP` value):\n\n```\ntorchrun --nproc_per_node 1 example_text_completion.py \\\n    --ckpt_dir llama-2-7b/ \\\n    --tokenizer_path tokenizer.model \\\n    --max_seq_len 128 --max_batch_size 4\n```\n\n### Fine-tuned Chat Models\n\nThe fine-tuned models were trained for dialogue applications. To get the expected features and performance for them, a specific formatting defined in [`chat_completion`](https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212)\nneeds to be followed, including the `INST` and `<<SYS>>` tags, `BOS` and `EOS` tokens, and the whitespaces and breaklines in between (we recommend calling `strip()` on inputs to avoid double-spaces).\n\nYou can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-cookbook repo for [an example](https://github.com/facebookresearch/llama-recipes/blob/main/examples/inference.py) of how to add a safety checker to the inputs and outputs of your inference code.\n\nExamples using llama-2-7b-chat:\n\n```\ntorchrun --nproc_per_node 1 example_chat_completion.py \\\n    --ckpt_dir llama-2-7b-chat/ \\\n    --tokenizer_path tokenizer.model \\\n    --max_seq_len 512 --max_batch_size 6\n```\n\nLlama 2 is a new technology that carries potential risks with use. Testing conducted to date has not \u2014 and could not \u2014 cover all scenarios.\nIn order to help developers address these risks, we have created the [Responsible Use Guide](Responsible-Use-Guide.pdf). More details can be found in our research paper as well.\n\n## Issues\n\nPlease report any software \u201cbug\u201d, or other problems with the models through one of the following means:\n- Reporting issues with the model: [github.com/facebookresearch/llama](http://github.com/facebookresearch/llama)\n- Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n## Model Card\nSee [MODEL_CARD.md](MODEL_CARD.md).\n\n## License\n\nOur model and weights are licensed for both researchers and commercial entities, upholding the principles of openness. Our mission is to empower individuals, and industry through this opportunity, while fostering an environment of discovery and ethical AI advancements. \n\nSee the [LICENSE](LICENSE) file, as well as our accompanying [Acceptable Use Policy](USE_POLICY.md)\n\n## References\n\n1. [Research Paper](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/)\n2. [Llama 2 technical overview](https://ai.meta.com/resources/models-and-libraries/llama)\n3. [Open Innovation AI Research Community](https://ai.meta.com/llama/open-innovation-ai-research-community/)\n\nFor common questions, the FAQ can be found [here](https://ai.meta.com/llama/faq/) which will be kept up to date over time as new questions arise. \n\n## Original Llama\nThe repo for the original llama release is in the [`llama_v1`](https://github.com/facebookresearch/llama/tree/llama_v1) branch.\n",
  "external_links_in_readme": [
    "https://github.com/facebookresearch/llama/tree/llama_v1",
    "https://github.com/facebookresearch/llama/blob/main/llama/generation.py#L212",
    "http://github.com/facebookresearch/llama",
    "https://github.com/facebookresearch/llama-recipes",
    "https://ai.meta.com/resources/models-and-libraries/llama-downloads/",
    "https://github.com/meta-llama/llama-models",
    "https://github.com/meta-llama/llama-agentic-system",
    "https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/",
    "https://huggingface.co/meta-llama",
    "https://github.com/meta-llama/llama-recipes",
    "https://github.com/facebookresearch/llama-recipes/blob/main/examples/inference.py",
    "https://ai.meta.com/llama/faq/",
    "https://ai.meta.com/resources/models-and-libraries/llama",
    "http://facebook.com/whitehat/info",
    "http://developers.facebook.com/llama_output_feedback",
    "https://github.com/meta-llama/PurpleLlama",
    "https://ai.meta.com/llama/open-innovation-ai-research-community/",
    "https://github.com/facebookresearch/llama-recipes/",
    "https://github.com/meta-llama/llama-toolchain"
  ]
}
```

</details>


---

## Repository 7: meta-llama/llama-models

# GitHub Repository Data

**Repository:** [meta-llama/llama-models](https://github.com/meta-llama/llama-models)

## Basic Information

- **Description:** Utilities intended for use with Llama models.
- **Created:** 2024-06-27T22:14:09+00:00
- **Last Updated:** 2025-06-21T19:26:30+00:00
- **Last Pushed:** 2025-06-02T14:30:54+00:00
- **Default Branch:** main
- **Size:** 4442 KB

## Statistics

- **Stars:** 7,085
- **Forks:** 1,188
- **Watchers:** 7,085
- **Open Issues:** 136
- **Total Issues:** 0
- **Pull Requests:** 143

## License

- **Type:** Other
- **SPDX ID:** NOASSERTION
- **URL:** [License](https://github.com/meta-llama/llama-models/blob/main/LICENSE)

## Languages

- **Python:** 308,457 bytes

## Top Contributors

1. **ashwinb** - 90 contributions
2. **yanxi0830** - 11 contributions
3. **dltn** - 11 contributions
4. **jspisak** - 10 contributions
5. **ehhuang** - 6 contributions
6. **wukaixingxp** - 6 contributions
7. **raghotham** - 6 contributions
8. **dvrogozh** - 5 contributions
9. **github-actions[bot]** - 5 contributions
10. **machina-source** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 110

- `.github` (tree)
- `.github/CODEOWNERS` (blob)
- `.github/workflows` (tree)
- `.github/workflows/publish-to-test-pypi.yml` (blob)
- `.gitignore` (blob)
- `.pre-commit-config.yaml` (blob)
- `.ruff.toml` (blob)
- `CODE_OF_CONDUCT.md` (blob)
- `CONTRIBUTING.md` (blob)
- `LICENSE` (blob)

## Recent Issues

- 游릭 **#370** Super slow inference using eager attention with Llama-4-Scout-17B-16E-Instruct (open)
- 游릭 **#369** Webi3 Llama (open)
- 游릭 **#368** Issue downloading Llama Models (open)
- 游릭 **#367** Create devcontainer.json (open)
- 游릭 **#366** 403 error downloading models (open)

## Recent Pull Requests

- 游릭 **#369** Webi3 Llama (open)
- 游릭 **#367** Create devcontainer.json (open)
- 游댮 **#357** chore: remove usage of load_tiktoken_bpe (closed)
- 游릭 **#353** Andriod Darvin Monteras (open)
- 游댮 **#345** sync prompt_format.md (closed)

## Recent Commits

- **01dc8ce4** chore: remove usage of load_tiktoken_bpe (#357) - S칠bastien Han (2025-06-02T14:30:54+00:00)
- **f3d16d73** sync prompt_format.md (#345) - ehhuang (2025-05-07T18:02:54+00:00)
- **02b654c9** Fix Llama 3.3 model size in README (#339) - Marek Jeli켻ski (2025-05-02T21:15:43+00:00)
- **9921d278** update llama4 prompt_format.md (#332) - ehhuang (2025-04-25T22:54:25+00:00)
- **823cd862** fix: re-add python_start, python_end tokens (#327) - Ashwin Bharambe (2025-04-12T03:29:27+00:00)
- **28fa4e3b** fix: on-the-fly int4 quantize parameter (#324) - Jiawen Liu (2025-04-09T20:28:43+00:00)
- **408c5771** fix: update rope scaling for Llama-4-Scout (#322) - Ashwin Bharambe (2025-04-09T06:25:11+00:00)
- **78eb422f** fix: Kill L2Norm since it was a misnomer, this is just an unscaled rmsnorm, name it as such (#320) - Ashwin Bharambe (2025-04-09T05:49:12+00:00)
- **cb246012** feat: support xccl distributed backend in llama3 (#319) - Dmitry Rogozhkin (2025-04-09T05:49:00+00:00)
- **5c4377d2** feat: add xpu support to llama3 completion scripts (#318) - Dmitry Rogozhkin (2025-04-09T05:48:12+00:00)

## External Links Found in README

- https://huggingface.co/meta-Llama">
- https://llama.meta.com/">Website</a>&nbsp
- https://img.shields.io/pypi/dm/llama-models
- https://llama.meta.com/llama-downloads/
- https://llama.meta.com/get-started/">Get
- https://huggingface.co/meta-llama
- https://ai.meta.com/static-resource/responsible-use-guide/
- https://github.com/meta-llama/llama-cookbook">Llama
- https://ai.meta.com/blog/">
- https://img.shields.io/discord/1257833999603335178
- https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E
- http://facebook.com/whitehat/info
- https://github.com/meta-llama/llama-stack
- https://discord.gg/TZAAYNVtrU
- https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues
- https://llama.meta.com/faq
- https://pypi.org/project/llama-models/
- http://developers.facebook.com/llama_output_feedback

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 821134374,
  "name": "llama-models",
  "full_name": "meta-llama/llama-models",
  "description": "Utilities intended for use with Llama models.",
  "html_url": "https://github.com/meta-llama/llama-models",
  "clone_url": "https://github.com/meta-llama/llama-models.git",
  "ssh_url": "git@github.com:meta-llama/llama-models.git",
  "homepage": null,
  "topics": [],
  "default_branch": "main",
  "created_at": "2024-06-27T22:14:09+00:00",
  "updated_at": "2025-06-21T19:26:30+00:00",
  "pushed_at": "2025-06-02T14:30:54+00:00",
  "size_kb": 4442,
  "watchers_count": 7085,
  "stargazers_count": 7085,
  "forks_count": 1188,
  "open_issues_count": 136,
  "license": {
    "key": "other",
    "name": "Other",
    "spdx_id": "NOASSERTION",
    "url": "https://github.com/meta-llama/llama-models/blob/main/LICENSE"
  },
  "languages": {
    "Python": 308457
  },
  "top_contributors": [
    {
      "login": "ashwinb",
      "contributions": 90
    },
    {
      "login": "yanxi0830",
      "contributions": 11
    },
    {
      "login": "dltn",
      "contributions": 11
    },
    {
      "login": "jspisak",
      "contributions": 10
    },
    {
      "login": "ehhuang",
      "contributions": 6
    },
    {
      "login": "wukaixingxp",
      "contributions": 6
    },
    {
      "login": "raghotham",
      "contributions": 6
    },
    {
      "login": "dvrogozh",
      "contributions": 5
    },
    {
      "login": "github-actions[bot]",
      "contributions": 5
    },
    {
      "login": "machina-source",
      "contributions": 3
    },
    {
      "login": "ConnorHack",
      "contributions": 3
    },
    {
      "login": "rohit-ptl",
      "contributions": 3
    },
    {
      "login": "hardikjshah",
      "contributions": 3
    },
    {
      "login": "samuelselvan",
      "contributions": 2
    },
    {
      "login": "wysuperfly",
      "contributions": 1
    },
    {
      "login": "varunfb",
      "contributions": 1
    },
    {
      "login": "pmysl",
      "contributions": 1
    },
    {
      "login": "noah23olsen",
      "contributions": 1
    },
    {
      "login": "minimalic",
      "contributions": 1
    },
    {
      "login": "liyunlu0618",
      "contributions": 1
    }
  ],
  "file_tree_count": 110,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/CODEOWNERS",
      "type": "blob"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/publish-to-test-pypi.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": ".pre-commit-config.yaml",
      "type": "blob"
    },
    {
      "path": ".ruff.toml",
      "type": "blob"
    },
    {
      "path": "CODE_OF_CONDUCT.md",
      "type": "blob"
    },
    {
      "path": "CONTRIBUTING.md",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 143,
  "recent_issues": [
    {
      "number": 370,
      "title": "Super slow inference using eager attention with Llama-4-Scout-17B-16E-Instruct",
      "state": "open"
    },
    {
      "number": 369,
      "title": "Webi3 Llama",
      "state": "open"
    },
    {
      "number": 368,
      "title": "Issue downloading Llama Models",
      "state": "open"
    },
    {
      "number": 367,
      "title": "Create devcontainer.json",
      "state": "open"
    },
    {
      "number": 366,
      "title": "403 error downloading models",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 369,
      "title": "Webi3 Llama",
      "state": "open"
    },
    {
      "number": 367,
      "title": "Create devcontainer.json",
      "state": "open"
    },
    {
      "number": 357,
      "title": "chore: remove usage of load_tiktoken_bpe",
      "state": "closed"
    },
    {
      "number": 353,
      "title": "Andriod Darvin Monteras",
      "state": "open"
    },
    {
      "number": 345,
      "title": "sync prompt_format.md",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "01dc8ce46fecf06b639598f715efbb4ab981fb4c",
      "author": "S\u00e9bastien Han",
      "date": "2025-06-02T14:30:54+00:00",
      "message": "chore: remove usage of load_tiktoken_bpe (#357)"
    },
    {
      "sha": "f3d16d734f4de7d5bb7427705399e350da5e200f",
      "author": "ehhuang",
      "date": "2025-05-07T18:02:54+00:00",
      "message": "sync prompt_format.md (#345)"
    },
    {
      "sha": "02b654c9d4923d0bede37d5f1a9a8c8c56022e4d",
      "author": "Marek Jeli\u0144ski",
      "date": "2025-05-02T21:15:43+00:00",
      "message": "Fix Llama 3.3 model size in README (#339)"
    },
    {
      "sha": "9921d278bf26cf4f3385e7bea399c2b6e7847f8b",
      "author": "ehhuang",
      "date": "2025-04-25T22:54:25+00:00",
      "message": "update llama4 prompt_format.md (#332)"
    },
    {
      "sha": "823cd8622e44d90b8e989e9f41ca364c06f5701d",
      "author": "Ashwin Bharambe",
      "date": "2025-04-12T03:29:27+00:00",
      "message": "fix: re-add python_start, python_end tokens (#327)"
    },
    {
      "sha": "28fa4e3b287e84f6a6a92aab3c931f7479c827c1",
      "author": "Jiawen Liu",
      "date": "2025-04-09T20:28:43+00:00",
      "message": "fix: on-the-fly int4 quantize parameter (#324)"
    },
    {
      "sha": "408c577168a187941f9359c916004fa3b21fbc3b",
      "author": "Ashwin Bharambe",
      "date": "2025-04-09T06:25:11+00:00",
      "message": "fix: update rope scaling for Llama-4-Scout (#322)"
    },
    {
      "sha": "78eb422f717d4de4707cebf520f0f8224537e4d3",
      "author": "Ashwin Bharambe",
      "date": "2025-04-09T05:49:12+00:00",
      "message": "fix: Kill L2Norm since it was a misnomer, this is just an unscaled rmsnorm, name it as such (#320)"
    },
    {
      "sha": "cb246012bc7177ac32ae17ed9611d3327b890cba",
      "author": "Dmitry Rogozhkin",
      "date": "2025-04-09T05:49:00+00:00",
      "message": "feat: support xccl distributed backend in llama3 (#319)"
    },
    {
      "sha": "5c4377d2a9be016b137cd20c765cdf52589f4b72",
      "author": "Dmitry Rogozhkin",
      "date": "2025-04-09T05:48:12+00:00",
      "message": "feat: add xpu support to llama3 completion scripts (#318)"
    },
    {
      "sha": "0ed6a1f4ce962bdf9c97b44d5032dcc3bc8c8394",
      "author": "Dmitry Rogozhkin",
      "date": "2025-04-09T05:47:35+00:00",
      "message": "fix: fix llama3 generation test (#317)"
    },
    {
      "sha": "f62cac6d2d145033df6016901aca53548193ead3",
      "author": "Ashwin Bharambe",
      "date": "2025-04-08T14:19:49+00:00",
      "message": "fix: ensure finetune_right_pad stays at the same position before the hiding of python_start, python_end"
    },
    {
      "sha": "63172b319ee6412d165095cc5a8f0fde12288ec0",
      "author": "Ashwin Bharambe",
      "date": "2025-04-07T22:06:03+00:00",
      "message": "fix: a couple minor bugs"
    },
    {
      "sha": "88e9f6aadc0067cb7a4724cc5e46eca384c11a91",
      "author": "Ashwin Bharambe",
      "date": "2025-04-07T18:00:41+00:00",
      "message": "fix: iron out differences between llama-models and llama-stack"
    },
    {
      "sha": "b12e46273bf002ab1318064bc0e14c49ecbe63a0",
      "author": "Ashwin Bharambe",
      "date": "2025-04-07T05:40:17+00:00",
      "message": "refactor: make llama3 generation closer to llama4 (#309)"
    },
    {
      "sha": "699a02993512fb36936b1b0741e13c06790bcf98",
      "author": "Ashwin Bharambe",
      "date": "2025-04-06T20:23:46+00:00",
      "message": "fix: quantization script had regressed (#308)"
    },
    {
      "sha": "4f45ca9d0522581aedcb7b2e2231e87b084bbadf",
      "author": "Ashwin Bharambe",
      "date": "2025-04-06T17:53:23+00:00",
      "message": "fix: make sure fp8 quantization only quantizes MoE layers"
    },
    {
      "sha": "2b2e5b2645c962f92dc004aa868696ec0e53b05c",
      "author": "Ashwin Bharambe",
      "date": "2025-04-06T02:05:57+00:00",
      "message": "fix: cleanup MoE docs"
    },
    {
      "sha": "ebe3527e5104647c0976af25ddbe88784778ed18",
      "author": "raghotham",
      "date": "2025-04-06T00:43:29+00:00",
      "message": "Create llama4 prompt_format.md (#303)"
    },
    {
      "sha": "eececc27d275a0f7d4d14ec83648d51c10a76560",
      "author": "Hamid Shojanazeri",
      "date": "2025-04-05T19:28:34+00:00",
      "message": "Update README.md (#300)"
    }
  ],
  "readme_text": "<p align=\"center\">\n  <img src=\"/Llama_Repo.jpeg\" width=\"400\"/>\n</p>\n\n<p align=\"center\">\n        \ud83e\udd17 <a href=\"https://huggingface.co/meta-Llama\"> Models on Hugging Face</a>&nbsp | <a href=\"https://ai.meta.com/blog/\"> Blog</a>&nbsp |  <a href=\"https://llama.meta.com/\">Website</a>&nbsp | <a href=\"https://llama.meta.com/get-started/\">Get Started</a>&nbsp | <a href=\"https://github.com/meta-llama/llama-cookbook\">Llama Cookbook</a>&nbsp\n<br>\n\n---\n\n# Llama Models\n\nLlama is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. A few key aspects:\n1. **Open access**: Easy accessibility to cutting-edge large language models, fostering collaboration and advancements among developers, researchers, and organizations\n2. **Broad ecosystem**: Llama models have been downloaded hundreds of millions of times, there are thousands of community projects built on Llama and platform support is broad from cloud providers to startups - the world is building with Llama!\n3. **Trust & safety**: Llama models are part of a comprehensive approach to trust and safety, releasing models and tools that are designed to enable community collaboration and encourage the standardization of the development and usage of trust and safety tools for generative AI\n\nOur mission is to empower individuals and industry through this opportunity while fostering an environment of discovery and ethical AI advancements. The model weights are licensed for researchers and commercial entities, upholding the principles of openness.\n\n## Llama Models\n\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-models)](https://pypi.org/project/llama-models/)\n[![Discord](https://img.shields.io/discord/1257833999603335178)](https://discord.gg/TZAAYNVtrU)\n\n|  **Model** | **Launch date** | **Model sizes** | **Context Length** | **Tokenizer** | **Acceptable use policy**  |  **License** | **Model Card** |\n| :----: | :----: | :----: | :----:|:----:|:----:|:----:|:----:|\n| Llama 2 | 7/18/2023 | 7B, 13B, 70B | 4K | Sentencepiece | [Use Policy](models/llama2/USE_POLICY.md) | [License](models/llama2/LICENSE) | [Model Card](models/llama2/MODEL_CARD.md) |\n| Llama 3 | 4/18/2024 | 8B, 70B | 8K | TikToken-based | [Use Policy](models/llama3/USE_POLICY.md) | [License](models/llama3/LICENSE) | [Model Card](models/llama3/MODEL_CARD.md) |\n| Llama 3.1 | 7/23/2024 | 8B, 70B, 405B | 128K | TikToken-based | [Use Policy](models/llama3_1/USE_POLICY.md) | [License](models/llama3_1/LICENSE) | [Model Card](models/llama3_1/MODEL_CARD.md) |\n| Llama 3.2 | 9/25/2024 | 1B, 3B | 128K | TikToken-based | [Use Policy](models/llama3_2/USE_POLICY.md) | [License](models/llama3_2/LICENSE) | [Model Card](models/llama3_2/MODEL_CARD.md) |\n| Llama 3.2-Vision | 9/25/2024 | 11B, 90B | 128K | TikToken-based | [Use Policy](models/llama3_2/USE_POLICY.md) | [License](models/llama3_2/LICENSE) | [Model Card](models/llama3_2/MODEL_CARD_VISION.md) |\n| Llama 3.3 | 12/04/2024 | 70B | 128K | TikToken-based | [Use Policy](models/llama3_3/USE_POLICY.md) | [License](models/llama3_3/LICENSE) | [Model Card](models/llama3_3/MODEL_CARD.md) |\n| Llama 4 | 4/5/2025 | Scout-17B-16E, Maverick-17B-128E | 10M, 1M | TikToken-based | [Use Policy](models/llama4/USE_POLICY.md) | [License](models/llama4/LICENSE) | [Model Card](models/llama4/MODEL_CARD.md) |\n\n## Download\n\nTo download the model weights and tokenizer:\n\n1. Visit the [Meta Llama website](https://llama.meta.com/llama-downloads/).\n2. Read and accept the license.\n3. Once your request is approved you will receive a signed URL via email.\n4. Install the [Llama CLI](https://github.com/meta-llama/llama-stack): `pip install llama-stack`. (**<-- Start Here if you have received an email already.**)\n5. Run `llama model list` to show the latest available models and determine the model ID you wish to download. **NOTE**:\nIf you want older versions of models, run `llama model list --show-all` to show all the available Llama models.\n\n6. Run: `llama download --source meta --model-id CHOSEN_MODEL_ID`\n7. Pass the URL provided when prompted to start the download.\n\nRemember that the links expire after 24 hours and a certain amount of downloads. You can always re-request a link if you start seeing errors such as `403: Forbidden`.\n\n## Running the models\n\nIn order to run the models, you will need to install dependencies after checking out the repository.\n\n```bash\n# Run this within a suitable Python environment (uv, conda, or virtualenv)\npip install .[torch]\n```\n\nExample scripts are available in `models/{ llama3, llama4 }/scripts/` sub-directory. Note that the Llama4 series of models require at least 4 GPUs to run inference at full (bf16) precision.\n\n```bash\n#!/bin/bash\n\nNGPUS=4\nCHECKPOINT_DIR=~/.llama/checkpoints/Llama-4-Scout-17B-16E-Instruct\nPYTHONPATH=$(git rev-parse --show-toplevel) \\\n  torchrun --nproc_per_node=$NGPUS \\\n  -m models.llama4.scripts.chat_completion $CHECKPOINT_DIR \\\n  --world_size $NGPUS\n```\n\nThe above script should be used with an Instruct (Chat) model. For a Base model, update the `CHECKPOINT_DIR` path and use the script `models.llama4.scripts.completion`.\n\n\n## Running inference with FP8 and Int4 Quantization\n\nYou can reduce the memory footprint of the models at the cost of minimal loss in accuracy by running inference with FP8 or Int4 quantization. Use the `--quantization-mode` flag to specify the quantization mode. There are two modes:\n- `fp8_mixed`: Mixed precision inference with FP8 for some weights and bfloat16 for activations.\n- `int4_mixed`: Mixed precision inference with Int4 for some weights and bfloat16 for activations.\n\nUsing FP8, running Llama-4-Scout-17B-16E-Instruct requires 2 GPUs with 80GB of memory. Using Int4, you need a single GPU with 80GB of memory.\n\n```bash\nMODE=fp8_mixed  # or int4_mixed\nif [ $MODE == \"fp8_mixed\" ]; then\n  NGPUS=2\nelse\n  NGPUS=1\nfi\nCHECKPOINT_DIR=~/.llama/checkpoints/Llama-4-Scout-17B-16E-Instruct\nPYTHONPATH=$(git rev-parse --show-toplevel) \\\n  torchrun --nproc_per_node=$NGPUS \\\n  -m models.llama4.scripts.chat_completion $CHECKPOINT_DIR \\\n  --world_size $NGPUS \\\n  --quantization-mode $MODE\n```\n\n\nFor more flexibility in running inference (including using other providers), please see the [`Llama Stack`](https://github.com/meta-llama/llama-stack) toolset.\n\n\n## Access to Hugging Face\n\nWe also provide downloads on [Hugging Face](https://huggingface.co/meta-llama), in both transformers and native `llama4` formats. To download the weights from Hugging Face, please follow these steps:\n\n- Visit one of the repos, for example [meta-llama/Llama-4-Scout-17B-16E](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E).\n- Read and accept the license. Once your request is approved, you'll be granted access to all Llama 3.1 models as well as previous versions. Note that requests used to take up to one hour to get processed.\n- To download the original native weights to use with this repo, click on the \"Files and versions\" tab and download the contents of the `original` folder. You can also download them from the command line if you `pip install huggingface-hub`:\n\n```bash\nhuggingface-cli download meta-llama/Llama-4-Scout-17B-16E-Instruct-Original --local-dir meta-llama/Llama-4-Scout-17B-16E-Instruct-Original\n```\n\n- To use with transformers, the following snippet will download and cache the weights:\n\n  ```python\n  # inference.py\n  from transformers import AutoTokenizer, Llama4ForConditionalGeneration\n  import torch\n\n  model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n\n  tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n  messages = [\n      {\"role\": \"user\", \"content\": \"Who are you?\"},\n  ]\n  inputs = tokenizer.apply_chat_template(\n      messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True\n  )\n\n  model = Llama4ForConditionalGeneration.from_pretrained(\n      model_id, device_map=\"auto\", torch_dtype=torch.bfloat16\n  )\n\n  outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n  outputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1] :])\n  print(outputs[0])\n  ```\n  ```bash\n   torchrun --nnodes=1 --nproc_per_node=8 inference.py\n   ```\n\n## Installations\n\nYou can install this repository as a [package](https://pypi.org/project/llama-models/) by just doing `pip install llama-models`\n\n## Responsible Use\n\nLlama models are a new technology that carries potential risks with use. Testing conducted to date has not \u2014 and could not \u2014 cover all scenarios.\nTo help developers address these risks, we have created the [Responsible Use Guide](https://ai.meta.com/static-resource/responsible-use-guide/).\n\n## Issues\n\nPlease report any software \u201cbug\u201d or other problems with the models through one of the following means:\n- Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\n- Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n\n## Questions\n\nFor common questions, the FAQ can be found [here](https://llama.meta.com/faq), which will be updated over time as new questions arise.\n",
  "external_links_in_readme": [
    "https://huggingface.co/meta-Llama\">",
    "https://llama.meta.com/\">Website</a>&nbsp",
    "https://img.shields.io/pypi/dm/llama-models",
    "https://llama.meta.com/llama-downloads/",
    "https://llama.meta.com/get-started/\">Get",
    "https://huggingface.co/meta-llama",
    "https://ai.meta.com/static-resource/responsible-use-guide/",
    "https://github.com/meta-llama/llama-cookbook\">Llama",
    "https://ai.meta.com/blog/\">",
    "https://img.shields.io/discord/1257833999603335178",
    "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E",
    "http://facebook.com/whitehat/info",
    "https://github.com/meta-llama/llama-stack",
    "https://discord.gg/TZAAYNVtrU",
    "https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues",
    "https://llama.meta.com/faq",
    "https://pypi.org/project/llama-models/",
    "http://developers.facebook.com/llama_output_feedback"
  ]
}
```

</details>


---

## Repository 8: meta-llama/llama-models

# GitHub Repository Data

**Repository:** [meta-llama/llama-models](https://github.com/meta-llama/llama-models)

## Basic Information

- **Description:** Utilities intended for use with Llama models.
- **Created:** 2024-06-27T22:14:09+00:00
- **Last Updated:** 2025-06-21T19:26:30+00:00
- **Last Pushed:** 2025-06-02T14:30:54+00:00
- **Default Branch:** main
- **Size:** 4442 KB

## Statistics

- **Stars:** 7,085
- **Forks:** 1,188
- **Watchers:** 7,085
- **Open Issues:** 136
- **Total Issues:** 0
- **Pull Requests:** 143

## License

- **Type:** Other
- **SPDX ID:** NOASSERTION
- **URL:** [License](https://github.com/meta-llama/llama-models/blob/main/LICENSE)

## Languages

- **Python:** 308,457 bytes

## Top Contributors

1. **ashwinb** - 90 contributions
2. **yanxi0830** - 11 contributions
3. **dltn** - 11 contributions
4. **jspisak** - 10 contributions
5. **ehhuang** - 6 contributions
6. **wukaixingxp** - 6 contributions
7. **raghotham** - 6 contributions
8. **dvrogozh** - 5 contributions
9. **github-actions[bot]** - 5 contributions
10. **machina-source** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 110

- `.github` (tree)
- `.github/CODEOWNERS` (blob)
- `.github/workflows` (tree)
- `.github/workflows/publish-to-test-pypi.yml` (blob)
- `.gitignore` (blob)
- `.pre-commit-config.yaml` (blob)
- `.ruff.toml` (blob)
- `CODE_OF_CONDUCT.md` (blob)
- `CONTRIBUTING.md` (blob)
- `LICENSE` (blob)

## Recent Issues

- 游릭 **#370** Super slow inference using eager attention with Llama-4-Scout-17B-16E-Instruct (open)
- 游릭 **#369** Webi3 Llama (open)
- 游릭 **#368** Issue downloading Llama Models (open)
- 游릭 **#367** Create devcontainer.json (open)
- 游릭 **#366** 403 error downloading models (open)

## Recent Pull Requests

- 游릭 **#369** Webi3 Llama (open)
- 游릭 **#367** Create devcontainer.json (open)
- 游댮 **#357** chore: remove usage of load_tiktoken_bpe (closed)
- 游릭 **#353** Andriod Darvin Monteras (open)
- 游댮 **#345** sync prompt_format.md (closed)

## Recent Commits

- **01dc8ce4** chore: remove usage of load_tiktoken_bpe (#357) - S칠bastien Han (2025-06-02T14:30:54+00:00)
- **f3d16d73** sync prompt_format.md (#345) - ehhuang (2025-05-07T18:02:54+00:00)
- **02b654c9** Fix Llama 3.3 model size in README (#339) - Marek Jeli켻ski (2025-05-02T21:15:43+00:00)
- **9921d278** update llama4 prompt_format.md (#332) - ehhuang (2025-04-25T22:54:25+00:00)
- **823cd862** fix: re-add python_start, python_end tokens (#327) - Ashwin Bharambe (2025-04-12T03:29:27+00:00)
- **28fa4e3b** fix: on-the-fly int4 quantize parameter (#324) - Jiawen Liu (2025-04-09T20:28:43+00:00)
- **408c5771** fix: update rope scaling for Llama-4-Scout (#322) - Ashwin Bharambe (2025-04-09T06:25:11+00:00)
- **78eb422f** fix: Kill L2Norm since it was a misnomer, this is just an unscaled rmsnorm, name it as such (#320) - Ashwin Bharambe (2025-04-09T05:49:12+00:00)
- **cb246012** feat: support xccl distributed backend in llama3 (#319) - Dmitry Rogozhkin (2025-04-09T05:49:00+00:00)
- **5c4377d2** feat: add xpu support to llama3 completion scripts (#318) - Dmitry Rogozhkin (2025-04-09T05:48:12+00:00)

## External Links Found in README

- https://huggingface.co/meta-Llama">
- https://llama.meta.com/">Website</a>&nbsp
- https://img.shields.io/pypi/dm/llama-models
- https://llama.meta.com/llama-downloads/
- https://llama.meta.com/get-started/">Get
- https://huggingface.co/meta-llama
- https://ai.meta.com/static-resource/responsible-use-guide/
- https://github.com/meta-llama/llama-cookbook">Llama
- https://ai.meta.com/blog/">
- https://img.shields.io/discord/1257833999603335178
- https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E
- http://facebook.com/whitehat/info
- https://github.com/meta-llama/llama-stack
- https://discord.gg/TZAAYNVtrU
- https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues
- https://llama.meta.com/faq
- https://pypi.org/project/llama-models/
- http://developers.facebook.com/llama_output_feedback

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 821134374,
  "name": "llama-models",
  "full_name": "meta-llama/llama-models",
  "description": "Utilities intended for use with Llama models.",
  "html_url": "https://github.com/meta-llama/llama-models",
  "clone_url": "https://github.com/meta-llama/llama-models.git",
  "ssh_url": "git@github.com:meta-llama/llama-models.git",
  "homepage": null,
  "topics": [],
  "default_branch": "main",
  "created_at": "2024-06-27T22:14:09+00:00",
  "updated_at": "2025-06-21T19:26:30+00:00",
  "pushed_at": "2025-06-02T14:30:54+00:00",
  "size_kb": 4442,
  "watchers_count": 7085,
  "stargazers_count": 7085,
  "forks_count": 1188,
  "open_issues_count": 136,
  "license": {
    "key": "other",
    "name": "Other",
    "spdx_id": "NOASSERTION",
    "url": "https://github.com/meta-llama/llama-models/blob/main/LICENSE"
  },
  "languages": {
    "Python": 308457
  },
  "top_contributors": [
    {
      "login": "ashwinb",
      "contributions": 90
    },
    {
      "login": "yanxi0830",
      "contributions": 11
    },
    {
      "login": "dltn",
      "contributions": 11
    },
    {
      "login": "jspisak",
      "contributions": 10
    },
    {
      "login": "ehhuang",
      "contributions": 6
    },
    {
      "login": "wukaixingxp",
      "contributions": 6
    },
    {
      "login": "raghotham",
      "contributions": 6
    },
    {
      "login": "dvrogozh",
      "contributions": 5
    },
    {
      "login": "github-actions[bot]",
      "contributions": 5
    },
    {
      "login": "machina-source",
      "contributions": 3
    },
    {
      "login": "ConnorHack",
      "contributions": 3
    },
    {
      "login": "rohit-ptl",
      "contributions": 3
    },
    {
      "login": "hardikjshah",
      "contributions": 3
    },
    {
      "login": "samuelselvan",
      "contributions": 2
    },
    {
      "login": "wysuperfly",
      "contributions": 1
    },
    {
      "login": "varunfb",
      "contributions": 1
    },
    {
      "login": "pmysl",
      "contributions": 1
    },
    {
      "login": "noah23olsen",
      "contributions": 1
    },
    {
      "login": "minimalic",
      "contributions": 1
    },
    {
      "login": "liyunlu0618",
      "contributions": 1
    }
  ],
  "file_tree_count": 110,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/CODEOWNERS",
      "type": "blob"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/publish-to-test-pypi.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": ".pre-commit-config.yaml",
      "type": "blob"
    },
    {
      "path": ".ruff.toml",
      "type": "blob"
    },
    {
      "path": "CODE_OF_CONDUCT.md",
      "type": "blob"
    },
    {
      "path": "CONTRIBUTING.md",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 143,
  "recent_issues": [
    {
      "number": 370,
      "title": "Super slow inference using eager attention with Llama-4-Scout-17B-16E-Instruct",
      "state": "open"
    },
    {
      "number": 369,
      "title": "Webi3 Llama",
      "state": "open"
    },
    {
      "number": 368,
      "title": "Issue downloading Llama Models",
      "state": "open"
    },
    {
      "number": 367,
      "title": "Create devcontainer.json",
      "state": "open"
    },
    {
      "number": 366,
      "title": "403 error downloading models",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 369,
      "title": "Webi3 Llama",
      "state": "open"
    },
    {
      "number": 367,
      "title": "Create devcontainer.json",
      "state": "open"
    },
    {
      "number": 357,
      "title": "chore: remove usage of load_tiktoken_bpe",
      "state": "closed"
    },
    {
      "number": 353,
      "title": "Andriod Darvin Monteras",
      "state": "open"
    },
    {
      "number": 345,
      "title": "sync prompt_format.md",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "01dc8ce46fecf06b639598f715efbb4ab981fb4c",
      "author": "S\u00e9bastien Han",
      "date": "2025-06-02T14:30:54+00:00",
      "message": "chore: remove usage of load_tiktoken_bpe (#357)"
    },
    {
      "sha": "f3d16d734f4de7d5bb7427705399e350da5e200f",
      "author": "ehhuang",
      "date": "2025-05-07T18:02:54+00:00",
      "message": "sync prompt_format.md (#345)"
    },
    {
      "sha": "02b654c9d4923d0bede37d5f1a9a8c8c56022e4d",
      "author": "Marek Jeli\u0144ski",
      "date": "2025-05-02T21:15:43+00:00",
      "message": "Fix Llama 3.3 model size in README (#339)"
    },
    {
      "sha": "9921d278bf26cf4f3385e7bea399c2b6e7847f8b",
      "author": "ehhuang",
      "date": "2025-04-25T22:54:25+00:00",
      "message": "update llama4 prompt_format.md (#332)"
    },
    {
      "sha": "823cd8622e44d90b8e989e9f41ca364c06f5701d",
      "author": "Ashwin Bharambe",
      "date": "2025-04-12T03:29:27+00:00",
      "message": "fix: re-add python_start, python_end tokens (#327)"
    },
    {
      "sha": "28fa4e3b287e84f6a6a92aab3c931f7479c827c1",
      "author": "Jiawen Liu",
      "date": "2025-04-09T20:28:43+00:00",
      "message": "fix: on-the-fly int4 quantize parameter (#324)"
    },
    {
      "sha": "408c577168a187941f9359c916004fa3b21fbc3b",
      "author": "Ashwin Bharambe",
      "date": "2025-04-09T06:25:11+00:00",
      "message": "fix: update rope scaling for Llama-4-Scout (#322)"
    },
    {
      "sha": "78eb422f717d4de4707cebf520f0f8224537e4d3",
      "author": "Ashwin Bharambe",
      "date": "2025-04-09T05:49:12+00:00",
      "message": "fix: Kill L2Norm since it was a misnomer, this is just an unscaled rmsnorm, name it as such (#320)"
    },
    {
      "sha": "cb246012bc7177ac32ae17ed9611d3327b890cba",
      "author": "Dmitry Rogozhkin",
      "date": "2025-04-09T05:49:00+00:00",
      "message": "feat: support xccl distributed backend in llama3 (#319)"
    },
    {
      "sha": "5c4377d2a9be016b137cd20c765cdf52589f4b72",
      "author": "Dmitry Rogozhkin",
      "date": "2025-04-09T05:48:12+00:00",
      "message": "feat: add xpu support to llama3 completion scripts (#318)"
    },
    {
      "sha": "0ed6a1f4ce962bdf9c97b44d5032dcc3bc8c8394",
      "author": "Dmitry Rogozhkin",
      "date": "2025-04-09T05:47:35+00:00",
      "message": "fix: fix llama3 generation test (#317)"
    },
    {
      "sha": "f62cac6d2d145033df6016901aca53548193ead3",
      "author": "Ashwin Bharambe",
      "date": "2025-04-08T14:19:49+00:00",
      "message": "fix: ensure finetune_right_pad stays at the same position before the hiding of python_start, python_end"
    },
    {
      "sha": "63172b319ee6412d165095cc5a8f0fde12288ec0",
      "author": "Ashwin Bharambe",
      "date": "2025-04-07T22:06:03+00:00",
      "message": "fix: a couple minor bugs"
    },
    {
      "sha": "88e9f6aadc0067cb7a4724cc5e46eca384c11a91",
      "author": "Ashwin Bharambe",
      "date": "2025-04-07T18:00:41+00:00",
      "message": "fix: iron out differences between llama-models and llama-stack"
    },
    {
      "sha": "b12e46273bf002ab1318064bc0e14c49ecbe63a0",
      "author": "Ashwin Bharambe",
      "date": "2025-04-07T05:40:17+00:00",
      "message": "refactor: make llama3 generation closer to llama4 (#309)"
    },
    {
      "sha": "699a02993512fb36936b1b0741e13c06790bcf98",
      "author": "Ashwin Bharambe",
      "date": "2025-04-06T20:23:46+00:00",
      "message": "fix: quantization script had regressed (#308)"
    },
    {
      "sha": "4f45ca9d0522581aedcb7b2e2231e87b084bbadf",
      "author": "Ashwin Bharambe",
      "date": "2025-04-06T17:53:23+00:00",
      "message": "fix: make sure fp8 quantization only quantizes MoE layers"
    },
    {
      "sha": "2b2e5b2645c962f92dc004aa868696ec0e53b05c",
      "author": "Ashwin Bharambe",
      "date": "2025-04-06T02:05:57+00:00",
      "message": "fix: cleanup MoE docs"
    },
    {
      "sha": "ebe3527e5104647c0976af25ddbe88784778ed18",
      "author": "raghotham",
      "date": "2025-04-06T00:43:29+00:00",
      "message": "Create llama4 prompt_format.md (#303)"
    },
    {
      "sha": "eececc27d275a0f7d4d14ec83648d51c10a76560",
      "author": "Hamid Shojanazeri",
      "date": "2025-04-05T19:28:34+00:00",
      "message": "Update README.md (#300)"
    }
  ],
  "readme_text": "<p align=\"center\">\n  <img src=\"/Llama_Repo.jpeg\" width=\"400\"/>\n</p>\n\n<p align=\"center\">\n        \ud83e\udd17 <a href=\"https://huggingface.co/meta-Llama\"> Models on Hugging Face</a>&nbsp | <a href=\"https://ai.meta.com/blog/\"> Blog</a>&nbsp |  <a href=\"https://llama.meta.com/\">Website</a>&nbsp | <a href=\"https://llama.meta.com/get-started/\">Get Started</a>&nbsp | <a href=\"https://github.com/meta-llama/llama-cookbook\">Llama Cookbook</a>&nbsp\n<br>\n\n---\n\n# Llama Models\n\nLlama is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. A few key aspects:\n1. **Open access**: Easy accessibility to cutting-edge large language models, fostering collaboration and advancements among developers, researchers, and organizations\n2. **Broad ecosystem**: Llama models have been downloaded hundreds of millions of times, there are thousands of community projects built on Llama and platform support is broad from cloud providers to startups - the world is building with Llama!\n3. **Trust & safety**: Llama models are part of a comprehensive approach to trust and safety, releasing models and tools that are designed to enable community collaboration and encourage the standardization of the development and usage of trust and safety tools for generative AI\n\nOur mission is to empower individuals and industry through this opportunity while fostering an environment of discovery and ethical AI advancements. The model weights are licensed for researchers and commercial entities, upholding the principles of openness.\n\n## Llama Models\n\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-models)](https://pypi.org/project/llama-models/)\n[![Discord](https://img.shields.io/discord/1257833999603335178)](https://discord.gg/TZAAYNVtrU)\n\n|  **Model** | **Launch date** | **Model sizes** | **Context Length** | **Tokenizer** | **Acceptable use policy**  |  **License** | **Model Card** |\n| :----: | :----: | :----: | :----:|:----:|:----:|:----:|:----:|\n| Llama 2 | 7/18/2023 | 7B, 13B, 70B | 4K | Sentencepiece | [Use Policy](models/llama2/USE_POLICY.md) | [License](models/llama2/LICENSE) | [Model Card](models/llama2/MODEL_CARD.md) |\n| Llama 3 | 4/18/2024 | 8B, 70B | 8K | TikToken-based | [Use Policy](models/llama3/USE_POLICY.md) | [License](models/llama3/LICENSE) | [Model Card](models/llama3/MODEL_CARD.md) |\n| Llama 3.1 | 7/23/2024 | 8B, 70B, 405B | 128K | TikToken-based | [Use Policy](models/llama3_1/USE_POLICY.md) | [License](models/llama3_1/LICENSE) | [Model Card](models/llama3_1/MODEL_CARD.md) |\n| Llama 3.2 | 9/25/2024 | 1B, 3B | 128K | TikToken-based | [Use Policy](models/llama3_2/USE_POLICY.md) | [License](models/llama3_2/LICENSE) | [Model Card](models/llama3_2/MODEL_CARD.md) |\n| Llama 3.2-Vision | 9/25/2024 | 11B, 90B | 128K | TikToken-based | [Use Policy](models/llama3_2/USE_POLICY.md) | [License](models/llama3_2/LICENSE) | [Model Card](models/llama3_2/MODEL_CARD_VISION.md) |\n| Llama 3.3 | 12/04/2024 | 70B | 128K | TikToken-based | [Use Policy](models/llama3_3/USE_POLICY.md) | [License](models/llama3_3/LICENSE) | [Model Card](models/llama3_3/MODEL_CARD.md) |\n| Llama 4 | 4/5/2025 | Scout-17B-16E, Maverick-17B-128E | 10M, 1M | TikToken-based | [Use Policy](models/llama4/USE_POLICY.md) | [License](models/llama4/LICENSE) | [Model Card](models/llama4/MODEL_CARD.md) |\n\n## Download\n\nTo download the model weights and tokenizer:\n\n1. Visit the [Meta Llama website](https://llama.meta.com/llama-downloads/).\n2. Read and accept the license.\n3. Once your request is approved you will receive a signed URL via email.\n4. Install the [Llama CLI](https://github.com/meta-llama/llama-stack): `pip install llama-stack`. (**<-- Start Here if you have received an email already.**)\n5. Run `llama model list` to show the latest available models and determine the model ID you wish to download. **NOTE**:\nIf you want older versions of models, run `llama model list --show-all` to show all the available Llama models.\n\n6. Run: `llama download --source meta --model-id CHOSEN_MODEL_ID`\n7. Pass the URL provided when prompted to start the download.\n\nRemember that the links expire after 24 hours and a certain amount of downloads. You can always re-request a link if you start seeing errors such as `403: Forbidden`.\n\n## Running the models\n\nIn order to run the models, you will need to install dependencies after checking out the repository.\n\n```bash\n# Run this within a suitable Python environment (uv, conda, or virtualenv)\npip install .[torch]\n```\n\nExample scripts are available in `models/{ llama3, llama4 }/scripts/` sub-directory. Note that the Llama4 series of models require at least 4 GPUs to run inference at full (bf16) precision.\n\n```bash\n#!/bin/bash\n\nNGPUS=4\nCHECKPOINT_DIR=~/.llama/checkpoints/Llama-4-Scout-17B-16E-Instruct\nPYTHONPATH=$(git rev-parse --show-toplevel) \\\n  torchrun --nproc_per_node=$NGPUS \\\n  -m models.llama4.scripts.chat_completion $CHECKPOINT_DIR \\\n  --world_size $NGPUS\n```\n\nThe above script should be used with an Instruct (Chat) model. For a Base model, update the `CHECKPOINT_DIR` path and use the script `models.llama4.scripts.completion`.\n\n\n## Running inference with FP8 and Int4 Quantization\n\nYou can reduce the memory footprint of the models at the cost of minimal loss in accuracy by running inference with FP8 or Int4 quantization. Use the `--quantization-mode` flag to specify the quantization mode. There are two modes:\n- `fp8_mixed`: Mixed precision inference with FP8 for some weights and bfloat16 for activations.\n- `int4_mixed`: Mixed precision inference with Int4 for some weights and bfloat16 for activations.\n\nUsing FP8, running Llama-4-Scout-17B-16E-Instruct requires 2 GPUs with 80GB of memory. Using Int4, you need a single GPU with 80GB of memory.\n\n```bash\nMODE=fp8_mixed  # or int4_mixed\nif [ $MODE == \"fp8_mixed\" ]; then\n  NGPUS=2\nelse\n  NGPUS=1\nfi\nCHECKPOINT_DIR=~/.llama/checkpoints/Llama-4-Scout-17B-16E-Instruct\nPYTHONPATH=$(git rev-parse --show-toplevel) \\\n  torchrun --nproc_per_node=$NGPUS \\\n  -m models.llama4.scripts.chat_completion $CHECKPOINT_DIR \\\n  --world_size $NGPUS \\\n  --quantization-mode $MODE\n```\n\n\nFor more flexibility in running inference (including using other providers), please see the [`Llama Stack`](https://github.com/meta-llama/llama-stack) toolset.\n\n\n## Access to Hugging Face\n\nWe also provide downloads on [Hugging Face](https://huggingface.co/meta-llama), in both transformers and native `llama4` formats. To download the weights from Hugging Face, please follow these steps:\n\n- Visit one of the repos, for example [meta-llama/Llama-4-Scout-17B-16E](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E).\n- Read and accept the license. Once your request is approved, you'll be granted access to all Llama 3.1 models as well as previous versions. Note that requests used to take up to one hour to get processed.\n- To download the original native weights to use with this repo, click on the \"Files and versions\" tab and download the contents of the `original` folder. You can also download them from the command line if you `pip install huggingface-hub`:\n\n```bash\nhuggingface-cli download meta-llama/Llama-4-Scout-17B-16E-Instruct-Original --local-dir meta-llama/Llama-4-Scout-17B-16E-Instruct-Original\n```\n\n- To use with transformers, the following snippet will download and cache the weights:\n\n  ```python\n  # inference.py\n  from transformers import AutoTokenizer, Llama4ForConditionalGeneration\n  import torch\n\n  model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n\n  tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n  messages = [\n      {\"role\": \"user\", \"content\": \"Who are you?\"},\n  ]\n  inputs = tokenizer.apply_chat_template(\n      messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True\n  )\n\n  model = Llama4ForConditionalGeneration.from_pretrained(\n      model_id, device_map=\"auto\", torch_dtype=torch.bfloat16\n  )\n\n  outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n  outputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1] :])\n  print(outputs[0])\n  ```\n  ```bash\n   torchrun --nnodes=1 --nproc_per_node=8 inference.py\n   ```\n\n## Installations\n\nYou can install this repository as a [package](https://pypi.org/project/llama-models/) by just doing `pip install llama-models`\n\n## Responsible Use\n\nLlama models are a new technology that carries potential risks with use. Testing conducted to date has not \u2014 and could not \u2014 cover all scenarios.\nTo help developers address these risks, we have created the [Responsible Use Guide](https://ai.meta.com/static-resource/responsible-use-guide/).\n\n## Issues\n\nPlease report any software \u201cbug\u201d or other problems with the models through one of the following means:\n- Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\n- Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n\n## Questions\n\nFor common questions, the FAQ can be found [here](https://llama.meta.com/faq), which will be updated over time as new questions arise.\n",
  "external_links_in_readme": [
    "https://huggingface.co/meta-Llama\">",
    "https://llama.meta.com/\">Website</a>&nbsp",
    "https://img.shields.io/pypi/dm/llama-models",
    "https://llama.meta.com/llama-downloads/",
    "https://llama.meta.com/get-started/\">Get",
    "https://huggingface.co/meta-llama",
    "https://ai.meta.com/static-resource/responsible-use-guide/",
    "https://github.com/meta-llama/llama-cookbook\">Llama",
    "https://ai.meta.com/blog/\">",
    "https://img.shields.io/discord/1257833999603335178",
    "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E",
    "http://facebook.com/whitehat/info",
    "https://github.com/meta-llama/llama-stack",
    "https://discord.gg/TZAAYNVtrU",
    "https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues",
    "https://llama.meta.com/faq",
    "https://pypi.org/project/llama-models/",
    "http://developers.facebook.com/llama_output_feedback"
  ]
}
```

</details>


---

## Repository 9: meta-llama/PurpleLlama

# GitHub Repository Data

**Repository:** [meta-llama/PurpleLlama](https://github.com/meta-llama/PurpleLlama)

## Basic Information

- **Description:** Set of tools to assess and improve LLM security.
- **Created:** 2023-12-06T21:29:41+00:00
- **Last Updated:** 2025-06-21T22:55:43+00:00
- **Last Pushed:** 2025-06-18T14:17:28+00:00
- **Default Branch:** main
- **Size:** 39636 KB

## Statistics

- **Stars:** 3,504
- **Forks:** 578
- **Watchers:** 3,504
- **Open Issues:** 16
- **Total Issues:** 0
- **Pull Requests:** 41

## License

- **Type:** Other
- **SPDX ID:** NOASSERTION
- **URL:** [License](https://github.com/meta-llama/PurpleLlama/blob/main/LICENSE)

## Languages

- **Python:** 944,185 bytes
- **C++:** 229,644 bytes
- **JavaScript:** 31,361 bytes
- **Jupyter Notebook:** 12,599 bytes
- **CSS:** 11,417 bytes
- **C:** 5,728 bytes
- **PHP:** 4,342 bytes
- **Shell:** 3,754 bytes
- **Dockerfile:** 1,352 bytes
- **MDX:** 439 bytes

## Top Contributors

1. **SimonWan** - 70 contributions
2. **dwjsong** - 37 contributions
3. **cynikolai** - 30 contributions
4. **csahana95** - 21 contributions
5. **onionymous** - 20 contributions
6. **vladionescu** - 20 contributions
7. **DhavalKapil** - 18 contributions
8. **YueLi28** - 15 contributions
9. **JFChi** - 12 contributions
10. **amontilla0** - 11 contributions

## File Structure (Sample of 10 files)

Total files: 995

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/lint.yml` (blob)
- `.github/workflows/sites_deployment.yml` (blob)
- `.github/workflows/tests.yml` (blob)
- `.gitignore` (blob)
- `CODE_OF_CONDUCT.md` (blob)
- `CONTRIBUTING.md` (blob)
- `CodeShield` (tree)
- `CodeShield/LICENSE` (blob)

## Recent Issues

- 游댮 **#115** Potential issue on `expansion` & `judge` step in MITRE evaluation (closed)
- 游릭 **#114** When using the code from the guide. the result is always the same value. (open)
- 游릭 **#113** Llama Prompt Guard Reproducibility  - Energy objective loss and training details (open)
- 游릭 **#112** Llama4ForCausalLM has no `_prepare_4d_causal_attention_mask_with_cache_position` method defined (open)
- 游댮 **#111** feat: Convert LlamaFirewall to C with D-Bus and Yocto integration (closed)

## Recent Pull Requests

- 游댮 **#111** feat: Convert LlamaFirewall to C with D-Bus and Yocto integration (closed)
- 游댮 **#110** Rename README.md to Meta.Ai (closed)
- 游댮 **#106** Fix Relative Import Error in demo_alignmentcheck.py (closed)
- 游댮 **#104** Update MODEL_CARD.md (closed)
- 游릭 **#100** Update lint.yml (open)

## Recent Commits

- **61104a7a** infra - separate base LLM into a new file - Shengye Wan (2025-06-18T14:14:52+00:00)
- **41aaf4be** CI refactor for LlamaFirewall tests - Abraham Montilla (2025-06-17T14:12:43+00:00)
- **30f25813** unblocking CI tests errors - Abraham Montilla (2025-06-13T16:24:28+00:00)
- **f0574022** unblocking CI lint errors - Abraham Montilla (2025-06-13T16:24:28+00:00)
- **23510a3c** CyberSecEval OSS PR Guidance - Shengye Wan (2025-06-12T20:40:59+00:00)
- **067ee439** PI Dataset Field Update - Alekhya Gampa (2025-06-09T17:44:16+00:00)
- **db023dcd** Fix CQS signal facebook-unused-include-check in fbcode/security/genai/CybersecurityBenchmarks/datasets/canary_exploit/memory_corruption - generatedunixname89002005287564 (2025-05-30T16:54:56+00:00)
- **55ff24c1** Adding support for multi-scanner outputs - Abraham Montilla (2025-05-28T20:59:26+00:00)
- **71edaeeb** CySE - update readme for interactions of sensitive content - Shengye Wan (2025-05-23T14:53:20+00:00)
- **1b37b0a1** Add 10 minutes of fuzzing on the GT patch for filtering - Wu Zhou (2025-05-16T17:58:31+00:00)

## External Links Found in README

- https://huggingface.co/spaces/facebook/CyberSecEval
- https://huggingface.co/meta-Llama">
- https://github.com/meta-llama/PurpleLlama/blob/main/CodeShield/notebook/CodeShieldUsageDemo.ipynb
- https://github.com/facebookresearch/PurpleLlama/blob/main/logo.png"
- https://ai.meta.com/research/publications/cyberseceval-2-a-wide-ranging-cybersecurity-evaluation-suite-for-large-language-models/
- https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai">
- https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/">Llama
- https://github.com/meta-llama/llama3/blob/main/LICENSE
- https://www.youtube.com/watch?v=ab_Fdp6FVDI
- https://ai.meta.com/llama/purple-llama">Website</a>&nbsp
- https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/">CyberSec
- https://github.com/facebookresearch/PurpleLlama/blob/main/LICENSE
- https://github.com/meta-llama/llama-agentic-system
- https://ai.meta.com/llama/responsible-use-guide/
- https://ai.meta.com/llama/faq/
- https://github.com/meta-llama/llama-recipes
- https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/
- https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 728399986,
  "name": "PurpleLlama",
  "full_name": "meta-llama/PurpleLlama",
  "description": "Set of tools to assess and improve LLM security.",
  "html_url": "https://github.com/meta-llama/PurpleLlama",
  "clone_url": "https://github.com/meta-llama/PurpleLlama.git",
  "ssh_url": "git@github.com:meta-llama/PurpleLlama.git",
  "homepage": "",
  "topics": [],
  "default_branch": "main",
  "created_at": "2023-12-06T21:29:41+00:00",
  "updated_at": "2025-06-21T22:55:43+00:00",
  "pushed_at": "2025-06-18T14:17:28+00:00",
  "size_kb": 39636,
  "watchers_count": 3504,
  "stargazers_count": 3504,
  "forks_count": 578,
  "open_issues_count": 16,
  "license": {
    "key": "other",
    "name": "Other",
    "spdx_id": "NOASSERTION",
    "url": "https://github.com/meta-llama/PurpleLlama/blob/main/LICENSE"
  },
  "languages": {
    "Python": 944185,
    "C++": 229644,
    "JavaScript": 31361,
    "Jupyter Notebook": 12599,
    "CSS": 11417,
    "C": 5728,
    "PHP": 4342,
    "Shell": 3754,
    "Dockerfile": 1352,
    "MDX": 439
  },
  "top_contributors": [
    {
      "login": "SimonWan",
      "contributions": 70
    },
    {
      "login": "dwjsong",
      "contributions": 37
    },
    {
      "login": "cynikolai",
      "contributions": 30
    },
    {
      "login": "csahana95",
      "contributions": 21
    },
    {
      "login": "onionymous",
      "contributions": 20
    },
    {
      "login": "vladionescu",
      "contributions": 20
    },
    {
      "login": "DhavalKapil",
      "contributions": 18
    },
    {
      "login": "YueLi28",
      "contributions": 15
    },
    {
      "login": "JFChi",
      "contributions": 12
    },
    {
      "login": "amontilla0",
      "contributions": 11
    },
    {
      "login": "laurendeason",
      "contributions": 8
    },
    {
      "login": "mbhatt1",
      "contributions": 7
    },
    {
      "login": "agampafb",
      "contributions": 5
    },
    {
      "login": "adam612",
      "contributions": 3
    },
    {
      "login": "r-barnes",
      "contributions": 3
    },
    {
      "login": "ujjwalkarn",
      "contributions": 3
    },
    {
      "login": "facebook-github-bot",
      "contributions": 2
    },
    {
      "login": "writerzhou",
      "contributions": 2
    },
    {
      "login": "kplawiak",
      "contributions": 2
    },
    {
      "login": "jspisak",
      "contributions": 2
    }
  ],
  "file_tree_count": 995,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/lint.yml",
      "type": "blob"
    },
    {
      "path": ".github/workflows/sites_deployment.yml",
      "type": "blob"
    },
    {
      "path": ".github/workflows/tests.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "CODE_OF_CONDUCT.md",
      "type": "blob"
    },
    {
      "path": "CONTRIBUTING.md",
      "type": "blob"
    },
    {
      "path": "CodeShield",
      "type": "tree"
    },
    {
      "path": "CodeShield/LICENSE",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 41,
  "recent_issues": [
    {
      "number": 115,
      "title": "Potential issue on `expansion` & `judge` step in MITRE evaluation",
      "state": "closed"
    },
    {
      "number": 114,
      "title": "When using the code from the guide. the result is always the same value.",
      "state": "open"
    },
    {
      "number": 113,
      "title": "Llama Prompt Guard Reproducibility  - Energy objective loss and training details",
      "state": "open"
    },
    {
      "number": 112,
      "title": "Llama4ForCausalLM has no `_prepare_4d_causal_attention_mask_with_cache_position` method defined",
      "state": "open"
    },
    {
      "number": 111,
      "title": "feat: Convert LlamaFirewall to C with D-Bus and Yocto integration",
      "state": "closed"
    }
  ],
  "recent_pulls": [
    {
      "number": 111,
      "title": "feat: Convert LlamaFirewall to C with D-Bus and Yocto integration",
      "state": "closed"
    },
    {
      "number": 110,
      "title": "Rename README.md to Meta.Ai",
      "state": "closed"
    },
    {
      "number": 106,
      "title": "Fix Relative Import Error in demo_alignmentcheck.py",
      "state": "closed"
    },
    {
      "number": 104,
      "title": "Update MODEL_CARD.md",
      "state": "closed"
    },
    {
      "number": 100,
      "title": "Update lint.yml",
      "state": "open"
    }
  ],
  "recent_commits": [
    {
      "sha": "61104a7a084a567211c892ff71f94e08b9fc6d0f",
      "author": "Shengye Wan",
      "date": "2025-06-18T14:14:52+00:00",
      "message": "infra - separate base LLM into a new file"
    },
    {
      "sha": "41aaf4be50a864010133128f5710fcc6a2d3f0df",
      "author": "Abraham Montilla",
      "date": "2025-06-17T14:12:43+00:00",
      "message": "CI refactor for LlamaFirewall tests"
    },
    {
      "sha": "30f25813583d5d62440724d504714b9844066f63",
      "author": "Abraham Montilla",
      "date": "2025-06-13T16:24:28+00:00",
      "message": "unblocking CI tests errors"
    },
    {
      "sha": "f05740226b0ef2939e69a07a05855629181d7520",
      "author": "Abraham Montilla",
      "date": "2025-06-13T16:24:28+00:00",
      "message": "unblocking CI lint errors"
    },
    {
      "sha": "23510a3c87b66271520abe228b4f7f0f14359bb1",
      "author": "Shengye Wan",
      "date": "2025-06-12T20:40:59+00:00",
      "message": "CyberSecEval OSS PR Guidance"
    },
    {
      "sha": "067ee439b94f06b49571d0828b78ec3efe241e04",
      "author": "Alekhya Gampa",
      "date": "2025-06-09T17:44:16+00:00",
      "message": "PI Dataset Field Update"
    },
    {
      "sha": "db023dcdf35971c8fb1def3a0ba460c7e1bbdf0c",
      "author": "generatedunixname89002005287564",
      "date": "2025-05-30T16:54:56+00:00",
      "message": "Fix CQS signal facebook-unused-include-check in fbcode/security/genai/CybersecurityBenchmarks/datasets/canary_exploit/memory_corruption"
    },
    {
      "sha": "55ff24c1baa317094bdde042c77bf5aac01eec84",
      "author": "Abraham Montilla",
      "date": "2025-05-28T20:59:26+00:00",
      "message": "Adding support for multi-scanner outputs"
    },
    {
      "sha": "71edaeeba2dae38f963f0f6898fca195a9353bc6",
      "author": "Shengye Wan",
      "date": "2025-05-23T14:53:20+00:00",
      "message": "CySE - update readme for interactions of sensitive content"
    },
    {
      "sha": "1b37b0a12fc36f002fa03c4f866205e4fd4db7f0",
      "author": "Wu Zhou",
      "date": "2025-05-16T17:58:31+00:00",
      "message": "Add 10 minutes of fuzzing on the GT patch for filtering"
    },
    {
      "sha": "d7996cbb3632dc1ab18b7c449b2e95459d8428b9",
      "author": "Abraham Montilla",
      "date": "2025-05-16T17:49:08+00:00",
      "message": "Update setuptools in requirements"
    },
    {
      "sha": "b4f0bd3fe8d6e82e4f63943de953eea81d43611c",
      "author": "Abraham Montilla",
      "date": "2025-05-16T17:48:47+00:00",
      "message": "Website logo multi-browser support"
    },
    {
      "sha": "e9484994d0197df1efc1c3944918033eeb4ac75f",
      "author": "Alekhya Gampa",
      "date": "2025-05-15T23:47:49+00:00",
      "message": "Update Return Messages"
    },
    {
      "sha": "0c2f39dde642128f1fb304f610f0de5522d5da12",
      "author": "Shengye Wan",
      "date": "2025-05-15T03:35:35+00:00",
      "message": "CyberSecEval - change the import to relative import"
    },
    {
      "sha": "65e8d8d56737e01a3ee26489e7402c6f936a54ba",
      "author": "Alekhya Gampa",
      "date": "2025-05-14T19:52:04+00:00",
      "message": "Add Public Service Providers to Support Gemini (GoogleGenAI)"
    },
    {
      "sha": "6e4cec2254ebc2ce1888e4418adc586728a07568",
      "author": "Alekhya Gampa",
      "date": "2025-05-14T19:52:04+00:00",
      "message": "Add Public Service Providers to Support Claude (Anthropic)"
    },
    {
      "sha": "ec65e5089cc829679d13ed2a856801261e93fd1e",
      "author": "Viewer-HX",
      "date": "2025-05-13T23:15:17+00:00",
      "message": "Fix Relative Import Error in demo_alignmentcheck.py (#106)"
    },
    {
      "sha": "7a47e8460a70b6572259a234dd9aa40a2b6b49cd",
      "author": "Alex Savage",
      "date": "2025-05-08T22:58:42+00:00",
      "message": "Update MODEL_CARD.md (#104)"
    },
    {
      "sha": "3a58f7f3725569520948a66abbd87b5305f85fac",
      "author": "Lauren Deason",
      "date": "2025-05-08T06:21:12+00:00",
      "message": "Pass guided_decode_json_schema into OPENAI query method"
    },
    {
      "sha": "3bb01650d6d694099c9d4eed7fb414c7058166a3",
      "author": "Mo",
      "date": "2025-05-07T14:38:13+00:00",
      "message": "Fixed typo website homepage (#96)"
    }
  ],
  "readme_text": "<p align=\"center\">\n  <img src=\"https://github.com/facebookresearch/PurpleLlama/blob/main/logo.png\" width=\"400\"/>\n</p>\n\n<p align=\"center\">\n        \ud83e\udd17 <a href=\"https://huggingface.co/meta-Llama\"> Models on Hugging Face</a>&nbsp | <a href=\"https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai\"> Blog</a>&nbsp |  <a href=\"https://ai.meta.com/llama/purple-llama\">Website</a>&nbsp | <a href=\"https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/\">CyberSec Eval Paper</a>&nbsp&nbsp | <a href=\"https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/\">Llama Guard Paper</a>&nbsp\n<br>\n\n---\n\n# Purple Llama\n\nPurple Llama is an umbrella project that over time will bring together tools\nand evals to help the community build responsibly with open generative AI\nmodels. The initial release will include tools and evals for Cyber Security and\nInput/Output safeguards but we plan to contribute more in the near future.\n\n## Why purple?\n\nBorrowing a [concept](https://www.youtube.com/watch?v=ab_Fdp6FVDI) from the\ncybersecurity world, we believe that to truly mitigate the challenges which\ngenerative AI presents, we need to take both attack (red team) and defensive\n(blue team) postures. Purple teaming, composed of both red and blue team\nresponsibilities, is a collaborative approach to evaluating and mitigating\npotential risks and the same ethos applies to generative AI and hence our\ninvestment in Purple Llama will be comprehensive.\n\n## License\n\nComponents within the Purple Llama project will be licensed permissively enabling both research and commercial usage.\nWe believe this is a major step towards enabling community collaboration and standardizing the development and usage of trust and safety tools for generative AI development.\nMore concretely evals and benchmarks are licensed under the MIT license while any models use the corresponding Llama Community license. See the table below:\n\n| **Component Type** |            **Components**            |                                          **License**                                           |\n| :----------------- | :----------------------------------: | :--------------------------------------------------------------------------------------------: |\n| Evals/Benchmarks   | Cyber Security Eval (others to come) |                                              MIT                                               |\n| Safeguard             |             Llama Guard              | [Llama 2 Community License](https://github.com/facebookresearch/PurpleLlama/blob/main/LICENSE) |\n| Safeguard             |             Llama Guard 2            | [Llama 3 Community License](https://github.com/meta-llama/llama3/blob/main/LICENSE) |\n| Safeguard             |             Llama Guard 3-8B            | [Llama 3.2 Community License](LICENSE) |\n| Safeguard             |             Llama Guard 3-1B            | [Llama 3.2 Community License](LICENSE) |\n| Safeguard             |             Llama Guard 3-11B-vision            | [Llama 3.2 Community License](LICENSE) |\n| Safeguard             |             Prompt Guard            | [Llama 3.2 Community License](LICENSE) |\n| Safeguard          |             Code Shield              | MIT |\n\n\n## System-Level Safeguards\n\nAs we outlined in Llama 3\u2019s\n[Responsible Use Guide](https://ai.meta.com/llama/responsible-use-guide/), we\nrecommend that all inputs and outputs to the LLM be checked and filtered in\naccordance with content guidelines appropriate to the application.\n\n### Llama Guard\n\nLlama Guard 3 consists of a series of high-performance input and output moderation models designed to support developers to detect various common types of violating content.\n\nThey were built by fine-tuning Meta-Llama 3.1 and 3.2 models and optimized to support the detection of the MLCommons standard hazards taxonomy, catering to a range of developer use cases.\nThey support the release of Llama 3.2 capabilities, including 7 new languages, a 128k context window, and image reasoning. Llama Guard 3 models were also optimized to detect helpful cyberattack responses and prevent malicious code output by LLMs to be executed in hosting environments for Llama systems using code interpreters.\n\n\n### Prompt Guard\nPrompt Guard is a powerful tool for protecting LLM powered applications from malicious prompts to ensure their security and integrity.\n\nCategories of prompt attacks include prompt injection and jailbreaking:\n\n* Prompt Injections are inputs that exploit the inclusion of untrusted data from third parties into the context window of a model to get it to execute unintended instructions.\n* Jailbreaks are malicious instructions designed to override the safety and security features built into a model.\n\n### Code Shield\n\nCode Shield adds support for inference-time filtering of insecure code produced by LLMs. Code Shield offers mitigation of insecure code suggestions risk, code interpreter abuse prevention, and secure command execution. [CodeShield Example Notebook](https://github.com/meta-llama/PurpleLlama/blob/main/CodeShield/notebook/CodeShieldUsageDemo.ipynb).\n\n\n\n## Evals & Benchmarks\n\n### Cybersecurity\n\n#### CyberSec Eval v1\nCyberSec Eval v1 was what we believe was the first industry-wide set of cybersecurity safety evaluations for LLMs. These benchmarks are based on industry guidance and standards (e.g., CWE and MITRE ATT&CK) and built in collaboration with our security subject matter experts. We aim to provide tools that will help address some risks outlined in the [White House commitments on developing responsible AI](https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/), including:\n* Metrics for quantifying LLM cybersecurity risks.\n* Tools to evaluate the frequency of insecure code suggestions.\n* Tools to evaluate LLMs to make it harder to generate malicious code or aid in carrying out cyberattacks.\n\nWe believe these tools will reduce the frequency of LLMs suggesting insecure AI-generated code and reduce their helpfulness to cyber adversaries. Our initial results show that there are meaningful cybersecurity risks for LLMs, both with recommending insecure code and for complying with malicious requests. See our [Cybersec Eval paper](https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/) for more details.\n\n#### CyberSec Eval 2\nCyberSec Eval 2 expands on its predecessor by measuring an LLM\u2019s propensity to abuse a code interpreter, offensive cybersecurity capabilities, and susceptibility to prompt injection. You can read the paper [here](https://ai.meta.com/research/publications/cyberseceval-2-a-wide-ranging-cybersecurity-evaluation-suite-for-large-language-models/).\n\nYou can also check out the \ud83e\udd17 leaderboard [here](https://huggingface.co/spaces/facebook/CyberSecEval).\n\n#### CyberSec Eval 3\nThe newly released CyberSec Eval 3 features three additional test suites: visual prompt injection tests, spear phishing capability tests, and autonomous offensive cyber operations tests.\n\n## Getting Started\n\nAs part of the [Llama reference system](https://github.com/meta-llama/llama-agentic-system), we\u2019re integrating a safety layer to facilitate adoption and deployment of these safeguards.\nResources to get started with the safeguards are available in the [Llama-recipe GitHub repository](https://github.com/meta-llama/llama-recipes).\n\n## FAQ\n\nFor a running list of frequently asked questions, for not only Purple Llama\ncomponents but also generally for Llama models, see the FAQ\n[here](https://ai.meta.com/llama/faq/).\n\n## Join the Purple Llama community\n\nSee the [CONTRIBUTING](CONTRIBUTING.md) file for how to help out.\n",
  "external_links_in_readme": [
    "https://huggingface.co/spaces/facebook/CyberSecEval",
    "https://huggingface.co/meta-Llama\">",
    "https://github.com/meta-llama/PurpleLlama/blob/main/CodeShield/notebook/CodeShieldUsageDemo.ipynb",
    "https://github.com/facebookresearch/PurpleLlama/blob/main/logo.png\"",
    "https://ai.meta.com/research/publications/cyberseceval-2-a-wide-ranging-cybersecurity-evaluation-suite-for-large-language-models/",
    "https://ai.meta.com/blog/purple-llama-open-trust-safety-generative-ai\">",
    "https://ai.meta.com/research/publications/llama-guard-llm-based-input-output-safeguard-for-human-ai-conversations/\">Llama",
    "https://github.com/meta-llama/llama3/blob/main/LICENSE",
    "https://www.youtube.com/watch?v=ab_Fdp6FVDI",
    "https://ai.meta.com/llama/purple-llama\">Website</a>&nbsp",
    "https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/\">CyberSec",
    "https://github.com/facebookresearch/PurpleLlama/blob/main/LICENSE",
    "https://github.com/meta-llama/llama-agentic-system",
    "https://ai.meta.com/llama/responsible-use-guide/",
    "https://ai.meta.com/llama/faq/",
    "https://github.com/meta-llama/llama-recipes",
    "https://ai.meta.com/research/publications/purple-llama-cyberseceval-a-benchmark-for-evaluating-the-cybersecurity-risks-of-large-language-models/",
    "https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/"
  ]
}
```

</details>


---

## Repository 10: meta-llama/llama-models

# GitHub Repository Data

**Repository:** [meta-llama/llama-models](https://github.com/meta-llama/llama-models)

## Basic Information

- **Description:** Utilities intended for use with Llama models.
- **Created:** 2024-06-27T22:14:09+00:00
- **Last Updated:** 2025-06-21T19:26:30+00:00
- **Last Pushed:** 2025-06-02T14:30:54+00:00
- **Default Branch:** main
- **Size:** 4442 KB

## Statistics

- **Stars:** 7,085
- **Forks:** 1,188
- **Watchers:** 7,085
- **Open Issues:** 136
- **Total Issues:** 0
- **Pull Requests:** 143

## License

- **Type:** Other
- **SPDX ID:** NOASSERTION
- **URL:** [License](https://github.com/meta-llama/llama-models/blob/main/LICENSE)

## Languages

- **Python:** 308,457 bytes

## Top Contributors

1. **ashwinb** - 90 contributions
2. **yanxi0830** - 11 contributions
3. **dltn** - 11 contributions
4. **jspisak** - 10 contributions
5. **ehhuang** - 6 contributions
6. **wukaixingxp** - 6 contributions
7. **raghotham** - 6 contributions
8. **dvrogozh** - 5 contributions
9. **github-actions[bot]** - 5 contributions
10. **machina-source** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 110

- `.github` (tree)
- `.github/CODEOWNERS` (blob)
- `.github/workflows` (tree)
- `.github/workflows/publish-to-test-pypi.yml` (blob)
- `.gitignore` (blob)
- `.pre-commit-config.yaml` (blob)
- `.ruff.toml` (blob)
- `CODE_OF_CONDUCT.md` (blob)
- `CONTRIBUTING.md` (blob)
- `LICENSE` (blob)

## Recent Issues

- 游릭 **#370** Super slow inference using eager attention with Llama-4-Scout-17B-16E-Instruct (open)
- 游릭 **#369** Webi3 Llama (open)
- 游릭 **#368** Issue downloading Llama Models (open)
- 游릭 **#367** Create devcontainer.json (open)
- 游릭 **#366** 403 error downloading models (open)

## Recent Pull Requests

- 游릭 **#369** Webi3 Llama (open)
- 游릭 **#367** Create devcontainer.json (open)
- 游댮 **#357** chore: remove usage of load_tiktoken_bpe (closed)
- 游릭 **#353** Andriod Darvin Monteras (open)
- 游댮 **#345** sync prompt_format.md (closed)

## Recent Commits

- **01dc8ce4** chore: remove usage of load_tiktoken_bpe (#357) - S칠bastien Han (2025-06-02T14:30:54+00:00)
- **f3d16d73** sync prompt_format.md (#345) - ehhuang (2025-05-07T18:02:54+00:00)
- **02b654c9** Fix Llama 3.3 model size in README (#339) - Marek Jeli켻ski (2025-05-02T21:15:43+00:00)
- **9921d278** update llama4 prompt_format.md (#332) - ehhuang (2025-04-25T22:54:25+00:00)
- **823cd862** fix: re-add python_start, python_end tokens (#327) - Ashwin Bharambe (2025-04-12T03:29:27+00:00)
- **28fa4e3b** fix: on-the-fly int4 quantize parameter (#324) - Jiawen Liu (2025-04-09T20:28:43+00:00)
- **408c5771** fix: update rope scaling for Llama-4-Scout (#322) - Ashwin Bharambe (2025-04-09T06:25:11+00:00)
- **78eb422f** fix: Kill L2Norm since it was a misnomer, this is just an unscaled rmsnorm, name it as such (#320) - Ashwin Bharambe (2025-04-09T05:49:12+00:00)
- **cb246012** feat: support xccl distributed backend in llama3 (#319) - Dmitry Rogozhkin (2025-04-09T05:49:00+00:00)
- **5c4377d2** feat: add xpu support to llama3 completion scripts (#318) - Dmitry Rogozhkin (2025-04-09T05:48:12+00:00)

## External Links Found in README

- https://huggingface.co/meta-Llama">
- https://llama.meta.com/">Website</a>&nbsp
- https://img.shields.io/pypi/dm/llama-models
- https://llama.meta.com/llama-downloads/
- https://llama.meta.com/get-started/">Get
- https://huggingface.co/meta-llama
- https://ai.meta.com/static-resource/responsible-use-guide/
- https://github.com/meta-llama/llama-cookbook">Llama
- https://ai.meta.com/blog/">
- https://img.shields.io/discord/1257833999603335178
- https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E
- http://facebook.com/whitehat/info
- https://github.com/meta-llama/llama-stack
- https://discord.gg/TZAAYNVtrU
- https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues
- https://llama.meta.com/faq
- https://pypi.org/project/llama-models/
- http://developers.facebook.com/llama_output_feedback

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 821134374,
  "name": "llama-models",
  "full_name": "meta-llama/llama-models",
  "description": "Utilities intended for use with Llama models.",
  "html_url": "https://github.com/meta-llama/llama-models",
  "clone_url": "https://github.com/meta-llama/llama-models.git",
  "ssh_url": "git@github.com:meta-llama/llama-models.git",
  "homepage": null,
  "topics": [],
  "default_branch": "main",
  "created_at": "2024-06-27T22:14:09+00:00",
  "updated_at": "2025-06-21T19:26:30+00:00",
  "pushed_at": "2025-06-02T14:30:54+00:00",
  "size_kb": 4442,
  "watchers_count": 7085,
  "stargazers_count": 7085,
  "forks_count": 1188,
  "open_issues_count": 136,
  "license": {
    "key": "other",
    "name": "Other",
    "spdx_id": "NOASSERTION",
    "url": "https://github.com/meta-llama/llama-models/blob/main/LICENSE"
  },
  "languages": {
    "Python": 308457
  },
  "top_contributors": [
    {
      "login": "ashwinb",
      "contributions": 90
    },
    {
      "login": "yanxi0830",
      "contributions": 11
    },
    {
      "login": "dltn",
      "contributions": 11
    },
    {
      "login": "jspisak",
      "contributions": 10
    },
    {
      "login": "ehhuang",
      "contributions": 6
    },
    {
      "login": "wukaixingxp",
      "contributions": 6
    },
    {
      "login": "raghotham",
      "contributions": 6
    },
    {
      "login": "dvrogozh",
      "contributions": 5
    },
    {
      "login": "github-actions[bot]",
      "contributions": 5
    },
    {
      "login": "machina-source",
      "contributions": 3
    },
    {
      "login": "ConnorHack",
      "contributions": 3
    },
    {
      "login": "rohit-ptl",
      "contributions": 3
    },
    {
      "login": "hardikjshah",
      "contributions": 3
    },
    {
      "login": "samuelselvan",
      "contributions": 2
    },
    {
      "login": "wysuperfly",
      "contributions": 1
    },
    {
      "login": "varunfb",
      "contributions": 1
    },
    {
      "login": "pmysl",
      "contributions": 1
    },
    {
      "login": "noah23olsen",
      "contributions": 1
    },
    {
      "login": "minimalic",
      "contributions": 1
    },
    {
      "login": "liyunlu0618",
      "contributions": 1
    }
  ],
  "file_tree_count": 110,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/CODEOWNERS",
      "type": "blob"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/publish-to-test-pypi.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": ".pre-commit-config.yaml",
      "type": "blob"
    },
    {
      "path": ".ruff.toml",
      "type": "blob"
    },
    {
      "path": "CODE_OF_CONDUCT.md",
      "type": "blob"
    },
    {
      "path": "CONTRIBUTING.md",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 143,
  "recent_issues": [
    {
      "number": 370,
      "title": "Super slow inference using eager attention with Llama-4-Scout-17B-16E-Instruct",
      "state": "open"
    },
    {
      "number": 369,
      "title": "Webi3 Llama",
      "state": "open"
    },
    {
      "number": 368,
      "title": "Issue downloading Llama Models",
      "state": "open"
    },
    {
      "number": 367,
      "title": "Create devcontainer.json",
      "state": "open"
    },
    {
      "number": 366,
      "title": "403 error downloading models",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 369,
      "title": "Webi3 Llama",
      "state": "open"
    },
    {
      "number": 367,
      "title": "Create devcontainer.json",
      "state": "open"
    },
    {
      "number": 357,
      "title": "chore: remove usage of load_tiktoken_bpe",
      "state": "closed"
    },
    {
      "number": 353,
      "title": "Andriod Darvin Monteras",
      "state": "open"
    },
    {
      "number": 345,
      "title": "sync prompt_format.md",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "01dc8ce46fecf06b639598f715efbb4ab981fb4c",
      "author": "S\u00e9bastien Han",
      "date": "2025-06-02T14:30:54+00:00",
      "message": "chore: remove usage of load_tiktoken_bpe (#357)"
    },
    {
      "sha": "f3d16d734f4de7d5bb7427705399e350da5e200f",
      "author": "ehhuang",
      "date": "2025-05-07T18:02:54+00:00",
      "message": "sync prompt_format.md (#345)"
    },
    {
      "sha": "02b654c9d4923d0bede37d5f1a9a8c8c56022e4d",
      "author": "Marek Jeli\u0144ski",
      "date": "2025-05-02T21:15:43+00:00",
      "message": "Fix Llama 3.3 model size in README (#339)"
    },
    {
      "sha": "9921d278bf26cf4f3385e7bea399c2b6e7847f8b",
      "author": "ehhuang",
      "date": "2025-04-25T22:54:25+00:00",
      "message": "update llama4 prompt_format.md (#332)"
    },
    {
      "sha": "823cd8622e44d90b8e989e9f41ca364c06f5701d",
      "author": "Ashwin Bharambe",
      "date": "2025-04-12T03:29:27+00:00",
      "message": "fix: re-add python_start, python_end tokens (#327)"
    },
    {
      "sha": "28fa4e3b287e84f6a6a92aab3c931f7479c827c1",
      "author": "Jiawen Liu",
      "date": "2025-04-09T20:28:43+00:00",
      "message": "fix: on-the-fly int4 quantize parameter (#324)"
    },
    {
      "sha": "408c577168a187941f9359c916004fa3b21fbc3b",
      "author": "Ashwin Bharambe",
      "date": "2025-04-09T06:25:11+00:00",
      "message": "fix: update rope scaling for Llama-4-Scout (#322)"
    },
    {
      "sha": "78eb422f717d4de4707cebf520f0f8224537e4d3",
      "author": "Ashwin Bharambe",
      "date": "2025-04-09T05:49:12+00:00",
      "message": "fix: Kill L2Norm since it was a misnomer, this is just an unscaled rmsnorm, name it as such (#320)"
    },
    {
      "sha": "cb246012bc7177ac32ae17ed9611d3327b890cba",
      "author": "Dmitry Rogozhkin",
      "date": "2025-04-09T05:49:00+00:00",
      "message": "feat: support xccl distributed backend in llama3 (#319)"
    },
    {
      "sha": "5c4377d2a9be016b137cd20c765cdf52589f4b72",
      "author": "Dmitry Rogozhkin",
      "date": "2025-04-09T05:48:12+00:00",
      "message": "feat: add xpu support to llama3 completion scripts (#318)"
    },
    {
      "sha": "0ed6a1f4ce962bdf9c97b44d5032dcc3bc8c8394",
      "author": "Dmitry Rogozhkin",
      "date": "2025-04-09T05:47:35+00:00",
      "message": "fix: fix llama3 generation test (#317)"
    },
    {
      "sha": "f62cac6d2d145033df6016901aca53548193ead3",
      "author": "Ashwin Bharambe",
      "date": "2025-04-08T14:19:49+00:00",
      "message": "fix: ensure finetune_right_pad stays at the same position before the hiding of python_start, python_end"
    },
    {
      "sha": "63172b319ee6412d165095cc5a8f0fde12288ec0",
      "author": "Ashwin Bharambe",
      "date": "2025-04-07T22:06:03+00:00",
      "message": "fix: a couple minor bugs"
    },
    {
      "sha": "88e9f6aadc0067cb7a4724cc5e46eca384c11a91",
      "author": "Ashwin Bharambe",
      "date": "2025-04-07T18:00:41+00:00",
      "message": "fix: iron out differences between llama-models and llama-stack"
    },
    {
      "sha": "b12e46273bf002ab1318064bc0e14c49ecbe63a0",
      "author": "Ashwin Bharambe",
      "date": "2025-04-07T05:40:17+00:00",
      "message": "refactor: make llama3 generation closer to llama4 (#309)"
    },
    {
      "sha": "699a02993512fb36936b1b0741e13c06790bcf98",
      "author": "Ashwin Bharambe",
      "date": "2025-04-06T20:23:46+00:00",
      "message": "fix: quantization script had regressed (#308)"
    },
    {
      "sha": "4f45ca9d0522581aedcb7b2e2231e87b084bbadf",
      "author": "Ashwin Bharambe",
      "date": "2025-04-06T17:53:23+00:00",
      "message": "fix: make sure fp8 quantization only quantizes MoE layers"
    },
    {
      "sha": "2b2e5b2645c962f92dc004aa868696ec0e53b05c",
      "author": "Ashwin Bharambe",
      "date": "2025-04-06T02:05:57+00:00",
      "message": "fix: cleanup MoE docs"
    },
    {
      "sha": "ebe3527e5104647c0976af25ddbe88784778ed18",
      "author": "raghotham",
      "date": "2025-04-06T00:43:29+00:00",
      "message": "Create llama4 prompt_format.md (#303)"
    },
    {
      "sha": "eececc27d275a0f7d4d14ec83648d51c10a76560",
      "author": "Hamid Shojanazeri",
      "date": "2025-04-05T19:28:34+00:00",
      "message": "Update README.md (#300)"
    }
  ],
  "readme_text": "<p align=\"center\">\n  <img src=\"/Llama_Repo.jpeg\" width=\"400\"/>\n</p>\n\n<p align=\"center\">\n        \ud83e\udd17 <a href=\"https://huggingface.co/meta-Llama\"> Models on Hugging Face</a>&nbsp | <a href=\"https://ai.meta.com/blog/\"> Blog</a>&nbsp |  <a href=\"https://llama.meta.com/\">Website</a>&nbsp | <a href=\"https://llama.meta.com/get-started/\">Get Started</a>&nbsp | <a href=\"https://github.com/meta-llama/llama-cookbook\">Llama Cookbook</a>&nbsp\n<br>\n\n---\n\n# Llama Models\n\nLlama is an accessible, open large language model (LLM) designed for developers, researchers, and businesses to build, experiment, and responsibly scale their generative AI ideas. Part of a foundational system, it serves as a bedrock for innovation in the global community. A few key aspects:\n1. **Open access**: Easy accessibility to cutting-edge large language models, fostering collaboration and advancements among developers, researchers, and organizations\n2. **Broad ecosystem**: Llama models have been downloaded hundreds of millions of times, there are thousands of community projects built on Llama and platform support is broad from cloud providers to startups - the world is building with Llama!\n3. **Trust & safety**: Llama models are part of a comprehensive approach to trust and safety, releasing models and tools that are designed to enable community collaboration and encourage the standardization of the development and usage of trust and safety tools for generative AI\n\nOur mission is to empower individuals and industry through this opportunity while fostering an environment of discovery and ethical AI advancements. The model weights are licensed for researchers and commercial entities, upholding the principles of openness.\n\n## Llama Models\n\n[![PyPI - Downloads](https://img.shields.io/pypi/dm/llama-models)](https://pypi.org/project/llama-models/)\n[![Discord](https://img.shields.io/discord/1257833999603335178)](https://discord.gg/TZAAYNVtrU)\n\n|  **Model** | **Launch date** | **Model sizes** | **Context Length** | **Tokenizer** | **Acceptable use policy**  |  **License** | **Model Card** |\n| :----: | :----: | :----: | :----:|:----:|:----:|:----:|:----:|\n| Llama 2 | 7/18/2023 | 7B, 13B, 70B | 4K | Sentencepiece | [Use Policy](models/llama2/USE_POLICY.md) | [License](models/llama2/LICENSE) | [Model Card](models/llama2/MODEL_CARD.md) |\n| Llama 3 | 4/18/2024 | 8B, 70B | 8K | TikToken-based | [Use Policy](models/llama3/USE_POLICY.md) | [License](models/llama3/LICENSE) | [Model Card](models/llama3/MODEL_CARD.md) |\n| Llama 3.1 | 7/23/2024 | 8B, 70B, 405B | 128K | TikToken-based | [Use Policy](models/llama3_1/USE_POLICY.md) | [License](models/llama3_1/LICENSE) | [Model Card](models/llama3_1/MODEL_CARD.md) |\n| Llama 3.2 | 9/25/2024 | 1B, 3B | 128K | TikToken-based | [Use Policy](models/llama3_2/USE_POLICY.md) | [License](models/llama3_2/LICENSE) | [Model Card](models/llama3_2/MODEL_CARD.md) |\n| Llama 3.2-Vision | 9/25/2024 | 11B, 90B | 128K | TikToken-based | [Use Policy](models/llama3_2/USE_POLICY.md) | [License](models/llama3_2/LICENSE) | [Model Card](models/llama3_2/MODEL_CARD_VISION.md) |\n| Llama 3.3 | 12/04/2024 | 70B | 128K | TikToken-based | [Use Policy](models/llama3_3/USE_POLICY.md) | [License](models/llama3_3/LICENSE) | [Model Card](models/llama3_3/MODEL_CARD.md) |\n| Llama 4 | 4/5/2025 | Scout-17B-16E, Maverick-17B-128E | 10M, 1M | TikToken-based | [Use Policy](models/llama4/USE_POLICY.md) | [License](models/llama4/LICENSE) | [Model Card](models/llama4/MODEL_CARD.md) |\n\n## Download\n\nTo download the model weights and tokenizer:\n\n1. Visit the [Meta Llama website](https://llama.meta.com/llama-downloads/).\n2. Read and accept the license.\n3. Once your request is approved you will receive a signed URL via email.\n4. Install the [Llama CLI](https://github.com/meta-llama/llama-stack): `pip install llama-stack`. (**<-- Start Here if you have received an email already.**)\n5. Run `llama model list` to show the latest available models and determine the model ID you wish to download. **NOTE**:\nIf you want older versions of models, run `llama model list --show-all` to show all the available Llama models.\n\n6. Run: `llama download --source meta --model-id CHOSEN_MODEL_ID`\n7. Pass the URL provided when prompted to start the download.\n\nRemember that the links expire after 24 hours and a certain amount of downloads. You can always re-request a link if you start seeing errors such as `403: Forbidden`.\n\n## Running the models\n\nIn order to run the models, you will need to install dependencies after checking out the repository.\n\n```bash\n# Run this within a suitable Python environment (uv, conda, or virtualenv)\npip install .[torch]\n```\n\nExample scripts are available in `models/{ llama3, llama4 }/scripts/` sub-directory. Note that the Llama4 series of models require at least 4 GPUs to run inference at full (bf16) precision.\n\n```bash\n#!/bin/bash\n\nNGPUS=4\nCHECKPOINT_DIR=~/.llama/checkpoints/Llama-4-Scout-17B-16E-Instruct\nPYTHONPATH=$(git rev-parse --show-toplevel) \\\n  torchrun --nproc_per_node=$NGPUS \\\n  -m models.llama4.scripts.chat_completion $CHECKPOINT_DIR \\\n  --world_size $NGPUS\n```\n\nThe above script should be used with an Instruct (Chat) model. For a Base model, update the `CHECKPOINT_DIR` path and use the script `models.llama4.scripts.completion`.\n\n\n## Running inference with FP8 and Int4 Quantization\n\nYou can reduce the memory footprint of the models at the cost of minimal loss in accuracy by running inference with FP8 or Int4 quantization. Use the `--quantization-mode` flag to specify the quantization mode. There are two modes:\n- `fp8_mixed`: Mixed precision inference with FP8 for some weights and bfloat16 for activations.\n- `int4_mixed`: Mixed precision inference with Int4 for some weights and bfloat16 for activations.\n\nUsing FP8, running Llama-4-Scout-17B-16E-Instruct requires 2 GPUs with 80GB of memory. Using Int4, you need a single GPU with 80GB of memory.\n\n```bash\nMODE=fp8_mixed  # or int4_mixed\nif [ $MODE == \"fp8_mixed\" ]; then\n  NGPUS=2\nelse\n  NGPUS=1\nfi\nCHECKPOINT_DIR=~/.llama/checkpoints/Llama-4-Scout-17B-16E-Instruct\nPYTHONPATH=$(git rev-parse --show-toplevel) \\\n  torchrun --nproc_per_node=$NGPUS \\\n  -m models.llama4.scripts.chat_completion $CHECKPOINT_DIR \\\n  --world_size $NGPUS \\\n  --quantization-mode $MODE\n```\n\n\nFor more flexibility in running inference (including using other providers), please see the [`Llama Stack`](https://github.com/meta-llama/llama-stack) toolset.\n\n\n## Access to Hugging Face\n\nWe also provide downloads on [Hugging Face](https://huggingface.co/meta-llama), in both transformers and native `llama4` formats. To download the weights from Hugging Face, please follow these steps:\n\n- Visit one of the repos, for example [meta-llama/Llama-4-Scout-17B-16E](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E).\n- Read and accept the license. Once your request is approved, you'll be granted access to all Llama 3.1 models as well as previous versions. Note that requests used to take up to one hour to get processed.\n- To download the original native weights to use with this repo, click on the \"Files and versions\" tab and download the contents of the `original` folder. You can also download them from the command line if you `pip install huggingface-hub`:\n\n```bash\nhuggingface-cli download meta-llama/Llama-4-Scout-17B-16E-Instruct-Original --local-dir meta-llama/Llama-4-Scout-17B-16E-Instruct-Original\n```\n\n- To use with transformers, the following snippet will download and cache the weights:\n\n  ```python\n  # inference.py\n  from transformers import AutoTokenizer, Llama4ForConditionalGeneration\n  import torch\n\n  model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\"\n\n  tokenizer = AutoTokenizer.from_pretrained(model_id)\n\n  messages = [\n      {\"role\": \"user\", \"content\": \"Who are you?\"},\n  ]\n  inputs = tokenizer.apply_chat_template(\n      messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True\n  )\n\n  model = Llama4ForConditionalGeneration.from_pretrained(\n      model_id, device_map=\"auto\", torch_dtype=torch.bfloat16\n  )\n\n  outputs = model.generate(**inputs.to(model.device), max_new_tokens=100)\n  outputs = tokenizer.batch_decode(outputs[:, inputs[\"input_ids\"].shape[-1] :])\n  print(outputs[0])\n  ```\n  ```bash\n   torchrun --nnodes=1 --nproc_per_node=8 inference.py\n   ```\n\n## Installations\n\nYou can install this repository as a [package](https://pypi.org/project/llama-models/) by just doing `pip install llama-models`\n\n## Responsible Use\n\nLlama models are a new technology that carries potential risks with use. Testing conducted to date has not \u2014 and could not \u2014 cover all scenarios.\nTo help developers address these risks, we have created the [Responsible Use Guide](https://ai.meta.com/static-resource/responsible-use-guide/).\n\n## Issues\n\nPlease report any software \u201cbug\u201d or other problems with the models through one of the following means:\n- Reporting issues with the model: [https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues)\n- Reporting risky content generated by the model: [developers.facebook.com/llama_output_feedback](http://developers.facebook.com/llama_output_feedback)\n- Reporting bugs and security concerns: [facebook.com/whitehat/info](http://facebook.com/whitehat/info)\n\n\n## Questions\n\nFor common questions, the FAQ can be found [here](https://llama.meta.com/faq), which will be updated over time as new questions arise.\n",
  "external_links_in_readme": [
    "https://huggingface.co/meta-Llama\">",
    "https://llama.meta.com/\">Website</a>&nbsp",
    "https://img.shields.io/pypi/dm/llama-models",
    "https://llama.meta.com/llama-downloads/",
    "https://llama.meta.com/get-started/\">Get",
    "https://huggingface.co/meta-llama",
    "https://ai.meta.com/static-resource/responsible-use-guide/",
    "https://github.com/meta-llama/llama-cookbook\">Llama",
    "https://ai.meta.com/blog/\">",
    "https://img.shields.io/discord/1257833999603335178",
    "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E",
    "http://facebook.com/whitehat/info",
    "https://github.com/meta-llama/llama-stack",
    "https://discord.gg/TZAAYNVtrU",
    "https://github.com/meta-llama/llama-models/issues](https://github.com/meta-llama/llama-models/issues",
    "https://llama.meta.com/faq",
    "https://pypi.org/project/llama-models/",
    "http://developers.facebook.com/llama_output_feedback"
  ]
}
```

</details>


---

