# GitHub Data for google_flan-t5-base

**Task Category:** Text Generation

## Repository 1: google-research/t5x

# GitHub Repository Data

**Repository:** [google-research/t5x](https://github.com/google-research/t5x)

## Basic Information

- **Description:** None
- **Created:** 2021-11-01T19:37:39+00:00
- **Last Updated:** 2025-06-18T22:09:59+00:00
- **Last Pushed:** 2025-06-04T16:08:08+00:00
- **Default Branch:** main
- **Size:** 9724 KB

## Statistics

- **Stars:** 2,834
- **Forks:** 318
- **Watchers:** 2,834
- **Open Issues:** 182
- **Total Issues:** 0
- **Pull Requests:** 1,428

## License

- **Type:** Apache License 2.0
- **SPDX ID:** Apache-2.0
- **URL:** [License](https://github.com/google-research/t5x/blob/main/LICENSE)

## Languages

- **Python:** 1,601,669 bytes
- **Jupyter Notebook:** 161,734 bytes
- **Shell:** 20,230 bytes
- **Dockerfile:** 466 bytes

## Top Contributors

1. **adarob** - 167 contributions
2. **cpgaffney1** - 134 contributions
3. **hwchung27** - 50 contributions
4. **kkenealy** - 30 contributions
5. **texasmichelle** - 25 contributions
6. **hawkinsp** - 24 contributions
7. **yashk2810** - 24 contributions
8. **KEHANG** - 23 contributions
9. **gauravmishra** - 18 contributions
10. **liangyaning33** - 16 contributions

## File Structure (Sample of 10 files)

Total files: 435

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/build.yaml` (blob)
- `CONTRIBUTING.md` (blob)
- `LICENSE` (blob)
- `README.md` (blob)
- `docs` (tree)
- `docs/_static` (tree)
- `docs/_static/t5x_theme.css` (blob)
- `docs/_templates` (tree)

## Recent Issues

- ðŸ”´ **#1626** Disable pytype for errors revealed by JAX build refactor (closed)
- ðŸ”´ **#1625** Disable pytype for errors revealed by JAX build refactor (closed)
- ðŸ”´ **#1624** Fix for seed-> key in optax (PR#1240 was not backward compatible) (closed)
- ðŸ”´ **#1623** Disable pytype for errors revealed by JAX build refactor (closed)
- ðŸ”´ **#1622** Fix DeprecationWarnings regarding `optax.DifferentiallyPrivateAggregateState` and `optax.dpsgd`. (closed)

## Recent Pull Requests

- ðŸ”´ **#1626** Disable pytype for errors revealed by JAX build refactor (closed)
- ðŸ”´ **#1625** Disable pytype for errors revealed by JAX build refactor (closed)
- ðŸ”´ **#1624** Fix for seed-> key in optax (PR#1240 was not backward compatible) (closed)
- ðŸ”´ **#1623** Disable pytype for errors revealed by JAX build refactor (closed)
- ðŸ”´ **#1622** Fix DeprecationWarnings regarding `optax.DifferentiallyPrivateAggregateState` and `optax.dpsgd`. (closed)

## Recent Commits

- **087c7242** Disable pytype for errors revealed by JAX build refactor - Jake VanderPlas (2025-06-04T16:07:02+00:00)
- **92c5b467** Disable pytype for errors revealed by JAX build refactor - Jake VanderPlas (2025-06-04T12:42:48+00:00)
- **c84e7a30** Fix for seed-> key in optax (PR#1240 was not backward compatible) - T5X Team (2025-06-03T17:12:58+00:00)
- **fb11a761** Add cast to fix conversion to int64 when JAX's x64 mode is on. - T5X Team (2025-05-02T01:25:10+00:00)
- **53ce5078** Fix DeprecationWarnings regarding `optax.DifferentiallyPrivateAggregateState` and `optax.dpsgd`. - T5X Team (2025-05-01T09:07:15+00:00)
- **9fecf300** No public description - Hana Joo (2025-04-28T18:43:40+00:00)
- **509d35e0** Reverting to previous logic of checking for export dir only when cpu dir is not specified. - Rohan Jain (2025-04-24T03:06:46+00:00)
- **0ec3eb52** Extending the T5X exporter library to support GPUs as well. - Rohan Jain (2025-04-22T18:17:27+00:00)
- **6ed10044** Handle `None` as a possible value of `output_vocabulary.eos_id` - Sergei Lebedev (2025-03-26T10:52:29+00:00)
- **ad6a629a** Allow users to save custom metadata with each checkpoint step. - Adam Cogdell (2025-01-30T21:03:01+00:00)

## External Links Found in README

- https://github.com/google-research/text-to-text-transfer-transformer
- https://github.com/google/seqio
- https://cloud.google.com/tpu/docs/system-architecture-tpu-vm
- https://github.com/google/jax
- https://github.com/NVIDIA/JAX-Toolbox/tree/main/rosetta/rosetta/projects/t5x
- https://github.com/google-research/t5x
- https://github.com/google-research/t5x/blob/main/docs/models.md
- https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms
- https://cloud.google.com/storage/docs/creating-buckets
- https://github.com/tensorflow/mesh
- https://www.tensorflow.org/tensorboard
- https://github.com/google/gin-config
- https://github.com/tensorflow/serving
- https://storage.googleapis.com/jax-releases/libtpu_releases.html
- https://www.tensorflow.org/guide/data
- https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm#create-vm
- https://github.com/deepmind/xmanager
- https://arxiv.org/abs/2203.17189},
- https://console.cloud.google.com/quotas
- https://github.com/google/flax

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 423593369,
  "name": "t5x",
  "full_name": "google-research/t5x",
  "description": null,
  "html_url": "https://github.com/google-research/t5x",
  "clone_url": "https://github.com/google-research/t5x.git",
  "ssh_url": "git@github.com:google-research/t5x.git",
  "homepage": null,
  "topics": [],
  "default_branch": "main",
  "created_at": "2021-11-01T19:37:39+00:00",
  "updated_at": "2025-06-18T22:09:59+00:00",
  "pushed_at": "2025-06-04T16:08:08+00:00",
  "size_kb": 9724,
  "watchers_count": 2834,
  "stargazers_count": 2834,
  "forks_count": 318,
  "open_issues_count": 182,
  "license": {
    "key": "apache-2.0",
    "name": "Apache License 2.0",
    "spdx_id": "Apache-2.0",
    "url": "https://github.com/google-research/t5x/blob/main/LICENSE"
  },
  "languages": {
    "Python": 1601669,
    "Jupyter Notebook": 161734,
    "Shell": 20230,
    "Dockerfile": 466
  },
  "top_contributors": [
    {
      "login": "adarob",
      "contributions": 167
    },
    {
      "login": "cpgaffney1",
      "contributions": 134
    },
    {
      "login": "hwchung27",
      "contributions": 50
    },
    {
      "login": "kkenealy",
      "contributions": 30
    },
    {
      "login": "texasmichelle",
      "contributions": 25
    },
    {
      "login": "hawkinsp",
      "contributions": 24
    },
    {
      "login": "yashk2810",
      "contributions": 24
    },
    {
      "login": "KEHANG",
      "contributions": 23
    },
    {
      "login": "gauravmishra",
      "contributions": 18
    },
    {
      "login": "liangyaning33",
      "contributions": 16
    },
    {
      "login": "SahilJain314",
      "contributions": 14
    },
    {
      "login": "Marvin182",
      "contributions": 13
    },
    {
      "login": "voutcn",
      "contributions": 12
    },
    {
      "login": "iansimon",
      "contributions": 11
    },
    {
      "login": "jacobaustin123",
      "contributions": 10
    },
    {
      "login": "gshennvm",
      "contributions": 10
    },
    {
      "login": "afrozenator",
      "contributions": 9
    },
    {
      "login": "nconstant-google",
      "contributions": 7
    },
    {
      "login": "levskaya",
      "contributions": 6
    },
    {
      "login": "blester125",
      "contributions": 6
    }
  ],
  "file_tree_count": 435,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/build.yaml",
      "type": "blob"
    },
    {
      "path": "CONTRIBUTING.md",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "docs",
      "type": "tree"
    },
    {
      "path": "docs/_static",
      "type": "tree"
    },
    {
      "path": "docs/_static/t5x_theme.css",
      "type": "blob"
    },
    {
      "path": "docs/_templates",
      "type": "tree"
    }
  ],
  "issues_count": 0,
  "pulls_count": 1428,
  "recent_issues": [
    {
      "number": 1626,
      "title": "Disable pytype for errors revealed by JAX build refactor",
      "state": "closed"
    },
    {
      "number": 1625,
      "title": "Disable pytype for errors revealed by JAX build refactor",
      "state": "closed"
    },
    {
      "number": 1624,
      "title": "Fix for seed-> key in optax (PR#1240 was not backward compatible)",
      "state": "closed"
    },
    {
      "number": 1623,
      "title": "Disable pytype for errors revealed by JAX build refactor",
      "state": "closed"
    },
    {
      "number": 1622,
      "title": "Fix DeprecationWarnings regarding `optax.DifferentiallyPrivateAggregateState` and `optax.dpsgd`.",
      "state": "closed"
    }
  ],
  "recent_pulls": [
    {
      "number": 1626,
      "title": "Disable pytype for errors revealed by JAX build refactor",
      "state": "closed"
    },
    {
      "number": 1625,
      "title": "Disable pytype for errors revealed by JAX build refactor",
      "state": "closed"
    },
    {
      "number": 1624,
      "title": "Fix for seed-> key in optax (PR#1240 was not backward compatible)",
      "state": "closed"
    },
    {
      "number": 1623,
      "title": "Disable pytype for errors revealed by JAX build refactor",
      "state": "closed"
    },
    {
      "number": 1622,
      "title": "Fix DeprecationWarnings regarding `optax.DifferentiallyPrivateAggregateState` and `optax.dpsgd`.",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "087c7242b43a158bf560bcb44ec5e8824cd1dbc6",
      "author": "Jake VanderPlas",
      "date": "2025-06-04T16:07:02+00:00",
      "message": "Disable pytype for errors revealed by JAX build refactor"
    },
    {
      "sha": "92c5b467a5964d06c351c7eae4aa4bcd341c7ded",
      "author": "Jake VanderPlas",
      "date": "2025-06-04T12:42:48+00:00",
      "message": "Disable pytype for errors revealed by JAX build refactor"
    },
    {
      "sha": "c84e7a30d460162d440baf700d2c9138ddd3d174",
      "author": "T5X Team",
      "date": "2025-06-03T17:12:58+00:00",
      "message": "Fix for seed-> key in optax (PR#1240 was not backward compatible)"
    },
    {
      "sha": "fb11a7619749b1686ab857f31667aa3303766c7e",
      "author": "T5X Team",
      "date": "2025-05-02T01:25:10+00:00",
      "message": "Add cast to fix conversion to int64 when JAX's x64 mode is on."
    },
    {
      "sha": "53ce507849e7c6898f79a6323e81385942990a64",
      "author": "T5X Team",
      "date": "2025-05-01T09:07:15+00:00",
      "message": "Fix DeprecationWarnings regarding `optax.DifferentiallyPrivateAggregateState` and `optax.dpsgd`."
    },
    {
      "sha": "9fecf300a9315fbeb1e014ae5feae03e912a8e09",
      "author": "Hana Joo",
      "date": "2025-04-28T18:43:40+00:00",
      "message": "No public description"
    },
    {
      "sha": "509d35e084c92780ac7c993b74fa0792d845a0da",
      "author": "Rohan Jain",
      "date": "2025-04-24T03:06:46+00:00",
      "message": "Reverting to previous logic of checking for export dir only when cpu dir is not specified."
    },
    {
      "sha": "0ec3eb52747a43eab6464638e122025d79bae5a1",
      "author": "Rohan Jain",
      "date": "2025-04-22T18:17:27+00:00",
      "message": "Extending the T5X exporter library to support GPUs as well."
    },
    {
      "sha": "6ed100440ee03410ebfe2d156675c60427284280",
      "author": "Sergei Lebedev",
      "date": "2025-03-26T10:52:29+00:00",
      "message": "Handle `None` as a possible value of `output_vocabulary.eos_id`"
    },
    {
      "sha": "ad6a629aa61f7353998482055feee597b651093f",
      "author": "Adam Cogdell",
      "date": "2025-01-30T21:03:01+00:00",
      "message": "Allow users to save custom metadata with each checkpoint step."
    },
    {
      "sha": "a1867702a42f856e00187117aa8dc80290760f0c",
      "author": "Adam Roberts",
      "date": "2025-01-17T00:13:42+00:00",
      "message": "Merge pull request #1609 from nvjax-svc-0/patch/fix-default-vocab"
    },
    {
      "sha": "9cc0d6feff1164cbf4a30e8997eac8846d76613d",
      "author": "ashors1",
      "date": "2024-12-19T19:14:06+00:00",
      "message": "fix default output features"
    },
    {
      "sha": "5f03619b0c5ebb44ae6adde1a2d8eea1a4b55fe0",
      "author": "Matteo Hessel",
      "date": "2024-12-17T11:21:16+00:00",
      "message": "Remove longtime deprecated functions."
    },
    {
      "sha": "fb95318cad9d2086d5b26936afdec2c81d4e0b26",
      "author": "Niket Kumar Bhumihar",
      "date": "2024-11-25T16:14:47+00:00",
      "message": "Remove redundant reference to `ocp.tree.serialize_tree(...)` by removing dead code."
    },
    {
      "sha": "c723ab00d52bff5cac64d1fd29078617a02c724e",
      "author": "George Necula",
      "date": "2024-10-28T17:33:53+00:00",
      "message": "No public description"
    },
    {
      "sha": "b642f3050b345caa1682a4b4ba8cf7107a4f52d0",
      "author": "Anselm Levskaya",
      "date": "2024-10-21T18:57:09+00:00",
      "message": "Remove dependence on old flax PRNG compat mode."
    },
    {
      "sha": "0ff82541a8481f6ebda98aae72594e3aedec79b4",
      "author": "T5X Team",
      "date": "2024-10-07T20:42:05+00:00",
      "message": "Fix partial checkpoint loading caused by changes in jax's flatten_up_to behaviour."
    },
    {
      "sha": "9bb1e85f957e27be1ccb8cf28eb6e5981ef86ff8",
      "author": "George Necula",
      "date": "2024-10-07T15:39:13+00:00",
      "message": "Replace usage of jax.experimental.host_callback.call with jax.experimental.io_callback."
    },
    {
      "sha": "705247b743d26a33d0c058b41c72ad030e51891b",
      "author": "Niket Kumar Bhumihar",
      "date": "2024-09-12T17:34:49+00:00",
      "message": "Rename `is_supported_empty_aggregation_type` and `is_supported_aggregation_type` functions."
    },
    {
      "sha": "c2f0c9c6bccf9084f02b19174651e983fdbcd2fe",
      "author": "Colin Gaffney",
      "date": "2024-09-03T18:06:00+00:00",
      "message": "Remove external direct references to `pytree_checkpoint_handler`."
    }
  ],
  "readme_text": "# T5X\n\n*Go to [T5X ReadTheDocs Documentation Page](https://t5x.readthedocs.io/).*\n\nT5X is a modular, composable, research-friendly framework for high-performance,\nconfigurable, self-service training, evaluation, and inference of sequence\nmodels (starting with language) at many scales.\n\nIt is essentially a new and improved implementation of the\n[T5 codebase](https://github.com/google-research/text-to-text-transfer-transformer)\n(based on [Mesh TensorFlow](https://github.com/tensorflow/mesh)) in [JAX](https://github.com/google/jax) and [Flax](https://github.com/google/flax). To learn\nmore, see the [T5X Paper](https://arxiv.org/abs/2203.17189).\n\nBelow is a quick start guide for training models with TPUs on Google Cloud. For\nadditional tutorials and background, see the [complete documentation](docs/index.md).\n\n## Quickstart (Recommended)\n\nT5X can be run with [XManager](https://github.com/deepmind/xmanager) on\n[Vertex AI](https://cloud.google.com/vertex-ai). Vertex AI is a platform for\ntraining that creates TPU instances and runs code on the TPUs. Vertex AI will\nalso shut down the TPUs when the jobs terminate. This is signifcantly easier\nthan managing GCE VMs and TPU VM instances.\n\n1. Follow the pre-requisites and directions to install [XManager](https://github.com/deepmind/xmanager).\n\n2. Request TPU quota as required. GCP projects come with 8 cores by default,\nwhich is enough to run one training experiment on a single TPU host. If you want\nto run multi-host training or run multiple trials in parallel, you will need\nmore quota. Navigate to [Quotas](https://console.cloud.google.com/quotas).\n\n  The quota you want is:\n\n  * Service: `Vertex AI API`\n  * Dimensions (location): `us-central1`\n  * If you want to run single-host experiments:\n    * `Custom model training TPU V2 cores per region`\n    * `Custom model training TPU V3 cores per region`\n  * If you want to run multi-host experiments:\n    * `Custom model training TPU V2 pod cores per region`\n    * `Custom model training TPU V3 pod cores per region`\n\n  TIP: You won't be able to run single-host experiments with multi-host quota.\n  (i.e. you can't run `tpu_v2=8` using `TPU V2 pod`)\n\n\n3. Launch the xmanager script located at `t5x/scripts/xm_launch.py`.\n\nAs a running example, we use the WMT14 En-De translation which is described in\nmore detail in the Examples section below.\n\n```sh\nexport GOOGLE_CLOUD_BUCKET_NAME=...\nexport TFDS_DATA_DIR=gs://$GOOGLE_CLOUD_BUCKET_NAME/t5x/data\nexport MODEL_DIR=gs://$GOOGLE_CLOUD_BUCKET_NAME/t5x/$(date +%Y%m%d)\n\n# Pre-download dataset in multi-host experiments.\ntfds build wmt_t2t_translate --data_dir=$TFDS_DATA_DIR\n\ngit clone https://github.com/google-research/t5x\ncd ./t5x/\n\npython3 ./t5x/scripts/xm_launch.py \\\n  --gin_file=t5x/examples/t5/t5_1_1/examples/base_wmt_from_scratch.gin \\\n  --model_dir=$MODEL_DIR \\\n  --tfds_data_dir=$TFDS_DATA_DIR\n```\n\nCheck `gs://$GOOGLE_CLOUD_BUCKET_NAME/t5x/` for the output artifacts, which can\nbe read by TensorBoard.\n\n## GPU Usage\nNote: NVIDIA has released an updated version of this repository with H100 FP8 support and broad GPU performance improvements. Please visit the [NVIDIA Rosetta](https://github.com/NVIDIA/JAX-Toolbox/tree/main/rosetta/rosetta/projects/t5x) repository for more details and usage instructions.\n\nT5X can be run easily on GPUs either in single-node configurations or multi-node configurations with a SLURM+pyxis cluster. Further instructions at [t5x/contrib/gpu](https://github.com/google-research/t5x/blob/main/t5x/contrib/gpu/README.md). The `t5x/contrib/gpu/scripts_gpu` folder contains example scripts for pretraining T5X on [The Pile](https://pile.eleuther.ai/) and for finetuning on SQuAD and MNLI. These scripts and associated `gin` configurations also contain additional GPU optimizations for better throughput. More examples and instructions can be found in the [NVIDIA Rosetta](https://github.com/NVIDIA/JAX-Toolbox/tree/main/rosetta/rosetta/projects/t5x) repository maintained by NVIDIA with H100 FP8 support and broad GPU performance improvements.\n\n\n## Installation\n\nNote that all the commands in this document should be run in the commandline of\nthe TPU VM instance unless otherwise stated.\n\n1.  Follow the\n    [instructions](https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm#install_the_google_cloud_sdk)\n    to set up a Google Cloud Platform (GCP) account and enable the Cloud TPU\n    API.\n\n    **Note:** T5X also works with GPU, please follow instructions in [t5x/contrib/gpu](https://github.com/google-research/t5x/blob/main/t5x/contrib/gpu/README.md) if you'd like to use GPU version.\n\n2.  Create a\n    [Cloud TPU VM instance](https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms)\n    following\n    [this instruction](https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm#create-vm).\n    We recommend that you develop your workflow in a single v3-8 TPU (i.e.,\n    `--accelerator-type=v3-8`) and scale up to pod slices once the pipeline is\n    ready. In this README, we focus on using a single v3-8 TPU. See\n    [here](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm) to\n    learn more about TPU architectures.\n\n3.  With Cloud TPU VMs, you ssh directly into the host machine of the TPU VM.\n    You can install packages, run your code run, etc. in the host machine. Once\n    the TPU instance is created, ssh into it with\n\n    ```sh\n    gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --zone=${ZONE}\n    ```\n\n    where `TPU_NAME` and `ZONE` are the name and the zone used in step 2.\n\n4.  Install T5X and the dependencies.\n\n    ```sh\n    git clone --branch=main https://github.com/google-research/t5x\n    cd t5x\n\n    python3 -m pip install -e '.[tpu]' -f \\\n      https://storage.googleapis.com/jax-releases/libtpu_releases.html\n\n    ```\n\n\n5.  Create Google Cloud Storage (GCS) bucket to store the dataset and model\n    checkpoints. To create a GCS bucket, see these\n    [instructions](https://cloud.google.com/storage/docs/creating-buckets).\n\n6.  (optional) If you prefer working with Jupyter/Colab style environment\n    you can setup a custom Colab runtime by following steps from\n    [t5x/notebooks](https://github.com/google-research/t5x/blob/main/t5x/notebooks/README.md).\n\n## Example: English to German translation\n\nAs a running example, we use the WMT14 En-De translation. The raw dataset is\navailable in TensorFlow Datasets as\n[\"wmt_t2t_translate\"](https://www.tensorflow.org/datasets/catalog/wmt_t2t_translate).\n\nT5 casts the translation task such as the following\n\n```py\n{'en': 'That is good.', 'de': 'Das ist gut.'}\n```\n\nto the form called \"text-to-text\":\n\n```py\n{'inputs': 'translate English to German: That is good.', 'targets': 'Das ist gut.'}\n```\n\nThis formulation allows many different classes of language tasks to be expressed\nin a uniform manner and a single encoder-decoder architecture can handle them\nwithout any task-specific parameters. For more detail, refer to the [T5 paper\n(Raffel et al. 2019)][t5_paper].\n\nFor a scalable data pipeline and an evaluation framework, we use\n[`SeqIO`](https://github.com/google/seqio), which was factored out of the [T5\nlibrary][t5_github]. A `seqio.Task` packages together the raw dataset, vocabulary,\npreprocessing such as tokenization and evaluation metrics such as\n[BLEU](https://aclanthology.org/P02-1040.pdf) and provides a\n[`tf.data`](https://www.tensorflow.org/guide/data) instance.\n\n[The T5 library][t5_github] provides a number of `seqio.Task`s that were used in the\n[T5 paper][t5_paper]. In this example, we use [wmt_t2t_ende_v003](https://github.com/google-research/text-to-text-transfer-transformer/blob/d81c0bab2a41b4d5dfbe4971de32f7d67df65f31/t5/data/tasks.py#L212).\n\nBefore training or fine-tuning you need to download [\"wmt_t2t_translate\"]\n(https://www.tensorflow.org/datasets/catalog/wmt_t2t_translate) dataset first.\n\n```sh\n# Data dir to save the processed dataset in \"gs://data_dir\" format.\nTFDS_DATA_DIR=\"...\"\n\n# Make sure that dataset package is up-to-date.\npython3 -m pip install --upgrade tfds-nightly\n\n# Pre-download dataset.\ntfds build wmt_t2t_translate ${TFDS_DATA_DIR}\n```\n\n### Training\n\nTo run a training job, we use the `t5x/train.py` script.\n\n```sh\n# Model dir to save logs, ckpts, etc. in \"gs://model_dir\" format.\nMODEL_DIR=\"...\"\nT5X_DIR=\"...\"  # directory where the T5X repo is cloned.\nTFDS_DATA_DIR=\"...\"\n\npython3 ${T5X_DIR}/t5x/train.py \\\n  --gin_file=\"t5x/examples/t5/t5_1_1/examples/base_wmt_from_scratch.gin\" \\\n  --gin.MODEL_DIR=\\\"${MODEL_DIR}\\\" \\\n  --tfds_data_dir=${TFDS_DATA_DIR}\n```\n\nThe configuration for this training run is defined in the Gin file\n[base_wmt_from_scratch.gin](t5x/examples/t5/t5_1_1/examples/base_wmt_from_scratch.gin).\n[Gin-config](https://github.com/google/gin-config) is a library to handle\nconfigurations based on dependency injection. Among many benefits, Gin allows\nusers to pass custom components such as a custom model to the T5X library\nwithout having to modify the core library. The [custom\ncomponents](#custom-components) section shows how this is done.\n\nWhile the core library is independent of Gin, it is central to the examples we\nprovide. Therefore, we provide a short [introduction][gin-primer] to Gin in the\ncontext of T5X.  All the configurations are written to a file \"config.gin\" in\n`MODEL_DIR`. This makes debugging as well as reproducing the experiment much\neasier.\n\nIn addition to the `config.json`, `model-info.txt` file summarizes the model\nparameters (shape, names of the axes, partitioning info) as well as the\noptimizer states.\n\n\n\n#### TensorBoard\n\nTo monitor the training in [TensorBoard](https://www.tensorflow.org/tensorboard), it is much easier (due to\nauthentification issues) to launch the TensorBoard on your own machine and _not_ in\nthe TPU VM. So in the commandline where you ssh'ed into the TPU VM, launch the\nTensorBoard with the `logdir` pointing to the `MODEL_DIR`.\n\n```sh\n# NB: run this on your machine not TPU VM!\nMODEL_DIR=\"...\"  # Copy from the TPU VM.\ntensorboard --logdir=${MODEL_DIR}\n```\n\nOr you can launch the TensorBoard inside a Colab. In a Colab cell, run\n\n```python\nfrom google.colab import auth\nauth.authenticate_user()\n```\n\nto authorize the Colab to access the GCS bucket and launch the TensorBoard.\n\n```python\n%load_ext tensorboard\nmodel_dir = \"...\"  # Copy from the TPU VM.\n%tensorboard --logdir=model_dir\n```\n\n\n### Fine-tuning\n\nWe can leverage the benefits of self-supervised pre-training by initializing\nfrom one of our pre-trained models. Here we use the T5.1.1 Base checkpoint.\n\n```sh\n# Model dir to save logs, ckpts, etc. in \"gs://model_dir\" format.\nMODEL_DIR=\"...\"\n\n# Data dir to save the processed dataset in \"gs://data_dir\" format.\nTFDS_DATA_DIR=\"...\"\nT5X_DIR=\"...\"  # directory where the T5X repo is cloned.\n\npython3 ${T5X_DIR}/t5x/train.py \\\n  --gin_file=\"t5x/examples/t5/t5_1_1/examples/base_wmt_finetune.gin\" \\\n  --gin.MODEL_DIR=\\\"${MODEL_DIR}\\\" \\\n  --tfds_data_dir=${TFDS_DATA_DIR}\n```\n\n**Note:** when supplying a string, dict, list, tuple value, or a bash variable\nvia a flag, you must put it in quotes. In the case of strings, it requires\nescaped quotes (`\\\"<string>\\\"`). For example:\n`--gin.utils.DatasetConfig.split=\\\"validation\\\"` or\n`--gin.MODEL_DIR=\\\"${MODEL_DIR}\\\"`.\n\nGin makes it easy to change a number of configurations. For example, you can\nchange the `partitioning.PjitPartitioner.num_partitions` (overriding\nthe value in\n[base_wmt_from_scratch.gin](t5x/examples/t5/t5_1_1/examples/base_wmt_from_scratch.gin))\nto chanage the parallelism strategy and pass it as a commandline arg.\n\n```sh\n--gin.partitioning.PjitPartitioner.num_partitions=8\n```\n\n### Evaluation\n\nTo run the offline (i.e. without training) evaluation, you can use `t5x/eval.py`\nscript.\n\n```sh\nEVAL_OUTPUT_DIR=\"...\"  # directory to write eval output\nT5X_DIR=\"...\"  # directory where the t5x is cloned, e.g., ${HOME}\"/t5x\".\nTFDS_DATA_DIR=\"...\"\nCHECKPOINT_PATH=\"...\"\n\npython3 ${T5X_DIR}/t5x/eval.py \\\n  --gin_file=\"t5x/examples/t5/t5_1_1/examples/base_wmt_eval.gin\" \\\n  --gin.CHECKPOINT_PATH=\\\"${CHECKPOINT_PATH}\\\" \\\n  --gin.EVAL_OUTPUT_DIR=\\\"${EVAL_OUTPUT_DIR}\\\" \\\n  --tfds_data_dir=${TFDS_DATA_DIR}\n```\n\n\n### Inference\n\nTo run inference, you can use `t5x/infer.py` script. Here we use the same\n`seqio.Task`, but for inference we do not use the targets features other than\nlogging them alongside the prediction in a JSON file.\n\n```sh\nINFER_OUTPUT_DIR=\"...\"  # directory to write infer output\nT5X_DIR=\"...\"  # directory where the t5x is cloned, e.g., ${HOME}\"/t5x\".\nTFDS_DATA_DIR=\"...\"\nCHECKPOINT_PATH=\"...\"\n\npython3 ${T5X_DIR}/t5x/infer.py \\\n  --gin_file=\"t5x/examples/t5/t5_1_1/examples/base_wmt_infer.gin\" \\\n  --gin.CHECKPOINT_PATH=\\\"${CHECKPOINT_PATH}\\\" \\\n  --gin.INFER_OUTPUT_DIR=\\\"${INFER_OUTPUT_DIR}\\\" \\\n  --tfds_data_dir=${TFDS_DATA_DIR}\n```\n\n### Exporting as TensorFlow Saved Model\n\nPretrained model can be exported as TensorFlow Saved Model, and deployed\nto Vertex AI Prediction service using [Optimized TensorFlow Runtime]\n(https://cloud.google.com/vertex-ai/docs/predictions/optimized-tensorflow-runtime).\nPlease note that exported model won't work on OSS based\n[TensorFlow Model Server](https://github.com/tensorflow/serving).\n\n```sh\nT5X_DIR=\"...\"  # directory where the t5x is cloned, e.g., ${HOME}\"/t5x\".\nCHECKPOINT_PATH=\"...\"\n\nBATCH_SIZE=None\nBEAM_SIZE=1\n\n# Use 'bfloat16' if you plan to run exported model on NVIDIA A100 or newer GPUs,\n# for other GPUs use 'float32'.\nACTIVATION_DTYPE=bfloat16\n\n# Version numbers must be numeric. We generate one based on datetime.\nVERSION=$(date +%Y%m%d%H%M%S)\n\nNAME=t5x_base_${ACTIVATION_DTYPE}  # Model name.\n\n# Path to export model to. Note that export script is going to add _cpu suffix\n# after model name.\nOUTPUT=${CHECKPOINT_PATH}/saved_model.${NAME}/${VERSION}\n\ndeclare -a ARGS=(\n--gin_file=t5x/examples/t5/t5_1_1/base.gin\n--gin_file=t5x/t5x/configs/runs/export.gin\n--gin.TASK_FEATURE_LENGTHS=\"{'inputs': 256, 'targets': 256}\"\n--gin.CHECKPOINT_PATH=\\\"${CHECKPOINT_PATH}\\\"\n--gin.MODEL_NAME=\\\"/ml/${USER}/t5x_base\\\"\n--gin.MODEL_OUTPUT_DIR=\\\"${OUTPUT}\\\"\n--gin.BEAM_SIZE=${BEAM_SIZE}\n--gin.BATCH_SIZE=${BATCH_SIZE}\n--gin.export_lib.save.partitioner=None\n--gin.export_lib.save.warmup_examples=\"['hello world']\"\n--gin.export_lib.ExportableModule.use_batch_function=False\n--gin.export_lib.ExportableModule.use_gpu=False\n--gin.export_lib.ExportableModule.jit_compile=False\n--gin.ACTIVATION_DTYPE=\\\"${ACTIVATION_DTYPE}\\\"\n--gin.network.T5Config.dtype=\\\"${ACTIVATION_DTYPE}\\\"\n--gin.utils.RestoreCheckpointConfig.dtype=\\\"${ACTIVATION_DTYPE}\\\"\n--gin.DROPOUT_RATE=0.0\n)\n\n(python3 ${T5X_DIR}/t5x/export.py \"${ARGS[@]}\")\n```\n\nFor detailed arguments definition refer to [export.gin]\n(t5x/configs/runs/export.gin).\n\nYou can run XL and smaller models on NVIDIA A100 40GB, and XXL models on\nNVIDIA A100 80GB.\n\n## Custom components\n\n[The translation example](#example-english-to-german-translation) uses the\nencoder-decoder model that T5X provides as well as the dataset from the T5\nlibrary. This section shows how you can use your own dataset and a model and\npass via Gin.\n\n### Example: custom dataset in a user directory\n\nFor this example, we have the following directory structure with\n`${HOME}/dir1/user_dir` representing a user directory with custom components.\n\n```\n${HOME}\n\u2514\u2500\u2500 dir1\n \u00a0\u00a0 \u2514\u2500\u2500 user_dir\n \u00a0\u00a0     \u251c\u2500\u2500 t5_1_1_base_de_en.gin\n \u00a0\u00a0     \u2514\u2500\u2500 tasks.py\n```\n\nAs an example, let's define a new dataset. Here we use the same Translation\ndataset but we define the translation task in the opposite direction, i.e.,\nGerman to English intead of English to German. We define this task in `tasks.py`\n\n```py\n# ${HOME}/dir1/user_dir/tasks.py\n\nimport functools\nimport seqio\nimport tensorflow_datasets as tfds\nfrom t5.evaluation import metrics\nfrom t5.data import preprocessors\n\nvocabulary = seqio.SentencePieceVocabulary(\n    'gs://t5-data/vocabs/cc_all.32000/sentencepiece.model', extra_ids=100)\noutput_features = {\n    'inputs': seqio.Feature(vocabulary=vocabulary),\n    'targets': seqio.Feature(vocabulary=vocabulary)\n}\n\nseqio.TaskRegistry.add(\n    'wmt_t2t_de_en_v003',\n    source=seqio.TfdsDataSource(tfds_name='wmt_t2t_translate/de-en:1.0.0'),\n    preprocessors=[\n        functools.partial(\n            preprocessors.translate,\n            source_language='de', target_language='en'),\n        seqio.preprocessors.tokenize,\n        seqio.CacheDatasetPlaceholder(),\n        seqio.preprocessors.append_eos_after_trim,\n    ],\n    metric_fns=[metrics.bleu],\n    output_features=output_features)\n```\n\nIn the Gin file, most of the settings are equivalent to those used in the\n[En->De example](#example-english-to-german-translation). So we include the Gin\nfile from that example. To use \"wmt_t2t_de_en_v003\" task we just defined, we\nneed to import the task module \"tasks.py\". Note that we use a relative path\ndefined with respect to the user directory. This will be specified as a\nflag.\n\n```py\n# ${HOME}/dir1/user_dir/t5_1_1_base_de_en.gin\nfrom __gin__ import dynamic_registration\nimport tasks  # This imports the task defined in dir1/user_dir/tasks.py.\n\ninclude \"t5x-tmp/t5x/examples/t5/t5_1_1/examples/base_wmt_from_scratch.gin\"\nMIXTURE_OR_TASK_NAME = \"wmt_t2t_de_en_v003\"\n```\n\nFinally, we launch training passing the user directory as a flag\n`gin_search_paths` such that the Gin file and python modules can be specified\nwith relative paths.\n\n```sh\nPROJECT_DIR=${HOME}\"/dir1/user_dir\"\nT5X_DIR=\"...\"  # directory where the t5x is cloned.\nTFDS_DATA_DIR=\"...\"\nMODEL_DIR=\"...\"\nexport PYTHONPATH=${PROJECT_DIR}\n\npython3 ${T5X_DIR}/t5x/train.py \\\n  --gin_search_paths=${PROJECT_DIR} \\\n  --gin_file=\"t5_1_1_base_de_en.gin\" \\\n  --gin.MODEL_DIR=\\\"${MODEL_DIR}\\\" \\\n  --tfds_data_dir=${TFDS_DATA_DIR}\n```\n\n## Checkpoints\n\n### Native Checkpoints\n\nWe have released the checkpoints of many of the original T5 models and their\nvariants a native T5X format for maximal efficiency.\nSee the [complete list](https://github.com/google-research/t5x/blob/main/docs/models.md) including the\nmatching Gin configuration files.\n\nThese are converted from the public [Mesh TensorFlow\ncheckpoints](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511)\n.\n\n\n### Compatibility with the Mesh TensorFlow checkpoints\nThe Mesh TensorFlow checkpoints trained using the [T5 library][t5_github] can be\ndirectly loaded into T5X. For example, we can rerun the fine-tuning example\ninitializing from the MTF checkpoint by changing the `INIT_CHECKPOINT` Gin\nmacro.\n\n```sh\n# Model dir to save logs, ckpts, etc. in \"gs://model_dir\" format.\nMODEL_DIR=\"...\"\n\n# Data dir to save the processed dataset in \"gs://data_dir\" format.\nTFDS_DATA_DIR=\"...\"\nT5X_DIR=\"...\"  # directory where the T5X repo is cloned.\n\npython3 ${T5X_DIR}/t5x/train.py \\\n  --gin_file=\"t5x/examples/t5/t5_1_1/examples/base_wmt19_ende_train.gin\" \\\n  --gin.MODEL_DIR=\\\"${MODEL_DIR}\\\" \\\n  --gin.MIXTURE_OR_TASK_NAME=\\\"wmt_t2t_ende_v003\\\" \\\n  --gin.INIT_CHECKPOINT=\\\"gs://t5-data/pretrained_models/t5.1.1.base/model.ckpt-1000000\\\" \\\n  --tfds_data_dir=${TFDS_DATA_DIR}\n```\n\nNote that restoring directly from the Mesh TensorFlow checkpoints can be\ninefficient if heavy model parallelism is used for large models. This is\nbecause each host loads the entire copy of the model first and then keep only\nthe relevant slices dictated by the model parallelism specification. If you have\nMesh TensorFlow checkpoints that you run often, we recommend converting the\ncheckpoints to T5X native format using the\n[convert_tf_checkpoint script](t5x/scripts/convert_tf_checkpoint.py).\n\n\n## Citing T5X\nPlease use the following bibtex entry to cite T5X.\n\n```\n@article{roberts2022t5x,\n  url = {https://arxiv.org/abs/2203.17189},\n  author = {Roberts, Adam and Chung, Hyung Won and Levskaya, Anselm and Mishra, Gaurav and Bradbury, James and Andor, Daniel and Narang, Sharan and Lester, Brian and Gaffney, Colin and Mohiuddin, Afroz and Hawthorne, Curtis and Lewkowycz, Aitor and Salcianu, Alex and van Zee, Marc and Austin, Jacob and Goodman, Sebastian and Soares, Livio Baldini and Hu, Haitang and Tsvyashchenko, Sasha and Chowdhery, Aakanksha and Bastings, Jasmijn and Bulian, Jannis and Garcia, Xavier and Ni, Jianmo and Chen, Andrew and Kenealy, Kathleen and Clark, Jonathan H. and Lee, Stephan and Garrette, Dan and Lee-Thorp, James and Raffel, Colin and Shazeer, Noam and Ritter, Marvin and Bosma, Maarten and Passos, Alexandre and Maitin-Shepard, Jeremy and Fiedel, Noah and Omernick, Mark and Saeta, Brennan and Sepassi, Ryan and Spiridonov, Alexander and Newlan, Joshua and Gesmundo, Andrea},\n  title = {Scaling Up Models and Data with $\\texttt{t5x}$ and $\\texttt{seqio}$},\n  journal={arXiv preprint arXiv:2203.17189},\n  year = {2022},\n}\n```\n\n\n## Note\nThis is not an officially supported Google product\n\n[t5_paper]: https://arxiv.org/abs/1910.10683\n[t5_github]: https://github.com/google-research/text-to-text-transfer-transformer\n[gin-primer]: docs/usage/gin.md\n",
  "external_links_in_readme": [
    "https://github.com/google-research/text-to-text-transfer-transformer",
    "https://github.com/google/seqio",
    "https://cloud.google.com/tpu/docs/system-architecture-tpu-vm",
    "https://github.com/google/jax",
    "https://github.com/NVIDIA/JAX-Toolbox/tree/main/rosetta/rosetta/projects/t5x",
    "https://github.com/google-research/t5x",
    "https://github.com/google-research/t5x/blob/main/docs/models.md",
    "https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms",
    "https://cloud.google.com/storage/docs/creating-buckets",
    "https://github.com/tensorflow/mesh",
    "https://www.tensorflow.org/tensorboard",
    "https://github.com/google/gin-config",
    "https://github.com/tensorflow/serving",
    "https://storage.googleapis.com/jax-releases/libtpu_releases.html",
    "https://www.tensorflow.org/guide/data",
    "https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm#create-vm",
    "https://github.com/deepmind/xmanager",
    "https://arxiv.org/abs/2203.17189},",
    "https://console.cloud.google.com/quotas",
    "https://github.com/google/flax",
    "https://aclanthology.org/P02-1040.pdf",
    "https://pile.eleuther.ai/",
    "https://github.com/google-research/t5x/blob/main/t5x/notebooks/README.md",
    "https://arxiv.org/abs/2203.17189",
    "https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511",
    "https://github.com/google-research/text-to-text-transfer-transformer/blob/d81c0bab2a41b4d5dfbe4971de32f7d67df65f31/t5/data/tasks.py#L212",
    "https://github.com/google-research/t5x/blob/main/t5x/contrib/gpu/README.md",
    "https://t5x.readthedocs.io/",
    "https://arxiv.org/abs/1910.10683",
    "https://cloud.google.com/vertex-ai/docs/predictions/optimized-tensorflow-runtime",
    "https://cloud.google.com/vertex-ai",
    "https://www.tensorflow.org/datasets/catalog/wmt_t2t_translate",
    "https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm#install_the_google_cloud_sdk"
  ]
}
```

</details>


---

## Repository 2: google-research/t5x

# GitHub Repository Data

**Repository:** [google-research/t5x](https://github.com/google-research/t5x)

## Basic Information

- **Description:** None
- **Created:** 2021-11-01T19:37:39+00:00
- **Last Updated:** 2025-06-18T22:09:59+00:00
- **Last Pushed:** 2025-06-04T16:08:08+00:00
- **Default Branch:** main
- **Size:** 9724 KB

## Statistics

- **Stars:** 2,834
- **Forks:** 318
- **Watchers:** 2,834
- **Open Issues:** 182
- **Total Issues:** 0
- **Pull Requests:** 1,428

## License

- **Type:** Apache License 2.0
- **SPDX ID:** Apache-2.0
- **URL:** [License](https://github.com/google-research/t5x/blob/main/LICENSE)

## Languages

- **Python:** 1,601,669 bytes
- **Jupyter Notebook:** 161,734 bytes
- **Shell:** 20,230 bytes
- **Dockerfile:** 466 bytes

## Top Contributors

1. **adarob** - 167 contributions
2. **cpgaffney1** - 134 contributions
3. **hwchung27** - 50 contributions
4. **kkenealy** - 30 contributions
5. **texasmichelle** - 25 contributions
6. **hawkinsp** - 24 contributions
7. **yashk2810** - 24 contributions
8. **KEHANG** - 23 contributions
9. **gauravmishra** - 18 contributions
10. **liangyaning33** - 16 contributions

## File Structure (Sample of 10 files)

Total files: 435

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/build.yaml` (blob)
- `CONTRIBUTING.md` (blob)
- `LICENSE` (blob)
- `README.md` (blob)
- `docs` (tree)
- `docs/_static` (tree)
- `docs/_static/t5x_theme.css` (blob)
- `docs/_templates` (tree)

## Recent Issues

- ðŸ”´ **#1626** Disable pytype for errors revealed by JAX build refactor (closed)
- ðŸ”´ **#1625** Disable pytype for errors revealed by JAX build refactor (closed)
- ðŸ”´ **#1624** Fix for seed-> key in optax (PR#1240 was not backward compatible) (closed)
- ðŸ”´ **#1623** Disable pytype for errors revealed by JAX build refactor (closed)
- ðŸ”´ **#1622** Fix DeprecationWarnings regarding `optax.DifferentiallyPrivateAggregateState` and `optax.dpsgd`. (closed)

## Recent Pull Requests

- ðŸ”´ **#1626** Disable pytype for errors revealed by JAX build refactor (closed)
- ðŸ”´ **#1625** Disable pytype for errors revealed by JAX build refactor (closed)
- ðŸ”´ **#1624** Fix for seed-> key in optax (PR#1240 was not backward compatible) (closed)
- ðŸ”´ **#1623** Disable pytype for errors revealed by JAX build refactor (closed)
- ðŸ”´ **#1622** Fix DeprecationWarnings regarding `optax.DifferentiallyPrivateAggregateState` and `optax.dpsgd`. (closed)

## Recent Commits

- **087c7242** Disable pytype for errors revealed by JAX build refactor - Jake VanderPlas (2025-06-04T16:07:02+00:00)
- **92c5b467** Disable pytype for errors revealed by JAX build refactor - Jake VanderPlas (2025-06-04T12:42:48+00:00)
- **c84e7a30** Fix for seed-> key in optax (PR#1240 was not backward compatible) - T5X Team (2025-06-03T17:12:58+00:00)
- **fb11a761** Add cast to fix conversion to int64 when JAX's x64 mode is on. - T5X Team (2025-05-02T01:25:10+00:00)
- **53ce5078** Fix DeprecationWarnings regarding `optax.DifferentiallyPrivateAggregateState` and `optax.dpsgd`. - T5X Team (2025-05-01T09:07:15+00:00)
- **9fecf300** No public description - Hana Joo (2025-04-28T18:43:40+00:00)
- **509d35e0** Reverting to previous logic of checking for export dir only when cpu dir is not specified. - Rohan Jain (2025-04-24T03:06:46+00:00)
- **0ec3eb52** Extending the T5X exporter library to support GPUs as well. - Rohan Jain (2025-04-22T18:17:27+00:00)
- **6ed10044** Handle `None` as a possible value of `output_vocabulary.eos_id` - Sergei Lebedev (2025-03-26T10:52:29+00:00)
- **ad6a629a** Allow users to save custom metadata with each checkpoint step. - Adam Cogdell (2025-01-30T21:03:01+00:00)

## External Links Found in README

- https://github.com/google-research/text-to-text-transfer-transformer
- https://github.com/google/seqio
- https://cloud.google.com/tpu/docs/system-architecture-tpu-vm
- https://github.com/google/jax
- https://github.com/NVIDIA/JAX-Toolbox/tree/main/rosetta/rosetta/projects/t5x
- https://github.com/google-research/t5x
- https://github.com/google-research/t5x/blob/main/docs/models.md
- https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms
- https://cloud.google.com/storage/docs/creating-buckets
- https://github.com/tensorflow/mesh
- https://www.tensorflow.org/tensorboard
- https://github.com/google/gin-config
- https://github.com/tensorflow/serving
- https://storage.googleapis.com/jax-releases/libtpu_releases.html
- https://www.tensorflow.org/guide/data
- https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm#create-vm
- https://github.com/deepmind/xmanager
- https://arxiv.org/abs/2203.17189},
- https://console.cloud.google.com/quotas
- https://github.com/google/flax

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 423593369,
  "name": "t5x",
  "full_name": "google-research/t5x",
  "description": null,
  "html_url": "https://github.com/google-research/t5x",
  "clone_url": "https://github.com/google-research/t5x.git",
  "ssh_url": "git@github.com:google-research/t5x.git",
  "homepage": null,
  "topics": [],
  "default_branch": "main",
  "created_at": "2021-11-01T19:37:39+00:00",
  "updated_at": "2025-06-18T22:09:59+00:00",
  "pushed_at": "2025-06-04T16:08:08+00:00",
  "size_kb": 9724,
  "watchers_count": 2834,
  "stargazers_count": 2834,
  "forks_count": 318,
  "open_issues_count": 182,
  "license": {
    "key": "apache-2.0",
    "name": "Apache License 2.0",
    "spdx_id": "Apache-2.0",
    "url": "https://github.com/google-research/t5x/blob/main/LICENSE"
  },
  "languages": {
    "Python": 1601669,
    "Jupyter Notebook": 161734,
    "Shell": 20230,
    "Dockerfile": 466
  },
  "top_contributors": [
    {
      "login": "adarob",
      "contributions": 167
    },
    {
      "login": "cpgaffney1",
      "contributions": 134
    },
    {
      "login": "hwchung27",
      "contributions": 50
    },
    {
      "login": "kkenealy",
      "contributions": 30
    },
    {
      "login": "texasmichelle",
      "contributions": 25
    },
    {
      "login": "hawkinsp",
      "contributions": 24
    },
    {
      "login": "yashk2810",
      "contributions": 24
    },
    {
      "login": "KEHANG",
      "contributions": 23
    },
    {
      "login": "gauravmishra",
      "contributions": 18
    },
    {
      "login": "liangyaning33",
      "contributions": 16
    },
    {
      "login": "SahilJain314",
      "contributions": 14
    },
    {
      "login": "Marvin182",
      "contributions": 13
    },
    {
      "login": "voutcn",
      "contributions": 12
    },
    {
      "login": "iansimon",
      "contributions": 11
    },
    {
      "login": "jacobaustin123",
      "contributions": 10
    },
    {
      "login": "gshennvm",
      "contributions": 10
    },
    {
      "login": "afrozenator",
      "contributions": 9
    },
    {
      "login": "nconstant-google",
      "contributions": 7
    },
    {
      "login": "levskaya",
      "contributions": 6
    },
    {
      "login": "blester125",
      "contributions": 6
    }
  ],
  "file_tree_count": 435,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/build.yaml",
      "type": "blob"
    },
    {
      "path": "CONTRIBUTING.md",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "docs",
      "type": "tree"
    },
    {
      "path": "docs/_static",
      "type": "tree"
    },
    {
      "path": "docs/_static/t5x_theme.css",
      "type": "blob"
    },
    {
      "path": "docs/_templates",
      "type": "tree"
    }
  ],
  "issues_count": 0,
  "pulls_count": 1428,
  "recent_issues": [
    {
      "number": 1626,
      "title": "Disable pytype for errors revealed by JAX build refactor",
      "state": "closed"
    },
    {
      "number": 1625,
      "title": "Disable pytype for errors revealed by JAX build refactor",
      "state": "closed"
    },
    {
      "number": 1624,
      "title": "Fix for seed-> key in optax (PR#1240 was not backward compatible)",
      "state": "closed"
    },
    {
      "number": 1623,
      "title": "Disable pytype for errors revealed by JAX build refactor",
      "state": "closed"
    },
    {
      "number": 1622,
      "title": "Fix DeprecationWarnings regarding `optax.DifferentiallyPrivateAggregateState` and `optax.dpsgd`.",
      "state": "closed"
    }
  ],
  "recent_pulls": [
    {
      "number": 1626,
      "title": "Disable pytype for errors revealed by JAX build refactor",
      "state": "closed"
    },
    {
      "number": 1625,
      "title": "Disable pytype for errors revealed by JAX build refactor",
      "state": "closed"
    },
    {
      "number": 1624,
      "title": "Fix for seed-> key in optax (PR#1240 was not backward compatible)",
      "state": "closed"
    },
    {
      "number": 1623,
      "title": "Disable pytype for errors revealed by JAX build refactor",
      "state": "closed"
    },
    {
      "number": 1622,
      "title": "Fix DeprecationWarnings regarding `optax.DifferentiallyPrivateAggregateState` and `optax.dpsgd`.",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "087c7242b43a158bf560bcb44ec5e8824cd1dbc6",
      "author": "Jake VanderPlas",
      "date": "2025-06-04T16:07:02+00:00",
      "message": "Disable pytype for errors revealed by JAX build refactor"
    },
    {
      "sha": "92c5b467a5964d06c351c7eae4aa4bcd341c7ded",
      "author": "Jake VanderPlas",
      "date": "2025-06-04T12:42:48+00:00",
      "message": "Disable pytype for errors revealed by JAX build refactor"
    },
    {
      "sha": "c84e7a30d460162d440baf700d2c9138ddd3d174",
      "author": "T5X Team",
      "date": "2025-06-03T17:12:58+00:00",
      "message": "Fix for seed-> key in optax (PR#1240 was not backward compatible)"
    },
    {
      "sha": "fb11a7619749b1686ab857f31667aa3303766c7e",
      "author": "T5X Team",
      "date": "2025-05-02T01:25:10+00:00",
      "message": "Add cast to fix conversion to int64 when JAX's x64 mode is on."
    },
    {
      "sha": "53ce507849e7c6898f79a6323e81385942990a64",
      "author": "T5X Team",
      "date": "2025-05-01T09:07:15+00:00",
      "message": "Fix DeprecationWarnings regarding `optax.DifferentiallyPrivateAggregateState` and `optax.dpsgd`."
    },
    {
      "sha": "9fecf300a9315fbeb1e014ae5feae03e912a8e09",
      "author": "Hana Joo",
      "date": "2025-04-28T18:43:40+00:00",
      "message": "No public description"
    },
    {
      "sha": "509d35e084c92780ac7c993b74fa0792d845a0da",
      "author": "Rohan Jain",
      "date": "2025-04-24T03:06:46+00:00",
      "message": "Reverting to previous logic of checking for export dir only when cpu dir is not specified."
    },
    {
      "sha": "0ec3eb52747a43eab6464638e122025d79bae5a1",
      "author": "Rohan Jain",
      "date": "2025-04-22T18:17:27+00:00",
      "message": "Extending the T5X exporter library to support GPUs as well."
    },
    {
      "sha": "6ed100440ee03410ebfe2d156675c60427284280",
      "author": "Sergei Lebedev",
      "date": "2025-03-26T10:52:29+00:00",
      "message": "Handle `None` as a possible value of `output_vocabulary.eos_id`"
    },
    {
      "sha": "ad6a629aa61f7353998482055feee597b651093f",
      "author": "Adam Cogdell",
      "date": "2025-01-30T21:03:01+00:00",
      "message": "Allow users to save custom metadata with each checkpoint step."
    },
    {
      "sha": "a1867702a42f856e00187117aa8dc80290760f0c",
      "author": "Adam Roberts",
      "date": "2025-01-17T00:13:42+00:00",
      "message": "Merge pull request #1609 from nvjax-svc-0/patch/fix-default-vocab"
    },
    {
      "sha": "9cc0d6feff1164cbf4a30e8997eac8846d76613d",
      "author": "ashors1",
      "date": "2024-12-19T19:14:06+00:00",
      "message": "fix default output features"
    },
    {
      "sha": "5f03619b0c5ebb44ae6adde1a2d8eea1a4b55fe0",
      "author": "Matteo Hessel",
      "date": "2024-12-17T11:21:16+00:00",
      "message": "Remove longtime deprecated functions."
    },
    {
      "sha": "fb95318cad9d2086d5b26936afdec2c81d4e0b26",
      "author": "Niket Kumar Bhumihar",
      "date": "2024-11-25T16:14:47+00:00",
      "message": "Remove redundant reference to `ocp.tree.serialize_tree(...)` by removing dead code."
    },
    {
      "sha": "c723ab00d52bff5cac64d1fd29078617a02c724e",
      "author": "George Necula",
      "date": "2024-10-28T17:33:53+00:00",
      "message": "No public description"
    },
    {
      "sha": "b642f3050b345caa1682a4b4ba8cf7107a4f52d0",
      "author": "Anselm Levskaya",
      "date": "2024-10-21T18:57:09+00:00",
      "message": "Remove dependence on old flax PRNG compat mode."
    },
    {
      "sha": "0ff82541a8481f6ebda98aae72594e3aedec79b4",
      "author": "T5X Team",
      "date": "2024-10-07T20:42:05+00:00",
      "message": "Fix partial checkpoint loading caused by changes in jax's flatten_up_to behaviour."
    },
    {
      "sha": "9bb1e85f957e27be1ccb8cf28eb6e5981ef86ff8",
      "author": "George Necula",
      "date": "2024-10-07T15:39:13+00:00",
      "message": "Replace usage of jax.experimental.host_callback.call with jax.experimental.io_callback."
    },
    {
      "sha": "705247b743d26a33d0c058b41c72ad030e51891b",
      "author": "Niket Kumar Bhumihar",
      "date": "2024-09-12T17:34:49+00:00",
      "message": "Rename `is_supported_empty_aggregation_type` and `is_supported_aggregation_type` functions."
    },
    {
      "sha": "c2f0c9c6bccf9084f02b19174651e983fdbcd2fe",
      "author": "Colin Gaffney",
      "date": "2024-09-03T18:06:00+00:00",
      "message": "Remove external direct references to `pytree_checkpoint_handler`."
    }
  ],
  "readme_text": "# T5X\n\n*Go to [T5X ReadTheDocs Documentation Page](https://t5x.readthedocs.io/).*\n\nT5X is a modular, composable, research-friendly framework for high-performance,\nconfigurable, self-service training, evaluation, and inference of sequence\nmodels (starting with language) at many scales.\n\nIt is essentially a new and improved implementation of the\n[T5 codebase](https://github.com/google-research/text-to-text-transfer-transformer)\n(based on [Mesh TensorFlow](https://github.com/tensorflow/mesh)) in [JAX](https://github.com/google/jax) and [Flax](https://github.com/google/flax). To learn\nmore, see the [T5X Paper](https://arxiv.org/abs/2203.17189).\n\nBelow is a quick start guide for training models with TPUs on Google Cloud. For\nadditional tutorials and background, see the [complete documentation](docs/index.md).\n\n## Quickstart (Recommended)\n\nT5X can be run with [XManager](https://github.com/deepmind/xmanager) on\n[Vertex AI](https://cloud.google.com/vertex-ai). Vertex AI is a platform for\ntraining that creates TPU instances and runs code on the TPUs. Vertex AI will\nalso shut down the TPUs when the jobs terminate. This is signifcantly easier\nthan managing GCE VMs and TPU VM instances.\n\n1. Follow the pre-requisites and directions to install [XManager](https://github.com/deepmind/xmanager).\n\n2. Request TPU quota as required. GCP projects come with 8 cores by default,\nwhich is enough to run one training experiment on a single TPU host. If you want\nto run multi-host training or run multiple trials in parallel, you will need\nmore quota. Navigate to [Quotas](https://console.cloud.google.com/quotas).\n\n  The quota you want is:\n\n  * Service: `Vertex AI API`\n  * Dimensions (location): `us-central1`\n  * If you want to run single-host experiments:\n    * `Custom model training TPU V2 cores per region`\n    * `Custom model training TPU V3 cores per region`\n  * If you want to run multi-host experiments:\n    * `Custom model training TPU V2 pod cores per region`\n    * `Custom model training TPU V3 pod cores per region`\n\n  TIP: You won't be able to run single-host experiments with multi-host quota.\n  (i.e. you can't run `tpu_v2=8` using `TPU V2 pod`)\n\n\n3. Launch the xmanager script located at `t5x/scripts/xm_launch.py`.\n\nAs a running example, we use the WMT14 En-De translation which is described in\nmore detail in the Examples section below.\n\n```sh\nexport GOOGLE_CLOUD_BUCKET_NAME=...\nexport TFDS_DATA_DIR=gs://$GOOGLE_CLOUD_BUCKET_NAME/t5x/data\nexport MODEL_DIR=gs://$GOOGLE_CLOUD_BUCKET_NAME/t5x/$(date +%Y%m%d)\n\n# Pre-download dataset in multi-host experiments.\ntfds build wmt_t2t_translate --data_dir=$TFDS_DATA_DIR\n\ngit clone https://github.com/google-research/t5x\ncd ./t5x/\n\npython3 ./t5x/scripts/xm_launch.py \\\n  --gin_file=t5x/examples/t5/t5_1_1/examples/base_wmt_from_scratch.gin \\\n  --model_dir=$MODEL_DIR \\\n  --tfds_data_dir=$TFDS_DATA_DIR\n```\n\nCheck `gs://$GOOGLE_CLOUD_BUCKET_NAME/t5x/` for the output artifacts, which can\nbe read by TensorBoard.\n\n## GPU Usage\nNote: NVIDIA has released an updated version of this repository with H100 FP8 support and broad GPU performance improvements. Please visit the [NVIDIA Rosetta](https://github.com/NVIDIA/JAX-Toolbox/tree/main/rosetta/rosetta/projects/t5x) repository for more details and usage instructions.\n\nT5X can be run easily on GPUs either in single-node configurations or multi-node configurations with a SLURM+pyxis cluster. Further instructions at [t5x/contrib/gpu](https://github.com/google-research/t5x/blob/main/t5x/contrib/gpu/README.md). The `t5x/contrib/gpu/scripts_gpu` folder contains example scripts for pretraining T5X on [The Pile](https://pile.eleuther.ai/) and for finetuning on SQuAD and MNLI. These scripts and associated `gin` configurations also contain additional GPU optimizations for better throughput. More examples and instructions can be found in the [NVIDIA Rosetta](https://github.com/NVIDIA/JAX-Toolbox/tree/main/rosetta/rosetta/projects/t5x) repository maintained by NVIDIA with H100 FP8 support and broad GPU performance improvements.\n\n\n## Installation\n\nNote that all the commands in this document should be run in the commandline of\nthe TPU VM instance unless otherwise stated.\n\n1.  Follow the\n    [instructions](https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm#install_the_google_cloud_sdk)\n    to set up a Google Cloud Platform (GCP) account and enable the Cloud TPU\n    API.\n\n    **Note:** T5X also works with GPU, please follow instructions in [t5x/contrib/gpu](https://github.com/google-research/t5x/blob/main/t5x/contrib/gpu/README.md) if you'd like to use GPU version.\n\n2.  Create a\n    [Cloud TPU VM instance](https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms)\n    following\n    [this instruction](https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm#create-vm).\n    We recommend that you develop your workflow in a single v3-8 TPU (i.e.,\n    `--accelerator-type=v3-8`) and scale up to pod slices once the pipeline is\n    ready. In this README, we focus on using a single v3-8 TPU. See\n    [here](https://cloud.google.com/tpu/docs/system-architecture-tpu-vm) to\n    learn more about TPU architectures.\n\n3.  With Cloud TPU VMs, you ssh directly into the host machine of the TPU VM.\n    You can install packages, run your code run, etc. in the host machine. Once\n    the TPU instance is created, ssh into it with\n\n    ```sh\n    gcloud alpha compute tpus tpu-vm ssh ${TPU_NAME} --zone=${ZONE}\n    ```\n\n    where `TPU_NAME` and `ZONE` are the name and the zone used in step 2.\n\n4.  Install T5X and the dependencies.\n\n    ```sh\n    git clone --branch=main https://github.com/google-research/t5x\n    cd t5x\n\n    python3 -m pip install -e '.[tpu]' -f \\\n      https://storage.googleapis.com/jax-releases/libtpu_releases.html\n\n    ```\n\n\n5.  Create Google Cloud Storage (GCS) bucket to store the dataset and model\n    checkpoints. To create a GCS bucket, see these\n    [instructions](https://cloud.google.com/storage/docs/creating-buckets).\n\n6.  (optional) If you prefer working with Jupyter/Colab style environment\n    you can setup a custom Colab runtime by following steps from\n    [t5x/notebooks](https://github.com/google-research/t5x/blob/main/t5x/notebooks/README.md).\n\n## Example: English to German translation\n\nAs a running example, we use the WMT14 En-De translation. The raw dataset is\navailable in TensorFlow Datasets as\n[\"wmt_t2t_translate\"](https://www.tensorflow.org/datasets/catalog/wmt_t2t_translate).\n\nT5 casts the translation task such as the following\n\n```py\n{'en': 'That is good.', 'de': 'Das ist gut.'}\n```\n\nto the form called \"text-to-text\":\n\n```py\n{'inputs': 'translate English to German: That is good.', 'targets': 'Das ist gut.'}\n```\n\nThis formulation allows many different classes of language tasks to be expressed\nin a uniform manner and a single encoder-decoder architecture can handle them\nwithout any task-specific parameters. For more detail, refer to the [T5 paper\n(Raffel et al. 2019)][t5_paper].\n\nFor a scalable data pipeline and an evaluation framework, we use\n[`SeqIO`](https://github.com/google/seqio), which was factored out of the [T5\nlibrary][t5_github]. A `seqio.Task` packages together the raw dataset, vocabulary,\npreprocessing such as tokenization and evaluation metrics such as\n[BLEU](https://aclanthology.org/P02-1040.pdf) and provides a\n[`tf.data`](https://www.tensorflow.org/guide/data) instance.\n\n[The T5 library][t5_github] provides a number of `seqio.Task`s that were used in the\n[T5 paper][t5_paper]. In this example, we use [wmt_t2t_ende_v003](https://github.com/google-research/text-to-text-transfer-transformer/blob/d81c0bab2a41b4d5dfbe4971de32f7d67df65f31/t5/data/tasks.py#L212).\n\nBefore training or fine-tuning you need to download [\"wmt_t2t_translate\"]\n(https://www.tensorflow.org/datasets/catalog/wmt_t2t_translate) dataset first.\n\n```sh\n# Data dir to save the processed dataset in \"gs://data_dir\" format.\nTFDS_DATA_DIR=\"...\"\n\n# Make sure that dataset package is up-to-date.\npython3 -m pip install --upgrade tfds-nightly\n\n# Pre-download dataset.\ntfds build wmt_t2t_translate ${TFDS_DATA_DIR}\n```\n\n### Training\n\nTo run a training job, we use the `t5x/train.py` script.\n\n```sh\n# Model dir to save logs, ckpts, etc. in \"gs://model_dir\" format.\nMODEL_DIR=\"...\"\nT5X_DIR=\"...\"  # directory where the T5X repo is cloned.\nTFDS_DATA_DIR=\"...\"\n\npython3 ${T5X_DIR}/t5x/train.py \\\n  --gin_file=\"t5x/examples/t5/t5_1_1/examples/base_wmt_from_scratch.gin\" \\\n  --gin.MODEL_DIR=\\\"${MODEL_DIR}\\\" \\\n  --tfds_data_dir=${TFDS_DATA_DIR}\n```\n\nThe configuration for this training run is defined in the Gin file\n[base_wmt_from_scratch.gin](t5x/examples/t5/t5_1_1/examples/base_wmt_from_scratch.gin).\n[Gin-config](https://github.com/google/gin-config) is a library to handle\nconfigurations based on dependency injection. Among many benefits, Gin allows\nusers to pass custom components such as a custom model to the T5X library\nwithout having to modify the core library. The [custom\ncomponents](#custom-components) section shows how this is done.\n\nWhile the core library is independent of Gin, it is central to the examples we\nprovide. Therefore, we provide a short [introduction][gin-primer] to Gin in the\ncontext of T5X.  All the configurations are written to a file \"config.gin\" in\n`MODEL_DIR`. This makes debugging as well as reproducing the experiment much\neasier.\n\nIn addition to the `config.json`, `model-info.txt` file summarizes the model\nparameters (shape, names of the axes, partitioning info) as well as the\noptimizer states.\n\n\n\n#### TensorBoard\n\nTo monitor the training in [TensorBoard](https://www.tensorflow.org/tensorboard), it is much easier (due to\nauthentification issues) to launch the TensorBoard on your own machine and _not_ in\nthe TPU VM. So in the commandline where you ssh'ed into the TPU VM, launch the\nTensorBoard with the `logdir` pointing to the `MODEL_DIR`.\n\n```sh\n# NB: run this on your machine not TPU VM!\nMODEL_DIR=\"...\"  # Copy from the TPU VM.\ntensorboard --logdir=${MODEL_DIR}\n```\n\nOr you can launch the TensorBoard inside a Colab. In a Colab cell, run\n\n```python\nfrom google.colab import auth\nauth.authenticate_user()\n```\n\nto authorize the Colab to access the GCS bucket and launch the TensorBoard.\n\n```python\n%load_ext tensorboard\nmodel_dir = \"...\"  # Copy from the TPU VM.\n%tensorboard --logdir=model_dir\n```\n\n\n### Fine-tuning\n\nWe can leverage the benefits of self-supervised pre-training by initializing\nfrom one of our pre-trained models. Here we use the T5.1.1 Base checkpoint.\n\n```sh\n# Model dir to save logs, ckpts, etc. in \"gs://model_dir\" format.\nMODEL_DIR=\"...\"\n\n# Data dir to save the processed dataset in \"gs://data_dir\" format.\nTFDS_DATA_DIR=\"...\"\nT5X_DIR=\"...\"  # directory where the T5X repo is cloned.\n\npython3 ${T5X_DIR}/t5x/train.py \\\n  --gin_file=\"t5x/examples/t5/t5_1_1/examples/base_wmt_finetune.gin\" \\\n  --gin.MODEL_DIR=\\\"${MODEL_DIR}\\\" \\\n  --tfds_data_dir=${TFDS_DATA_DIR}\n```\n\n**Note:** when supplying a string, dict, list, tuple value, or a bash variable\nvia a flag, you must put it in quotes. In the case of strings, it requires\nescaped quotes (`\\\"<string>\\\"`). For example:\n`--gin.utils.DatasetConfig.split=\\\"validation\\\"` or\n`--gin.MODEL_DIR=\\\"${MODEL_DIR}\\\"`.\n\nGin makes it easy to change a number of configurations. For example, you can\nchange the `partitioning.PjitPartitioner.num_partitions` (overriding\nthe value in\n[base_wmt_from_scratch.gin](t5x/examples/t5/t5_1_1/examples/base_wmt_from_scratch.gin))\nto chanage the parallelism strategy and pass it as a commandline arg.\n\n```sh\n--gin.partitioning.PjitPartitioner.num_partitions=8\n```\n\n### Evaluation\n\nTo run the offline (i.e. without training) evaluation, you can use `t5x/eval.py`\nscript.\n\n```sh\nEVAL_OUTPUT_DIR=\"...\"  # directory to write eval output\nT5X_DIR=\"...\"  # directory where the t5x is cloned, e.g., ${HOME}\"/t5x\".\nTFDS_DATA_DIR=\"...\"\nCHECKPOINT_PATH=\"...\"\n\npython3 ${T5X_DIR}/t5x/eval.py \\\n  --gin_file=\"t5x/examples/t5/t5_1_1/examples/base_wmt_eval.gin\" \\\n  --gin.CHECKPOINT_PATH=\\\"${CHECKPOINT_PATH}\\\" \\\n  --gin.EVAL_OUTPUT_DIR=\\\"${EVAL_OUTPUT_DIR}\\\" \\\n  --tfds_data_dir=${TFDS_DATA_DIR}\n```\n\n\n### Inference\n\nTo run inference, you can use `t5x/infer.py` script. Here we use the same\n`seqio.Task`, but for inference we do not use the targets features other than\nlogging them alongside the prediction in a JSON file.\n\n```sh\nINFER_OUTPUT_DIR=\"...\"  # directory to write infer output\nT5X_DIR=\"...\"  # directory where the t5x is cloned, e.g., ${HOME}\"/t5x\".\nTFDS_DATA_DIR=\"...\"\nCHECKPOINT_PATH=\"...\"\n\npython3 ${T5X_DIR}/t5x/infer.py \\\n  --gin_file=\"t5x/examples/t5/t5_1_1/examples/base_wmt_infer.gin\" \\\n  --gin.CHECKPOINT_PATH=\\\"${CHECKPOINT_PATH}\\\" \\\n  --gin.INFER_OUTPUT_DIR=\\\"${INFER_OUTPUT_DIR}\\\" \\\n  --tfds_data_dir=${TFDS_DATA_DIR}\n```\n\n### Exporting as TensorFlow Saved Model\n\nPretrained model can be exported as TensorFlow Saved Model, and deployed\nto Vertex AI Prediction service using [Optimized TensorFlow Runtime]\n(https://cloud.google.com/vertex-ai/docs/predictions/optimized-tensorflow-runtime).\nPlease note that exported model won't work on OSS based\n[TensorFlow Model Server](https://github.com/tensorflow/serving).\n\n```sh\nT5X_DIR=\"...\"  # directory where the t5x is cloned, e.g., ${HOME}\"/t5x\".\nCHECKPOINT_PATH=\"...\"\n\nBATCH_SIZE=None\nBEAM_SIZE=1\n\n# Use 'bfloat16' if you plan to run exported model on NVIDIA A100 or newer GPUs,\n# for other GPUs use 'float32'.\nACTIVATION_DTYPE=bfloat16\n\n# Version numbers must be numeric. We generate one based on datetime.\nVERSION=$(date +%Y%m%d%H%M%S)\n\nNAME=t5x_base_${ACTIVATION_DTYPE}  # Model name.\n\n# Path to export model to. Note that export script is going to add _cpu suffix\n# after model name.\nOUTPUT=${CHECKPOINT_PATH}/saved_model.${NAME}/${VERSION}\n\ndeclare -a ARGS=(\n--gin_file=t5x/examples/t5/t5_1_1/base.gin\n--gin_file=t5x/t5x/configs/runs/export.gin\n--gin.TASK_FEATURE_LENGTHS=\"{'inputs': 256, 'targets': 256}\"\n--gin.CHECKPOINT_PATH=\\\"${CHECKPOINT_PATH}\\\"\n--gin.MODEL_NAME=\\\"/ml/${USER}/t5x_base\\\"\n--gin.MODEL_OUTPUT_DIR=\\\"${OUTPUT}\\\"\n--gin.BEAM_SIZE=${BEAM_SIZE}\n--gin.BATCH_SIZE=${BATCH_SIZE}\n--gin.export_lib.save.partitioner=None\n--gin.export_lib.save.warmup_examples=\"['hello world']\"\n--gin.export_lib.ExportableModule.use_batch_function=False\n--gin.export_lib.ExportableModule.use_gpu=False\n--gin.export_lib.ExportableModule.jit_compile=False\n--gin.ACTIVATION_DTYPE=\\\"${ACTIVATION_DTYPE}\\\"\n--gin.network.T5Config.dtype=\\\"${ACTIVATION_DTYPE}\\\"\n--gin.utils.RestoreCheckpointConfig.dtype=\\\"${ACTIVATION_DTYPE}\\\"\n--gin.DROPOUT_RATE=0.0\n)\n\n(python3 ${T5X_DIR}/t5x/export.py \"${ARGS[@]}\")\n```\n\nFor detailed arguments definition refer to [export.gin]\n(t5x/configs/runs/export.gin).\n\nYou can run XL and smaller models on NVIDIA A100 40GB, and XXL models on\nNVIDIA A100 80GB.\n\n## Custom components\n\n[The translation example](#example-english-to-german-translation) uses the\nencoder-decoder model that T5X provides as well as the dataset from the T5\nlibrary. This section shows how you can use your own dataset and a model and\npass via Gin.\n\n### Example: custom dataset in a user directory\n\nFor this example, we have the following directory structure with\n`${HOME}/dir1/user_dir` representing a user directory with custom components.\n\n```\n${HOME}\n\u2514\u2500\u2500 dir1\n \u00a0\u00a0 \u2514\u2500\u2500 user_dir\n \u00a0\u00a0     \u251c\u2500\u2500 t5_1_1_base_de_en.gin\n \u00a0\u00a0     \u2514\u2500\u2500 tasks.py\n```\n\nAs an example, let's define a new dataset. Here we use the same Translation\ndataset but we define the translation task in the opposite direction, i.e.,\nGerman to English intead of English to German. We define this task in `tasks.py`\n\n```py\n# ${HOME}/dir1/user_dir/tasks.py\n\nimport functools\nimport seqio\nimport tensorflow_datasets as tfds\nfrom t5.evaluation import metrics\nfrom t5.data import preprocessors\n\nvocabulary = seqio.SentencePieceVocabulary(\n    'gs://t5-data/vocabs/cc_all.32000/sentencepiece.model', extra_ids=100)\noutput_features = {\n    'inputs': seqio.Feature(vocabulary=vocabulary),\n    'targets': seqio.Feature(vocabulary=vocabulary)\n}\n\nseqio.TaskRegistry.add(\n    'wmt_t2t_de_en_v003',\n    source=seqio.TfdsDataSource(tfds_name='wmt_t2t_translate/de-en:1.0.0'),\n    preprocessors=[\n        functools.partial(\n            preprocessors.translate,\n            source_language='de', target_language='en'),\n        seqio.preprocessors.tokenize,\n        seqio.CacheDatasetPlaceholder(),\n        seqio.preprocessors.append_eos_after_trim,\n    ],\n    metric_fns=[metrics.bleu],\n    output_features=output_features)\n```\n\nIn the Gin file, most of the settings are equivalent to those used in the\n[En->De example](#example-english-to-german-translation). So we include the Gin\nfile from that example. To use \"wmt_t2t_de_en_v003\" task we just defined, we\nneed to import the task module \"tasks.py\". Note that we use a relative path\ndefined with respect to the user directory. This will be specified as a\nflag.\n\n```py\n# ${HOME}/dir1/user_dir/t5_1_1_base_de_en.gin\nfrom __gin__ import dynamic_registration\nimport tasks  # This imports the task defined in dir1/user_dir/tasks.py.\n\ninclude \"t5x-tmp/t5x/examples/t5/t5_1_1/examples/base_wmt_from_scratch.gin\"\nMIXTURE_OR_TASK_NAME = \"wmt_t2t_de_en_v003\"\n```\n\nFinally, we launch training passing the user directory as a flag\n`gin_search_paths` such that the Gin file and python modules can be specified\nwith relative paths.\n\n```sh\nPROJECT_DIR=${HOME}\"/dir1/user_dir\"\nT5X_DIR=\"...\"  # directory where the t5x is cloned.\nTFDS_DATA_DIR=\"...\"\nMODEL_DIR=\"...\"\nexport PYTHONPATH=${PROJECT_DIR}\n\npython3 ${T5X_DIR}/t5x/train.py \\\n  --gin_search_paths=${PROJECT_DIR} \\\n  --gin_file=\"t5_1_1_base_de_en.gin\" \\\n  --gin.MODEL_DIR=\\\"${MODEL_DIR}\\\" \\\n  --tfds_data_dir=${TFDS_DATA_DIR}\n```\n\n## Checkpoints\n\n### Native Checkpoints\n\nWe have released the checkpoints of many of the original T5 models and their\nvariants a native T5X format for maximal efficiency.\nSee the [complete list](https://github.com/google-research/t5x/blob/main/docs/models.md) including the\nmatching Gin configuration files.\n\nThese are converted from the public [Mesh TensorFlow\ncheckpoints](https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511)\n.\n\n\n### Compatibility with the Mesh TensorFlow checkpoints\nThe Mesh TensorFlow checkpoints trained using the [T5 library][t5_github] can be\ndirectly loaded into T5X. For example, we can rerun the fine-tuning example\ninitializing from the MTF checkpoint by changing the `INIT_CHECKPOINT` Gin\nmacro.\n\n```sh\n# Model dir to save logs, ckpts, etc. in \"gs://model_dir\" format.\nMODEL_DIR=\"...\"\n\n# Data dir to save the processed dataset in \"gs://data_dir\" format.\nTFDS_DATA_DIR=\"...\"\nT5X_DIR=\"...\"  # directory where the T5X repo is cloned.\n\npython3 ${T5X_DIR}/t5x/train.py \\\n  --gin_file=\"t5x/examples/t5/t5_1_1/examples/base_wmt19_ende_train.gin\" \\\n  --gin.MODEL_DIR=\\\"${MODEL_DIR}\\\" \\\n  --gin.MIXTURE_OR_TASK_NAME=\\\"wmt_t2t_ende_v003\\\" \\\n  --gin.INIT_CHECKPOINT=\\\"gs://t5-data/pretrained_models/t5.1.1.base/model.ckpt-1000000\\\" \\\n  --tfds_data_dir=${TFDS_DATA_DIR}\n```\n\nNote that restoring directly from the Mesh TensorFlow checkpoints can be\ninefficient if heavy model parallelism is used for large models. This is\nbecause each host loads the entire copy of the model first and then keep only\nthe relevant slices dictated by the model parallelism specification. If you have\nMesh TensorFlow checkpoints that you run often, we recommend converting the\ncheckpoints to T5X native format using the\n[convert_tf_checkpoint script](t5x/scripts/convert_tf_checkpoint.py).\n\n\n## Citing T5X\nPlease use the following bibtex entry to cite T5X.\n\n```\n@article{roberts2022t5x,\n  url = {https://arxiv.org/abs/2203.17189},\n  author = {Roberts, Adam and Chung, Hyung Won and Levskaya, Anselm and Mishra, Gaurav and Bradbury, James and Andor, Daniel and Narang, Sharan and Lester, Brian and Gaffney, Colin and Mohiuddin, Afroz and Hawthorne, Curtis and Lewkowycz, Aitor and Salcianu, Alex and van Zee, Marc and Austin, Jacob and Goodman, Sebastian and Soares, Livio Baldini and Hu, Haitang and Tsvyashchenko, Sasha and Chowdhery, Aakanksha and Bastings, Jasmijn and Bulian, Jannis and Garcia, Xavier and Ni, Jianmo and Chen, Andrew and Kenealy, Kathleen and Clark, Jonathan H. and Lee, Stephan and Garrette, Dan and Lee-Thorp, James and Raffel, Colin and Shazeer, Noam and Ritter, Marvin and Bosma, Maarten and Passos, Alexandre and Maitin-Shepard, Jeremy and Fiedel, Noah and Omernick, Mark and Saeta, Brennan and Sepassi, Ryan and Spiridonov, Alexander and Newlan, Joshua and Gesmundo, Andrea},\n  title = {Scaling Up Models and Data with $\\texttt{t5x}$ and $\\texttt{seqio}$},\n  journal={arXiv preprint arXiv:2203.17189},\n  year = {2022},\n}\n```\n\n\n## Note\nThis is not an officially supported Google product\n\n[t5_paper]: https://arxiv.org/abs/1910.10683\n[t5_github]: https://github.com/google-research/text-to-text-transfer-transformer\n[gin-primer]: docs/usage/gin.md\n",
  "external_links_in_readme": [
    "https://github.com/google-research/text-to-text-transfer-transformer",
    "https://github.com/google/seqio",
    "https://cloud.google.com/tpu/docs/system-architecture-tpu-vm",
    "https://github.com/google/jax",
    "https://github.com/NVIDIA/JAX-Toolbox/tree/main/rosetta/rosetta/projects/t5x",
    "https://github.com/google-research/t5x",
    "https://github.com/google-research/t5x/blob/main/docs/models.md",
    "https://cloud.google.com/blog/products/compute/introducing-cloud-tpu-vms",
    "https://cloud.google.com/storage/docs/creating-buckets",
    "https://github.com/tensorflow/mesh",
    "https://www.tensorflow.org/tensorboard",
    "https://github.com/google/gin-config",
    "https://github.com/tensorflow/serving",
    "https://storage.googleapis.com/jax-releases/libtpu_releases.html",
    "https://www.tensorflow.org/guide/data",
    "https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm#create-vm",
    "https://github.com/deepmind/xmanager",
    "https://arxiv.org/abs/2203.17189},",
    "https://console.cloud.google.com/quotas",
    "https://github.com/google/flax",
    "https://aclanthology.org/P02-1040.pdf",
    "https://pile.eleuther.ai/",
    "https://github.com/google-research/t5x/blob/main/t5x/notebooks/README.md",
    "https://arxiv.org/abs/2203.17189",
    "https://github.com/google-research/text-to-text-transfer-transformer/blob/main/released_checkpoints.md#t511",
    "https://github.com/google-research/text-to-text-transfer-transformer/blob/d81c0bab2a41b4d5dfbe4971de32f7d67df65f31/t5/data/tasks.py#L212",
    "https://github.com/google-research/t5x/blob/main/t5x/contrib/gpu/README.md",
    "https://t5x.readthedocs.io/",
    "https://arxiv.org/abs/1910.10683",
    "https://cloud.google.com/vertex-ai/docs/predictions/optimized-tensorflow-runtime",
    "https://cloud.google.com/vertex-ai",
    "https://www.tensorflow.org/datasets/catalog/wmt_t2t_translate",
    "https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm#install_the_google_cloud_sdk"
  ]
}
```

</details>


---

## Repository 3: google/jax

# GitHub Repository Data

**Repository:** [jax-ml/jax](https://github.com/jax-ml/jax)

## Basic Information

- **Description:** Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more
- **Created:** 2018-10-25T21:25:02+00:00
- **Last Updated:** 2025-06-22T01:52:15+00:00
- **Last Pushed:** 2025-06-22T01:52:13+00:00
- **Default Branch:** main
- **Size:** 122328 KB

## Statistics

- **Stars:** 32,588
- **Forks:** 3,067
- **Watchers:** 32,588
- **Open Issues:** 2,099
- **Total Issues:** 0
- **Pull Requests:** 20,142

## License

- **Type:** Apache License 2.0
- **SPDX ID:** Apache-2.0
- **URL:** [License](https://github.com/jax-ml/jax/blob/main/LICENSE)

## Languages

- **Python:** 17,365,022 bytes
- **C++:** 2,417,197 bytes
- **Starlark:** 347,938 bytes
- **Jupyter Notebook:** 90,058 bytes
- **Shell:** 57,838 bytes
- **C:** 21,953 bytes
- **MAXScript:** 3,178 bytes
- **Makefile:** 441 bytes
- **Linker Script:** 285 bytes

## Topics

- `jax`

## Top Contributors

1. **hawkinsp** - 3077 contributions
2. **mattjj** - 2597 contributions
3. **jakevdp** - 2375 contributions
4. **Google-ML-Automation** - 1968 contributions
5. **yashk2810** - 1574 contributions
6. **gnecula** - 1163 contributions
7. **froystig** - 681 contributions
8. **apaszke** - 667 contributions
9. **superbobry** - 509 contributions
10. **skye** - 501 contributions

## File Structure (Sample of 10 files)

Total files: 1,809

- `.bazelrc` (blob)
- `.bazelversion` (blob)
- `.editorconfig` (blob)
- `.github` (tree)
- `.github/ISSUE_TEMPLATE` (tree)
- `.github/ISSUE_TEMPLATE/Feature_request.md` (blob)
- `.github/ISSUE_TEMPLATE/bug-report.yml` (blob)
- `.github/ISSUE_TEMPLATE/config.yml` (blob)
- `.github/actionlint.yaml` (blob)
- `.github/dependabot.yml` (blob)

## Recent Issues

- ðŸ”´ **#29633** Fix some sharding-in-types error messages (closed)
- ðŸ”´ **#29632** Go via device_put in reshard's impl rule if the mesh in context and the mesh is not multi-process (closed)
- ðŸ”´ **#29631** Expose reshard, auto_axes and explicit_axes via the `jax.sharding` endpoint graduating them from `jax.experimental`. (closed)
- ðŸŸ¢ **#29630** Add support for unroll=0 to lax.scan. (open)
- ðŸ”´ **#29629** instantiate symbolic zero cotangents in _linear_solve_transpose_rule (closed)

## Recent Pull Requests

- ðŸ”´ **#29633** Fix some sharding-in-types error messages (closed)
- ðŸ”´ **#29632** Go via device_put in reshard's impl rule if the mesh in context and the mesh is not multi-process (closed)
- ðŸ”´ **#29631** Expose reshard, auto_axes and explicit_axes via the `jax.sharding` endpoint graduating them from `jax.experimental`. (closed)
- ðŸŸ¢ **#29630** Add support for unroll=0 to lax.scan. (open)
- ðŸ”´ **#29629** instantiate symbolic zero cotangents in _linear_solve_transpose_rule (closed)

## Recent Commits

- **53ff15a3** Fix some sharding-in-types error messages - Yash Katariya (2025-06-22T01:51:09+00:00)
- **2d0df4d7** Go via device_put in reshard's impl rule if the mesh in context and the mesh is not multi-process - Yash Katariya (2025-06-22T01:34:51+00:00)
- **4f0eeea6** Add pow to fusor. - jax authors (2025-06-22T01:29:01+00:00)
- **ff402188** Expose reshard, auto_axes and explicit_axes via the `jax.sharding` endpoint graduating them from `jax.experimental`. - Yash Katariya (2025-06-22T01:14:07+00:00)
- **48feb750** Merge pull request #29565 from Cjkkkk:disable_cudnn_sdpa_on_gb300 - jax authors (2025-06-22T00:08:27+00:00)
- **1f75e16c** Merge pull request #29629 from mattjj:linear-solve-fix - jax authors (2025-06-22T00:00:08+00:00)
- **a26a1a72** instantiate symbolic zero cotangents in _linear_solve_transpose_rule - Matthew Johnson (2025-06-10T21:55:02+00:00)
- **f141c365** Expose local/global topology exchange timeouts for CPU client with collectives. - jax authors (2025-06-20T23:16:25+00:00)
- **636691bb** [Pallas Fuser] Use partial_eval instead of DCE for fusible short-circuit check - Jacob Burnim (2025-06-20T20:12:10+00:00)
- **0320c354** [Pallas Fuser] Add support for custom_vjp functions - Sharad Vikram (2025-06-20T19:50:49+00:00)

## External Links Found in README

- https://docs.jax.dev/en/latest/control-flow.html
- https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html
- https://docs.jax.dev/en/latest/advanced_guide.html
- http://github.com/jax-ml/jax},
- https://mlsys.org/Conferences/2019/doc/2018/146.pdf
- https://docs.jax.dev/en/latest/
- https://docs.jax.dev/en/latest/notebooks/autodiff_cookbook.html
- https://github.com/jax-ml/jax/actions/workflows/ci-build.yaml/badge.svg
- https://img.shields.io/pypi/v/jax
- https://docs.jax.dev/en/latest/notebooks/explicit-sharding.html
- https://github.com/jax-ml/jax/actions/workflows/ci-build.yaml
- https://docs.jax.dev/en/latest/developer.html
- https://www.tensorflow.org/xla
- https://pypi.org/project/jax/
- https://github.com/intel/intel-extension-for-openxla/blob/main/docs/acc_jax.md
- https://docs.jax.dev/en/latest/changelog.html
- https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html
- https://developer.apple.com/metal/jax/
- https://docs.jax.dev/en/latest/jax.html#vectorization-vmap
- https://raw.githubusercontent.com/jax-ml/jax/main/images/jax_logo_250px.png"

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 154739597,
  "name": "jax",
  "full_name": "jax-ml/jax",
  "description": "Composable transformations of Python+NumPy programs: differentiate, vectorize, JIT to GPU/TPU, and more",
  "html_url": "https://github.com/jax-ml/jax",
  "clone_url": "https://github.com/jax-ml/jax.git",
  "ssh_url": "git@github.com:jax-ml/jax.git",
  "homepage": "https://docs.jax.dev",
  "topics": [
    "jax"
  ],
  "default_branch": "main",
  "created_at": "2018-10-25T21:25:02+00:00",
  "updated_at": "2025-06-22T01:52:15+00:00",
  "pushed_at": "2025-06-22T01:52:13+00:00",
  "size_kb": 122328,
  "watchers_count": 32588,
  "stargazers_count": 32588,
  "forks_count": 3067,
  "open_issues_count": 2099,
  "license": {
    "key": "apache-2.0",
    "name": "Apache License 2.0",
    "spdx_id": "Apache-2.0",
    "url": "https://github.com/jax-ml/jax/blob/main/LICENSE"
  },
  "languages": {
    "Python": 17365022,
    "C++": 2417197,
    "Starlark": 347938,
    "Jupyter Notebook": 90058,
    "Shell": 57838,
    "C": 21953,
    "MAXScript": 3178,
    "Makefile": 441,
    "Linker Script": 285
  },
  "top_contributors": [
    {
      "login": "hawkinsp",
      "contributions": 3077
    },
    {
      "login": "mattjj",
      "contributions": 2597
    },
    {
      "login": "jakevdp",
      "contributions": 2375
    },
    {
      "login": "Google-ML-Automation",
      "contributions": 1968
    },
    {
      "login": "yashk2810",
      "contributions": 1574
    },
    {
      "login": "gnecula",
      "contributions": 1163
    },
    {
      "login": "froystig",
      "contributions": 681
    },
    {
      "login": "apaszke",
      "contributions": 667
    },
    {
      "login": "superbobry",
      "contributions": 509
    },
    {
      "login": "skye",
      "contributions": 501
    },
    {
      "login": "sharadmv",
      "contributions": 360
    },
    {
      "login": "dfm",
      "contributions": 282
    },
    {
      "login": "bchetioui",
      "contributions": 232
    },
    {
      "login": "pschuh",
      "contributions": 181
    },
    {
      "login": "shoyer",
      "contributions": 152
    },
    {
      "login": "tlongeri",
      "contributions": 149
    },
    {
      "login": "justinjfu",
      "contributions": 137
    },
    {
      "login": "bythew3i",
      "contributions": 131
    },
    {
      "login": "jekbradbury",
      "contributions": 127
    },
    {
      "login": "chr1sj0nes",
      "contributions": 114
    }
  ],
  "file_tree_count": 1809,
  "file_tree_sample": [
    {
      "path": ".bazelrc",
      "type": "blob"
    },
    {
      "path": ".bazelversion",
      "type": "blob"
    },
    {
      "path": ".editorconfig",
      "type": "blob"
    },
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/Feature_request.md",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/bug-report.yml",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/config.yml",
      "type": "blob"
    },
    {
      "path": ".github/actionlint.yaml",
      "type": "blob"
    },
    {
      "path": ".github/dependabot.yml",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 20142,
  "recent_issues": [
    {
      "number": 29633,
      "title": "Fix some sharding-in-types error messages",
      "state": "closed"
    },
    {
      "number": 29632,
      "title": "Go via device_put in reshard's impl rule if the mesh in context and the mesh is not multi-process",
      "state": "closed"
    },
    {
      "number": 29631,
      "title": "Expose reshard, auto_axes and explicit_axes via the `jax.sharding` endpoint graduating them from `jax.experimental`.",
      "state": "closed"
    },
    {
      "number": 29630,
      "title": "Add support for unroll=0 to lax.scan.",
      "state": "open"
    },
    {
      "number": 29629,
      "title": "instantiate symbolic zero cotangents in _linear_solve_transpose_rule",
      "state": "closed"
    }
  ],
  "recent_pulls": [
    {
      "number": 29633,
      "title": "Fix some sharding-in-types error messages",
      "state": "closed"
    },
    {
      "number": 29632,
      "title": "Go via device_put in reshard's impl rule if the mesh in context and the mesh is not multi-process",
      "state": "closed"
    },
    {
      "number": 29631,
      "title": "Expose reshard, auto_axes and explicit_axes via the `jax.sharding` endpoint graduating them from `jax.experimental`.",
      "state": "closed"
    },
    {
      "number": 29630,
      "title": "Add support for unroll=0 to lax.scan.",
      "state": "open"
    },
    {
      "number": 29629,
      "title": "instantiate symbolic zero cotangents in _linear_solve_transpose_rule",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "53ff15a3b1807228e34910188fe5d2fcc09442c1",
      "author": "Yash Katariya",
      "date": "2025-06-22T01:51:09+00:00",
      "message": "Fix some sharding-in-types error messages"
    },
    {
      "sha": "2d0df4d79b7d9560d3f10b3f46249bd026746192",
      "author": "Yash Katariya",
      "date": "2025-06-22T01:34:51+00:00",
      "message": "Go via device_put in reshard's impl rule if the mesh in context and the mesh is not multi-process"
    },
    {
      "sha": "4f0eeea6291f5da1c595263c9c5e202dc00163e1",
      "author": "jax authors",
      "date": "2025-06-22T01:29:01+00:00",
      "message": "Add pow to fusor."
    },
    {
      "sha": "ff4021885f85cc53db419efe1b96371ff078373b",
      "author": "Yash Katariya",
      "date": "2025-06-22T01:14:07+00:00",
      "message": "Expose reshard, auto_axes and explicit_axes via the `jax.sharding` endpoint graduating them from `jax.experimental`."
    },
    {
      "sha": "48feb75065b59c0e0795e4bf98c2f51bcad3a7c1",
      "author": "jax authors",
      "date": "2025-06-22T00:08:27+00:00",
      "message": "Merge pull request #29565 from Cjkkkk:disable_cudnn_sdpa_on_gb300"
    },
    {
      "sha": "1f75e16c8b9739348f0b538527b162168c0cf955",
      "author": "jax authors",
      "date": "2025-06-22T00:00:08+00:00",
      "message": "Merge pull request #29629 from mattjj:linear-solve-fix"
    },
    {
      "sha": "a26a1a7228d8e2fca209a9f45049ff03c4138a3b",
      "author": "Matthew Johnson",
      "date": "2025-06-10T21:55:02+00:00",
      "message": "instantiate symbolic zero cotangents in _linear_solve_transpose_rule"
    },
    {
      "sha": "f141c3652a2588cfb721570e4589ff15197c3114",
      "author": "jax authors",
      "date": "2025-06-20T23:16:25+00:00",
      "message": "Expose local/global topology exchange timeouts for CPU client with collectives."
    },
    {
      "sha": "636691bba40b936b8b64a4792c1d2158296e9dd4",
      "author": "Jacob Burnim",
      "date": "2025-06-20T20:12:10+00:00",
      "message": "[Pallas Fuser] Use partial_eval instead of DCE for fusible short-circuit check"
    },
    {
      "sha": "0320c354117d8d7e03dcac93c513adc66cec2dde",
      "author": "Sharad Vikram",
      "date": "2025-06-20T19:50:49+00:00",
      "message": "[Pallas Fuser] Add support for custom_vjp functions"
    },
    {
      "sha": "5f0b64e370e764bc9aad2fd6a97f23f2a9391102",
      "author": "jax authors",
      "date": "2025-06-20T19:45:28+00:00",
      "message": "Add REG type for GPUs"
    },
    {
      "sha": "25c72d1f8650ef3498e95ecbd70f4afedeaf6ea4",
      "author": "Justin Fu",
      "date": "2025-06-20T19:29:51+00:00",
      "message": "[Pallas][Mosaic GPU] Fix SMEM aliasing of RHS for tcgen05.mma"
    },
    {
      "sha": "2958740b42565ff0b2fb7607e9e00597525dcf9b",
      "author": "Tom\u00e1s Longeri",
      "date": "2025-06-20T18:34:59+00:00",
      "message": "[Mosaic:TPU] Extend support for ext to be symmetrical with trunc"
    },
    {
      "sha": "64736c9bd655d8da8378da17c45550a2016efa66",
      "author": "jax authors",
      "date": "2025-06-20T17:42:41+00:00",
      "message": "Merge pull request #29421 from mattjj:mutable-array-custom-vjp-scan2-again"
    },
    {
      "sha": "9f9a7d41f6df48ec414ca87d211d81c342e61fc1",
      "author": "jax authors",
      "date": "2025-06-20T16:14:05+00:00",
      "message": "Add more models to auto tune and update tuned block."
    },
    {
      "sha": "234b87a8f71d0be6b8dc94e7bb1b0a963e130444",
      "author": "Adam Paszke",
      "date": "2025-06-20T16:07:07+00:00",
      "message": "[Pallas:MGPU] Return the same grid as passed to MGPU from the Pallas:MGPU lowering"
    },
    {
      "sha": "6e39a181013144dc4a773d3451b38069eb9ccfd5",
      "author": "Yash Katariya",
      "date": "2025-06-20T15:22:44+00:00",
      "message": "[Take 2][Roll-forward] Remove Layout, .layout, .input_layouts and .output_layouts and replace it with Format, .format, .input_formats and .output_formats in JAX"
    },
    {
      "sha": "ea9fa64dfd84630b8dc348c39d8135e8871181f5",
      "author": "jax authors",
      "date": "2025-06-20T14:29:26+00:00",
      "message": "Reverts bfc07e2f10093ae169e2578872b04e8eb204a1d6"
    },
    {
      "sha": "ddb783afccb09820f318603cba4ab5db862ea69e",
      "author": "Sergei Lebedev",
      "date": "2025-06-20T13:16:16+00:00",
      "message": "[pallas:mosaic] Aligned squeezed dimension inference for `tpu.memref_squeeze` with MLIR"
    },
    {
      "sha": "22f7b7b5cc2cfb8ed43b15fdad491b2268f4f3de",
      "author": "Adam Paszke",
      "date": "2025-06-20T10:06:54+00:00",
      "message": "[Pallas:MGPU] Add a reference to the software pipelining guide"
    }
  ],
  "readme_text": "<div align=\"center\">\n<img src=\"https://raw.githubusercontent.com/jax-ml/jax/main/images/jax_logo_250px.png\" alt=\"logo\"></img>\n</div>\n\n# Transformable numerical computing at scale\n\n[![Continuous integration](https://github.com/jax-ml/jax/actions/workflows/ci-build.yaml/badge.svg)](https://github.com/jax-ml/jax/actions/workflows/ci-build.yaml)\n[![PyPI version](https://img.shields.io/pypi/v/jax)](https://pypi.org/project/jax/)\n\n[**Transformations**](#transformations)\n| [**Scaling**](#scaling)\n| [**Install guide**](#installation)\n| [**Change logs**](https://docs.jax.dev/en/latest/changelog.html)\n| [**Reference docs**](https://docs.jax.dev/en/latest/)\n\n\n## What is JAX?\n\nJAX is a Python library for accelerator-oriented array computation and program transformation,\ndesigned for high-performance numerical computing and large-scale machine learning.\n\nJAX can automatically differentiate native\nPython and NumPy functions. It can differentiate through loops, branches,\nrecursion, and closures, and it can take derivatives of derivatives of\nderivatives. It supports reverse-mode differentiation (a.k.a. backpropagation)\nvia [`jax.grad`](#automatic-differentiation-with-grad) as well as forward-mode differentiation,\nand the two can be composed arbitrarily to any order.\n\nJAX uses [XLA](https://www.tensorflow.org/xla)\nto compile and scale your NumPy programs on TPUs, GPUs, and other hardware accelerators.\nYou can compile your own pure functions with [`jax.jit`](#compilation-with-jit).\nCompilation and automatic differentiation can be composed arbitrarily.\n\nDig a little deeper, and you'll see that JAX is really an extensible system for\n[composable function transformations](#transformations) at [scale](#scaling).\n\nThis is a research project, not an official Google product. Expect\n[sharp edges](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html).\nPlease help by trying it out, [reporting bugs](https://github.com/jax-ml/jax/issues),\nand letting us know what you think!\n\n```python\nimport jax\nimport jax.numpy as jnp\n\ndef predict(params, inputs):\n  for W, b in params:\n    outputs = jnp.dot(inputs, W) + b\n    inputs = jnp.tanh(outputs)  # inputs to the next layer\n  return outputs                # no activation on last layer\n\ndef loss(params, inputs, targets):\n  preds = predict(params, inputs)\n  return jnp.sum((preds - targets)**2)\n\ngrad_loss = jax.jit(jax.grad(loss))  # compiled gradient evaluation function\nperex_grads = jax.jit(jax.vmap(grad_loss, in_axes=(None, 0, 0)))  # fast per-example grads\n```\n\n### Contents\n* [Transformations](#transformations)\n* [Scaling](#scaling)\n* [Current gotchas](#gotchas-and-sharp-bits)\n* [Installation](#installation)\n* [Neural net libraries](#neural-network-libraries)\n* [Citing JAX](#citing-jax)\n* [Reference documentation](#reference-documentation)\n\n## Transformations\n\nAt its core, JAX is an extensible system for transforming numerical functions.\nHere are three: `jax.grad`, `jax.jit`, and `jax.vmap`.\n\n### Automatic differentiation with `grad`\n\nUse [`jax.grad`](https://docs.jax.dev/en/latest/jax.html#jax.grad)\nto efficiently compute reverse-mode gradients:\n\n```python\nimport jax\nimport jax.numpy as jnp\n\ndef tanh(x):\n  y = jnp.exp(-2.0 * x)\n  return (1.0 - y) / (1.0 + y)\n\ngrad_tanh = jax.grad(tanh)\nprint(grad_tanh(1.0))\n# prints 0.4199743\n```\n\nYou can differentiate to any order with `grad`:\n\n```python\nprint(jax.grad(jax.grad(jax.grad(tanh)))(1.0))\n# prints 0.62162673\n```\n\nYou're free to use differentiation with Python control flow:\n\n```python\ndef abs_val(x):\n  if x > 0:\n    return x\n  else:\n    return -x\n\nabs_val_grad = jax.grad(abs_val)\nprint(abs_val_grad(1.0))   # prints 1.0\nprint(abs_val_grad(-1.0))  # prints -1.0 (abs_val is re-evaluated)\n```\n\nSee the [JAX Autodiff\nCookbook](https://docs.jax.dev/en/latest/notebooks/autodiff_cookbook.html)\nand the [reference docs on automatic\ndifferentiation](https://docs.jax.dev/en/latest/jax.html#automatic-differentiation)\nfor more.\n\n### Compilation with `jit`\n\nUse XLA to compile your functions end-to-end with\n[`jit`](https://docs.jax.dev/en/latest/jax.html#just-in-time-compilation-jit),\nused either as an `@jit` decorator or as a higher-order function.\n\n```python\nimport jax\nimport jax.numpy as jnp\n\ndef slow_f(x):\n  # Element-wise ops see a large benefit from fusion\n  return x * x + x * 2.0\n\nx = jnp.ones((5000, 5000))\nfast_f = jax.jit(slow_f)\n%timeit -n10 -r3 fast_f(x)\n%timeit -n10 -r3 slow_f(x)\n```\n\nUsing `jax.jit` constrains the kind of Python control flow\nthe function can use; see\nthe tutorial on [Control Flow and Logical Operators with JIT](https://docs.jax.dev/en/latest/control-flow.html)\nfor more.\n\n### Auto-vectorization with `vmap`\n\n[`vmap`](https://docs.jax.dev/en/latest/jax.html#vectorization-vmap) maps\na function along array axes.\nBut instead of just looping over function applications, it pushes the loop down\nonto the function\u2019s primitive operations, e.g. turning matrix-vector multiplies into\nmatrix-matrix multiplies for better performance.\n\nUsing `vmap` can save you from having to carry around batch dimensions in your\ncode:\n\n```python\nimport jax\nimport jax.numpy as jnp\n\ndef l1_distance(x, y):\n  assert x.ndim == y.ndim == 1  # only works on 1D inputs\n  return jnp.sum(jnp.abs(x - y))\n\ndef pairwise_distances(dist1D, xs):\n  return jax.vmap(jax.vmap(dist1D, (0, None)), (None, 0))(xs, xs)\n\nxs = jax.random.normal(jax.random.key(0), (100, 3))\ndists = pairwise_distances(l1_distance, xs)\ndists.shape  # (100, 100)\n```\n\nBy composing `jax.vmap` with `jax.grad` and `jax.jit`, we can get efficient\nJacobian matrices, or per-example gradients:\n\n```python\nper_example_grads = jax.jit(jax.vmap(jax.grad(loss), in_axes=(None, 0, 0)))\n```\n\n## Scaling\n\nTo scale your computations across thousands of devices, you can use any\ncomposition of these:\n* [**Compiler-based automatic parallelization**](https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html)\nwhere you program as if using a single global machine, and the compiler chooses\nhow to shard data and partition computation (with some user-provided constraints);\n* [**Explicit sharding and automatic partitioning**](https://docs.jax.dev/en/latest/notebooks/explicit-sharding.html)\nwhere you still have a global view but data shardings are\nexplicit in JAX types, inspectable using `jax.typeof`;\n* [**Manual per-device programming**](https://docs.jax.dev/en/latest/notebooks/shard_map.html)\nwhere you have a per-device view of data\nand computation, and can communicate with explicit collectives.\n\n| Mode | View? | Explicit sharding? | Explicit Collectives? |\n|---|---|---|---|\n| Auto | Global | \u274c | \u274c |\n| Explicit | Global | \u2705 | \u274c |\n| Manual | Per-device | \u2705 | \u2705 |\n\n```python\nfrom jax.sharding import set_mesh, AxisType, PartitionSpec as P\nmesh = jax.make_mesh((8,), ('data',), axis_types=(AxisType.Explicit,))\nset_mesh(mesh)\n\n# parameters are sharded for FSDP:\nfor W, b in params:\n  print(f'{jax.typeof(W)}')  # f32[512@data,512]\n  print(f'{jax.typeof(b)}')  # f32[512]\n\n# shard data for batch parallelism:\ninputs, targets = jax.device_put((inputs, targets), P('data'))\n\n# evaluate gradients, automatically parallelized!\ngradfun = jax.jit(jax.grad(loss))\nparam_grads = gradfun(params, (inputs, targets))\n```\n\nSee the [tutorial](https://docs.jax.dev/en/latest/sharded-computation.html) and\n[advanced guides](https://docs.jax.dev/en/latest/advanced_guide.html) for more.\n\n## Gotchas and sharp bits\n\nSee the [Gotchas\nNotebook](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html).\n\n## Installation\n\n### Supported platforms\n\n|            | Linux x86_64 | Linux aarch64 | Mac aarch64  | Windows x86_64 | Windows WSL2 x86_64 |\n|------------|--------------|---------------|--------------|----------------|---------------------|\n| CPU        | yes          | yes           | yes          | yes            | yes                 |\n| NVIDIA GPU | yes          | yes           | n/a          | no             | experimental        |\n| Google TPU | yes          | n/a           | n/a          | n/a            | n/a                 |\n| AMD GPU    | yes          | no            | n/a          | no             | no                  |\n| Apple GPU  | n/a          | no            | experimental | n/a            | n/a                 |\n| Intel GPU  | experimental | n/a           | n/a          | no             | no                  |\n\n\n### Instructions\n\n| Platform        | Instructions                                                                                                    |\n|-----------------|-----------------------------------------------------------------------------------------------------------------|\n| CPU             | `pip install -U jax`                                                                                            |\n| NVIDIA GPU      | `pip install -U \"jax[cuda12]\"`                                                                                  |\n| Google TPU      | `pip install -U \"jax[tpu]\"`                                                                                     |\n| AMD GPU (Linux) | Follow [AMD's instructions](https://github.com/jax-ml/jax/blob/main/build/rocm/README.md).                      |\n| Mac GPU         | Follow [Apple's instructions](https://developer.apple.com/metal/jax/).                                          |\n| Intel GPU       | Follow [Intel's instructions](https://github.com/intel/intel-extension-for-openxla/blob/main/docs/acc_jax.md).  |\n\nSee [the documentation](https://docs.jax.dev/en/latest/installation.html)\nfor information on alternative installation strategies. These include compiling\nfrom source, installing with Docker, using other versions of CUDA, a\ncommunity-supported conda build, and answers to some frequently-asked questions.\n\n## Citing JAX\n\nTo cite this repository:\n\n```\n@software{jax2018github,\n  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},\n  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},\n  url = {http://github.com/jax-ml/jax},\n  version = {0.3.13},\n  year = {2018},\n}\n```\n\nIn the above bibtex entry, names are in alphabetical order, the version number\nis intended to be that from [jax/version.py](../main/jax/version.py), and\nthe year corresponds to the project's open-source release.\n\nA nascent version of JAX, supporting only automatic differentiation and\ncompilation to XLA, was described in a [paper that appeared at SysML\n2018](https://mlsys.org/Conferences/2019/doc/2018/146.pdf). We're currently working on\ncovering JAX's ideas and capabilities in a more comprehensive and up-to-date\npaper.\n\n## Reference documentation\n\nFor details about the JAX API, see the\n[reference documentation](https://docs.jax.dev/).\n\nFor getting started as a JAX developer, see the\n[developer documentation](https://docs.jax.dev/en/latest/developer.html).\n",
  "external_links_in_readme": [
    "https://docs.jax.dev/en/latest/control-flow.html",
    "https://docs.jax.dev/en/latest/notebooks/Distributed_arrays_and_automatic_parallelization.html",
    "https://docs.jax.dev/en/latest/advanced_guide.html",
    "http://github.com/jax-ml/jax},",
    "https://mlsys.org/Conferences/2019/doc/2018/146.pdf",
    "https://docs.jax.dev/en/latest/",
    "https://docs.jax.dev/en/latest/notebooks/autodiff_cookbook.html",
    "https://github.com/jax-ml/jax/actions/workflows/ci-build.yaml/badge.svg",
    "https://img.shields.io/pypi/v/jax",
    "https://docs.jax.dev/en/latest/notebooks/explicit-sharding.html",
    "https://github.com/jax-ml/jax/actions/workflows/ci-build.yaml",
    "https://docs.jax.dev/en/latest/developer.html",
    "https://www.tensorflow.org/xla",
    "https://pypi.org/project/jax/",
    "https://github.com/intel/intel-extension-for-openxla/blob/main/docs/acc_jax.md",
    "https://docs.jax.dev/en/latest/changelog.html",
    "https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html",
    "https://developer.apple.com/metal/jax/",
    "https://docs.jax.dev/en/latest/jax.html#vectorization-vmap",
    "https://raw.githubusercontent.com/jax-ml/jax/main/images/jax_logo_250px.png\"",
    "https://docs.jax.dev/en/latest/installation.html",
    "https://docs.jax.dev/",
    "https://docs.jax.dev/en/latest/notebooks/shard_map.html",
    "https://docs.jax.dev/en/latest/sharded-computation.html",
    "https://docs.jax.dev/en/latest/jax.html#just-in-time-compilation-jit",
    "https://docs.jax.dev/en/latest/jax.html#jax.grad",
    "https://github.com/jax-ml/jax/blob/main/build/rocm/README.md",
    "https://github.com/jax-ml/jax/issues",
    "https://docs.jax.dev/en/latest/jax.html#automatic-differentiation"
  ]
}
```

</details>


---

