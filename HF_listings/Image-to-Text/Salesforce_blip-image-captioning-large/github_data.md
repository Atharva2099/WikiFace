# GitHub Data for Salesforce_blip-image-captioning-large

**Task Category:** Image-to-Text

## Repository 1: salesforce/BLIP

# GitHub Repository Data

**Repository:** [salesforce/BLIP](https://github.com/salesforce/BLIP)

## Basic Information

- **Description:** PyTorch code for BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation  
- **Created:** 2022-01-25T01:19:25+00:00
- **Last Updated:** 2025-06-21T19:46:36+00:00
- **Last Pushed:** 2024-08-05T12:45:49+00:00
- **Default Branch:** main
- **Size:** 6647 KB

## Statistics

- **Stars:** 5,327
- **Forks:** 696
- **Watchers:** 5,327
- **Open Issues:** 132
- **Total Issues:** 0
- **Pull Requests:** 22

## License

- **Type:** BSD 3-Clause "New" or "Revised" License
- **SPDX ID:** BSD-3-Clause
- **URL:** [License](https://github.com/salesforce/BLIP/blob/main/LICENSE.txt)

## Languages

- **Jupyter Notebook:** 687,080 bytes
- **Python:** 260,500 bytes

## Topics

- `vision-language`
- `vision-and-language-pre-training`
- `image-text-retrieval`
- `image-captioning`
- `visual-question-answering`
- `visual-reasoning`
- `vision-language-transformer`

## Top Contributors

1. **AK391** - 1 contributions

## File Structure (Sample of 10 files)

Total files: 52

- `BLIP.gif` (blob)
- `CODEOWNERS` (blob)
- `CODE_OF_CONDUCT.md` (blob)
- `LICENSE.txt` (blob)
- `README.md` (blob)
- `SECURITY.md` (blob)
- `cog.yaml` (blob)
- `configs` (tree)
- `configs/bert_config.json` (blob)
- `configs/caption_coco.yaml` (blob)

## Recent Issues

- 游댮 **#231** BertLMHeadModel' object has no attribute 'generate' (closed)
- 游릭 **#230** "NotImplementedError" (open)
- 游릭 **#229** Question about blip-itm-base-flickr released at hugging face (open)
- 游댮 **#228** How to make the description longer and more detailed? (closed)
- 游릭 **#227** How to identify specific roles by fine tune BLIP Image Captioning with custom dataset (open)

## Recent Pull Requests

- 游릭 **#213** fix the wrong words (open)
- 游릭 **#210** fixed runtime device error (open)
- 游댮 **#185** Add auto-label guide (closed)
- 游릭 **#171** Update med.py: Fixed the issue of BERTEncoder.forward() not returning cross-attentions when requested (open)
- 游릭 **#162** Bump transformers from 4.15.0 to 4.30.0 (open)

## Recent Commits

- **3a29b741** Update README.md - Junnan Li (2022-09-20T04:57:15+00:00)
- **6b5ca0da** Update README.md - Junnan Li (2022-09-20T03:43:44+00:00)
- **48211a15** Update README.md - Junnan Li (2022-06-07T07:42:11+00:00)
- **2415455a** Update README.md - Junnan Li (2022-05-23T02:21:30+00:00)
- **e54c6017** Update README.md - Junnan Li (2022-03-29T07:10:05+00:00)
- **b7bb1eeb** update retrieval model script - root (2022-03-26T12:14:31+00:00)
- **17dcdd9b** Update README.md - Junnan Li (2022-03-03T00:28:05+00:00)
- **21aaf5d6** update model url - root (2022-03-02T23:59:09+00:00)
- **96a956cf** Update README.md - Junnan Li (2022-02-28T01:48:51+00:00)
- **91d740c2** Update README.md - Junnan Li (2022-02-27T01:59:57+00:00)

## External Links Found in README

- https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/ccs_synthetic_filtered.json">Download</a>|
- https://arxiv.org/abs/2201.12086">BLIP
- https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth">Download</a>|
- https://replicate.com/salesforce/blip
- https://huggingface.co/spaces/Salesforce/BLIP
- https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_flickr.pth">Download</a>
- https://replicate.com/salesforce/blip/badge
- https://github.com/rwightman/pytorch-image-models/tree/master/timm">timm</a>.
- https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_coco.pth">Download</a>
- https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth">Download</a>
- https://github.com/salesforce/LAVIS
- https://colab.research.google.com/github/salesforce/BLIP/blob/main/demo.ipynb
- https://github.com/salesforce/ALPRO,
- https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/laion_filtered.json">Download</a>|
- https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/ccs_filtered.json">Download</a>|
- https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth".
- https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/ccs_synthetic_filtered_large.json">Download</a>
- https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_14M.pth">Download</a>|
- https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth">Download</a>
- https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/laion_synthetic_filtered.json">Download</a>|

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 451691984,
  "name": "BLIP",
  "full_name": "salesforce/BLIP",
  "description": "PyTorch code for BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation  ",
  "html_url": "https://github.com/salesforce/BLIP",
  "clone_url": "https://github.com/salesforce/BLIP.git",
  "ssh_url": "git@github.com:salesforce/BLIP.git",
  "homepage": "",
  "topics": [
    "vision-language",
    "vision-and-language-pre-training",
    "image-text-retrieval",
    "image-captioning",
    "visual-question-answering",
    "visual-reasoning",
    "vision-language-transformer"
  ],
  "default_branch": "main",
  "created_at": "2022-01-25T01:19:25+00:00",
  "updated_at": "2025-06-21T19:46:36+00:00",
  "pushed_at": "2024-08-05T12:45:49+00:00",
  "size_kb": 6647,
  "watchers_count": 5327,
  "stargazers_count": 5327,
  "forks_count": 696,
  "open_issues_count": 132,
  "license": {
    "key": "bsd-3-clause",
    "name": "BSD 3-Clause \"New\" or \"Revised\" License",
    "spdx_id": "BSD-3-Clause",
    "url": "https://github.com/salesforce/BLIP/blob/main/LICENSE.txt"
  },
  "languages": {
    "Jupyter Notebook": 687080,
    "Python": 260500
  },
  "top_contributors": [
    {
      "login": "AK391",
      "contributions": 1
    }
  ],
  "file_tree_count": 52,
  "file_tree_sample": [
    {
      "path": "BLIP.gif",
      "type": "blob"
    },
    {
      "path": "CODEOWNERS",
      "type": "blob"
    },
    {
      "path": "CODE_OF_CONDUCT.md",
      "type": "blob"
    },
    {
      "path": "LICENSE.txt",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "SECURITY.md",
      "type": "blob"
    },
    {
      "path": "cog.yaml",
      "type": "blob"
    },
    {
      "path": "configs",
      "type": "tree"
    },
    {
      "path": "configs/bert_config.json",
      "type": "blob"
    },
    {
      "path": "configs/caption_coco.yaml",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 22,
  "recent_issues": [
    {
      "number": 231,
      "title": "BertLMHeadModel' object has no attribute 'generate'",
      "state": "closed"
    },
    {
      "number": 230,
      "title": "\"NotImplementedError\"",
      "state": "open"
    },
    {
      "number": 229,
      "title": "Question about blip-itm-base-flickr released at hugging face",
      "state": "open"
    },
    {
      "number": 228,
      "title": "How to make the description longer and more detailed?",
      "state": "closed"
    },
    {
      "number": 227,
      "title": "How to identify specific roles by fine tune BLIP Image Captioning with custom dataset",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 213,
      "title": "fix the wrong words",
      "state": "open"
    },
    {
      "number": 210,
      "title": "fixed runtime device error",
      "state": "open"
    },
    {
      "number": 185,
      "title": "Add auto-label guide",
      "state": "closed"
    },
    {
      "number": 171,
      "title": "Update med.py: Fixed the issue of BERTEncoder.forward() not returning cross-attentions when requested",
      "state": "open"
    },
    {
      "number": 162,
      "title": "Bump transformers from 4.15.0 to 4.30.0",
      "state": "open"
    }
  ],
  "recent_commits": [
    {
      "sha": "3a29b7410476bf5f2ba0955827390eb6ea1f4f9d",
      "author": "Junnan Li",
      "date": "2022-09-20T04:57:15+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "6b5ca0da1e2e022ecfe6d3154414f75e28f2dd28",
      "author": "Junnan Li",
      "date": "2022-09-20T03:43:44+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "48211a1594f1321b00f14c9f7a5b4813144b2fb9",
      "author": "Junnan Li",
      "date": "2022-06-07T07:42:11+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "2415455a94ae3229ca0f477ee873ffa8f4f8d987",
      "author": "Junnan Li",
      "date": "2022-05-23T02:21:30+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "e54c601766f72aa9476e4dc9376d8bec71642bd1",
      "author": "Junnan Li",
      "date": "2022-03-29T07:10:05+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "b7bb1eeb6e901044a9eb1016f408ee908b216bc7",
      "author": "root",
      "date": "2022-03-26T12:14:31+00:00",
      "message": "update retrieval model script"
    },
    {
      "sha": "17dcdd9b7ec6ef88f82dc1319b066d4c3d710f77",
      "author": "Junnan Li",
      "date": "2022-03-03T00:28:05+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "21aaf5d67bda30412e7a3060ca79a652491e0575",
      "author": "root",
      "date": "2022-03-02T23:59:09+00:00",
      "message": "update model url"
    },
    {
      "sha": "96a956cfe22ffb438ccdcc762c0804b126214fce",
      "author": "Junnan Li",
      "date": "2022-02-28T01:48:51+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "91d740c20e1d33d91fc02f6a8265f162af1719de",
      "author": "Junnan Li",
      "date": "2022-02-27T01:59:57+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "e69bd1853f7debdd314ded1f83c9c8896e8b5c8f",
      "author": "root",
      "date": "2022-02-27T01:54:08+00:00",
      "message": "Merge branch 'main' of github.com:salesforce/BLIP into main"
    },
    {
      "sha": "fe95e577d9c034fc57d76a4fba58fae641285763",
      "author": "root",
      "date": "2022-02-27T01:53:45+00:00",
      "message": "add video-text retrieval"
    },
    {
      "sha": "7be8be6757a41ded755550af163dac230b7bec4e",
      "author": "Junnan Li",
      "date": "2022-02-22T04:08:00+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "3bb0173abc455204c8f78ab6c1cc5e47a0948bfe",
      "author": "Junnan Li",
      "date": "2022-02-22T04:04:48+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "85f80572694a8579a26669dc655af78646956ba0",
      "author": "root",
      "date": "2022-02-18T03:42:19+00:00",
      "message": "Merge branch 'main' of github.com:salesforce/BLIP into main"
    },
    {
      "sha": "23af1be3ffaa16524428b64c9227ea164aaa50e4",
      "author": "root",
      "date": "2022-02-18T03:41:34+00:00",
      "message": "add bert_config"
    },
    {
      "sha": "073b821aa2a1ea9dfc56c66d7cfc17e25d755608",
      "author": "Junnan Li",
      "date": "2022-02-15T03:20:04+00:00",
      "message": "Update blip_pretrain.py"
    },
    {
      "sha": "ad5eec314c0b82005c361249acc4918d36cb0d90",
      "author": "root",
      "date": "2022-02-11T07:55:17+00:00",
      "message": "update demo"
    },
    {
      "sha": "a3a2577a2084acbdfe1a8afdc6f8b21e12c5663b",
      "author": "Junnan Li",
      "date": "2022-02-08T00:30:51+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "d10be550b2974e17ea72e74edc7948c9e5eab884",
      "author": "Junnan Li",
      "date": "2022-02-08T00:28:02+00:00",
      "message": "Merge pull request #9 from CJWBW/replicate"
    }
  ],
  "readme_text": "## BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation\n\n## Announcement: BLIP is now officially integrated into [LAVIS](https://github.com/salesforce/LAVIS) - a one-stop library for language-and-vision research and applications!\n\n<img src=\"BLIP.gif\" width=\"700\">\n\nThis is the PyTorch code of the <a href=\"https://arxiv.org/abs/2201.12086\">BLIP paper</a> [[blog](https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/)]. The code has been tested on PyTorch 1.10.\nTo install the dependencies, run <pre/>pip install -r requirements.txt</pre> \n\nCatalog:\n- [x] Inference demo\n- [x] Pre-trained and finetuned checkpoints\n- [x] Finetuning code for Image-Text Retrieval, Image Captioning, VQA, and NLVR2\n- [x] Pre-training code\n- [x] Zero-shot video-text retrieval\n- [x] Download of bootstrapped pre-training datasets \n\n\n### Inference demo:\nRun our interactive demo using [Colab notebook](https://colab.research.google.com/github/salesforce/BLIP/blob/main/demo.ipynb) (no GPU needed).\nThe demo includes code for: \n1. Image captioning\n2. Open-ended visual question answering\n3. Multimodal / unimodal feature extraction\n4. Image-text matching\n\nTry out the [Web demo](https://huggingface.co/spaces/Salesforce/BLIP), integrated into [Huggingface Spaces \ud83e\udd17](https://huggingface.co/spaces) using [Gradio](https://github.com/gradio-app/gradio). \n\nReplicate web demo and Docker image is also available at [![Replicate](https://replicate.com/salesforce/blip/badge)](https://replicate.com/salesforce/blip)\n\n### Pre-trained checkpoints:\nNum. pre-train images | BLIP w/ ViT-B | BLIP w/ ViT-B and CapFilt-L | BLIP w/ ViT-L \n--- | :---: | :---: | :---: \n14M | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_14M.pth\">Download</a>| - | -\n129M | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth\">Download</a>| <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth\">Download</a> | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large.pth\">Download</a>\n\n### Finetuned checkpoints:\nTask | BLIP w/ ViT-B | BLIP w/ ViT-B and CapFilt-L | BLIP w/ ViT-L \n--- | :---: | :---: | :---:\nImage-Text Retrieval (COCO) | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth\">Download</a>| - | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_coco.pth\">Download</a>\nImage-Text Retrieval (Flickr30k) | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_flickr.pth\">Download</a>|  - | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_flickr.pth\">Download</a>\nImage Captioning (COCO) | - | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth\">Download</a>| <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\">Download</a> | \nVQA | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_vqa.pth\">Download</a>| <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth\">Download</a> | - \nNLVR2 | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_nlvr.pth\">Download</a>| - | - \n\n\n### Image-Text Retrieval:\n1. Download COCO and Flickr30k datasets from the original websites, and set 'image_root' in configs/retrieval_{dataset}.yaml accordingly.\n2. To evaluate the finetuned BLIP model on COCO, run:\n<pre>python -m torch.distributed.run --nproc_per_node=8 train_retrieval.py \\\n--config ./configs/retrieval_coco.yaml \\\n--output_dir output/retrieval_coco \\\n--evaluate</pre> \n3. To finetune the pre-trained checkpoint using 8 A100 GPUs, first set 'pretrained' in configs/retrieval_coco.yaml as \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth\". Then run:\n<pre>python -m torch.distributed.run --nproc_per_node=8 train_retrieval.py \\\n--config ./configs/retrieval_coco.yaml \\\n--output_dir output/retrieval_coco </pre> \n\n### Image-Text Captioning:\n1. Download COCO and NoCaps datasets from the original websites, and set 'image_root' in configs/caption_coco.yaml and configs/nocaps.yaml accordingly.\n2. To evaluate the finetuned BLIP model on COCO, run:\n<pre>python -m torch.distributed.run --nproc_per_node=8 train_caption.py --evaluate</pre> \n3. To evaluate the finetuned BLIP model on NoCaps, generate results with: (evaluation needs to be performed on official server)\n<pre>python -m torch.distributed.run --nproc_per_node=8 eval_nocaps.py </pre> \n4. To finetune the pre-trained checkpoint using 8 A100 GPUs, first set 'pretrained' in configs/caption_coco.yaml as \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth\". Then run:\n<pre>python -m torch.distributed.run --nproc_per_node=8 train_caption.py </pre> \n\n### VQA:\n1. Download VQA v2 dataset and Visual Genome dataset from the original websites, and set 'vqa_root' and 'vg_root' in configs/vqa.yaml.\n2. To evaluate the finetuned BLIP model, generate results with: (evaluation needs to be performed on official server)\n<pre>python -m torch.distributed.run --nproc_per_node=8 train_vqa.py --evaluate</pre> \n3. To finetune the pre-trained checkpoint using 16 A100 GPUs, first set 'pretrained' in configs/vqa.yaml as \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth\". Then run:\n<pre>python -m torch.distributed.run --nproc_per_node=16 train_vqa.py </pre> \n\n### NLVR2:\n1. Download NLVR2 dataset from the original websites, and set 'image_root' in configs/nlvr.yaml.\n2. To evaluate the finetuned BLIP model, run\n<pre>python -m torch.distributed.run --nproc_per_node=8 train_nlvr.py --evaluate</pre> \n3. To finetune the pre-trained checkpoint using 16 A100 GPUs, first set 'pretrained' in configs/nlvr.yaml as \"https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth\". Then run:\n<pre>python -m torch.distributed.run --nproc_per_node=16 train_nlvr.py </pre> \n\n### Finetune with ViT-L:\nIn order to finetune a model with ViT-L, simply change the config file to set 'vit' as large. Batch size and learning rate may also need to be adjusted accordingly (please see the paper's appendix for hyper-parameter details). <a href=\"https://github.com/facebookresearch/fairscale\">Gradient checkpoint</a> can also be activated in the config file to reduce GPU memory usage. \n\n### Pre-train:\n1. Prepare training json files where each json file contains a list. Each item in the list is a dictonary with two key-value pairs: {'image': path_of_image, 'caption': text_of_image}. \n2. In configs/pretrain.yaml, set 'train_file' as the paths for the json files .\n3. Pre-train the model using 8 A100 GPUs:\n<pre>python -m torch.distributed.run --nproc_per_node=8 pretrain.py --config ./configs/Pretrain.yaml --output_dir output/Pretrain </pre> \n\n### Zero-shot video-text retrieval:\n1. Download MSRVTT dataset following the instructions from https://github.com/salesforce/ALPRO, and set 'video_root' accordingly in configs/retrieval_msrvtt.yaml.\n2. Install [decord](https://github.com/dmlc/decord) with <pre>pip install decord</pre> \n3. To perform zero-shot evaluation, run\n<pre>python -m torch.distributed.run --nproc_per_node=8 eval_retrieval_video.py</pre> \n\n### Pre-training datasets download:\nWe provide bootstrapped pre-training datasets as json files. Each json file contains a list. Each item in the list is a dictonary with two key-value pairs: {'url': url_of_image, 'caption': text_of_image}. \n\nImage source | Filtered web caption | Filtered synthetic caption by ViT-B | Filtered synthetic caption by ViT-L\n--- | :---: | :---: | :---:\nCC3M+CC12M+SBU |  <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/ccs_filtered.json\">Download</a>|  <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/ccs_synthetic_filtered.json\">Download</a>|  <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/ccs_synthetic_filtered_large.json\">Download</a>\nLAION115M | <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/laion_filtered.json\">Download</a>|  <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/laion_synthetic_filtered.json\">Download</a>|  <a href=\"https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/laion_synthetic_filtered_large.json\">Download</a>\n\n### Citation\nIf you find this code to be useful for your research, please consider citing.\n<pre>\n@inproceedings{li2022blip,\n      title={BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation}, \n      author={Junnan Li and Dongxu Li and Caiming Xiong and Steven Hoi},\n      year={2022},\n      booktitle={ICML},\n}</pre>\n\n### Acknowledgement\nThe implementation of BLIP relies on resources from <a href=\"https://github.com/salesforce/ALBEF\">ALBEF</a>, <a href=\"https://github.com/huggingface/transformers\">Huggingface Transformers</a>, and <a href=\"https://github.com/rwightman/pytorch-image-models/tree/master/timm\">timm</a>. We thank the original authors for their open-sourcing.\n",
  "external_links_in_readme": [
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/ccs_synthetic_filtered.json\">Download</a>|",
    "https://arxiv.org/abs/2201.12086\">BLIP",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth\">Download</a>|",
    "https://replicate.com/salesforce/blip",
    "https://huggingface.co/spaces/Salesforce/BLIP",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_flickr.pth\">Download</a>",
    "https://replicate.com/salesforce/blip/badge",
    "https://github.com/rwightman/pytorch-image-models/tree/master/timm\">timm</a>.",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_retrieval_coco.pth\">Download</a>",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth\">Download</a>",
    "https://github.com/salesforce/LAVIS",
    "https://colab.research.google.com/github/salesforce/BLIP/blob/main/demo.ipynb",
    "https://github.com/salesforce/ALPRO,",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/laion_filtered.json\">Download</a>|",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/ccs_filtered.json\">Download</a>|",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_capfilt_large.pth\".",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/ccs_synthetic_filtered_large.json\">Download</a>",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_14M.pth\">Download</a>|",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_vqa_capfilt_large.pth\">Download</a>",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/laion_synthetic_filtered.json\">Download</a>|",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_vqa.pth\">Download</a>|",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/datasets/laion_synthetic_filtered_large.json\">Download</a>",
    "https://github.com/huggingface/transformers\">Huggingface",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_nlvr.pth\">Download</a>|",
    "https://github.com/facebookresearch/fairscale\">Gradient",
    "https://github.com/salesforce/ALBEF\">ALBEF</a>,",
    "https://huggingface.co/spaces",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_flickr.pth\">Download</a>|",
    "https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_retrieval_coco.pth\">Download</a>|",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large.pth\">Download</a>",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base.pth\".",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth\">Download</a>|",
    "https://github.com/gradio-app/gradio",
    "https://github.com/dmlc/decord",
    "https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\">Download</a>"
  ]
}
```

</details>


---

