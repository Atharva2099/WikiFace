# GitHub Data for OpenGVLab_InternVL3-78B

**Task Category:** Image-Text-to-Text

## Repository 1: InternLM/xtuner

# GitHub Repository Data

**Repository:** [InternLM/xtuner](https://github.com/InternLM/xtuner)

## Basic Information

- **Description:** An efficient, flexible and full-featured toolkit for fine-tuning LLM (InternLM2, Llama3, Phi3, Qwen, Mistral, ...)
- **Created:** 2023-07-11T03:18:13+00:00
- **Last Updated:** 2025-06-21T07:15:50+00:00
- **Last Pushed:** 2025-05-29T15:17:57+00:00
- **Default Branch:** main
- **Size:** 2241 KB

## Statistics

- **Stars:** 4,607
- **Forks:** 350
- **Watchers:** 4,607
- **Open Issues:** 249
- **Total Issues:** 0
- **Pull Requests:** 453

## License

- **Type:** Apache License 2.0
- **SPDX ID:** Apache-2.0
- **URL:** [License](https://github.com/InternLM/xtuner/blob/main/LICENSE)

## Languages

- **Python:** 5,741,471 bytes

## Topics

- `baichuan`
- `chatglm2`
- `internlm`
- `large-language-models`
- `llama2`
- `llm`
- `llm-training`
- `peft`
- `qwen`
- `chatbot`
- `conversational-ai`
- `supervised-finetuning`
- `agent`
- `chatglm3`
- `msagent`
- `llava`
- `mixtral`
- `llama3`
- `phi3`

## Top Contributors

1. **LZHgrla** - 160 contributions
2. **HIT-cwh** - 107 contributions
3. **pppppM** - 21 contributions
4. **RangiLyu** - 5 contributions
5. **xiaohangguo** - 5 contributions
6. **hhaAndroid** - 3 contributions
7. **LDLINGLINGLING** - 3 contributions
8. **tpoisonooo** - 2 contributions
9. **humu789** - 2 contributions
10. **fanqiNO1** - 2 contributions

## File Structure (Sample of 10 files)

Total files: 1,265

- `.github` (tree)
- `.github/CONTRIBUTING.md` (blob)
- `.github/workflows` (tree)
- `.github/workflows/deploy.yml` (blob)
- `.github/workflows/lint.yml` (blob)
- `.gitignore` (blob)
- `.owners.yml` (blob)
- `.pre-commit-config.yaml` (blob)
- `LICENSE` (blob)
- `MANIFEST.in` (blob)

## Recent Issues

- ðŸŸ¢ **#1036** è®­ç»ƒæ•°æ® (open)
- ðŸŸ¢ **#1035** è®­ç»ƒå¡æ­» (open)
- ðŸ”´ **#1034** resume or load_from Error (closed)
- ðŸŸ¢ **#1033** docs(README.md): add GraphGen for data preparation (open)
- ðŸŸ¢ **#1032** how can I use Xtuner in Cuda12.8? (open)

## Recent Pull Requests

- ðŸŸ¢ **#1033** docs(README.md): add GraphGen for data preparation (open)
- ðŸ”´ **#1028** Fix gmm (closed)
- ðŸ”´ **#1027** Update jsonl dataset (closed)
- ðŸ”´ **#1026** support global average (closed)
- ðŸ”´ **#1024** Remove torchao (closed)

## Recent Commits

- **53f2429d** Update README.md - pppppM (2025-02-21T15:28:43+00:00)
- **bbcd94bb** bump version to v0.2.0rc0 (#990) - pppppM (2025-02-21T15:16:43+00:00)
- **4cade9f5** [Fix]MLU Device Mesh (#987) - pppppM (2025-01-22T13:00:43+00:00)
- **4a521fb5** [Feature] Auto patch for different devices (#986) - pppppM (2025-01-21T10:16:51+00:00)
- **4ee82150** [Feature]Support transformers==4.48 (#985) - whcao (2025-01-14T11:04:48+00:00)
- **2c06115f** Add Ascend NPU as a backend (#983) - Tonyztj (2025-01-14T02:58:44+00:00)
- **90192ffe** Add functionality to download models from sources other than HuggingFace (#946) - starmountain1997 (2024-11-08T07:04:01+00:00)
- **697bc77e** å¯¹Minicpm3è¿›è¡Œäº†æ”¯æŒ (#954) - é˜¿ä¸¹(adan) (2024-10-22T12:11:38+00:00)
- **4a1b2013** [Bugs] fix qlora convert bugs (#930) - whcao (2024-09-29T02:45:26+00:00)
- **081c8ca8** Add internlm2 5 cfgs (#872) - whcao (2024-08-09T10:01:01+00:00)

## External Links Found in README

- https://huggingface.co/datasets/HuggingFaceH4/CodeAlpaca_20K">Code
- https://img.shields.io/badge/-grey?style=social&logo=twitter&label=Twitter
- https://img.shields.io/badge/-grey?style=social&logo=discord&label=Discord
- https://github.com/microsoft/DeepSpeed
- https://arxiv.org/abs/2403.07691">ORPO</a></li>
- https://huggingface.co/THUDM/chatglm3-6b">ChatGLM3</a></li>
- https://img.shields.io/github/issues-closed-raw/InternLM/xtuner
- https://huggingface.co/mistralai
- https://www.modelscope.cn/organization/xtuner
- https://img.shields.io/badge/-gery?style=social&label=ðŸ¤—%20Huggingface
- https://github.com/InternLM/lmdeploy/tree/main#quantization
- https://github.com/InternLM/xtuner}},
- https://huggingface.co/openbmb">MiniCPM</a></li>
- https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1
- https://huggingface.co/xtuner/llava-llama-3-8b
- https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3">Phi-3</a></li>
- https://github.com/LiuHC0428/LAW-GPT">Chinese
- https://github.com/InternLM/xtuner/blob/docs/docs/zh_cn/acceleration/benchmark.rst
- https://huggingface.co/datasets/shibing624/medical">Medical
- https://modelscope.cn/datasets/damo/MSAgent-Bench

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 664913876,
  "name": "xtuner",
  "full_name": "InternLM/xtuner",
  "description": "An efficient, flexible and full-featured toolkit for fine-tuning LLM (InternLM2, Llama3, Phi3, Qwen, Mistral, ...)",
  "html_url": "https://github.com/InternLM/xtuner",
  "clone_url": "https://github.com/InternLM/xtuner.git",
  "ssh_url": "git@github.com:InternLM/xtuner.git",
  "homepage": "https://xtuner.readthedocs.io/zh-cn/latest/",
  "topics": [
    "baichuan",
    "chatglm2",
    "internlm",
    "large-language-models",
    "llama2",
    "llm",
    "llm-training",
    "peft",
    "qwen",
    "chatbot",
    "conversational-ai",
    "supervised-finetuning",
    "agent",
    "chatglm3",
    "msagent",
    "llava",
    "mixtral",
    "llama3",
    "phi3"
  ],
  "default_branch": "main",
  "created_at": "2023-07-11T03:18:13+00:00",
  "updated_at": "2025-06-21T07:15:50+00:00",
  "pushed_at": "2025-05-29T15:17:57+00:00",
  "size_kb": 2241,
  "watchers_count": 4607,
  "stargazers_count": 4607,
  "forks_count": 350,
  "open_issues_count": 249,
  "license": {
    "key": "apache-2.0",
    "name": "Apache License 2.0",
    "spdx_id": "Apache-2.0",
    "url": "https://github.com/InternLM/xtuner/blob/main/LICENSE"
  },
  "languages": {
    "Python": 5741471
  },
  "top_contributors": [
    {
      "login": "LZHgrla",
      "contributions": 160
    },
    {
      "login": "HIT-cwh",
      "contributions": 107
    },
    {
      "login": "pppppM",
      "contributions": 21
    },
    {
      "login": "RangiLyu",
      "contributions": 5
    },
    {
      "login": "xiaohangguo",
      "contributions": 5
    },
    {
      "login": "hhaAndroid",
      "contributions": 3
    },
    {
      "login": "LDLINGLINGLING",
      "contributions": 3
    },
    {
      "login": "tpoisonooo",
      "contributions": 2
    },
    {
      "login": "humu789",
      "contributions": 2
    },
    {
      "login": "fanqiNO1",
      "contributions": 2
    },
    {
      "login": "amulil",
      "contributions": 2
    },
    {
      "login": "xu-song",
      "contributions": 2
    },
    {
      "login": "KooSung",
      "contributions": 2
    },
    {
      "login": "KMnO4-zx",
      "contributions": 1
    },
    {
      "login": "aJupyter",
      "contributions": 1
    },
    {
      "login": "starmountain1997",
      "contributions": 1
    },
    {
      "login": "ooooo-create",
      "contributions": 1
    },
    {
      "login": "maxchiron",
      "contributions": 1
    },
    {
      "login": "gzlong96",
      "contributions": 1
    },
    {
      "login": "del-zhenwu",
      "contributions": 1
    }
  ],
  "file_tree_count": 1265,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/CONTRIBUTING.md",
      "type": "blob"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/deploy.yml",
      "type": "blob"
    },
    {
      "path": ".github/workflows/lint.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": ".owners.yml",
      "type": "blob"
    },
    {
      "path": ".pre-commit-config.yaml",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "MANIFEST.in",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 453,
  "recent_issues": [
    {
      "number": 1036,
      "title": "\u8bad\u7ec3\u6570\u636e",
      "state": "open"
    },
    {
      "number": 1035,
      "title": "\u8bad\u7ec3\u5361\u6b7b",
      "state": "open"
    },
    {
      "number": 1034,
      "title": "resume or load_from Error",
      "state": "closed"
    },
    {
      "number": 1033,
      "title": "docs(README.md): add GraphGen for data preparation",
      "state": "open"
    },
    {
      "number": 1032,
      "title": "how can I use Xtuner in Cuda12.8?",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1033,
      "title": "docs(README.md): add GraphGen for data preparation",
      "state": "open"
    },
    {
      "number": 1028,
      "title": "Fix gmm",
      "state": "closed"
    },
    {
      "number": 1027,
      "title": "Update jsonl dataset",
      "state": "closed"
    },
    {
      "number": 1026,
      "title": "support global average",
      "state": "closed"
    },
    {
      "number": 1024,
      "title": "Remove torchao",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "53f2429d8a4662c04a8a4a2dc5c941672f4d3bdd",
      "author": "pppppM",
      "date": "2025-02-21T15:28:43+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "bbcd94bb0b6922d25fd8b2e1158fcd5076e225e9",
      "author": "pppppM",
      "date": "2025-02-21T15:16:43+00:00",
      "message": "bump version to v0.2.0rc0 (#990)"
    },
    {
      "sha": "4cade9f547cbdf1ff5bada1c8a7f4a15a01c49ef",
      "author": "pppppM",
      "date": "2025-01-22T13:00:43+00:00",
      "message": "[Fix]MLU Device Mesh (#987)"
    },
    {
      "sha": "4a521fb5ffeb5f017d8b93efd9fb4cca4253cc44",
      "author": "pppppM",
      "date": "2025-01-21T10:16:51+00:00",
      "message": "[Feature] Auto patch for different devices (#986)"
    },
    {
      "sha": "4ee821501cbd09d2c19d3bf3c6854591b95b910f",
      "author": "whcao",
      "date": "2025-01-14T11:04:48+00:00",
      "message": "[Feature]Support transformers==4.48 (#985)"
    },
    {
      "sha": "2c06115f6e4e7c3ed5525b30e2020452b57246cd",
      "author": "Tonyztj",
      "date": "2025-01-14T02:58:44+00:00",
      "message": "Add Ascend NPU as a backend (#983)"
    },
    {
      "sha": "90192ffe42612b0f88409432e7b4860294432bcc",
      "author": "starmountain1997",
      "date": "2024-11-08T07:04:01+00:00",
      "message": "Add functionality to download models from sources other than HuggingFace (#946)"
    },
    {
      "sha": "697bc77eeb3770d932b93bfd6a7d3535fa2bff6f",
      "author": "\u963f\u4e39(adan)",
      "date": "2024-10-22T12:11:38+00:00",
      "message": "\u5bf9Minicpm3\u8fdb\u884c\u4e86\u652f\u6301 (#954)"
    },
    {
      "sha": "4a1b201370e4b462e180950e926f7b092335e29c",
      "author": "whcao",
      "date": "2024-09-29T02:45:26+00:00",
      "message": "[Bugs] fix qlora convert bugs (#930)"
    },
    {
      "sha": "081c8ca874bdbf7a7f8cd0a9e4cba503eaaa7bba",
      "author": "whcao",
      "date": "2024-08-09T10:01:01+00:00",
      "message": "Add internlm2 5 cfgs (#872)"
    },
    {
      "sha": "d81b366d5bd5795d5892b0c9ae42ef229ac719f4",
      "author": "whcao",
      "date": "2024-08-09T10:00:34+00:00",
      "message": "support transformers >= 4.43 (#878)"
    },
    {
      "sha": "7dd779b3b4c5a7fb8e00f7511c1635b8ff33979b",
      "author": "whcao",
      "date": "2024-08-09T09:59:54+00:00",
      "message": "[Bug] fix openai_map_fn bugs (#885)"
    },
    {
      "sha": "01640b00790453de39a63b5019fdd14f7cc6e4d8",
      "author": "whcao",
      "date": "2024-07-31T10:54:56+00:00",
      "message": "[Bug] fix dsv2 attn dispatch (softmax_scale) (#873)"
    },
    {
      "sha": "d2a173a284d5969e80669c6e59dffd055d2570f3",
      "author": "LDLINGLINGLING",
      "date": "2024-07-29T08:29:27+00:00",
      "message": "readme\u4e2d\u589e\u52a0\u4e86MiniCPM\u7684\u652f\u6301 (#869)"
    },
    {
      "sha": "94a4fcb0f53d63ceb85334bb456c073251669ccb",
      "author": "whcao",
      "date": "2024-07-22T12:17:44+00:00",
      "message": "bump version to 0.1.23 (#862)"
    },
    {
      "sha": "9444b3423d3f9b0a409e879eb24d06a4a190a9b5",
      "author": "whcao",
      "date": "2024-07-22T06:46:06+00:00",
      "message": "[Bug] fix preference_collate_fn attn_mask (#859)"
    },
    {
      "sha": "f30ad4c4302573f64e703bcf2aa12de3fce98f75",
      "author": "Haian Huang(\u6df1\u5ea6\u7738)",
      "date": "2024-07-22T06:45:09+00:00",
      "message": "Support InternVL 1.5/2.0 finetune (#737)"
    },
    {
      "sha": "5a93e7d7518d347a9c381ce261fa4390a4385b7b",
      "author": "whcao",
      "date": "2024-07-19T09:50:31+00:00",
      "message": "bump version to 0.1.22 (#855)"
    },
    {
      "sha": "f49ac9895336ff8f6936b91ab04725020b5c53b3",
      "author": "whcao",
      "date": "2024-07-19T09:50:12+00:00",
      "message": "fix lint (#856)"
    },
    {
      "sha": "27cf856135ef9db63ec96257c1cb8caf4b87d329",
      "author": "LDLINGLINGLING",
      "date": "2024-07-19T09:14:55+00:00",
      "message": "Added minicpm config file to support sft\u3001qlora\u3001lora\u3001dpo (#847)"
    }
  ],
  "readme_text": "<div align=\"center\">\n  <img src=\"https://github.com/InternLM/lmdeploy/assets/36994684/0cf8d00f-e86b-40ba-9b54-dc8f1bc6c8d8\" width=\"600\"/>\n  <br /><br />\n\n[![GitHub Repo stars](https://img.shields.io/github/stars/InternLM/xtuner?style=social)](https://github.com/InternLM/xtuner/stargazers)\n[![license](https://img.shields.io/github/license/InternLM/xtuner.svg)](https://github.com/InternLM/xtuner/blob/main/LICENSE)\n[![PyPI](https://img.shields.io/pypi/v/xtuner)](https://pypi.org/project/xtuner/)\n[![Downloads](https://static.pepy.tech/badge/xtuner)](https://pypi.org/project/xtuner/)\n[![issue resolution](https://img.shields.io/github/issues-closed-raw/InternLM/xtuner)](https://github.com/InternLM/xtuner/issues)\n[![open issues](https://img.shields.io/github/issues-raw/InternLM/xtuner)](https://github.com/InternLM/xtuner/issues)\n\n\ud83d\udc4b join us on [![Static Badge](https://img.shields.io/badge/-grey?style=social&logo=wechat&label=WeChat)](https://cdn.vansin.top/internlm/xtuner.jpg)\n[![Static Badge](https://img.shields.io/badge/-grey?style=social&logo=twitter&label=Twitter)](https://twitter.com/intern_lm)\n[![Static Badge](https://img.shields.io/badge/-grey?style=social&logo=discord&label=Discord)](https://discord.gg/xa29JuW87d)\n\n\ud83d\udd0d Explore our models on\n[![Static Badge](https://img.shields.io/badge/-gery?style=social&label=\ud83e\udd17%20Huggingface)](https://huggingface.co/xtuner)\n[![Static Badge](https://img.shields.io/badge/-gery?style=social&label=\ud83e\udd16%20ModelScope)](https://www.modelscope.cn/organization/xtuner)\n[![Static Badge](https://img.shields.io/badge/-gery?style=social&label=\ud83e\uddf0%20OpenXLab)](https://openxlab.org.cn/usercenter/xtuner)\n[![Static Badge](https://img.shields.io/badge/-gery?style=social&label=\ud83e\udde0%20WiseModel)](https://www.wisemodel.cn/organization/xtuner)\n\nEnglish | [\u7b80\u4f53\u4e2d\u6587](README_zh-CN.md)\n\n</div>\n\n## \ud83d\ude80 Speed Benchmark\n\n- Llama2 7B Training Speed\n\n<div align=center>\n  <img src=\"https://github.com/InternLM/xtuner/assets/41630003/9c9dfdf4-1efb-4daf-84bf-7c379ae40b8b\" style=\"width:80%\">\n</div>\n\n- Llama2 70B Training Speed\n\n<div align=center>\n  <img src=\"https://github.com/InternLM/xtuner/assets/41630003/5ba973b8-8885-4b72-b51b-c69fa1583bdd\" style=\"width:80%\">\n</div>\n\n## \ud83c\udf89 News\n- **\\[2025/02\\]** Support [OREAL](https://github.com/InternLM/OREAL), a new RL method for math reasoning!\n- **\\[2025/01\\]** Support [InternLM3 8B Instruct](https://huggingface.co/internlm/internlm3-8b-instruct)!\n- **\\[2024/07\\]** Support [MiniCPM](xtuner/configs/minicpm/) models!\n- **\\[2024/07\\]** Support [DPO](https://github.com/InternLM/xtuner/tree/main/xtuner/configs/dpo), [ORPO](https://github.com/InternLM/xtuner/tree/main/xtuner/configs/orpo) and [Reward Model](https://github.com/InternLM/xtuner/tree/main/xtuner/configs/reward_model) training with packed data and sequence parallel! See [documents](https://xtuner.readthedocs.io/en/latest/dpo/overview.html) for more details.\n- **\\[2024/07\\]** Support [InternLM 2.5](xtuner/configs/internlm/internlm2_5_chat_7b/) models!\n- **\\[2024/06\\]** Support [DeepSeek V2](xtuner/configs/deepseek/deepseek_v2_chat/) models! **2x faster!**\n- **\\[2024/04\\]** [LLaVA-Phi-3-mini](https://huggingface.co/xtuner/llava-phi-3-mini-hf) is released! Click [here](xtuner/configs/llava/phi3_mini_4k_instruct_clip_vit_large_p14_336) for details!\n- **\\[2024/04\\]** [LLaVA-Llama-3-8B](https://huggingface.co/xtuner/llava-llama-3-8b) and [LLaVA-Llama-3-8B-v1.1](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1) are released! Click [here](xtuner/configs/llava/llama3_8b_instruct_clip_vit_large_p14_336) for details!\n- **\\[2024/04\\]** Support [Llama 3](xtuner/configs/llama) models!\n- **\\[2024/04\\]** Support Sequence Parallel for enabling highly efficient and scalable LLM training with extremely long sequence lengths! \\[[Usage](https://github.com/InternLM/xtuner/blob/docs/docs/zh_cn/acceleration/train_extreme_long_sequence.rst)\\] \\[[Speed Benchmark](https://github.com/InternLM/xtuner/blob/docs/docs/zh_cn/acceleration/benchmark.rst)\\]\n- **\\[2024/02\\]** Support [Gemma](xtuner/configs/gemma) models!\n- **\\[2024/02\\]** Support [Qwen1.5](xtuner/configs/qwen/qwen1_5) models!\n- **\\[2024/01\\]** Support [InternLM2](xtuner/configs/internlm) models! The latest VLM [LLaVA-Internlm2-7B](https://huggingface.co/xtuner/llava-internlm2-7b) / [20B](https://huggingface.co/xtuner/llava-internlm2-20b) models are released, with impressive performance!\n- **\\[2024/01\\]** Support [DeepSeek-MoE](https://huggingface.co/deepseek-ai/deepseek-moe-16b-chat) models! 20GB GPU memory is enough for QLoRA fine-tuning, and 4x80GB for full-parameter fine-tuning. Click [here](xtuner/configs/deepseek/) for details!\n- **\\[2023/12\\]** \ud83d\udd25 Support multi-modal VLM pretraining and fine-tuning with [LLaVA-v1.5](https://github.com/haotian-liu/LLaVA) architecture! Click [here](xtuner/configs/llava/README.md) for details!\n- **\\[2023/12\\]** \ud83d\udd25 Support [Mixtral 8x7B](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1) models! Click [here](xtuner/configs/mixtral/README.md) for details!\n- **\\[2023/11\\]** Support [ChatGLM3-6B](xtuner/configs/chatglm) model!\n- **\\[2023/10\\]** Support [MSAgent-Bench](https://modelscope.cn/datasets/damo/MSAgent-Bench) dataset, and the fine-tuned LLMs can be applied by [Lagent](https://github.com/InternLM/lagent)!\n- **\\[2023/10\\]** Optimize the data processing to accommodate `system` context. More information can be found on [Docs](docs/en/user_guides/dataset_format.md)!\n- **\\[2023/09\\]** Support [InternLM-20B](xtuner/configs/internlm) models!\n- **\\[2023/09\\]** Support [Baichuan2](xtuner/configs/baichuan) models!\n- **\\[2023/08\\]** XTuner is released, with multiple fine-tuned adapters on [Hugging Face](https://huggingface.co/xtuner).\n\n## \ud83d\udcd6 Introduction\n\nXTuner is an efficient, flexible and full-featured toolkit for fine-tuning large models.\n\n**Efficient**\n\n- Support LLM, VLM pre-training / fine-tuning on almost all GPUs. XTuner is capable of fine-tuning 7B LLM on a single 8GB GPU, as well as multi-node fine-tuning of models exceeding 70B.\n- Automatically dispatch high-performance operators such as FlashAttention and Triton kernels to increase training throughput.\n- Compatible with [DeepSpeed](https://github.com/microsoft/DeepSpeed) \ud83d\ude80, easily utilizing a variety of ZeRO optimization techniques.\n\n**Flexible**\n\n- Support various LLMs ([InternLM](https://huggingface.co/internlm), [Mixtral-8x7B](https://huggingface.co/mistralai), [Llama 2](https://huggingface.co/meta-llama), [ChatGLM](https://huggingface.co/THUDM), [Qwen](https://huggingface.co/Qwen), [Baichuan](https://huggingface.co/baichuan-inc), ...).\n- Support VLM ([LLaVA](https://github.com/haotian-liu/LLaVA)). The performance of [LLaVA-InternLM2-20B](https://huggingface.co/xtuner/llava-internlm2-20b) is outstanding.\n- Well-designed data pipeline, accommodating datasets in any format, including but not limited to open-source and custom formats.\n- Support various training algorithms ([QLoRA](http://arxiv.org/abs/2305.14314), [LoRA](http://arxiv.org/abs/2106.09685), full-parameter fune-tune), allowing users to choose the most suitable solution for their requirements.\n\n**Full-featured**\n\n- Support continuous pre-training, instruction fine-tuning, and agent fine-tuning.\n- Support chatting with large models with pre-defined templates.\n- The output models can seamlessly integrate with deployment and server toolkit ([LMDeploy](https://github.com/InternLM/lmdeploy)), and large-scale evaluation toolkit ([OpenCompass](https://github.com/open-compass/opencompass), [VLMEvalKit](https://github.com/open-compass/VLMEvalKit)).\n\n## \ud83d\udd25 Supports\n\n<table>\n<tbody>\n<tr align=\"center\" valign=\"middle\">\n<td>\n  <b>Models</b>\n</td>\n<td>\n  <b>SFT Datasets</b>\n</td>\n<td>\n  <b>Data Pipelines</b>\n</td>\n <td>\n  <b>Algorithms</b>\n</td>\n</tr>\n<tr valign=\"top\">\n<td align=\"left\" valign=\"top\">\n<ul>\n  <li><a href=\"https://huggingface.co/internlm\">InternLM2 / 2.5</a></li>\n  <li><a href=\"https://huggingface.co/meta-llama\">Llama 2 / 3</a></li>\n  <li><a href=\"https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3\">Phi-3</a></li>\n  <li><a href=\"https://huggingface.co/THUDM/chatglm2-6b\">ChatGLM2</a></li>\n  <li><a href=\"https://huggingface.co/THUDM/chatglm3-6b\">ChatGLM3</a></li>\n  <li><a href=\"https://huggingface.co/Qwen/Qwen-7B\">Qwen</a></li>\n  <li><a href=\"https://huggingface.co/baichuan-inc/Baichuan2-7B-Base\">Baichuan2</a></li>\n  <li><a href=\"https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\">Mixtral</a></li>\n  <li><a href=\"https://huggingface.co/deepseek-ai/DeepSeek-V2-Chat\">DeepSeek V2</a></li>\n  <li><a href=\"https://huggingface.co/google\">Gemma</a></li>\n  <li><a href=\"https://huggingface.co/openbmb\">MiniCPM</a></li>\n  <li>...</li>\n</ul>\n</td>\n<td>\n<ul>\n  <li><a href=\"https://modelscope.cn/datasets/damo/MSAgent-Bench\">MSAgent-Bench</a></li>\n  <li><a href=\"https://huggingface.co/datasets/fnlp/moss-003-sft-data\">MOSS-003-SFT</a> \ud83d\udd27</li>\n  <li><a href=\"https://huggingface.co/datasets/tatsu-lab/alpaca\">Alpaca en</a> / <a href=\"https://huggingface.co/datasets/silk-road/alpaca-data-gpt4-chinese\">zh</a></li>\n  <li><a href=\"https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k\">WizardLM</a></li>\n  <li><a href=\"https://huggingface.co/datasets/timdettmers/openassistant-guanaco\">oasst1</a></li>\n  <li><a href=\"https://huggingface.co/datasets/garage-bAInd/Open-Platypus\">Open-Platypus</a></li>\n  <li><a href=\"https://huggingface.co/datasets/HuggingFaceH4/CodeAlpaca_20K\">Code Alpaca</a></li>\n  <li><a href=\"https://huggingface.co/datasets/burkelibbey/colors\">Colorist</a> \ud83c\udfa8</li>\n  <li><a href=\"https://github.com/WangRongsheng/ChatGenTitle\">Arxiv GenTitle</a></li>\n  <li><a href=\"https://github.com/LiuHC0428/LAW-GPT\">Chinese Law</a></li>\n  <li><a href=\"https://huggingface.co/datasets/Open-Orca/OpenOrca\">OpenOrca</a></li>\n  <li><a href=\"https://huggingface.co/datasets/shibing624/medical\">Medical Dialogue</a></li>\n  <li>...</li>\n</ul>\n</td>\n<td>\n<ul>\n  <li><a href=\"docs/zh_cn/user_guides/incremental_pretraining.md\">Incremental Pre-training</a> </li>\n  <li><a href=\"docs/zh_cn/user_guides/single_turn_conversation.md\">Single-turn Conversation SFT</a> </li>\n  <li><a href=\"docs/zh_cn/user_guides/multi_turn_conversation.md\">Multi-turn Conversation SFT</a> </li>\n</ul>\n</td>\n<td>\n<ul>\n  <li><a href=\"http://arxiv.org/abs/2305.14314\">QLoRA</a></li>\n  <li><a href=\"http://arxiv.org/abs/2106.09685\">LoRA</a></li>\n  <li>Full parameter fine-tune</li>\n  <li><a href=\"https://arxiv.org/abs/2305.18290\">DPO</a></li>\n  <li><a href=\"https://arxiv.org/abs/2403.07691\">ORPO</a></li>\n  <li>Reward Model</a></li>\n</ul>\n</td>\n</tr>\n</tbody>\n</table>\n\n## \ud83d\udee0\ufe0f Quick Start\n\n### Installation\n\n- It is recommended to build a Python-3.10 virtual environment using conda\n\n  ```bash\n  conda create --name xtuner-env python=3.10 -y\n  conda activate xtuner-env\n  ```\n\n- Install XTuner via pip\n\n  ```shell\n  pip install -U xtuner\n  ```\n\n  or with DeepSpeed integration\n\n  ```shell\n  pip install -U 'xtuner[deepspeed]'\n  ```\n\n- Install XTuner from source\n\n  ```shell\n  git clone https://github.com/InternLM/xtuner.git\n  cd xtuner\n  pip install -e '.[all]'\n  ```\n\n### Fine-tune\n\nXTuner supports the efficient fine-tune (*e.g.*, QLoRA) for LLMs. Dataset prepare guides can be found on [dataset_prepare.md](./docs/en/user_guides/dataset_prepare.md).\n\n- **Step 0**, prepare the config. XTuner provides many ready-to-use configs and we can view all configs by\n\n  ```shell\n  xtuner list-cfg\n  ```\n\n  Or, if the provided configs cannot meet the requirements, please copy the provided config to the specified directory and make specific modifications by\n\n  ```shell\n  xtuner copy-cfg ${CONFIG_NAME} ${SAVE_PATH}\n  vi ${SAVE_PATH}/${CONFIG_NAME}_copy.py\n  ```\n\n- **Step 1**, start fine-tuning.\n\n  ```shell\n  xtuner train ${CONFIG_NAME_OR_PATH}\n  ```\n\n  For example, we can start the QLoRA fine-tuning of InternLM2.5-Chat-7B with oasst1 dataset by\n\n  ```shell\n  # On a single GPU\n  xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2\n  # On multiple GPUs\n  (DIST) NPROC_PER_NODE=${GPU_NUM} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --deepspeed deepspeed_zero2\n  (SLURM) srun ${SRUN_ARGS} xtuner train internlm2_5_chat_7b_qlora_oasst1_e3 --launcher slurm --deepspeed deepspeed_zero2\n  ```\n\n  - `--deepspeed` means using [DeepSpeed](https://github.com/microsoft/DeepSpeed) \ud83d\ude80 to optimize the training. XTuner comes with several integrated strategies including ZeRO-1, ZeRO-2, and ZeRO-3. If you wish to disable this feature, simply remove this argument.\n\n  - For more examples, please see [finetune.md](./docs/en/user_guides/finetune.md).\n\n- **Step 2**, convert the saved PTH model (if using DeepSpeed, it will be a directory) to Hugging Face model, by\n\n  ```shell\n  xtuner convert pth_to_hf ${CONFIG_NAME_OR_PATH} ${PTH} ${SAVE_PATH}\n  ```\n\n### Chat\n\nXTuner provides tools to chat with pretrained / fine-tuned LLMs.\n\n```shell\nxtuner chat ${NAME_OR_PATH_TO_LLM} --adapter {NAME_OR_PATH_TO_ADAPTER} [optional arguments]\n```\n\nFor example, we can start the chat with InternLM2.5-Chat-7B :\n\n```shell\nxtuner chat internlm/internlm2_5-chat-7b --prompt-template internlm2_chat\n```\n\nFor more examples, please see [chat.md](./docs/en/user_guides/chat.md).\n\n### Deployment\n\n- **Step 0**, merge the Hugging Face adapter to pretrained LLM, by\n\n  ```shell\n  xtuner convert merge \\\n      ${NAME_OR_PATH_TO_LLM} \\\n      ${NAME_OR_PATH_TO_ADAPTER} \\\n      ${SAVE_PATH} \\\n      --max-shard-size 2GB\n  ```\n\n- **Step 1**, deploy fine-tuned LLM with any other framework, such as [LMDeploy](https://github.com/InternLM/lmdeploy) \ud83d\ude80.\n\n  ```shell\n  pip install lmdeploy\n  python -m lmdeploy.pytorch.chat ${NAME_OR_PATH_TO_LLM} \\\n      --max_new_tokens 256 \\\n      --temperture 0.8 \\\n      --top_p 0.95 \\\n      --seed 0\n  ```\n\n  \ud83d\udd25 Seeking efficient inference with less GPU memory? Try 4-bit quantization from [LMDeploy](https://github.com/InternLM/lmdeploy)! For more details, see [here](https://github.com/InternLM/lmdeploy/tree/main#quantization).\n\n### Evaluation\n\n- We recommend using [OpenCompass](https://github.com/InternLM/opencompass), a comprehensive and systematic LLM evaluation library, which currently supports 50+ datasets with about 300,000 questions.\n\n## \ud83e\udd1d Contributing\n\nWe appreciate all contributions to XTuner. Please refer to [CONTRIBUTING.md](.github/CONTRIBUTING.md) for the contributing guideline.\n\n## \ud83c\udf96\ufe0f Acknowledgement\n\n- [Llama 2](https://github.com/facebookresearch/llama)\n- [DeepSpeed](https://github.com/microsoft/DeepSpeed)\n- [QLoRA](https://github.com/artidoro/qlora)\n- [LMDeploy](https://github.com/InternLM/lmdeploy)\n- [LLaVA](https://github.com/haotian-liu/LLaVA)\n\n## \ud83d\udd8a\ufe0f Citation\n\n```bibtex\n@misc{2023xtuner,\n    title={XTuner: A Toolkit for Efficiently Fine-tuning LLM},\n    author={XTuner Contributors},\n    howpublished = {\\url{https://github.com/InternLM/xtuner}},\n    year={2023}\n}\n```\n\n## License\n\nThis project is released under the [Apache License 2.0](LICENSE). Please also adhere to the Licenses of models and datasets being used.\n",
  "external_links_in_readme": [
    "https://huggingface.co/datasets/HuggingFaceH4/CodeAlpaca_20K\">Code",
    "https://img.shields.io/badge/-grey?style=social&logo=twitter&label=Twitter",
    "https://img.shields.io/badge/-grey?style=social&logo=discord&label=Discord",
    "https://github.com/microsoft/DeepSpeed",
    "https://arxiv.org/abs/2403.07691\">ORPO</a></li>",
    "https://huggingface.co/THUDM/chatglm3-6b\">ChatGLM3</a></li>",
    "https://img.shields.io/github/issues-closed-raw/InternLM/xtuner",
    "https://huggingface.co/mistralai",
    "https://www.modelscope.cn/organization/xtuner",
    "https://img.shields.io/badge/-gery?style=social&label=\ud83e\udd17%20Huggingface",
    "https://github.com/InternLM/lmdeploy/tree/main#quantization",
    "https://github.com/InternLM/xtuner}},",
    "https://huggingface.co/openbmb\">MiniCPM</a></li>",
    "https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1",
    "https://huggingface.co/xtuner/llava-llama-3-8b",
    "https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3\">Phi-3</a></li>",
    "https://github.com/LiuHC0428/LAW-GPT\">Chinese",
    "https://github.com/InternLM/xtuner/blob/docs/docs/zh_cn/acceleration/benchmark.rst",
    "https://huggingface.co/datasets/shibing624/medical\">Medical",
    "https://modelscope.cn/datasets/damo/MSAgent-Bench",
    "https://github.com/open-compass/VLMEvalKit",
    "https://huggingface.co/xtuner/llava-phi-3-mini-hf",
    "https://huggingface.co/internlm\">InternLM2",
    "https://img.shields.io/badge/-gery?style=social&label=\ud83e\udd16%20ModelScope",
    "https://huggingface.co/internlm/internlm3-8b-instruct",
    "http://arxiv.org/abs/2106.09685",
    "https://huggingface.co/THUDM/chatglm2-6b\">ChatGLM2</a></li>",
    "https://huggingface.co/Qwen",
    "https://huggingface.co/baichuan-inc/Baichuan2-7B-Base\">Baichuan2</a></li>",
    "https://github.com/InternLM/xtuner/stargazers",
    "https://huggingface.co/datasets/silk-road/alpaca-data-gpt4-chinese\">zh</a></li>",
    "https://github.com/InternLM/xtuner/assets/41630003/5ba973b8-8885-4b72-b51b-c69fa1583bdd\"",
    "https://github.com/open-compass/opencompass",
    "https://img.shields.io/github/stars/InternLM/xtuner?style=social",
    "https://huggingface.co/Qwen/Qwen-7B\">Qwen</a></li>",
    "https://huggingface.co/datasets/timdettmers/openassistant-guanaco\">oasst1</a></li>",
    "https://img.shields.io/github/issues-raw/InternLM/xtuner",
    "https://huggingface.co/deepseek-ai/deepseek-moe-16b-chat",
    "https://huggingface.co/datasets/fnlp/moss-003-sft-data\">MOSS-003-SFT</a>",
    "https://github.com/InternLM/lagent",
    "http://arxiv.org/abs/2305.14314",
    "https://github.com/facebookresearch/llama",
    "https://huggingface.co/xtuner",
    "https://github.com/InternLM/lmdeploy/assets/36994684/0cf8d00f-e86b-40ba-9b54-dc8f1bc6c8d8\"",
    "https://pypi.org/project/xtuner/",
    "https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k\">WizardLM</a></li>",
    "https://github.com/InternLM/xtuner/blob/docs/docs/zh_cn/acceleration/train_extreme_long_sequence.rst",
    "https://huggingface.co/datasets/burkelibbey/colors\">Colorist</a>",
    "https://img.shields.io/badge/-grey?style=social&logo=wechat&label=WeChat",
    "https://huggingface.co/baichuan-inc",
    "https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\">Mixtral</a></li>",
    "https://img.shields.io/pypi/v/xtuner",
    "https://github.com/InternLM/xtuner/tree/main/xtuner/configs/reward_model",
    "https://huggingface.co/datasets/garage-bAInd/Open-Platypus\">Open-Platypus</a></li>",
    "https://github.com/InternLM/xtuner.git",
    "https://huggingface.co/meta-llama\">Llama",
    "https://github.com/InternLM/xtuner/tree/main/xtuner/configs/orpo",
    "http://arxiv.org/abs/2305.14314\">QLoRA</a></li>",
    "https://github.com/InternLM/lmdeploy",
    "https://static.pepy.tech/badge/xtuner",
    "https://github.com/haotian-liu/LLaVA",
    "https://modelscope.cn/datasets/damo/MSAgent-Bench\">MSAgent-Bench</a></li>",
    "https://img.shields.io/badge/-gery?style=social&label=\ud83e\uddf0%20OpenXLab",
    "https://twitter.com/intern_lm",
    "https://huggingface.co/meta-llama",
    "http://arxiv.org/abs/2106.09685\">LoRA</a></li>",
    "https://xtuner.readthedocs.io/en/latest/dpo/overview.html",
    "https://huggingface.co/xtuner/llava-internlm2-7b",
    "https://github.com/InternLM/xtuner/tree/main/xtuner/configs/dpo",
    "https://huggingface.co/deepseek-ai/DeepSeek-V2-Chat\">DeepSeek",
    "https://huggingface.co/google\">Gemma</a></li>",
    "https://www.wisemodel.cn/organization/xtuner",
    "https://github.com/artidoro/qlora",
    "https://img.shields.io/github/license/InternLM/xtuner.svg",
    "https://img.shields.io/badge/-gery?style=social&label=\ud83e\udde0%20WiseModel",
    "https://huggingface.co/THUDM",
    "https://discord.gg/xa29JuW87d",
    "https://github.com/InternLM/xtuner/blob/main/LICENSE",
    "https://huggingface.co/xtuner/llava-llama-3-8b-v1_1",
    "https://github.com/InternLM/OREAL",
    "https://huggingface.co/xtuner/llava-internlm2-20b",
    "https://cdn.vansin.top/internlm/xtuner.jpg",
    "https://arxiv.org/abs/2305.18290\">DPO</a></li>",
    "https://huggingface.co/internlm",
    "https://github.com/InternLM/xtuner/issues",
    "https://github.com/InternLM/opencompass",
    "https://openxlab.org.cn/usercenter/xtuner",
    "https://huggingface.co/datasets/tatsu-lab/alpaca\">Alpaca",
    "https://github.com/InternLM/xtuner/assets/41630003/9c9dfdf4-1efb-4daf-84bf-7c379ae40b8b\"",
    "https://huggingface.co/datasets/Open-Orca/OpenOrca\">OpenOrca</a></li>",
    "https://github.com/WangRongsheng/ChatGenTitle\">Arxiv"
  ]
}
```

</details>


---

## Repository 2: OpenGVLab/InternVL

# GitHub Repository Data

**Repository:** [OpenGVLab/InternVL](https://github.com/OpenGVLab/InternVL)

## Basic Information

- **Description:** [CVPR 2024 Oral] InternVL Family: A Pioneering Open-Source Alternative to GPT-4o.  æŽ¥è¿‘GPT-4oè¡¨çŽ°çš„å¼€æºå¤šæ¨¡æ€å¯¹è¯æ¨¡åž‹
- **Created:** 2023-11-22T08:08:08+00:00
- **Last Updated:** 2025-06-21T20:37:14+00:00
- **Last Pushed:** 2025-05-29T16:21:15+00:00
- **Default Branch:** main
- **Size:** 40355 KB

## Statistics

- **Stars:** 8,390
- **Forks:** 643
- **Watchers:** 8,390
- **Open Issues:** 221
- **Total Issues:** 0
- **Pull Requests:** 81

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/OpenGVLab/InternVL/blob/main/LICENSE)

## Languages

- **Python:** 2,732,522 bytes
- **Jupyter Notebook:** 2,353,994 bytes
- **Shell:** 341,911 bytes
- **JavaScript:** 9,991 bytes
- **HTML:** 7,669 bytes
- **Makefile:** 2,480 bytes
- **CSS:** 1,822 bytes

## Topics

- `image-classification`
- `image-text-retrieval`
- `llm`
- `semantic-segmentation`
- `video-classification`
- `vision-language-model`
- `vit-22b`
- `vit-6b`
- `multi-modal`
- `gpt`
- `gpt-4v`
- `gpt-4o`

## Top Contributors

1. **czczup** - 171 contributions
2. **Weiyun1025** - 21 contributions
3. **whai362** - 20 contributions
4. **G-z-w** - 11 contributions
5. **ErfeiCui** - 4 contributions
6. **hjh0119** - 2 contributions
7. **Adushar** - 1 contributions
8. **lvhan028** - 1 contributions
9. **vishwamartur** - 1 contributions
10. **qishisuren123** - 1 contributions

## File Structure (Sample of 10 files)

Total files: 1,010

- `.flake8` (blob)
- `.github` (tree)
- `.github/CONTRIBUTING.md` (blob)
- `.github/ISSUE_TEMPLATE` (tree)
- `.github/ISSUE_TEMPLATE/1-bug-report.yml` (blob)
- `.github/ISSUE_TEMPLATE/2-feature-request.yml` (blob)
- `.github/ISSUE_TEMPLATE/3-documentation.yml` (blob)
- `.gitignore` (blob)
- `.isort.cfg` (blob)
- `.pre-commit-config.yaml` (blob)

## Recent Issues

- ðŸŸ¢ **#1080** [Bug] Inference InternVL3 with Transformers | ç”¨å®˜æ–¹ç¤ºä¾‹ä»£ç çš„transformersæŽ¨ç†InternVL3å¤±è´¥ (open)
- ðŸŸ¢ **#1079** Issue on page /internvl3.0/deployment.html (open)
- ðŸŸ¢ **#1078** [Feature] Does the vl2.5 lora finetuning scripts support vl3 ? (open)
- ðŸŸ¢ **#1077** internvl2.5-8B å…³äºŽchartqaçš„æ€§èƒ½ (open)
- ðŸŸ¢ **#1076** ä¸ºä»€ä¹ˆ Intern3 æ²¡æœ‰ 4B çš„æ¨¡åž‹ï¼Ÿ (open)

## Recent Pull Requests

- ðŸ”´ **#1075** `internvl_chat` depends on `flash-attn` (closed)
- ðŸ”´ **#1074** Update dependencies (closed)
- ðŸ”´ **#1052** merge from ccx (closed)
- ðŸ”´ **#1051** Add InternVL3 shell (closed)
- ðŸŸ¢ **#1044** removed redundant code (open)

## Recent Commits

- **d779db3b** Add InternVL3 shell (#1051) - ErfeiCui (2025-05-29T16:21:15+00:00)
- **dd363520** Sync device with language_model to prevent multi-GPU errors (#975) - zodymm (2025-04-27T14:09:46+00:00)
- **0dd08cdd** Update README and open-source scripts (#999) - WeiyunWang (2025-04-17T18:12:38+00:00)
- **19b0da2d** update mpo training scripts and mpo/visualprm data pipeline (#986) - Zhe Chen (2025-04-11T16:30:07+00:00)
- **34a81000** update README (#959) - WeiyunWang (2025-03-20T09:36:28+00:00)
- **9d3a709b** Update API document (#929) - Zhe Chen (2025-02-26T08:05:24+00:00)
- **2d57e218** Update InternVL 2.5 README (#798) - Zhe Chen (2024-12-25T15:18:05+00:00)
- **e8dd6f8f** Update internvl_chat.txt (#795) - Zhe Chen (2024-12-25T13:00:54+00:00)
- **db06f8b9** Update README.md (#794) - WeiyunWang (2024-12-24T14:15:11+00:00)
- **aeab5814** Fix multi GPU inference on V100 (#789) - Vishwanath Martur (2024-12-24T05:53:05+00:00)

## External Links Found in README

- https://github.com/openai/CLIP
- https://github.com/PaddlePaddle/PaddleMIX
- https://internvl.readthedocs.io/en/latest/tutorials/coco_caption_finetune.html
- https://internvl.readthedocs.io/en/latest/get_started/chat_data_format.html#single-image-data
- https://huggingface.co/internlm/internlm2_5-20b-chat">internlm2_5-20b-chat</a></td>
- https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2-Plus">ðŸ¤—
- https://github.com/huggingface/transformers
- https://huggingface.co/papers/2412.09616
- https://huggingface.co/OpenGVLab/InternVL2_5-1B">ðŸ¤—
- https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5">ðŸ¤—
- https://github.com/BradyFU/Video-MME
- https://internvl.readthedocs.io/en/latest/get_started/chat_data_format.html#multi-image-data
- https://modelscope.cn/models/OpenGVLab/InternVL3-38B">ðŸ¤–
- https://huggingface.co/OpenGVLab/InternVL2_5-78B
- https://internvl.readthedocs.io/en/latest/internvl1.0/internvl_g.html
- https://internvl.opengvlab.com/
- https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1
- https://huggingface.co/OpenGVLab/InternViT-300M-448px">ðŸ¤—
- https://github.com/mulanai/MuLan
- https://github.com/salesforce/LAVIS/tree/main/projects/blip2

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 721995615,
  "name": "InternVL",
  "full_name": "OpenGVLab/InternVL",
  "description": "[CVPR 2024 Oral] InternVL Family: A Pioneering Open-Source Alternative to GPT-4o.  \u63a5\u8fd1GPT-4o\u8868\u73b0\u7684\u5f00\u6e90\u591a\u6a21\u6001\u5bf9\u8bdd\u6a21\u578b",
  "html_url": "https://github.com/OpenGVLab/InternVL",
  "clone_url": "https://github.com/OpenGVLab/InternVL.git",
  "ssh_url": "git@github.com:OpenGVLab/InternVL.git",
  "homepage": "https://internvl.readthedocs.io/en/latest/",
  "topics": [
    "image-classification",
    "image-text-retrieval",
    "llm",
    "semantic-segmentation",
    "video-classification",
    "vision-language-model",
    "vit-22b",
    "vit-6b",
    "multi-modal",
    "gpt",
    "gpt-4v",
    "gpt-4o"
  ],
  "default_branch": "main",
  "created_at": "2023-11-22T08:08:08+00:00",
  "updated_at": "2025-06-21T20:37:14+00:00",
  "pushed_at": "2025-05-29T16:21:15+00:00",
  "size_kb": 40355,
  "watchers_count": 8390,
  "stargazers_count": 8390,
  "forks_count": 643,
  "open_issues_count": 221,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/OpenGVLab/InternVL/blob/main/LICENSE"
  },
  "languages": {
    "Python": 2732522,
    "Jupyter Notebook": 2353994,
    "Shell": 341911,
    "JavaScript": 9991,
    "HTML": 7669,
    "Makefile": 2480,
    "CSS": 1822
  },
  "top_contributors": [
    {
      "login": "czczup",
      "contributions": 171
    },
    {
      "login": "Weiyun1025",
      "contributions": 21
    },
    {
      "login": "whai362",
      "contributions": 20
    },
    {
      "login": "G-z-w",
      "contributions": 11
    },
    {
      "login": "ErfeiCui",
      "contributions": 4
    },
    {
      "login": "hjh0119",
      "contributions": 2
    },
    {
      "login": "Adushar",
      "contributions": 1
    },
    {
      "login": "lvhan028",
      "contributions": 1
    },
    {
      "login": "vishwamartur",
      "contributions": 1
    },
    {
      "login": "qishisuren123",
      "contributions": 1
    },
    {
      "login": "dlutwy",
      "contributions": 1
    },
    {
      "login": "luyao-cv",
      "contributions": 1
    },
    {
      "login": "opengvlab-admin",
      "contributions": 1
    },
    {
      "login": "vansin",
      "contributions": 1
    },
    {
      "login": "zodymm",
      "contributions": 1
    }
  ],
  "file_tree_count": 1010,
  "file_tree_sample": [
    {
      "path": ".flake8",
      "type": "blob"
    },
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/CONTRIBUTING.md",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/1-bug-report.yml",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/2-feature-request.yml",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/3-documentation.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": ".isort.cfg",
      "type": "blob"
    },
    {
      "path": ".pre-commit-config.yaml",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 81,
  "recent_issues": [
    {
      "number": 1080,
      "title": "[Bug] Inference InternVL3 with Transformers | \u7528\u5b98\u65b9\u793a\u4f8b\u4ee3\u7801\u7684transformers\u63a8\u7406InternVL3\u5931\u8d25",
      "state": "open"
    },
    {
      "number": 1079,
      "title": "Issue on page /internvl3.0/deployment.html",
      "state": "open"
    },
    {
      "number": 1078,
      "title": "[Feature] Does the vl2.5 lora finetuning scripts support vl3 ?",
      "state": "open"
    },
    {
      "number": 1077,
      "title": "internvl2.5-8B \u5173\u4e8echartqa\u7684\u6027\u80fd",
      "state": "open"
    },
    {
      "number": 1076,
      "title": "\u4e3a\u4ec0\u4e48 Intern3 \u6ca1\u6709 4B \u7684\u6a21\u578b\uff1f",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1075,
      "title": "`internvl_chat` depends on `flash-attn`",
      "state": "closed"
    },
    {
      "number": 1074,
      "title": "Update dependencies",
      "state": "closed"
    },
    {
      "number": 1052,
      "title": "merge from ccx",
      "state": "closed"
    },
    {
      "number": 1051,
      "title": "Add InternVL3 shell",
      "state": "closed"
    },
    {
      "number": 1044,
      "title": "removed redundant code",
      "state": "open"
    }
  ],
  "recent_commits": [
    {
      "sha": "d779db3b0581859753069c4113f69d367eff799b",
      "author": "ErfeiCui",
      "date": "2025-05-29T16:21:15+00:00",
      "message": "Add InternVL3 shell (#1051)"
    },
    {
      "sha": "dd3635206874c92386185d586fffeda1026d3a76",
      "author": "zodymm",
      "date": "2025-04-27T14:09:46+00:00",
      "message": "Sync device with language_model to prevent multi-GPU errors (#975)"
    },
    {
      "sha": "0dd08cdd1a7606de0a0492d6e3d17070ce8c6c4f",
      "author": "WeiyunWang",
      "date": "2025-04-17T18:12:38+00:00",
      "message": "Update README and open-source scripts (#999)"
    },
    {
      "sha": "19b0da2d1986fedc406c8dd02ffc2969fcfc4443",
      "author": "Zhe Chen",
      "date": "2025-04-11T16:30:07+00:00",
      "message": "update mpo training scripts and mpo/visualprm data pipeline (#986)"
    },
    {
      "sha": "34a81000402bf8f716bab8c9b57aff1f6b436bd0",
      "author": "WeiyunWang",
      "date": "2025-03-20T09:36:28+00:00",
      "message": "update README (#959)"
    },
    {
      "sha": "9d3a709b16874e73ffdd38b9cf53296fae4589b9",
      "author": "Zhe Chen",
      "date": "2025-02-26T08:05:24+00:00",
      "message": "Update API document (#929)"
    },
    {
      "sha": "2d57e2181dad4c1c3c50f8da8a23600d4ee19fda",
      "author": "Zhe Chen",
      "date": "2024-12-25T15:18:05+00:00",
      "message": "Update InternVL 2.5 README (#798)"
    },
    {
      "sha": "e8dd6f8f473afee01d23f67b1f8be3d757c79aa7",
      "author": "Zhe Chen",
      "date": "2024-12-25T13:00:54+00:00",
      "message": "Update internvl_chat.txt (#795)"
    },
    {
      "sha": "db06f8b9ae99305c8b2a4d3c9820dc7232d8bade",
      "author": "WeiyunWang",
      "date": "2024-12-24T14:15:11+00:00",
      "message": "Update README.md (#794)"
    },
    {
      "sha": "aeab58141b2b0d4cf98bede792ee616f4aff7196",
      "author": "Vishwanath Martur",
      "date": "2024-12-24T05:53:05+00:00",
      "message": "Fix multi GPU inference on V100 (#789)"
    },
    {
      "sha": "94be99c1004fb6c29350410339a5d99707b32de1",
      "author": "zhe chen",
      "date": "2024-12-21T14:11:47+00:00",
      "message": "Reformat code"
    },
    {
      "sha": "b048980c1aa5f2230f7405bf08765c70032a589c",
      "author": "WeiyunWang",
      "date": "2024-12-21T14:09:00+00:00",
      "message": "Update InternVL2.5-MPO (#784)"
    },
    {
      "sha": "a0fe6ed31435a70a136e9aa7414674b26ce81a1f",
      "author": "luyao-cv",
      "date": "2024-12-19T03:12:34+00:00",
      "message": "InternVL2/2.5 is supported in PaddleMIX by Paddle Team (#766)"
    },
    {
      "sha": "dc2bdc6861533ee138ffca4ded68c78558a3e85f",
      "author": "zhe chen",
      "date": "2024-12-17T17:21:18+00:00",
      "message": "Release InternVL 2.5 training scripts"
    },
    {
      "sha": "22839cef8aa46925b45db2d3d6f72f63d237bb0f",
      "author": "zhe chen",
      "date": "2024-12-17T14:01:37+00:00",
      "message": "Update InternVL 2.5 training code"
    },
    {
      "sha": "80fbfa32096ec08a217e5137c74706577f1b6b3a",
      "author": "zhe chen",
      "date": "2024-12-17T13:57:49+00:00",
      "message": "Update InternVL 2.5 evaluation code"
    },
    {
      "sha": "869d3be88d40d79162ca23b1ff5380d657883b55",
      "author": "vansin",
      "date": "2024-12-10T09:11:53+00:00",
      "message": "docs: update assistant qrcode to live chatroom qrcode (#745)"
    },
    {
      "sha": "d77b75ce2f724db23b427e5b1c0c4d43b2807ee9",
      "author": "zhe chen",
      "date": "2024-12-09T12:41:44+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "28096c88487b543c327eb57a99c26b3ff14b7539",
      "author": "zhe chen",
      "date": "2024-12-09T07:09:13+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "dab6160927ca2ef12dfe78d7632ca3cab13af00d",
      "author": "Zhe Chen",
      "date": "2024-12-06T11:36:27+00:00",
      "message": "Release InternVL2.5 report"
    }
  ],
  "readme_text": "<div align=\"center\">\n\n# InternVL Family: Closing the Gap to Commercial Multimodal Models with Open-Source Suites \u2014\u2014 A Pioneering Open-Source Alternative to GPT-4o\n\n<div align=\"center\">\n  <img width=\"500\" alt=\"image\" src=\"https://github.com/user-attachments/assets/930e6814-8a9f-43e1-a284-118a5732daa4\">\n  <br>\n</div>\n\n[\\[\ud83c\udd95 Blog\\]](https://internvl.github.io/blog/)  [\\[\ud83e\udd14 FAQs\\]](https://internvl.readthedocs.io/en/latest/tutorials/faqs.html)   [\\[\ud83d\udde8\ufe0f Chat Demo\\]](https://internvl.opengvlab.com/)  [\\[\ud83e\udd17 HF Demo\\]](https://huggingface.co/spaces/OpenGVLab/InternVL)  [\\[\ud83d\udcd6 Document\\]](https://internvl.readthedocs.io/en/latest/)  [\\[\ud83c\udf10 API\\]](https://internlm.intern-ai.org.cn/api/document)  [\\[\ud83d\ude80 Quick Start\\]](#quick-start-with-huggingface)\n\n[\\[\ud83d\udd25 InternVL3.0 Report\\]](https://huggingface.co/papers/2504.10479) [\\[\ud83d\udd25 InternVL2.5 MPO\\]](https://huggingface.co/papers/2411.10442) [\\[\ud83d\udd25 InternVL2.5 Report\\]](https://huggingface.co/papers/2412.05271) [\\[Mini-InternVL Paper\\]](https://arxiv.org/abs/2410.16261) [\\[InternVL2 Blog\\]](https://internvl.github.io/blog/2024-07-02-InternVL-2.0/)  [\\[\ud83d\udcdc InternVL 1.5 Paper\\]](https://huggingface.co/papers/2404.16821)  [\\[\ud83d\udcdc InternVL 1.0 Paper\\]](https://huggingface.co/papers/2312.14238)\n\n[\\[\ud83d\udcd6 2.0 \u4e2d\u6587\u89e3\u8bfb\\]](https://zhuanlan.zhihu.com/p/706547971)  [\\[\ud83d\udcd6 1.5 \u4e2d\u6587\u89e3\u8bfb\\]](https://zhuanlan.zhihu.com/p/699439759)  [\\[\ud83d\udcd6 1.0 \u4e2d\u6587\u89e3\u8bfb\\]](https://zhuanlan.zhihu.com/p/702946079)\n\n[Switch to the Chinese version (\u5207\u6362\u81f3\u4e2d\u6587\u7248)](/README_zh.md)\n\n<a href=\"https://trendshift.io/repositories/9803\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/9803\" alt=\"OpenGVLab%2FInternVL | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n<img height=\"55\" alt=\"image\" src=\"https://github.com/user-attachments/assets/bd62ab46-f0ea-40c6-ab10-7fde671716cc\">\n\n![image/png](https://huggingface.co/datasets/Weiyun1025/InternVL-Performance/resolve/main/internvl3/overall.png)\n\n</div>\n\n## News \ud83d\ude80\ud83d\ude80\ud83d\ude80\n\n- `2025/04/17`: We open-source the [data construction pipeline](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/tools/reasoning_data_pipeline) and [training scripts](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl3.0/mpo) of [MPO](https://huggingface.co/papers/2411.10442) and [VisualPRM](https://huggingface.co/papers/2503.10291). Additionally, the data construction scripts for [MPO](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl3.0/mpo_data_construction) and [VisualPRM](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl3.0/visualprm_data_construction) are also released for reference.\n- `2025/04/11`: \ud83d\ude80 We introduce [InternVL3](https://huggingface.co/collections/OpenGVLab/internvl3-67f7f690be79c2fe9d74fe9d), an advanced multimodal large language model (MLLM) series that demonstrates superior overall performance. InternVL3-78B achieves SoTA performance in both [perception](https://rank.opencompass.org.cn/leaderboard-multimodal/?m=REALTIME) and [reasoning performance](https://rank.opencompass.org.cn/leaderboard-multimodal-reasoning/?m=REALTIME) among open-source MLLMs. The key designs of InternVL3-78B include [Variable Visual Position Encoding](https://huggingface.co/papers/2412.09616), [Native Multimodal Pre-Training](https://huggingface.co/papers/2504.10479), [Mixed Preference Optimization](https://huggingface.co/papers/2411.10442), and [Multimodal Test-Time Scaling](https://huggingface.co/papers/2503.10291).\n- `2025/03/13`: \ud83d\udd25 We introduce [VisualPRM](https://huggingface.co/OpenGVLab/VisualPRM-8B), an advanced multimodal Process Reward Model (PRM) with 8B parameters, which improves the overall reasoning performance of InternVL2.5-8B and InternVL2.5-78B by 8.4 and 5.9 points, respectively. The training data for this model, termed [VisualPRM400K](https://huggingface.co/datasets/OpenGVLab/VisualPRM400K), is also open-sourced. Please refer to our [paper](https://huggingface.co/papers/2503.10291) and [project page](https://internvl.github.io/blog/2025-03-13-VisualPRM/) for more details.\n- `2024/12/20`: \ud83d\udd25 We release the [InternVL2.5-MPO](https://internvl.github.io/blog/2024-12-20-InternVL-2.5-MPO/), which is finetuned with [Mixed Preference Optimization](https://huggingface.co/papers/2411.10442) on [MMPR-v1.1](https://huggingface.co/datasets/OpenGVLab/MMPR-v1.1). **The resulting models outperform their counterparts without MPO by an average of 2 points across all model scales on the OpenCompass leaderboard.** These models are available at [HF link](https://huggingface.co/collections/OpenGVLab/internvl25-mpo-6753fed98cd828219b12f849).\n- `2024/12/17`: \ud83d\ude80 [InternVL2/2.5](https://github.com/PaddlePaddle/PaddleMIX/tree/develop/paddlemix/examples/internvl2) is supported in [PaddleMIX](https://github.com/PaddlePaddle/PaddleMIX) by Paddle Team.\n- `2024/12/05`: \ud83d\ude80 We release the [InternVL2.5](https://huggingface.co/collections/OpenGVLab/internvl-25-673e1019b66e2218f68d7c1c), an advanced multimodal large language model (MLLM) series with parameter coverage ranging from 1B to 78B. [InternVL2_5-78B](https://huggingface.co/OpenGVLab/InternVL2_5-78B) is the first open-source MLLMs to achieve over **70%** on the **MMMU benchmark**, matching the performance of leading closed-source commercial models like GPT-4o. These models are available at [HF link](https://huggingface.co/collections/OpenGVLab/internvl-25-673e1019b66e2218f68d7c1c).\n- `2024/11/14`: We introduce [MMPR](https://huggingface.co/datasets/OpenGVLab/MMPR), a high-quality, large-scale multimodal reasoning preference dataset, and [MPO](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl2.0_mpo), an effective preference optimization algorithm. The resulting model, [InternVL2-8B-MPO](https://huggingface.co/OpenGVLab/InternVL2-8B-MPO), achieves an accuracy of 67.0 on MathVista. Please refer to our [paper](https://arxiv.org/abs/2411.10442), [project page](https://internvl.github.io/blog/2024-11-14-InternVL-2.0-MPO/) and [document](https://internvl.readthedocs.io/en/latest/internvl2.0/preference_optimization.html) for more details.\n- `2024/10/21`: We release the Mini-InternVL series. These models achieve impressive performance with minimal size: the 4B model achieves 90% of the performance with just 5% of the model size. For more details, please check our [project page](https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/mini_internvl) and [document](https://internvl.readthedocs.io/en/latest/internvl2.0/domain_adaptation.html).\n- `2024/08/01`: The [Chartmimic](https://chartmimic.github.io/) team evaluated the InternVL2 series models on their benchmark. The InternVL2-26B and 76B models achieved the top two performances among open-source models, with the InternVL2 76B model surpassing GeminiProVision and exhibiting comparable results to Claude-3-opus.\n- `2024/08/01`: InternVL2-Pro achieved the SOTA performance among open-source models on the [CharXiv](https://charxiv.github.io/#leaderboard) dataset, surpassing many closed-source models such as GPT-4V, Gemini 1.5 Flash, and Claude 3 Sonnet.\n- `2024/07/24`: The [MLVU](https://github.com/JUNJIE99/MLVU) team evaluated InternVL-1.5 on their benchmark. The average performance on the multiple-choice task was 50.4%, while the performance on the generative tasks was 4.02. The performance on the multiple-choice task ranked #1 among all open-source MLLMs.\n- `2024/07/04`: We release the [InternVL2 series](https://huggingface.co/collections/OpenGVLab/internvl-20-667d3961ab5eb12c7ed1463e). InternVL2-Pro achieved a 62.0% accuracy on the MMMU benchmark, matching the performance of leading closed-source commercial models like GPT-4o.\n\n<details>\n<summary>More News</summary>\n\n- `2024/07/18`: InternVL2-40B achieved SOTA performance among open-source models on the [Video-MME](https://github.com/BradyFU/Video-MME) dataset, scoring 61.2 when inputting 16 frames and 64.4 when inputting 32 frames. It significantly outperforms other open-source models and is the closest open-source model to GPT-4o mini.\n- `2024/07/18`: InternVL2-Pro achieved the SOTA performance on the [DocVQA](https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1) and [InfoVQA](https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=3) benchmarks.\n- `2024/06/19`: We propose Needle In A Multimodal Haystack ([MM-NIAH](https://github.com/OpenGVLab/MM-NIAH)), the first benchmark designed to systematically evaluate the capability of existing MLLMs to comprehend long multimodal documents.\n- `2024/05/30`: We release [ShareGPT-4o](https://sharegpt4o.github.io/), a large-scale dataset that we plan to open-source with 200K images, 10K videos, and 10K audios with detailed descriptions.\n- `2024/05/28`: Thanks to the [lmdeploy](https://github.com/InternLM/lmdeploy) team for providing AWQ quantization support. The 4-bit model is available at [OpenGVLab/InternVL-Chat-V1-5-AWQ](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5-AWQ).\n- `2024/05/13`: InternVL 1.0 can now be used as the [text encoder](https://huggingface.co/OpenGVLab/InternVL-14B-224px) for diffusion models to support multilingual generation natively in over 110 languages worldwide. See [MuLan](https://github.com/mulanai/MuLan) for more details.\n- `2024/04/18`: InternVL-Chat-V1-5 has been released at [HF link](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5), approaching the performance of GPT-4V and Gemini Pro on various benchmarks like MMMU, DocVQA, ChartQA, MathVista, etc.\n- `2024/02/27`: InternVL is accepted by CVPR 2024 (Oral)! \ud83c\udf89\n- `2024/02/21`: [InternVL-Chat-V1-2-Plus](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2-Plus) achieved SOTA performance on MathVista (59.9), MMBench (83.8), and MMVP (58.7). See our [blog](https://internvl.github.io/blog/2024-02-21-InternVL-1.2/) for more details.\n- `2024/02/12`: InternVL-Chat-V1-2 has been released. It achieves 51.6 on MMMU val and 82.3 on MMBench test. For more details, please refer to our [blog](https://internvl.github.io/blog/2024-02-21-InternVL-1.2/) and [SFT data](./internvl_chat#prepare-training-datasets). The model is now available on [HuggingFace](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2), and both training / evaluation data and scripts are open-sourced.\n- `2024/01/24`: InternVL-Chat-V1-1 is released, it supports Chinese and has stronger OCR capability, see [here](https://huggingface.co/OpenGVLab/InternVL-Chat-V1-1).\n- `2024/01/16`: We release our [customized mmcv/mmsegmentation/mmdetection code](https://github.com/OpenGVLab/InternVL-MMDetSeg), integrated with DeepSpeed, which can be used for training large-scale detection and segmentation models.\n\n</details>\n\n## Documents\n\n### \ud83c\udf1f **Get Started**\n\n- **Installation**: \ud83c\udf31 [Installation Guide](https://internvl.readthedocs.io/en/latest/get_started/installation.html) | \ud83d\udcc4 [requirements.txt](./requirements.txt)\n- **Chat Data Format**: \ud83d\udcdd [Meta File](https://internvl.readthedocs.io/en/latest/get_started/chat_data_format.html#meta-file) | \u270f\ufe0f [Text](https://internvl.readthedocs.io/en/latest/get_started/chat_data_format.html#pure-text-data) | \ud83d\uddbc\ufe0f [Single-Image](https://internvl.readthedocs.io/en/latest/get_started/chat_data_format.html#single-image-data) | \ud83d\uddbc\ufe0f\ud83d\uddbc\ufe0f [Multi-Image](https://internvl.readthedocs.io/en/latest/get_started/chat_data_format.html#multi-image-data) | \ud83c\udfa5 [Video](https://internvl.readthedocs.io/en/latest/get_started/chat_data_format.html#video-data)\n- **Local Chat Demo**: \ud83e\udd16 [Streamlit Demo](https://internvl.readthedocs.io/en/latest/get_started/local_chat_demo.html#streamlit-demo)\n- **InternVL-Chat API**: \ud83c\udf10 [InternVL2.5 API](https://internlm.intern-ai.org.cn/api/document)\n- **Tutorials**: \ud83d\ude80 [Enhancing InternVL2 on COCO Caption Using LoRA Fine-Tuning](https://internvl.readthedocs.io/en/latest/tutorials/coco_caption_finetune.html)\n\n### \ud83c\udfc6 **InternVL Family**\n\n- **InternVL 3.0**: \ud83d\udcd6 [Intro](https://internvl.readthedocs.io/en/latest/internvl3.0/introduction.html) | \u26a1 [Quick Start](https://internvl.readthedocs.io/en/latest/internvl3.0/quick_start.html) | \u2728 [Finetune](https://internvl.readthedocs.io/en/latest/internvl3.0/finetune.html) | \ud83d\udcca [Evaluate](https://internvl.readthedocs.io/en/latest/internvl3.0/evaluation.html) | \ud83d\udce6 [Deploy](https://internvl.readthedocs.io/en/latest/internvl3.0/deployment.html) | \ud83c\udfaf [MPO](https://internvl.readthedocs.io/en/latest/internvl3.0/preference_optimization.html)\n- **InternVL 2.5**: \ud83d\udcd6 [Intro](https://internvl.readthedocs.io/en/latest/internvl2.5/introduction.html) | \u26a1 [Quick Start](https://internvl.readthedocs.io/en/latest/internvl2.5/quick_start.html) | \u2728 [Finetune](https://internvl.readthedocs.io/en/latest/internvl2.5/finetune.html) | \ud83d\udcca [Evaluate](https://internvl.readthedocs.io/en/latest/internvl2.5/evaluation.html) | \ud83d\udce6 [Deploy](https://internvl.readthedocs.io/en/latest/internvl2.5/deployment.html) | \ud83c\udfaf [MPO](https://internvl.readthedocs.io/en/latest/internvl2.5/preference_optimization.html)\n- **InternVL 2.0**: \ud83d\udcd6 [Intro](https://internvl.readthedocs.io/en/latest/internvl2.0/introduction.html) | \u26a1 [Quick Start](https://internvl.readthedocs.io/en/latest/internvl2.0/quick_start.html) | \u2728 [Finetune](https://internvl.readthedocs.io/en/latest/internvl2.0/finetune.html) | \ud83d\udcca [Evaluate](https://internvl.readthedocs.io/en/latest/internvl2.0/evaluation.html) | \ud83d\udce6 [Deploy](https://internvl.readthedocs.io/en/latest/internvl2.0/deployment.html) | \ud83c\udfaf [MPO](https://internvl.readthedocs.io/en/latest/internvl2.0/preference_optimization.html)\n- **InternVL 1.5**: \ud83d\udcd6 [Intro](https://internvl.readthedocs.io/en/latest/internvl1.5/introduction.html) | \u26a1 [Quick Start](https://internvl.readthedocs.io/en/latest/internvl1.5/quick_start.html) | \u2728 [Finetune](https://internvl.readthedocs.io/en/latest/internvl1.5/finetune.html) | \ud83d\udcca [Evaluate](https://internvl.readthedocs.io/en/latest/internvl1.5/evaluation.html) | \ud83d\udce6 [Deploy](https://internvl.readthedocs.io/en/latest/internvl1.5/deployment.html)\n- **InternVL 1.2**: \ud83d\udcd6 [Intro](https://internvl.readthedocs.io/en/latest/internvl1.2/introduction.html) | \u26a1 [Quick Start](https://internvl.readthedocs.io/en/latest/internvl1.2/quick_start.html) | \u2728 [Finetune](https://internvl.readthedocs.io/en/latest/internvl1.2/finetune.html) | \ud83d\udcca [Evaluate](https://internvl.readthedocs.io/en/latest/internvl1.2/evaluation.html)\n- **InternVL 1.1**: \ud83d\udcd6 [Intro](https://internvl.readthedocs.io/en/latest/internvl1.1/introduction.html) | \u26a1 [Quick Start](https://internvl.readthedocs.io/en/latest/internvl1.1/quick_start.html) | \ud83d\udcca [Evaluation](https://internvl.readthedocs.io/en/latest/internvl1.1/evaluation.html)\n- **InternVL 1.0**: \ud83d\uddbc\ufe0f [Classification](https://internvl.readthedocs.io/en/latest/internvl1.0/classification.html) | \ud83d\udcca [CLIP-Benchmark](https://internvl.readthedocs.io/en/latest/internvl1.0/clip_benchmark.html) | \ud83c\udfa8 [Segmentation](https://internvl.readthedocs.io/en/latest/internvl1.0/segmentation.html) | \ud83d\udcac [Chat-LLaVA](https://internvl.readthedocs.io/en/latest/internvl1.0/internvl_chat_llava.html) | \u2728 [InternVL-G](https://internvl.readthedocs.io/en/latest/internvl1.0/internvl_g.html)\n\n## Model Zoo\n\n#### Multimodal Large Language Model (InternVL 3.0)\n<table>\n  <tr>\n    <th>Model Name</th>\n    <th>Vision Part</th>\n    <th>Language Part</th>\n    <th>HF&nbsp;Link</th>\n    <th>MS&nbsp;Link</th>\n  </tr>\n  <tr>\n    <td>InternVL3-1B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5\">InternViT&#8209;300M&#8209;448px&#8209;V2_5</a></td>\n    <td><a href=\"https://huggingface.co/Qwen/Qwen2.5-0.5B\">Qwen2.5&#8209;0.5B</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL3-1B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL3-1B\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL3-2B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5\">InternViT-300M-448px-V2_5</a></td>\n    <td><a href=\"https://huggingface.co/Qwen/Qwen2.5-1.5B\">Qwen2.5-1.5B</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL3-2B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL3-2B\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL3-8B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5\">InternViT-300M-448px-V2_5</a></td>\n    <td><a href=\"https://huggingface.co/Qwen/Qwen2.5-7B\">Qwen2.5-7B</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL3-8B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL3-8B\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL3-9B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5\">InternViT-300M-448px-V2_5</a></td>\n    <td><a href=\"https://huggingface.co/internlm/internlm3-8b-instruct\">internlm3-8b-instruct</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL3-9B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL3-9B\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL3-14B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5\">InternViT-300M-448px-V2_5</a></td>\n    <td><a href=\"https://huggingface.co/Qwen/Qwen2.5-14B\">Qwen2.5-14B</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL3-14B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL3-14B\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL3-38B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-6B-448px-V2_5\">InternViT-6B-448px-V2_5</a></td>\n    <td><a href=\"https://huggingface.co/Qwen/Qwen2.5-32B\">Qwen2.5-32B</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL3-38B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL3-38B\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL3-78B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-6B-448px-V2_5\">InternViT-6B-448px-V2_5</a></td>\n    <td><a href=\"https://huggingface.co/Qwen/Qwen2.5-72B\">Qwen2.5-72B</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL3-78B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL3-78B\">\ud83e\udd16 link</a></td>\n  </tr>\n</table>\n\n#### Multimodal Large Language Model (InternVL 2.5)\n\n<table>\n  <tr>\n    <th>Model Name</th>\n    <th>Vision Part</th>\n    <th>Language Part</th>\n    <th>HF&nbsp;Link</th>\n    <th>MS&nbsp;Link</th>\n  </tr>\n  <tr>\n    <td>InternVL2_5-1B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5\">InternViT&#8209;300M&#8209;448px&#8209;V2_5</a></td>\n    <td><a href=\"https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct\">Qwen2.5&#8209;0.5B&#8209;Instruct</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2_5-1B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2_5-1B\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL2_5-2B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5\">InternViT-300M-448px-V2_5</a></td>\n    <td><a href=\"https://huggingface.co/internlm/internlm2_5-1_8b-chat\">internlm2_5-1_8b-chat</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2_5-2B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2_5-2B\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL2_5-4B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5\">InternViT-300M-448px-V2_5</a></td>\n    <td><a href=\"https://huggingface.co/Qwen/Qwen2.5-3B-Instruct\">Qwen2.5-3B-Instruct</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2_5-4B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2_5-4B\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL2_5-8B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5\">InternViT-300M-448px-V2_5</a></td>\n    <td><a href=\"https://huggingface.co/internlm/internlm2_5-7b-chat\">internlm2_5-7b-chat</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2_5-8B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2_5-8B\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL2_5-26B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-6B-448px-V2_5\">InternViT-6B-448px-V2_5</a></td>\n    <td><a href=\"https://huggingface.co/internlm/internlm2_5-20b-chat\">internlm2_5-20b-chat</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2_5-26B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2_5-26B\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL2_5-38B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-6B-448px-V2_5\">InternViT-6B-448px-V2_5</a></td>\n    <td><a href=\"https://huggingface.co/Qwen/Qwen2.5-32B-Instruct\">Qwen2.5-32B-Instruct</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2_5-38B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2_5-38B\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL2_5-78B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-6B-448px-V2_5\">InternViT-6B-448px-V2_5</a></td>\n    <td><a href=\"https://huggingface.co/Qwen/Qwen2.5-72B-Instruct\">Qwen2.5-72B-Instruct</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2_5-78B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2_5-78B\">\ud83e\udd16 link</a></td>\n  </tr>\n</table>\n\n<table>\n  <tr>\n    <th>Model Name</th>\n    <th>Vision Part</th>\n    <th>Language Part</th>\n    <th>HF&nbsp;Link</th>\n    <th>MS&nbsp;Link</th>\n  </tr>\n  <tr>\n    <td>InternVL2_5-1B-MPO</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5\">InternViT&#8209;300M&#8209;448px&#8209;V2_5</a></td>\n    <td><a href=\"https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct\">Qwen2.5&#8209;0.5B&#8209;Instruct</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2_5-1B-MPO\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2_5-1B-MPO\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL2_5-2B-MPO</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5\">InternViT-300M-448px-V2_5</a></td>\n    <td><a href=\"https://huggingface.co/internlm/internlm2_5-1_8b-chat\">internlm2_5-1_8b-chat</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2_5-2B-MPO\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2_5-2B-MPO\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL2_5-4B-MPO</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5\">InternViT-300M-448px-V2_5</a></td>\n    <td><a href=\"https://huggingface.co/Qwen/Qwen2.5-3B-Instruct\">Qwen2.5-3B-Instruct</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2_5-4B-MPO\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2_5-4B-MPO\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL2_5-8B-MPO</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5\">InternViT-300M-448px-V2_5</a></td>\n    <td><a href=\"https://huggingface.co/internlm/internlm2_5-7b-chat\">internlm2_5-7b-chat</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2_5-8B-MPO\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2_5-8B-MPO\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL2_5-26B-MPO</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-6B-448px-V2_5\">InternViT-6B-448px-V2_5</a></td>\n    <td><a href=\"https://huggingface.co/internlm/internlm2_5-20b-chat\">internlm2_5-20b-chat</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2_5-26B-MPO\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2_5-26B-MPO\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL2_5-38B-MPO</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-6B-448px-V2_5\">InternViT-6B-448px-V2_5</a></td>\n    <td><a href=\"https://huggingface.co/Qwen/Qwen2.5-32B-Instruct\">Qwen2.5-32B-Instruct</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2_5-38B-MPO\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2_5-38B-MPO\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL2_5-78B-MPO</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-6B-448px-V2_5\">InternViT-6B-448px-V2_5</a></td>\n    <td><a href=\"https://huggingface.co/Qwen/Qwen2.5-72B-Instruct\">Qwen2.5-72B-Instruct</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2_5-78B-MPO\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2_5-78B-MPO\">\ud83e\udd16 link</a></td>\n  </tr>\n</table>\n\n#### Multimodal Large Language Model (InternVL 2.0)\n\n<table>\n  <tr>\n    <th>Model Name</th>\n    <th>Vision Part</th>\n    <th>Language Part</th>\n    <th>HF&nbsp;Link</th>\n    <th>MS&nbsp;Link</th>\n  </tr>\n  <tr>\n    <td>InternVL2-1B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px\">InternViT-300M-448px</a></td>\n    <td><a href=\"https://huggingface.co/Qwen/Qwen2-0.5B-Instruct\">Qwen2-0.5B-Instruct</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2-1B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2-1B\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL2-2B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px\">InternViT-300M-448px</a></td>\n    <td><a href=\"https://huggingface.co/internlm/internlm2-chat-1_8b\">internlm2-chat-1-8b</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2-2B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2-2B\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL2-4B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px\">InternViT-300M-448px</a></td>\n    <td><a href=\"https://huggingface.co/microsoft/Phi-3-mini-128k-instruct\">Phi&#8209;3&#8209;mini&#8209;128k&#8209;instruct</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2-4B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2-4B\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL2-8B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px\">InternViT-300M-448px</a></td>\n    <td><a href=\"https://huggingface.co/internlm/internlm2_5-7b-chat\">internlm2_5-7b-chat</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2-8B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2-8B\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL2-26B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5\">InternViT-6B-448px-V1-5</a></td>\n    <td><a href=\"https://huggingface.co/internlm/internlm2-chat-20b\">internlm2-chat-20b</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2-26B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2-26B\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL2-40B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5\">InternViT&#8209;6B&#8209;448px&#8209;V1&#8209;5</a></td>\n    <td><a href=\"https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B\">Nous&#8209;Hermes&#8209;2&#8209;Yi&#8209;34B</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2-40B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2-40B\">\ud83e\udd16 link</a></td>\n  </tr>\n  <tr>\n    <td>InternVL2&#8209;Llama3-76B</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5\">InternViT-6B-448px-V1-5</a></td>\n    <td><a href=\"https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-70B\">Hermes\u20112\u2011Theta\u2011<br>Llama\u20113\u201170B</a></td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL2-Llama3-76B\">\ud83e\udd16 link</a></td>\n  </tr>\n</table>\n\n#### Multimodal Large Language Model (InternVL 1.0-1.5)\n\n<table>\n  <tr>\n    <th>Model</th>\n    <th>Date</th>\n    <th>HF&nbsp;Link</th>\n    <th>MS&nbsp;Link</th>\n    <th>Note</th>\n  </tr>\n  <tr>\n    <td>Mini&#8209;InternVL&#8209;Chat&#8209;4B&#8209;V1&#8209;5</td>\n    <td>2024.05.28</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-4B-V1-5\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/Mini-InternVL-Chat-4B-V1-5\">\ud83e\udd16 link</a></td>\n    <td>\ud83d\ude80\ud83d\ude80 16% of the model size, 90% of the performance</td>\n  </tr>\n  <tr>\n    <td>Mini-InternVL-Chat-2B-V1-5</td>\n    <td>2024.05.19</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-2B-V1-5\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/Mini-InternVL-Chat-2B-V1-5\">\ud83e\udd16 link</a></td>\n    <td>\ud83d\ude80 8% of the model size, 80% of the performance</td>\n  </tr>\n  <tr>\n    <td>InternVL-Chat-V1-5</td>\n    <td>2024.04.18</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL-Chat-V1-5\">\ud83e\udd16 link</a></td>\n    <td>support 4K image; super strong OCR; Approaching the performance of GPT-4V and Gemini Pro on various benchmarks like MMMU, DocVQA, ChartQA, MathVista, etc.</td>\n  </tr>\n  <tr>\n    <td>InternVL-Chat-V1-2-Plus</td>\n    <td>2024.02.21</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2-Plus\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL-Chat-V1-2-Plus\">\ud83e\udd16 link</a></td>\n    <td>more SFT data and stronger</td>\n  </tr>\n  <tr>\n    <td>InternVL-Chat-V1-2</td>\n    <td>2024.02.11</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL-Chat-V1-2\">\ud83e\udd16 link</a></td>\n    <td>scaling up LLM to 34B</td>\n  </tr>\n  <tr>\n    <td>InternVL-Chat-V1-1</td>\n    <td>2024.01.24</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL-Chat-V1-1\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL-Chat-V1-1\">\ud83e\udd16 link</a></td>\n    <td>support Chinese and stronger OCR</td>\n  </tr>\n  <tr>\n    <td>InternVL-Chat-19B</td>\n    <td>2023.12.25</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-13B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-13B\">\ud83e\udd16 link</a></td>\n    <td>English multimodal dialogue</td>\n  </tr>\n  <tr>\n    <td>InternVL-Chat-13B</td>\n    <td>2023.12.25</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-7B\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-7B\">\ud83e\udd16 link</a></td>\n    <td>English multimodal dialogue</td>\n  </tr>\n</table>\n\n#### CLIP-like Model (InternVL 1.0-2.5)\n\n<table>\n  <tr>\n    <th>Model</th>\n    <th>Date</th>\n    <th>HF&nbsp;Link</th>\n    <th>MS&nbsp;Link</th>\n    <th>Note</th>\n  </tr>\n  <tr>\n    <td>InternViT-300M-448px-V2_5</td>\n    <td>2024.12.05</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternViT-300M-448px-V2_5\">\ud83e\udd16 link</a></td>\n    <td>\ud83d\ude80\ud83d\ude80 A more powerful lightweight visual encoder. (\ud83d\udd25new)</td>\n  </tr>\n  <tr>\n    <td>InternViT-6B-448px-V2_5</td>\n    <td>2024.12.05</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-6B-448px-V2_5\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternViT-6B-448px-V2_5\">\ud83e\udd16 link</a></td>\n    <td>\ud83d\ude80\ud83d\ude80 A stronger visual encoder to extract visual features. (\ud83d\udd25new)</td>\n  </tr>\n  <tr>\n    <td>InternViT-300M-448px</td>\n    <td>2024.05.25</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-300M-448px\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternViT-300M-448px\">\ud83e\udd16 link</a></td>\n    <td>distilled small vision foundation model with 300M parameters </td>\n  </tr>\n  <tr>\n    <td>InternViT&#8209;6B&#8209;448px&#8209;V1&#8209;5</td>\n    <td>2024.04.20</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternViT-6B-448px-V1-5\">\ud83e\udd16 link</a></td>\n    <td>support dynamic resolution and super strong OCR feature extraction capability by incremental pre-training</td>\n  </tr>\n  <tr>\n    <td>InternViT-6B-448px-V1-2</td>\n    <td>2024.02.11</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-2\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternViT-6B-448px-V1-2\">\ud83e\udd16 link</a></td>\n    <td>support 448 resolution by incremental pre-training</td>\n  </tr>\n  <tr>\n    <td>InternViT-6B-448px-V1-0</td>\n    <td>2024.01.30</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-0\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternViT-6B-448px-V1-0\">\ud83e\udd16 link</a></td>\n    <td>support 448 resolution by incremental pre-training</td>\n  </tr>\n  <tr>\n    <td>InternViT-6B-224px</td>\n    <td>2023.12.22</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternViT-6B-224px\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternViT-6B-224px\">\ud83e\udd16 link</a></td>\n    <td>the first version of InternViT-6B, extracted from InternVL\u201114B\u2011224px</td>\n  </tr>\n</table>\n\n#### Vision-Language Foundation Model (InternVL 1.0)\n\n<table>\n  <tr>\n    <th>Model</th>\n    <th>Date</th>\n    <th>HF&nbsp;Link</th>\n    <th>MS&nbsp;Link</th>\n    <th>Note</th>\n  </tr>\n  <tr>\n    <td>InternVL&#8209;14B&#8209;224px</td>\n    <td>2023.12.22</td>\n    <td><a href=\"https://huggingface.co/OpenGVLab/InternVL-14B-224px\">\ud83e\udd17 link</a></td>\n    <td><a href=\"https://modelscope.cn/models/OpenGVLab/InternVL-14B-224px\">\ud83e\udd16 link</a></td>\n    <td>vision-language foundation model, InternViT-6B + QLLaMA, can be used for image-text retrieval like CLIP</td>\n  </tr>\n</table>\n\n## TODO List\n\n- [x] Release training / evaluation code for InternVL2.5 series\n- [x] Support liger kernels to save GPU memory\n- [x] Release the code, model, and data of MPO\n- [x] Support multimodal packed dataset\n- [ ] Support vLLM and Ollama\n- [ ] Support video and PDF input in online demo\n- [ ] Release InternVL2 with VisionLLMv2 integration\n- [x] Rebuild documents using readthedocs\n- [x] Support fine-tuning different LLMs with LoRA\n- [x] Release `requirements.txt` for InternVL2\n- [x] Release training / evaluation code for InternVL2 series\n- [x] Release Streamlit web UI for InternVL1.5 and InternVL2\n\n## What can InternVL do?\n\n<details>\n  <summary>Visual Perception (click to expand)</summary>\n\n- Linear-Probe Image Classification [\\[see details\\]](./classification#-evaluation)\n\n  ViT-22B uses the private JFT-3B dataset.\n\n  | method              | #param | IN-1K | IN-ReaL | IN-V2 | IN-A  | IN-R  | IN-Sketch |\n  | ------------------- | :----: | :---: | :-----: | :---: | :---: | :---: | :-------: |\n  | OpenCLIP-G          |  1.8B  | 86.2  |  89.4   | 77.2  | 63.8  | 87.8  |   66.4    |\n  | DINOv2-g            |  1.1B  | 86.5  |  89.6   | 78.4  | 75.9  | 78.8  |   62.5    |\n  | EVA-01-CLIP-g       |  1.1B  | 86.5  |  89.3   | 77.4  | 70.5  | 87.7  |   63.1    |\n  | MAWS-ViT-6.5B       |  6.5B  | 87.8  |    -    |   -   |   -   |   -   |     -     |\n  | ViT-22B\\*           | 21.7B  | 89.5  |  90.9   | 83.2  | 83.8  | 87.4  |     -     |\n  | InternViT-6B (ours) |  5.9B  | 88.2  |  90.4   | 79.9  | 77.5  | 89.8  |   69.1    |\n\n- Semantic Segmentation [\\[see details\\]](./segmentation#-evaluation)\n\n  | method                | decoder | #param (train/total) | crop size | mIoU         |\n  | --------------------- | :-----: | :------------------: | :-------: | ------------ |\n  | OpenCLIP-G (frozen)   | Linear  |     0.3M / 1.8B      |    512    | 39.3         |\n  | ViT-22B (frozen)      | Linear  |     0.9M / 21.7B     |    504    | 34.6         |\n  | InternViT-6B (frozen) | Linear  |     0.5M / 5.9B      |    504    | 47.2 (+12.6) |\n  | ViT-22B (frozen)      | UperNet |     0.8B / 22.5B     |    504    | 52.7         |\n  | InternViT-6B (frozen) | UperNet |     0.4B / 6.3B      |    504    | 54.9 (+2.2)  |\n  | ViT-22B               | UperNet |    22.5B / 22.5B     |    504    | 55.3         |\n  | InternViT-6B          | UperNet |     6.3B / 6.3B      |    504    | 58.9 (+3.6)  |\n\n- Zero-Shot Image Classification [\\[see details\\]](./clip_benchmark#imagenet-variants-and-objectnet)\n\n  | method            | IN-1K | IN-A  | IN-R  | IN-V2 | IN-Sketch | ObjectNet |\n  | ----------------- | :---: | :---: | :---: | :---: | :-------: | :-------: |\n  | OpenCLIP-G        | 80.1  | 69.3  | 92.1  | 73.6  |   68.9    |   73.0    |\n  | EVA-02-CLIP-E+    | 82.0  | 82.1  | 94.5  | 75.7  |   71.6    |   79.6    |\n  | ViT-22B\\*         | 85.9  | 90.1  | 96.0  | 80.9  |     -     |   87.6    |\n  | InternVL-C (ours) | 83.2  | 83.8  | 95.5  | 77.3  |   73.9    |   80.6    |\n\n- Multilingual Zero-Shot Image Classification [\\[see details\\]](./clip_benchmark#multilingual-imagenet-1k)\n\n  EN: English, ZH: Chinese, JP: Japanese, Ar: Arabic, IT: Italian\n\n  | method            | IN-1K (EN) | IN-1K (ZH) | IN-1K (JP) | IN-1K (AR) | IN-1K (IT) |\n  | ----------------- | :--------: | :--------: | :--------: | :--------: | :--------: |\n  | Taiyi-CLIP-ViT-H  |     -      |    54.4    |     -      |     -      |     -      |\n  | WuKong-ViT-L-G    |     -      |    57.5    |     -      |     -      |     -      |\n  | CN-CLIP-ViT-H     |     -      |    59.6    |     -      |     -      |     -      |\n  | AltCLIP-ViT-L     |    74.5    |    59.6    |     -      |     -      |     -      |\n  | EVA-02-CLIP-E+    |    82.0    |     -      |     -      |     -      |    41.2    |\n  | OpenCLIP-XLM-R-H  |    77.0    |    55.7    |    53.1    |    37.0    |    56.8    |\n  | InternVL-C (ours) |    83.2    |    64.5    |    61.5    |    44.9    |    65.7    |\n\n- Zero-Shot Video Classification\n\n  | method            | #frame | K400  | K600  | K700  |\n  | ----------------- | :----: | :---: | :---: | :---: |\n  | OpenCLIP-G        |   1    | 65.9  | 66.1  | 59.2  |\n  | EVA-02-CLIP-E+    |   1    | 69.8  | 69.3  | 63.4  |\n  | InternVL-C (ours) |   1    | 71.0  | 71.3  | 65.7  |\n  | ViCLIP            |   8    | 75.7  | 73.5  | 66.4  |\n  | InternVL-C (ours) |   8    | 79.4  | 78.8  | 71.5  |\n\n</details>\n\n<details>\n  <summary>Cross-Modal Retrieval (click to expand)</summary>\n\n- English Zero-Shot Image-Text Retrieval [\\[see details\\]](./clip_benchmark#flickr30k--coco)\n\n  <table>\n    <tr align=center>\n        <td rowspan=\"3\" align=left><b>model</b></td>\n        <td colspan=\"6\" align=center><b>Flickr30K</b></td>\n        <td colspan=\"6\" align=center><b>COCO</b></td>\n        <td rowspan=\"3\" align=center><b>avg</b></td>\n    </tr>\n     <tr align=center>\n        <td colspan=\"3\" align=center><b>image-to-text</b></td>\n        <td colspan=\"3\" align=center><b>text-to-image</b></td>\n         <td colspan=\"3\" align=center><b>image-to-text</b></td>\n        <td colspan=\"3\" align=center><b>text-to-image</b></td>\n     </tr>\n     <tr>\n        <td>R@1</td>\n        <td>R@5</td>\n        <td>R@10</td>\n        <td>R@1</td>\n        <td>R@5</td>\n        <td>R@10</td>\n        <td>R@1</td>\n        <td>R@5</td>\n        <td>R@10</td>\n        <td>R@1</td>\n        <td>R@5</td>\n        <td>R@10</td>\n     </tr>\n  <tr align=center>\n        <td align=left>OpenCLIP-G</td>\n        <td>92.9</td>\n        <td>99.3</td>\n        <td>99.8</td>\n        <td>79.5</td>\n        <td>95.0</td>\n        <td>97.1</td>\n        <td>67.3</td>\n        <td>86.9</td>\n        <td>92.6</td>\n        <td>51.4</td>\n        <td>74.9</td>\n        <td>83.0</td>\n        <td>85.0</td>\n     </tr>\n  <tr align=center>\n        <td align=left>EVA-02-CLIP-E+</td>\n        <td>93.9</td>\n        <td>99.4</td>\n        <td>99.8</td>\n        <td>78.8</td>\n        <td>94.2</td>\n        <td>96.8</td>\n        <td>68.8</td>\n        <td>87.8</td>\n        <td>92.8</td>\n        <td>51.1</td>\n        <td>75.0</td>\n        <td>82.7</td>\n        <td>85.1</td>\n     </tr>\n    <tr align=center>\n        <td align=left>EVA-CLIP-8B</td>\n        <td>95.6</td>\n        <td>99.6</td>\n        <td>99.9</td>\n        <td>80.8</td>\n        <td>95.5</td>\n        <td>97.6</td>\n        <td>70.3</td>\n        <td>89.3</td>\n        <td>93.9</td>\n        <td>53.0</td>\n        <td>76.0</td>\n        <td>83.4</td>\n        <td>86.2</td>\n     </tr>\n  <tr align=center>\n        <td align=left>InternVL-C (ours)</td>\n        <td>94.7</td>\n        <td>99.6</td>\n        <td>99.9</td>\n        <td>81.7</td>\n        <td>96.0</td>\n        <td>98.2</td>\n        <td>70.6</td>\n        <td>89.0</td>\n        <td>93.5</td>\n        <td>54.1</td>\n        <td>77.3</td>\n        <td>84.6</td>\n        <td>86.6</td>\n     </tr>\n  <tr align=center>\n        <td align=left>InternVL-G (ours)</td>\n        <td>95.7</td>\n        <td>99.7</td>\n        <td>99.9</td>\n        <td>85.0</td>\n        <td>97.0</td>\n        <td>98.6</td>\n        <td>74.9</td>\n        <td>91.3</td>\n        <td>95.2</td>\n        <td>58.6</td>\n        <td>81.3</td>\n        <td>88.0</td>\n        <td>88.8</td>\n     </tr>\n\n  </table>\n\n- Chinese Zero-Shot Image-Text Retrieval [\\[see details\\]](./clip_benchmark#flickr30k-cn--coco-cn)\n\n  <table>\n    <tr  align=center>\n        <td rowspan=\"3\" align=left><b>model</b></td>\n        <td colspan=\"6\" align=center><b>Flickr30K-CN</b></td>\n        <td colspan=\"6\" align=center><b>COCO-CN</b></td>\n        <td rowspan=\"3\" align=center><b>avg</b></td>\n\n  </tr>\n     <tr  align=center>\n        <td colspan=\"3\" align=center><b>image-to-text</b></td>\n        <td colspan=\"3\" align=center><b>text-to-image</b></td>\n         <td colspan=\"3\" align=center><b>image-to-text</b></td>\n        <td colspan=\"3\" align=center><b>text-to-image</b></td>\n     </tr>\n     <tr>\n        <td>R@1</td>\n        <td>R@5</td>\n        <td>R@10</td>\n        <td>R@1</td>\n        <td>R@5</td>\n        <td>R@10</td>\n        <td>R@1</td>\n        <td>R@5</td>\n        <td>R@10</td>\n        <td>R@1</td>\n        <td>R@5</td>\n        <td>R@10</td>\n     </tr>\n\n  <tr align=center>\n        <td align=left>CN-CLIP-ViT-H</td>\n        <td>81.6</td>\n        <td>97.5</td>\n        <td>98.8</td>\n        <td>71.2</td>\n        <td>91.4</td>\n        <td>95.5</td>\n        <td>63.0</td>\n        <td>86.6</td>\n        <td>92.9</td>\n        <td>69.2</td>\n        <td>89.9</td>\n        <td>96.1</td>\n        <td>86.1</td>\n     </tr>\n\n  <tr align=center>\n        <td align=left>OpenCLIP-XLM-R-H</td>\n        <td>86.1</td>\n        <td>97.5</td>\n        <td>99.2</td>\n        <td>71.0</td>\n        <td>90.5</td>\n        <td>94.9</td>\n        <td>70.0</td>\n        <td>91.5</td>\n        <td>97.0</td>\n        <td>66.1</td>\n        <td>90.8</td>\n        <td>96.0</td>\n        <td>87.6</td>\n     </tr>\n\n  <tr align=center>\n        <td align=left>InternVL-C (ours)</td>\n        <td>90.3</td>\n        <td>98.8</td>\n        <td>99.7</td>\n        <td>75.1</td>\n        <td>92.9</td>\n        <td>96.4</td>\n        <td>68.8</td>\n        <td>92.0</td>\n        <td>96.7</td>\n        <td>68.9</td>\n        <td>91.9</td>\n        <td>96.5</td>\n        <td>89.0</td>\n     </tr>\n  <tr align=center>\n        <td align=left>InternVL-G (ours)</td>\n        <td>92.9</td>\n        <td>99.4</td>\n        <td>99.8</td>\n        <td>77.7</td>\n        <td>94.8</td>\n        <td>97.3</td>\n        <td>71.4</td>\n        <td>93.9</td>\n        <td>97.7</td>\n        <td>73.8</td>\n        <td>94.4</td>\n        <td>98.1</td>\n        <td>90.9</td>\n     </tr>\n\n  </table>\n\n- Multilingual Zero-Shot Image-Text Retrieval on XTD [\\[see details\\]](./clip_benchmark#xtd)\n\n  | method            |  EN   |  ES   |  FR   |  ZH   |  IT   |  KO   |  RU   |  JP   | average |\n  | ----------------- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :-----: |\n  | AltCLIP           | 95.4  | 94.1  | 92.9  | 95.1  | 94.2  | 94.4  | 91.8  | 91.7  |  93.7   |\n  | OpenCLIP-XLM-R-H  | 97.3  | 96.1  | 94.5  | 94.7  | 96.0  | 90.2  | 93.9  | 94.0  |  94.6   |\n  | InternVL-C (ours) | 97.3  | 95.7  | 95.1  | 95.6  | 96.0  | 92.2  | 93.3  | 95.5  |  95.1   |\n  | InternVL-G (ours) | 98.6  | 97.7  | 96.5  | 96.7  | 96.9  | 95.1  | 94.8  | 96.1  |  96.6   |\n\n</details>\n\n<details>\n  <summary>Multimodal Dialogue</summary>\n\n</details>\n\n## Quick Start with HuggingFace\n\n<details>\n  <summary>using InternViT-6B for visual feature extraction (click to expand)</summary>\n\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, CLIPImageProcessor\n\nmodel = AutoModel.from_pretrained(\n    'OpenGVLab/InternViT-6B-448px-V2_5',\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True).cuda().eval()\n\nimage = Image.open('./examples/image1.jpg').convert('RGB')\n\nimage_processor = CLIPImageProcessor.from_pretrained('OpenGVLab/InternViT-6B-448px-V1-5')\n\npixel_values = image_processor(images=image, return_tensors='pt').pixel_values\npixel_values = pixel_values.to(torch.bfloat16).cuda()\n\noutputs = model(pixel_values)\n```\n\n</details>\n\n<details>\n  <summary>using InternVL-C(ontrastive) and InternVL-G(enerative) for cross-modal retrieval (click to expand)</summary>\n\n```python\nimport torch\nfrom PIL import Image\nfrom transformers import AutoModel, CLIPImageProcessor\nfrom transformers import AutoTokenizer\n\n\nmodel = AutoModel.from_pretrained(\n    'OpenGVLab/InternVL-14B-224px',\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True).cuda().eval()\n\nimage_processor = CLIPImageProcessor.from_pretrained('OpenGVLab/InternVL-14B-224px')\n\ntokenizer = AutoTokenizer.from_pretrained(\n    'OpenGVLab/InternVL-14B-224px', use_fast=False, add_eos_token=True)\ntokenizer.pad_token_id = 0  # set pad_token_id to 0\n\nimages = [\n    Image.open('./examples/image1.jpg').convert('RGB'),\n    Image.open('./examples/image2.jpg').convert('RGB'),\n    Image.open('./examples/image3.jpg').convert('RGB')\n]\nprefix = 'summarize:'\ntexts = [\n    prefix + 'a photo of a red panda',  # English\n    prefix + '\u4e00\u5f20\u718a\u732b\u7684\u7167\u7247',  # Chinese\n    prefix + '\u4e8c\u5339\u306e\u732b\u306e\u5199\u771f'  # Japanese\n]\n\npixel_values = image_processor(images=images, return_tensors='pt').pixel_values\npixel_values = pixel_values.to(torch.bfloat16).cuda()\ninput_ids = tokenizer(texts, return_tensors='pt', max_length=80,\n                      truncation=True, padding='max_length').input_ids.cuda()\n\n# InternVL-C\nlogits_per_image, logits_per_text = model(\n    image=pixel_values, text=input_ids, mode='InternVL-C')\nprobs = logits_per_image.softmax(dim=-1)\n# tensor([[9.9609e-01, 5.2185e-03, 6.0070e-08],\n#         [2.2949e-02, 9.7656e-01, 5.9903e-06],\n#         [3.2932e-06, 7.4863e-05, 1.0000e+00]], device='cuda:0',\n#        dtype=torch.bfloat16, grad_fn=<SoftmaxBackward0>)\n\n# InternVL-G\nlogits_per_image, logits_per_text = model(\n    image=pixel_values, text=input_ids, mode='InternVL-G')\nprobs = logits_per_image.softmax(dim=-1)\n# tensor([[9.9609e-01, 3.1738e-03, 3.6322e-08],\n#         [8.6060e-03, 9.9219e-01, 2.8759e-06],\n#         [1.7583e-06, 3.1233e-05, 1.0000e+00]], device='cuda:0',\n#        dtype=torch.bfloat16, grad_fn=<SoftmaxBackward0>)\n\n# please set add_eos_token to False for generation\ntokenizer.add_eos_token = False\nimage = Image.open('./examples/image1.jpg').convert('RGB')\npixel_values = image_processor(images=image, return_tensors='pt').pixel_values\npixel_values = pixel_values.to(torch.bfloat16).cuda()\n\ntokenized = tokenizer(\"English caption:\", return_tensors='pt')\npred = model.generate(\n    pixel_values=pixel_values,\n    input_ids=tokenized.input_ids.cuda(),\n    attention_mask=tokenized.attention_mask.cuda(),\n    num_beams=5,\n    min_new_tokens=8,\n)\ncaption = tokenizer.decode(pred[0].cpu(), skip_special_tokens=True).strip()\n# English caption: a red panda sitting on top of a wooden platform\n```\n\n</details>\n\n<details>\n  <summary>using InternVL 2.5 for multimodal chat (click to expand)</summary>\n\nHere, we take the smaller `OpenGVLab/InternVL2_5-8B` as an example:\n\n```python\nimport numpy as np\nimport torch\nimport torchvision.transforms as T\nfrom decord import VideoReader, cpu\nfrom PIL import Image\nfrom torchvision.transforms.functional import InterpolationMode\nfrom transformers import AutoModel, AutoTokenizer\n\nIMAGENET_MEAN = (0.485, 0.456, 0.406)\nIMAGENET_STD = (0.229, 0.224, 0.225)\n\ndef build_transform(input_size):\n    MEAN, STD = IMAGENET_MEAN, IMAGENET_STD\n    transform = T.Compose([\n        T.Lambda(lambda img: img.convert('RGB') if img.mode != 'RGB' else img),\n        T.Resize((input_size, input_size), interpolation=InterpolationMode.BICUBIC),\n        T.ToTensor(),\n        T.Normalize(mean=MEAN, std=STD)\n    ])\n    return transform\n\ndef find_closest_aspect_ratio(aspect_ratio, target_ratios, width, height, image_size):\n    best_ratio_diff = float('inf')\n    best_ratio = (1, 1)\n    area = width * height\n    for ratio in target_ratios:\n        target_aspect_ratio = ratio[0] / ratio[1]\n        ratio_diff = abs(aspect_ratio - target_aspect_ratio)\n        if ratio_diff < best_ratio_diff:\n            best_ratio_diff = ratio_diff\n            best_ratio = ratio\n        elif ratio_diff == best_ratio_diff:\n            if area > 0.5 * image_size * image_size * ratio[0] * ratio[1]:\n                best_ratio = ratio\n    return best_ratio\n\ndef dynamic_preprocess(image, min_num=1, max_num=12, image_size=448, use_thumbnail=False):\n    orig_width, orig_height = image.size\n    aspect_ratio = orig_width / orig_height\n\n    # calculate the existing image aspect ratio\n    target_ratios = set(\n        (i, j) for n in range(min_num, max_num + 1) for i in range(1, n + 1) for j in range(1, n + 1) if\n        i * j <= max_num and i * j >= min_num)\n    target_ratios = sorted(target_ratios, key=lambda x: x[0] * x[1])\n\n    # find the closest aspect ratio to the target\n    target_aspect_ratio = find_closest_aspect_ratio(\n        aspect_ratio, target_ratios, orig_width, orig_height, image_size)\n\n    # calculate the target width and height\n    target_width = image_size * target_aspect_ratio[0]\n    target_height = image_size * target_aspect_ratio[1]\n    blocks = target_aspect_ratio[0] * target_aspect_ratio[1]\n\n    # resize the image\n    resized_img = image.resize((target_width, target_height))\n    processed_images = []\n    for i in range(blocks):\n        box = (\n            (i % (target_width // image_size)) * image_size,\n            (i // (target_width // image_size)) * image_size,\n            ((i % (target_width // image_size)) + 1) * image_size,\n            ((i // (target_width // image_size)) + 1) * image_size\n        )\n        # split the image\n        split_img = resized_img.crop(box)\n        processed_images.append(split_img)\n    assert len(processed_images) == blocks\n    if use_thumbnail and len(processed_images) != 1:\n        thumbnail_img = image.resize((image_size, image_size))\n        processed_images.append(thumbnail_img)\n    return processed_images\n\ndef load_image(image_file, input_size=448, max_num=12):\n    image = Image.open(image_file).convert('RGB')\n    transform = build_transform(input_size=input_size)\n    images = dynamic_preprocess(image, image_size=input_size, use_thumbnail=True, max_num=max_num)\n    pixel_values = [transform(image) for image in images]\n    pixel_values = torch.stack(pixel_values)\n    return pixel_values\n\n# If you have an 80G A100 GPU, you can put the entire model on a single GPU.\n# Otherwise, you need to load a model using multiple GPUs, please refer to the `Multiple GPUs` section.\npath = 'OpenGVLab/InternVL2_5-8B'\nmodel = AutoModel.from_pretrained(\n    path,\n    torch_dtype=torch.bfloat16,\n    low_cpu_mem_usage=True,\n    trust_remote_code=True).eval().cuda()\ntokenizer = AutoTokenizer.from_pretrained(path, trust_remote_code=True, use_fast=False)\n\n# set the max number of tiles in `max_num`\npixel_values = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\ngeneration_config = dict(max_new_tokens=1024, do_sample=False)\n\n# pure-text conversation (\u7eaf\u6587\u672c\u5bf9\u8bdd)\nquestion = 'Hello, who are you?'\nresponse, history = model.chat(tokenizer, None, question, generation_config, history=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n\nquestion = 'Can you tell me a story?'\nresponse, history = model.chat(tokenizer, None, question, generation_config, history=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n\n# single-image single-round conversation (\u5355\u56fe\u5355\u8f6e\u5bf9\u8bdd)\nquestion = '<image>\\nPlease describe the image shortly.'\nresponse = model.chat(tokenizer, pixel_values, question, generation_config)\nprint(f'User: {question}\\nAssistant: {response}')\n\n# single-image multi-round conversation (\u5355\u56fe\u591a\u8f6e\u5bf9\u8bdd)\nquestion = '<image>\\nPlease describe the image in detail.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config, history=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n\nquestion = 'Please write a poem according to the image.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config, history=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n\n# multi-image multi-round conversation, combined images (\u591a\u56fe\u591a\u8f6e\u5bf9\u8bdd\uff0c\u62fc\u63a5\u56fe\u50cf)\npixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n\nquestion = '<image>\\nDescribe the two images in detail.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\n                               history=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n\nquestion = 'What are the similarities and differences between these two images.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\n                               history=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n\n# multi-image multi-round conversation, separate images (\u591a\u56fe\u591a\u8f6e\u5bf9\u8bdd\uff0c\u72ec\u7acb\u56fe\u50cf)\npixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\nnum_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\n\nquestion = 'Image-1: <image>\\nImage-2: <image>\\nDescribe the two images in detail.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\n                               num_patches_list=num_patches_list,\n                               history=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n\nquestion = 'What are the similarities and differences between these two images.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\n                               num_patches_list=num_patches_list,\n                               history=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n\n# batch inference, single image per sample (\u5355\u56fe\u6279\u5904\u7406)\npixel_values1 = load_image('./examples/image1.jpg', max_num=12).to(torch.bfloat16).cuda()\npixel_values2 = load_image('./examples/image2.jpg', max_num=12).to(torch.bfloat16).cuda()\nnum_patches_list = [pixel_values1.size(0), pixel_values2.size(0)]\npixel_values = torch.cat((pixel_values1, pixel_values2), dim=0)\n\nquestions = ['<image>\\nDescribe the image in detail.'] * len(num_patches_list)\nresponses = model.batch_chat(tokenizer, pixel_values,\n                             num_patches_list=num_patches_list,\n                             questions=questions,\n                             generation_config=generation_config)\nfor question, response in zip(questions, responses):\n    print(f'User: {question}\\nAssistant: {response}')\n\n# video multi-round conversation (\u89c6\u9891\u591a\u8f6e\u5bf9\u8bdd)\ndef get_index(bound, fps, max_frame, first_idx=0, num_segments=32):\n    if bound:\n        start, end = bound[0], bound[1]\n    else:\n        start, end = -100000, 100000\n    start_idx = max(first_idx, round(start * fps))\n    end_idx = min(round(end * fps), max_frame)\n    seg_size = float(end_idx - start_idx) / num_segments\n    frame_indices = np.array([\n        int(start_idx + (seg_size / 2) + np.round(seg_size * idx))\n        for idx in range(num_segments)\n    ])\n    return frame_indices\n\ndef load_video(video_path, bound=None, input_size=448, max_num=1, num_segments=32):\n    vr = VideoReader(video_path, ctx=cpu(0), num_threads=1)\n    max_frame = len(vr) - 1\n    fps = float(vr.get_avg_fps())\n\n    pixel_values_list, num_patches_list = [], []\n    transform = build_transform(input_size=input_size)\n    frame_indices = get_index(bound, fps, max_frame, first_idx=0, num_segments=num_segments)\n    for frame_index in frame_indices:\n        img = Image.fromarray(vr[frame_index].asnumpy()).convert('RGB')\n        img = dynamic_preprocess(img, image_size=input_size, use_thumbnail=True, max_num=max_num)\n        pixel_values = [transform(tile) for tile in img]\n        pixel_values = torch.stack(pixel_values)\n        num_patches_list.append(pixel_values.shape[0])\n        pixel_values_list.append(pixel_values)\n    pixel_values = torch.cat(pixel_values_list)\n    return pixel_values, num_patches_list\n\nvideo_path = './examples/red-panda.mp4'\npixel_values, num_patches_list = load_video(video_path, num_segments=8, max_num=1)\npixel_values = pixel_values.to(torch.bfloat16).cuda()\nvideo_prefix = ''.join([f'Frame-{i+1}: <image>\\n' for i in range(len(num_patches_list))])\nquestion = video_prefix + 'What is the red panda doing?'\n# Frame1: <image>\\nFrame2: <image>\\n...\\nFrame8: <image>\\n{question}\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\n                               num_patches_list=num_patches_list, history=None, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n\nquestion = 'Describe this video in detail.'\nresponse, history = model.chat(tokenizer, pixel_values, question, generation_config,\n                               num_patches_list=num_patches_list, history=history, return_history=True)\nprint(f'User: {question}\\nAssistant: {response}')\n```\n\n</details>\n\n## License\n\nThis project is released under the [MIT license](LICENSE). Parts of this project contain code and models from other sources, which are subject to their respective licenses.\n\n## Citation\n\nIf you find this project useful in your research, please consider cite:\n\n```BibTeX\n@misc{zhu2025internvl3exploringadvancedtraining,\n      title={InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models}, \n      author={Jinguo Zhu and Weiyun Wang and Zhe Chen and Zhaoyang Liu and Shenglong Ye and Lixin Gu and Hao Tian and Yuchen Duan and Weijie Su and Jie Shao and Zhangwei Gao and Erfei Cui and Xuehui Wang and Yue Cao and Yangzhou Liu and Xingguang Wei and Hongjie Zhang and Haomin Wang and Weiye Xu and Hao Li and Jiahao Wang and Nianchen Deng and Songze Li and Yinan He and Tan Jiang and Jiapeng Luo and Yi Wang and Conghui He and Botian Shi and Xingcheng Zhang and Wenqi Shao and Junjun He and Yingtong Xiong and Wenwen Qu and Peng Sun and Penglong Jiao and Han Lv and Lijun Wu and Kaipeng Zhang and Huipeng Deng and Jiaye Ge and Kai Chen and Limin Wang and Min Dou and Lewei Lu and Xizhou Zhu and Tong Lu and Dahua Lin and Yu Qiao and Jifeng Dai and Wenhai Wang},\n      year={2025},\n      eprint={2504.10479},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2504.10479}, \n}\n@article{chen2024expanding,\n  title={Expanding Performance Boundaries of Open-Source Multimodal Models with Model, Data, and Test-Time Scaling},\n  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},\n  journal={arXiv preprint arXiv:2412.05271},\n  year={2024}\n}\n@article{wang2024mpo,\n  title={Enhancing the Reasoning Ability of Multimodal Large Language Models via Mixed Preference Optimization},\n  author={Wang, Weiyun and Chen, Zhe and Wang, Wenhai and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Zhu, Jinguo and Zhu, Xizhou and Lu, Lewei and Qiao, Yu and Dai, Jifeng},\n  journal={arXiv preprint arXiv:2411.10442},\n  year={2024}\n}\n@article{gao2024mini,\n  title={Mini-InternVL: a flexible-transfer pocket multi-modal model with 5\\% parameters and 90\\% performance},\n  author={Gao, Zhangwei and Chen, Zhe and Cui, Erfei and Ren, Yiming and Wang, Weiyun and Zhu, Jinguo and Tian, Hao and Ye, Shenglong and He, Junjun and Zhu, Xizhou and others},\n  journal={Visual Intelligence},\n  volume={2},\n  number={1},\n  pages={1--17},\n  year={2024},\n  publisher={Springer}\n}\n@article{chen2024far,\n  title={How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites},\n  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},\n  journal={Science China Information Sciences},\n  volume={67},\n  number={12},\n  pages={220101},\n  year={2024},\n  publisher={Springer}\n}\n@inproceedings{chen2024internvl,\n  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},\n  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={24185--24198},\n  year={2024}\n}\n```\n\n## Acknowledgement\n\nInternVL is built with reference to the code of the following projects: [OpenAI CLIP](https://github.com/openai/CLIP), [Open CLIP](https://github.com/mlfoundations/open_clip), [CLIP Benchmark](https://github.com/LAION-AI/CLIP_benchmark), [EVA](https://github.com/baaivision/EVA/tree/master), [InternImage](https://github.com/OpenGVLab/InternImage), [ViT-Adapter](https://github.com/czczup/ViT-Adapter), [MMSegmentation](https://github.com/open-mmlab/mmsegmentation), [Transformers](https://github.com/huggingface/transformers), [DINOv2](https://github.com/facebookresearch/dinov2), [BLIP-2](https://github.com/salesforce/LAVIS/tree/main/projects/blip2), [Qwen-VL](https://github.com/QwenLM/Qwen-VL/tree/master/eval_mm), and [LLaVA-1.5](https://github.com/haotian-liu/LLaVA). Thanks for their awesome work!\n\n______________________________________________________________________\n\nScan the following QR Code, join our WeChat group.\n\n<p align=\"center\"><img width=\"300\" alt=\"image\" src=\"https://github.com/user-attachments/assets/f776df09-ebba-4fd5-80c2-fec4ff1518be\"></p>\n",
  "external_links_in_readme": [
    "https://github.com/openai/CLIP",
    "https://github.com/PaddlePaddle/PaddleMIX",
    "https://internvl.readthedocs.io/en/latest/tutorials/coco_caption_finetune.html",
    "https://internvl.readthedocs.io/en/latest/get_started/chat_data_format.html#single-image-data",
    "https://huggingface.co/internlm/internlm2_5-20b-chat\">internlm2_5-20b-chat</a></td>",
    "https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2-Plus\">\ud83e\udd17",
    "https://github.com/huggingface/transformers",
    "https://huggingface.co/papers/2412.09616",
    "https://huggingface.co/OpenGVLab/InternVL2_5-1B\">\ud83e\udd17",
    "https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5\">\ud83e\udd17",
    "https://github.com/BradyFU/Video-MME",
    "https://internvl.readthedocs.io/en/latest/get_started/chat_data_format.html#multi-image-data",
    "https://modelscope.cn/models/OpenGVLab/InternVL3-38B\">\ud83e\udd16",
    "https://huggingface.co/OpenGVLab/InternVL2_5-78B",
    "https://internvl.readthedocs.io/en/latest/internvl1.0/internvl_g.html",
    "https://internvl.opengvlab.com/",
    "https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=1",
    "https://huggingface.co/OpenGVLab/InternViT-300M-448px\">\ud83e\udd17",
    "https://github.com/mulanai/MuLan",
    "https://github.com/salesforce/LAVIS/tree/main/projects/blip2",
    "https://huggingface.co/NousResearch/Nous-Hermes-2-Yi-34B\">Nous&#8209;Hermes&#8209;2&#8209;Yi&#8209;34B</a></td>",
    "https://huggingface.co/papers/2503.10291",
    "https://modelscope.cn/models/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-7B\">\ud83e\udd16",
    "https://sharegpt4o.github.io/",
    "https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5",
    "https://huggingface.co/OpenGVLab/InternVL2-Llama3-76B\">\ud83e\udd17",
    "https://modelscope.cn/models/OpenGVLab/InternVL3-78B\">\ud83e\udd16",
    "https://modelscope.cn/models/OpenGVLab/InternViT-300M-448px\">\ud83e\udd16",
    "https://internvl.readthedocs.io/en/latest/internvl1.5/quick_start.html",
    "https://huggingface.co/collections/OpenGVLab/internvl-25-673e1019b66e2218f68d7c1c",
    "https://huggingface.co/OpenGVLab/InternViT-6B-448px-V2_5\">InternViT-6B-448px-V2_5</a></td>",
    "https://huggingface.co/Qwen/Qwen2.5-14B\">Qwen2.5-14B</a></td>",
    "https://huggingface.co/OpenGVLab/InternVL3-14B\">\ud83e\udd17",
    "https://huggingface.co/internlm/internlm2-chat-20b\">internlm2-chat-20b</a></td>",
    "https://internvl.readthedocs.io/en/latest/internvl3.0/introduction.html",
    "https://huggingface.co/Qwen/Qwen2.5-3B-Instruct\">Qwen2.5-3B-Instruct</a></td>",
    "https://rank.opencompass.org.cn/leaderboard-multimodal-reasoning/?m=REALTIME",
    "https://modelscope.cn/models/OpenGVLab/InternVL3-14B\">\ud83e\udd16",
    "https://huggingface.co/OpenGVLab/InternVL2_5-38B\">\ud83e\udd17",
    "https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5\">InternViT-300M-448px-V2_5</a></td>",
    "https://huggingface.co/OpenGVLab/InternVL-14B-224px\">\ud83e\udd17",
    "https://github.com/OpenGVLab/MM-NIAH",
    "https://huggingface.co/Qwen/Qwen2.5-0.5B\">Qwen2.5&#8209;0.5B</a></td>",
    "https://huggingface.co/Qwen/Qwen2.5-72B\">Qwen2.5-72B</a></td>",
    "https://modelscope.cn/models/OpenGVLab/InternVL2-4B\">\ud83e\udd16",
    "https://internvl.readthedocs.io/en/latest/internvl1.5/deployment.html",
    "https://huggingface.co/OpenGVLab/InternVL2-4B\">\ud83e\udd17",
    "https://huggingface.co/OpenGVLab/InternVL2-40B\">\ud83e\udd17",
    "https://modelscope.cn/models/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-13B\">\ud83e\udd16",
    "https://huggingface.co/OpenGVLab/InternVL2_5-78B-MPO\">\ud83e\udd17",
    "https://chartmimic.github.io/",
    "https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-2\">\ud83e\udd17",
    "https://internvl.readthedocs.io/en/latest/",
    "https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-2B-V1-5\">\ud83e\udd17",
    "https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5-AWQ",
    "https://modelscope.cn/models/OpenGVLab/InternViT-6B-448px-V1-5\">\ud83e\udd16",
    "https://internvl.readthedocs.io/en/latest/internvl2.5/preference_optimization.html",
    "https://internvl.github.io/blog/2024-11-14-InternVL-2.0-MPO/",
    "https://huggingface.co/OpenGVLab/InternVL3-8B\">\ud83e\udd17",
    "https://github.com/PaddlePaddle/PaddleMIX/tree/develop/paddlemix/examples/internvl2",
    "https://github.com/JUNJIE99/MLVU",
    "https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2",
    "https://internvl.readthedocs.io/en/latest/internvl2.0/evaluation.html",
    "https://modelscope.cn/models/OpenGVLab/InternVL2-1B\">\ud83e\udd16",
    "https://internvl.readthedocs.io/en/latest/get_started/chat_data_format.html#meta-file",
    "https://internvl.readthedocs.io/en/latest/internvl1.1/evaluation.html",
    "https://github.com/facebookresearch/dinov2",
    "https://github.com/user-attachments/assets/930e6814-8a9f-43e1-a284-118a5732daa4\">",
    "https://internvl.readthedocs.io/en/latest/internvl2.0/quick_start.html",
    "https://huggingface.co/OpenGVLab/InternViT-300M-448px\">InternViT-300M-448px</a></td>",
    "https://modelscope.cn/models/OpenGVLab/InternVL2-2B\">\ud83e\udd16",
    "https://modelscope.cn/models/OpenGVLab/InternViT-6B-448px-V1-2\">\ud83e\udd16",
    "https://huggingface.co/internlm/internlm2-chat-1_8b\">internlm2-chat-1-8b</a></td>",
    "https://modelscope.cn/models/OpenGVLab/InternVL2_5-8B\">\ud83e\udd16",
    "https://rank.opencompass.org.cn/leaderboard-multimodal/?m=REALTIME",
    "https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/tools/reasoning_data_pipeline",
    "https://modelscope.cn/models/OpenGVLab/InternVL2_5-1B\">\ud83e\udd16",
    "https://modelscope.cn/models/OpenGVLab/InternViT-300M-448px-V2_5\">\ud83e\udd16",
    "https://modelscope.cn/models/OpenGVLab/InternVL-14B-224px\">\ud83e\udd16",
    "https://github.com/OpenGVLab/InternVL-MMDetSeg",
    "https://github.com/baaivision/EVA/tree/master",
    "https://huggingface.co/collections/OpenGVLab/internvl-20-667d3961ab5eb12c7ed1463e",
    "https://huggingface.co/OpenGVLab/InternVL2-1B\">\ud83e\udd17",
    "https://internvl.readthedocs.io/en/latest/internvl1.1/quick_start.html",
    "https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5\">InternViT&#8209;300M&#8209;448px&#8209;V2_5</a></td>",
    "https://modelscope.cn/models/OpenGVLab/InternViT-6B-448px-V2_5\">\ud83e\udd16",
    "https://huggingface.co/papers/2411.10442",
    "https://internvl.github.io/blog/2024-12-20-InternVL-2.5-MPO/",
    "https://modelscope.cn/models/OpenGVLab/InternVL2_5-38B-MPO\">\ud83e\udd16",
    "https://huggingface.co/Qwen/Qwen2.5-72B-Instruct\">Qwen2.5-72B-Instruct</a></td>",
    "https://huggingface.co/Qwen/Qwen2.5-1.5B\">Qwen2.5-1.5B</a></td>",
    "https://internvl.readthedocs.io/en/latest/internvl1.5/introduction.html",
    "https://modelscope.cn/models/OpenGVLab/InternVL3-1B\">\ud83e\udd16",
    "https://modelscope.cn/models/OpenGVLab/InternVL2-26B\">\ud83e\udd16",
    "https://internvl.readthedocs.io/en/latest/internvl1.0/classification.html",
    "https://internvl.readthedocs.io/en/latest/internvl1.2/evaluation.html",
    "https://charxiv.github.io/#leaderboard",
    "https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl2.0_mpo",
    "https://huggingface.co/OpenGVLab/InternVL2-2B\">\ud83e\udd17",
    "https://internvl.github.io/blog/2024-07-02-InternVL-2.0/",
    "https://huggingface.co/OpenGVLab/InternViT-6B-224px\">\ud83e\udd17",
    "https://modelscope.cn/models/OpenGVLab/InternVL2_5-78B-MPO\">\ud83e\udd16",
    "https://internvl.readthedocs.io/en/latest/internvl2.0/finetune.html",
    "https://internvl.readthedocs.io/en/latest/internvl2.5/deployment.html",
    "https://modelscope.cn/models/OpenGVLab/InternVL-Chat-V1-1\">\ud83e\udd16",
    "https://github.com/open-mmlab/mmsegmentation",
    "https://modelscope.cn/models/OpenGVLab/InternViT-6B-224px\">\ud83e\udd16",
    "https://huggingface.co/microsoft/Phi-3-mini-128k-instruct\">Phi&#8209;3&#8209;mini&#8209;128k&#8209;instruct</a></td>",
    "https://huggingface.co/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-13B\">\ud83e\udd17",
    "https://internvl.readthedocs.io/en/latest/internvl2.5/evaluation.html",
    "https://internvl.readthedocs.io/en/latest/internvl2.0/introduction.html",
    "https://modelscope.cn/models/OpenGVLab/InternVL2_5-8B-MPO\">\ud83e\udd16",
    "https://huggingface.co/OpenGVLab/InternVL-Chat-V1-5\">\ud83e\udd17",
    "https://modelscope.cn/models/OpenGVLab/InternViT-6B-448px-V1-0\">\ud83e\udd16",
    "https://huggingface.co/Qwen/Qwen2-0.5B-Instruct\">Qwen2-0.5B-Instruct</a></td>",
    "https://internvl.readthedocs.io/en/latest/internvl3.0/finetune.html",
    "https://internvl.readthedocs.io/en/latest/internvl3.0/evaluation.html",
    "https://huggingface.co/Qwen/Qwen2.5-7B\">Qwen2.5-7B</a></td>",
    "https://huggingface.co/OpenGVLab/InternVL-Chat-V1-1",
    "https://modelscope.cn/models/OpenGVLab/InternVL2_5-26B\">\ud83e\udd16",
    "https://trendshift.io/api/badge/repositories/9803\"",
    "https://modelscope.cn/models/OpenGVLab/InternVL-Chat-V1-2-Plus\">\ud83e\udd16",
    "https://huggingface.co/OpenGVLab/InternVL3-9B\">\ud83e\udd17",
    "https://modelscope.cn/models/OpenGVLab/Mini-InternVL-Chat-2B-V1-5\">\ud83e\udd16",
    "https://huggingface.co/OpenGVLab/InternVL2_5-38B-MPO\">\ud83e\udd17",
    "https://internvl.readthedocs.io/en/latest/internvl1.5/evaluation.html",
    "https://huggingface.co/OpenGVLab/InternVL3-38B\">\ud83e\udd17",
    "https://modelscope.cn/models/OpenGVLab/InternVL2_5-4B-MPO\">\ud83e\udd16",
    "https://huggingface.co/papers/2312.14238",
    "https://github.com/InternLM/lmdeploy",
    "https://arxiv.org/abs/2411.10442",
    "https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl3.0/visualprm_data_construction",
    "https://huggingface.co/datasets/OpenGVLab/MMPR",
    "https://huggingface.co/OpenGVLab/InternVL2-8B-MPO",
    "https://github.com/haotian-liu/LLaVA",
    "https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct\">Qwen2.5&#8209;0.5B&#8209;Instruct</a></td>",
    "https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2\">\ud83e\udd17",
    "https://internvl.readthedocs.io/en/latest/internvl1.2/finetune.html",
    "https://huggingface.co/OpenGVLab/VisualPRM-8B",
    "https://modelscope.cn/models/OpenGVLab/InternVL2_5-26B-MPO\">\ud83e\udd16",
    "https://internvl.readthedocs.io/en/latest/internvl2.0/preference_optimization.html",
    "https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/mini_internvl",
    "https://internvl.readthedocs.io/en/latest/internvl3.0/preference_optimization.html",
    "https://internvl.readthedocs.io/en/latest/internvl2.5/introduction.html",
    "https://internvl.readthedocs.io/en/latest/internvl1.2/introduction.html",
    "https://internvl.readthedocs.io/en/latest/internvl2.5/quick_start.html",
    "https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5\">InternViT&#8209;6B&#8209;448px&#8209;V1&#8209;5</a></td>",
    "https://github.com/user-attachments/assets/bd62ab46-f0ea-40c6-ab10-7fde671716cc\">",
    "https://modelscope.cn/models/OpenGVLab/InternVL3-2B\">\ud83e\udd16",
    "https://huggingface.co/OpenGVLab/InternVL2_5-8B-MPO\">\ud83e\udd17",
    "https://huggingface.co/datasets/OpenGVLab/VisualPRM400K",
    "https://huggingface.co/OpenGVLab/InternVL2_5-8B\">\ud83e\udd17",
    "https://huggingface.co/OpenGVLab/InternVL2_5-4B\">\ud83e\udd17",
    "https://huggingface.co/OpenGVLab/InternVL-14B-224px",
    "https://internvl.readthedocs.io/en/latest/internvl1.2/quick_start.html",
    "https://trendshift.io/repositories/9803\"",
    "https://internvl.readthedocs.io/en/latest/internvl2.0/domain_adaptation.html",
    "https://github.com/mlfoundations/open_clip",
    "https://huggingface.co/collections/OpenGVLab/internvl3-67f7f690be79c2fe9d74fe9d",
    "https://modelscope.cn/models/OpenGVLab/InternVL2-40B\">\ud83e\udd16",
    "https://internvl.readthedocs.io/en/latest/get_started/chat_data_format.html#video-data",
    "https://internvl.readthedocs.io/en/latest/internvl3.0/deployment.html",
    "https://huggingface.co/datasets/Weiyun1025/InternVL-Performance/resolve/main/internvl3/overall.png",
    "https://github.com/czczup/ViT-Adapter",
    "https://modelscope.cn/models/OpenGVLab/InternVL2_5-78B\">\ud83e\udd16",
    "https://internvl.readthedocs.io/en/latest/internvl1.1/introduction.html",
    "https://internvl.readthedocs.io/en/latest/internvl2.0/deployment.html",
    "https://modelscope.cn/models/OpenGVLab/InternVL2_5-2B-MPO\">\ud83e\udd16",
    "https://internvl.readthedocs.io/en/latest/get_started/installation.html",
    "https://huggingface.co/OpenGVLab/InternVL2_5-4B-MPO\">\ud83e\udd17",
    "https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-0\">\ud83e\udd17",
    "https://internvl.readthedocs.io/en/latest/internvl1.0/clip_benchmark.html",
    "https://internvl.readthedocs.io/en/latest/internvl1.5/finetune.html",
    "https://huggingface.co/internlm/internlm3-8b-instruct\">internlm3-8b-instruct</a></td>",
    "https://huggingface.co/OpenGVLab/InternVL2_5-26B-MPO\">\ud83e\udd17",
    "https://modelscope.cn/models/OpenGVLab/InternVL2_5-38B\">\ud83e\udd16",
    "https://huggingface.co/OpenGVLab/InternVL2_5-2B-MPO\">\ud83e\udd17",
    "https://arxiv.org/abs/2504.10479},",
    "https://internvl.readthedocs.io/en/latest/internvl1.0/segmentation.html",
    "https://huggingface.co/papers/2504.10479",
    "https://huggingface.co/OpenGVLab/InternVL2_5-78B\">\ud83e\udd17",
    "https://huggingface.co/OpenGVLab/InternVL-Chat-ViT-6B-Vicuna-7B\">\ud83e\udd17",
    "https://huggingface.co/internlm/internlm2_5-1_8b-chat\">internlm2_5-1_8b-chat</a></td>",
    "https://internlm.intern-ai.org.cn/api/document",
    "https://huggingface.co/OpenGVLab/InternVL2-8B\">\ud83e\udd17",
    "https://huggingface.co/papers/2412.05271",
    "https://internvl.readthedocs.io/en/latest/internvl3.0/quick_start.html",
    "https://modelscope.cn/models/OpenGVLab/InternVL2-Llama3-76B\">\ud83e\udd16",
    "https://huggingface.co/OpenGVLab/Mini-InternVL-Chat-4B-V1-5\">\ud83e\udd17",
    "https://huggingface.co/Qwen/Qwen2.5-32B-Instruct\">Qwen2.5-32B-Instruct</a></td>",
    "https://internvl.readthedocs.io/en/latest/get_started/chat_data_format.html#pure-text-data",
    "https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl3.0/mpo",
    "https://internvl.readthedocs.io/en/latest/internvl1.0/internvl_chat_llava.html",
    "https://modelscope.cn/models/OpenGVLab/InternVL3-9B\">\ud83e\udd16",
    "https://modelscope.cn/models/OpenGVLab/InternVL2_5-4B\">\ud83e\udd16",
    "https://zhuanlan.zhihu.com/p/702946079",
    "https://huggingface.co/OpenGVLab/InternVL-Chat-V1-1\">\ud83e\udd17",
    "https://zhuanlan.zhihu.com/p/699439759",
    "https://github.com/OpenGVLab/InternImage",
    "https://huggingface.co/OpenGVLab/InternVL2_5-26B\">\ud83e\udd17",
    "https://huggingface.co/OpenGVLab/InternVL3-2B\">\ud83e\udd17",
    "https://internvl.github.io/blog/2024-02-21-InternVL-1.2/",
    "https://huggingface.co/OpenGVLab/InternVL2_5-2B\">\ud83e\udd17",
    "https://github.com/user-attachments/assets/f776df09-ebba-4fd5-80c2-fec4ff1518be\"></p>",
    "https://zhuanlan.zhihu.com/p/706547971",
    "https://huggingface.co/datasets/OpenGVLab/MMPR-v1.1",
    "https://rrc.cvc.uab.es/?ch=17&com=evaluation&task=3",
    "https://arxiv.org/abs/2410.16261",
    "https://github.com/OpenGVLab/InternVL/tree/main/internvl_chat/shell/internvl3.0/mpo_data_construction",
    "https://huggingface.co/OpenGVLab/InternViT-6B-448px-V2_5\">\ud83e\udd17",
    "https://modelscope.cn/models/OpenGVLab/InternVL3-8B\">\ud83e\udd16",
    "https://huggingface.co/collections/OpenGVLab/internvl25-mpo-6753fed98cd828219b12f849",
    "https://modelscope.cn/models/OpenGVLab/InternVL-Chat-V1-5\">\ud83e\udd16",
    "https://internvl.readthedocs.io/en/latest/tutorials/faqs.html",
    "https://github.com/QwenLM/Qwen-VL/tree/master/eval_mm",
    "https://github.com/LAION-AI/CLIP_benchmark",
    "https://huggingface.co/OpenGVLab/InternViT-300M-448px-V2_5\">\ud83e\udd17",
    "https://modelscope.cn/models/OpenGVLab/InternVL2_5-1B-MPO\">\ud83e\udd16",
    "https://huggingface.co/OpenGVLab/InternViT-6B-448px-V1-5\">InternViT-6B-448px-V1-5</a></td>",
    "https://huggingface.co/NousResearch/Hermes-2-Theta-Llama-3-70B\">Hermes\u20112\u2011Theta\u2011<br>Llama\u20113\u201170B</a></td>",
    "https://modelscope.cn/models/OpenGVLab/Mini-InternVL-Chat-4B-V1-5\">\ud83e\udd16",
    "https://huggingface.co/OpenGVLab/InternVL3-1B\">\ud83e\udd17",
    "https://modelscope.cn/models/OpenGVLab/InternVL2_5-2B\">\ud83e\udd16",
    "https://modelscope.cn/models/OpenGVLab/InternVL2-8B\">\ud83e\udd16",
    "https://huggingface.co/spaces/OpenGVLab/InternVL",
    "https://huggingface.co/OpenGVLab/InternVL2_5-1B-MPO\">\ud83e\udd17",
    "https://internvl.readthedocs.io/en/latest/get_started/local_chat_demo.html#streamlit-demo",
    "https://internvl.github.io/blog/2025-03-13-VisualPRM/",
    "https://huggingface.co/Qwen/Qwen2.5-32B\">Qwen2.5-32B</a></td>",
    "https://huggingface.co/OpenGVLab/InternVL2-26B\">\ud83e\udd17",
    "https://huggingface.co/internlm/internlm2_5-7b-chat\">internlm2_5-7b-chat</a></td>",
    "https://internvl.readthedocs.io/en/latest/internvl2.5/finetune.html",
    "https://modelscope.cn/models/OpenGVLab/InternVL-Chat-V1-2\">\ud83e\udd16",
    "https://internvl.github.io/blog/",
    "https://huggingface.co/OpenGVLab/InternVL-Chat-V1-2-Plus",
    "https://huggingface.co/papers/2404.16821",
    "https://huggingface.co/OpenGVLab/InternVL3-78B\">\ud83e\udd17"
  ]
}
```

</details>


---

## Repository 3: modelscope/ms-swift

# GitHub Repository Data

**Repository:** [modelscope/ms-swift](https://github.com/modelscope/ms-swift)

## Basic Information

- **Description:** Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 500+ LLMs (Qwen3, Qwen3-MoE, Llama4, InternLM3, DeepSeek-R1, ...) and 200+ MLLMs (Qwen2.5-VL, Qwen2.5-Omni, Qwen2-Audio, Ovis2, InternVL3, Llava, GLM4v, Phi4, ...) (AAAI 2025).
- **Created:** 2023-08-01T15:06:39+00:00
- **Last Updated:** 2025-06-22T01:38:35+00:00
- **Last Pushed:** 2025-06-21T18:30:09+00:00
- **Default Branch:** main
- **Size:** 65328 KB

## Statistics

- **Stars:** 8,207
- **Forks:** 706
- **Watchers:** 8,207
- **Open Issues:** 769
- **Total Issues:** 0
- **Pull Requests:** 2,035

## License

- **Type:** Apache License 2.0
- **SPDX ID:** Apache-2.0
- **URL:** [License](https://github.com/modelscope/ms-swift/blob/main/LICENSE)

## Languages

- **Python:** 2,586,588 bytes
- **Shell:** 6,134 bytes
- **Makefile:** 359 bytes

## Topics

- `llm`
- `lora`
- `llama`
- `sft`
- `deploy`
- `multimodal`
- `peft`
- `internvl`
- `liger`
- `qwen2-vl`
- `rft`
- `deepseek-r1`
- `embedding`
- `grpo`
- `open-r1`
- `megatron`
- `omni`
- `llama4`
- `qwen3`
- `qwen3-moe`

## Top Contributors

1. **Jintao-Huang** - 1111 contributions
2. **tastelikefeet** - 501 contributions
3. **hjh0119** - 199 contributions
4. **slin000111** - 26 contributions
5. **DaozeZhang** - 18 contributions
6. **baoleai** - 16 contributions
7. **mi804** - 13 contributions
8. **wenmengzhou** - 12 contributions
9. **Yunnglin** - 8 contributions
10. **lxline** - 6 contributions

## File Structure (Sample of 10 files)

Total files: 1,033

- `.dev_scripts` (tree)
- `.dev_scripts/build_docs.sh` (blob)
- `.dev_scripts/ci_container_test.sh` (blob)
- `.dev_scripts/dockerci.sh` (blob)
- `.github` (tree)
- `.github/ISSUE_TEMPLATE` (tree)
- `.github/ISSUE_TEMPLATE/bug_report.md` (blob)
- `.github/ISSUE_TEMPLATE/custom.md` (blob)
- `.github/ISSUE_TEMPLATE/feature_request.md` (blob)
- `.github/PULL_REQUEST_TEMPLATE.md` (blob)

## Recent Issues

- ðŸŸ¢ **#4659** support deepseek-r1 (open)
- ðŸ”´ **#4658** [gkd] support use_logits_to_keep/padding_free/packing & update gkd shell (closed)
- ðŸ”´ **#4657** [docs] update gkd (closed)
- ðŸ”´ **#4656** 'weight' must be 2-D (closed)
- ðŸ”´ **#4655** compat megatron-core 0.11 (closed)

## Recent Pull Requests

- ðŸŸ¢ **#4659** support deepseek-r1 (open)
- ðŸ”´ **#4658** [gkd] support use_logits_to_keep/padding_free/packing & update gkd shell (closed)
- ðŸ”´ **#4657** [docs] update gkd (closed)
- ðŸ”´ **#4655** compat megatron-core 0.11 (closed)
- ðŸ”´ **#4654** [megatron] fix eval data_collator (closed)

## Recent Commits

- **67549f29** [gkd] support gkd use_logits_to_keep & padding_free & packing (#4658) - Jintao (2025-06-21T18:30:09+00:00)
- **871278cd** [docs] update gkd (#4657) - Jintao (2025-06-20T16:09:21+00:00)
- **cdf23345** [megatron] compat megatron-core 0.11 (#4655) - Jintao (2025-06-20T10:05:36+00:00)
- **a71d11c5** [megatron] fix eval data_collator (#4654) - Jintao (2025-06-20T09:16:37+00:00)
- **778b1bdf** fix device_map & ddp rank0 (#4650) - Jintao (2025-06-20T07:35:49+00:00)
- **8dcace6c** fix packing & load_from_cache_file (#4649) - Jintao (2025-06-20T06:22:40+00:00)
- **2fcdd11b** [model] fix model_meta (#4647) - Jintao (2025-06-20T05:44:41+00:00)
- **4be8f5d5** [template] optimize get_length (#4641) - Jintao (2025-06-20T03:33:07+00:00)
- **0f31a5a9** [docs] update qwen3 best_practice (#4300) - Jintao (2025-06-19T15:50:35+00:00)
- **5f5b5739** update docs readme (#4639) - Jintao (2025-06-19T12:07:14+00:00)

## External Links Found in README

- https://img.shields.io/github/license/modelscope/ms-swift"></a>
- https://trendshift.io/repositories/6427"
- https://github.com/modelscope/ms-swift/tree/main/examples/train/grpo/external
- https://github.com/modelscope/ms-swift/blob/main/requirements/install_all.sh
- https://github.com/modelscope/ms-swift/blob/main/examples/notebook/qwen2_5-self-cognition/self-cognition-sft.ipynb
- https://img.shields.io/badge/PR-welcome-55EB99.svg"></a>
- https://pypi.org/project/ms-swift/"><img
- https://github.com/modelscope/modelscope/blob/master/LICENSE
- https://github.com/modelscope/ms-swift/blob/main/examples/train/multimodal/rlhf/dpo
- https://img.shields.io/badge/pytorch-%E2%89%A52.0-orange.svg">
- https://github.com/modelscope/ms-swift/tree/main/examples/train/long_text
- https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/gkd
- https://github.com/modelscope/ms-swift/blob/main/examples/train/pretrain/train.sh
- https://ojs.aaai.org/index.php/AAAI/article/view/35383
- https://swift.readthedocs.io/en/latest/GetStarted/Web-UI.html
- https://github.com/vllm-project/vllm
- https://github.com/InternLM/lmdeploy
- https://github.com/modelscope/evalscope/
- https://github.com/modelscope/ms-swift/blob/main/examples/train/grpo/internal
- https://github.com/modelscope/ms-swift/blob/main/examples/infer/sglang

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 673412924,
  "name": "ms-swift",
  "full_name": "modelscope/ms-swift",
  "description": "Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 500+ LLMs (Qwen3, Qwen3-MoE, Llama4, InternLM3, DeepSeek-R1, ...) and 200+ MLLMs (Qwen2.5-VL, Qwen2.5-Omni, Qwen2-Audio, Ovis2, InternVL3, Llava, GLM4v, Phi4, ...) (AAAI 2025).",
  "html_url": "https://github.com/modelscope/ms-swift",
  "clone_url": "https://github.com/modelscope/ms-swift.git",
  "ssh_url": "git@github.com:modelscope/ms-swift.git",
  "homepage": "https://swift.readthedocs.io/zh-cn/latest/",
  "topics": [
    "llm",
    "lora",
    "llama",
    "sft",
    "deploy",
    "multimodal",
    "peft",
    "internvl",
    "liger",
    "qwen2-vl",
    "rft",
    "deepseek-r1",
    "embedding",
    "grpo",
    "open-r1",
    "megatron",
    "omni",
    "llama4",
    "qwen3",
    "qwen3-moe"
  ],
  "default_branch": "main",
  "created_at": "2023-08-01T15:06:39+00:00",
  "updated_at": "2025-06-22T01:38:35+00:00",
  "pushed_at": "2025-06-21T18:30:09+00:00",
  "size_kb": 65328,
  "watchers_count": 8207,
  "stargazers_count": 8207,
  "forks_count": 706,
  "open_issues_count": 769,
  "license": {
    "key": "apache-2.0",
    "name": "Apache License 2.0",
    "spdx_id": "Apache-2.0",
    "url": "https://github.com/modelscope/ms-swift/blob/main/LICENSE"
  },
  "languages": {
    "Python": 2586588,
    "Shell": 6134,
    "Makefile": 359
  },
  "top_contributors": [
    {
      "login": "Jintao-Huang",
      "contributions": 1111
    },
    {
      "login": "tastelikefeet",
      "contributions": 501
    },
    {
      "login": "hjh0119",
      "contributions": 199
    },
    {
      "login": "slin000111",
      "contributions": 26
    },
    {
      "login": "DaozeZhang",
      "contributions": 18
    },
    {
      "login": "baoleai",
      "contributions": 16
    },
    {
      "login": "mi804",
      "contributions": 13
    },
    {
      "login": "wenmengzhou",
      "contributions": 12
    },
    {
      "login": "Yunnglin",
      "contributions": 8
    },
    {
      "login": "lxline",
      "contributions": 6
    },
    {
      "login": "jiangzeyinzi",
      "contributions": 5
    },
    {
      "login": "Zhikaiiii",
      "contributions": 5
    },
    {
      "login": "yingdachen",
      "contributions": 4
    },
    {
      "login": "Leoyzen",
      "contributions": 3
    },
    {
      "login": "co63oc",
      "contributions": 3
    },
    {
      "login": "lincq2000",
      "contributions": 3
    },
    {
      "login": "anw90",
      "contributions": 3
    },
    {
      "login": "wning13",
      "contributions": 2
    },
    {
      "login": "LukeForeverYoung",
      "contributions": 2
    },
    {
      "login": "zsxm1998",
      "contributions": 2
    }
  ],
  "file_tree_count": 1033,
  "file_tree_sample": [
    {
      "path": ".dev_scripts",
      "type": "tree"
    },
    {
      "path": ".dev_scripts/build_docs.sh",
      "type": "blob"
    },
    {
      "path": ".dev_scripts/ci_container_test.sh",
      "type": "blob"
    },
    {
      "path": ".dev_scripts/dockerci.sh",
      "type": "blob"
    },
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/bug_report.md",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/custom.md",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/feature_request.md",
      "type": "blob"
    },
    {
      "path": ".github/PULL_REQUEST_TEMPLATE.md",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 2035,
  "recent_issues": [
    {
      "number": 4659,
      "title": "support deepseek-r1",
      "state": "open"
    },
    {
      "number": 4658,
      "title": "[gkd] support use_logits_to_keep/padding_free/packing & update gkd shell",
      "state": "closed"
    },
    {
      "number": 4657,
      "title": "[docs] update gkd",
      "state": "closed"
    },
    {
      "number": 4656,
      "title": "'weight' must be 2-D",
      "state": "closed"
    },
    {
      "number": 4655,
      "title": "compat megatron-core 0.11",
      "state": "closed"
    }
  ],
  "recent_pulls": [
    {
      "number": 4659,
      "title": "support deepseek-r1",
      "state": "open"
    },
    {
      "number": 4658,
      "title": "[gkd] support use_logits_to_keep/padding_free/packing & update gkd shell",
      "state": "closed"
    },
    {
      "number": 4657,
      "title": "[docs] update gkd",
      "state": "closed"
    },
    {
      "number": 4655,
      "title": "compat megatron-core 0.11",
      "state": "closed"
    },
    {
      "number": 4654,
      "title": "[megatron] fix eval data_collator",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "67549f29b5fd756d2af2c073470c6d6b9e705e66",
      "author": "Jintao",
      "date": "2025-06-21T18:30:09+00:00",
      "message": "[gkd] support gkd use_logits_to_keep & padding_free & packing (#4658)"
    },
    {
      "sha": "871278cd7ac6dbaa7c64c7a2e18d0f0fdd89f1a6",
      "author": "Jintao",
      "date": "2025-06-20T16:09:21+00:00",
      "message": "[docs] update gkd (#4657)"
    },
    {
      "sha": "cdf233452555641bbdc4bc7701a454c43e215db6",
      "author": "Jintao",
      "date": "2025-06-20T10:05:36+00:00",
      "message": "[megatron] compat megatron-core 0.11 (#4655)"
    },
    {
      "sha": "a71d11c5c76747fe3e03944f8161dc29dcc40acd",
      "author": "Jintao",
      "date": "2025-06-20T09:16:37+00:00",
      "message": "[megatron] fix eval data_collator (#4654)"
    },
    {
      "sha": "778b1bdfb42bfd2c0e59e5ee119baa7b1516c878",
      "author": "Jintao",
      "date": "2025-06-20T07:35:49+00:00",
      "message": "fix device_map & ddp rank0 (#4650)"
    },
    {
      "sha": "8dcace6c48fbee52553219f3bf02866b441f6cd0",
      "author": "Jintao",
      "date": "2025-06-20T06:22:40+00:00",
      "message": "fix packing & load_from_cache_file (#4649)"
    },
    {
      "sha": "2fcdd11b32f6301d4c7e61b13fe04adb9ac1ed95",
      "author": "Jintao",
      "date": "2025-06-20T05:44:41+00:00",
      "message": "[model] fix model_meta (#4647)"
    },
    {
      "sha": "4be8f5d598414afc2fc74358716e204e4e51ac68",
      "author": "Jintao",
      "date": "2025-06-20T03:33:07+00:00",
      "message": "[template] optimize get_length (#4641)"
    },
    {
      "sha": "0f31a5a9ebe0de1240db56e128ed0251c9b9d223",
      "author": "Jintao",
      "date": "2025-06-19T15:50:35+00:00",
      "message": "[docs] update qwen3 best_practice (#4300)"
    },
    {
      "sha": "5f5b57394c5eb47082d41ee1b220a6cc071112bb",
      "author": "Jintao",
      "date": "2025-06-19T12:07:14+00:00",
      "message": "update docs readme (#4639)"
    },
    {
      "sha": "36fdf381e5e88cb8a71c9d69c1d8936a989318cc",
      "author": "Jintao",
      "date": "2025-06-19T08:36:55+00:00",
      "message": "update docs & shell (#4637)"
    },
    {
      "sha": "1ceba3534ce6c81913e8184de9f7ca49b3e03c88",
      "author": "Jintao",
      "date": "2025-06-19T05:44:13+00:00",
      "message": "[infer/deploy/eval/app] support sglang engine (#3810)"
    },
    {
      "sha": "c611569b1cad36f35b43703b72ca868f11ce72fe",
      "author": "jinghanhu",
      "date": "2025-06-18T13:20:38+00:00",
      "message": "[doc] LaTeX rendering (#4629)"
    },
    {
      "sha": "b1624a2eceffda175c528ac6f98c85ddb347c37b",
      "author": "Jintao",
      "date": "2025-06-17T13:11:04+00:00",
      "message": "[rollout] swift rollout add template (#4626)"
    },
    {
      "sha": "336520adf06ae5e434a802ab6d267b2145fdfead",
      "author": "sosofun",
      "date": "2025-06-17T12:11:05+00:00",
      "message": "[loss_scale] support last_round_with_ignore_empty_think for rag (#4623)"
    },
    {
      "sha": "6abc4de1880ec82a986955794491a5463a7c0223",
      "author": "Jintao",
      "date": "2025-06-17T12:07:58+00:00",
      "message": "[megatron] fix max_epochs tp (#4624)"
    },
    {
      "sha": "ac1d2c86d84dfce020903ac195d3b9e2c0162450",
      "author": "Jintao",
      "date": "2025-06-17T11:26:34+00:00",
      "message": "[ppo] fix ppo (#4622)"
    },
    {
      "sha": "3f502bec09e74aeb99fe1b96317da5b7c3ed5b40",
      "author": "Jintao",
      "date": "2025-06-17T08:54:49+00:00",
      "message": "[docs] remove Qwen3-32B-Base (#4621)"
    },
    {
      "sha": "85c16d44eaa71294e4fef8e78699178ec9bcd42a",
      "author": "Jintao",
      "date": "2025-06-17T07:22:26+00:00",
      "message": "[gkd] support gkd_trainer (#4587)"
    },
    {
      "sha": "29b8395a51028d9d03262561087c3c31ce01832d",
      "author": "Jintao",
      "date": "2025-06-17T06:21:11+00:00",
      "message": "Fix minimax & fix agent_template (#4618)"
    }
  ],
  "readme_text": "# SWIFT (Scalable lightWeight Infrastructure for Fine-Tuning)\n\n<p align=\"center\">\n    <br>\n    <img src=\"asset/banner.png\"/>\n    <br>\n<p>\n<p align=\"center\">\n<a href=\"https://modelscope.cn/home\">ModelScope Community Website</a>\n<br>\n        <a href=\"README_CN.md\">\u4e2d\u6587</a> &nbsp \uff5c &nbsp English &nbsp\n</p>\n\n<p align=\"center\">\n<img src=\"https://img.shields.io/badge/python-3.10-5be.svg\">\n<img src=\"https://img.shields.io/badge/pytorch-%E2%89%A52.0-orange.svg\">\n<a href=\"https://github.com/modelscope/modelscope/\"><img src=\"https://img.shields.io/badge/modelscope-%E2%89%A51.19-5D91D4.svg\"></a>\n<a href=\"https://pypi.org/project/ms-swift/\"><img src=\"https://badge.fury.io/py/ms-swift.svg\"></a>\n<a href=\"https://github.com/modelscope/ms-swift/blob/main/LICENSE\"><img src=\"https://img.shields.io/github/license/modelscope/ms-swift\"></a>\n<a href=\"https://pepy.tech/project/ms-swift\"><img src=\"https://pepy.tech/badge/ms-swift\"></a>\n<a href=\"https://github.com/modelscope/ms-swift/pulls\"><img src=\"https://img.shields.io/badge/PR-welcome-55EB99.svg\"></a>\n</p>\n\n<p align=\"center\">\n<a href=\"https://trendshift.io/repositories/6427\" target=\"_blank\"><img src=\"https://trendshift.io/api/badge/repositories/6427\" alt=\"modelscope%2Fswift | Trendshift\" style=\"width: 250px; height: 55px;\" width=\"250\" height=\"55\"/></a>\n</p>\n\n<p align=\"center\">\n        <a href=\"https://arxiv.org/abs/2408.05517\">Paper</a> &nbsp \uff5c <a href=\"https://swift.readthedocs.io/en/latest/\">English Documentation</a> &nbsp \uff5c &nbsp <a href=\"https://swift.readthedocs.io/zh-cn/latest/\">\u4e2d\u6587\u6587\u6863</a> &nbsp\n</p>\n\n## \ud83d\udcd6 Table of Contents\n- [Groups](#-Groups)\n- [Introduction](#-introduction)\n- [News](#-news)\n- [Installation](#%EF%B8%8F-installation)\n- [Quick Start](#-quick-Start)\n- [Usage](#-Usage)\n- [License](#-License)\n- [Citation](#-citation)\n\n\n## \u260e Groups\n\nYou can contact us and communicate with us by adding our group:\n\n\n[Discord Group](https://discord.com/invite/D27yfEFVz5)              |  WeChat Group\n:-------------------------:|:-------------------------:\n<img src=\"asset/discord_qr.jpg\" width=\"200\" height=\"200\">  |  <img src=\"asset/wechat.png\" width=\"200\" height=\"200\">\n\n\n## \ud83d\udcdd Introduction\n\ud83c\udf72 ms-swift is an official framework provided by the ModelScope community for fine-tuning and deploying large language models and multi-modal large models. It currently supports the training (pre-training, fine-tuning, human alignment), inference, evaluation, quantization, and deployment of 500+ large models and 200+ multi-modal large models. These large language models (LLMs) include models such as Qwen3, Qwen3-MoE, Qwen2.5, InternLM3, GLM4, Mistral, DeepSeek-R1, Yi1.5, TeleChat2, Baichuan2, and Gemma2. The multi-modal LLMs include models such as Qwen2.5-VL, Qwen2-Audio, Llama4, Llava, InternVL3, MiniCPM-V-2.6, GLM4v, Xcomposer2.5, Yi-VL, DeepSeek-VL2, Phi3.5-Vision, and GOT-OCR2.\n\n\ud83c\udf54 Additionally, ms-swift incorporates the latest training technologies, including lightweight techniques such as LoRA, QLoRA, Llama-Pro, LongLoRA, GaLore, Q-GaLore, LoRA+, LISA, DoRA, FourierFt, ReFT, UnSloth, and Liger, as well as human alignment training methods like DPO, GRPO, RM, PPO, GKD, KTO, CPO, SimPO, and ORPO. ms-swift supports acceleration of inference, evaluation, and deployment modules using vLLM, SGLang and LMDeploy, and it supports model quantization with technologies like GPTQ, AWQ, and BNB. Furthermore, ms-swift offers a Gradio-based Web UI and a wealth of best practices.\n\n**Why choose ms-swift?**\n\n- \ud83c\udf4e **Model Types**: Supports 500+ pure text large models, **200+ multi-modal large models**, as well as All-to-All multi-modal models, sequence classification models, and embedding models, **covering the entire process from training to deployment**.\n- **Dataset Types**: Comes with 150+ pre-training, fine-tuning, human alignment, multi-modal datasets, and supports custom datasets.\n- **Hardware Support**: Compatible with CPU, RTX series, T4/V100, A10/A100/H100, Ascend NPU, MPS, etc.\n- \ud83c\udf4a **Lightweight Training**: Supports lightweight fine-tuning methods like LoRA, QLoRA, DoRA, LoRA+, ReFT, RS-LoRA, LLaMAPro, Adapter, GaLore, Q-Galore, LISA, UnSloth, Liger-Kernel.\n- **Distributed Training**: Supports distributed data parallel (DDP), device_map simple model parallelism, DeepSpeed ZeRO2/ZeRO3, FSDP, and other distributed training techniques.\n- **Quantization Training**: Supports training quantized models like BNB, AWQ, GPTQ, AQLM, HQQ, EETQ.\n- **RLHF Training**: Supports human alignment training methods such as DPO, GRPO, RM, PPO, GKD, KTO, CPO, SimPO, ORPO for both pure text and multi-modal large models.\n- \ud83c\udf53 **Multi-Modal Training**: Supports training on different modalities like images, videos, and audio, for tasks like VQA, captioning, OCR, and grounding.\n- **Interface Training**: Provides capabilities for training, inference, evaluation, quantization through an interface, completing the whole large model pipeline.\n- **Plugin and Extension**: Supports custom model and dataset extensions, as well as customization of components like loss, metric, trainer, loss-scale, callback, optimizer.\n- \ud83c\udf49 **Toolbox Capabilities**: Offers not only training support for large models and multi-modal large models but also covers the entire process of inference, evaluation, quantization, and deployment.\n- **Inference Acceleration**: Supports inference acceleration engines like PyTorch, vLLM, SGLang, LmDeploy, and provides OpenAI API for accelerating inference, deployment, and evaluation modules.\n- **Model Evaluation**: Uses EvalScope as the evaluation backend and supports evaluation on 100+ datasets for both pure text and multi-modal models.\n- **Model Quantization**: Supports AWQ, GPTQ, and BNB quantized exports, with models that can use vLLM/SGLang/LmDeploy for inference acceleration and continue training.\n\n\n## \ud83c\udf89 News\n- \ud83c\udf81 2025.06.18: Support for accelerating the ms-swift [inference](https://github.com/modelscope/ms-swift/blob/main/examples/infer/sglang), deployment, evaluation, and UI modules using the [sglang](https://github.com/sgl-project/sglang) inference acceleration engine. Simply set `--infer_backend sglang` to enable it.\n- \ud83c\udf81 2025.06.15: Support for GKD training on both pure text large models and multimodal models. Training scripts can be found here: [Pure Text](https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/gkd), [Multimodal](https://github.com/modelscope/ms-swift/blob/main/examples/train/multimodal/rlhf/gkd).\n- \ud83c\udf81 2025.06.11: Support for using Megatron parallelism techniques for RLHF training. The training script can be found [here](https://github.com/modelscope/ms-swift/tree/main/examples/train/megatron/rlhf).\n- \ud83c\udf81 2025.05.29: Support sequence parallel in pt, sft, dpo and grpo, check script [here](https://github.com/modelscope/ms-swift/tree/main/examples/train/long_text).\n- \ud83c\udf81 2025.05.11: GRPO now supports custom processing logic for reward models. See the GenRM example [here](./docs/source_en/Instruction/GRPO.md#customized-reward-models).\n- \ud83c\udf81 2025.04.15: The ms-swift paper has been accepted by AAAI 2025. You can find the paper at [this link](https://ojs.aaai.org/index.php/AAAI/article/view/35383).\n- \ud83c\udf81 2025.03.23: Multi-round GRPO is now supported for training multi-turn dialogue scenarios (e.g., agent tool calling). Please refer to the [training script](examples/train/grpo/internal/vllm_multi_round.sh).\n- \ud83c\udf81 2025.03.16: Support for Megatron's parallel training techniques is now available. Please see the [Megatron-SWIFT training documentation](https://swift.readthedocs.io/en/latest/Instruction/Megatron-SWIFT-Training.html).\n- \ud83c\udf81 2025.03.15: Fine-tuning of embedding models for both pure text and multimodal models is supported. Please check the [training script](examples/train/embedding).\n- \ud83c\udf81 2025.03.05: The hybrid mode for GRPO is supported, with a script for training a 72B model on 4 GPUs (4*80G) available [here](examples/train/grpo/internal/vllm_72b_4gpu.sh). Tensor parallelism with vllm is also supported, with the training script available [here](examples/train/grpo/internal).\n- \ud83c\udf81 2025.02.21: The GRPO algorithm now supports LMDeploy, with the training script available [here](examples/train/grpo/internal/full_lmdeploy.sh). Additionally, the performance of the GRPO algorithm has been tested, achieving a training speed increase of up to 300% using various tricks. Please check the WanDB table [here](https://wandb.ai/tastelikefeet/grpo_perf_test?nw=nwuseryuzezyz).\n- \ud83c\udf81 2025.02.21: The `swift sample` command is now supported. The reinforcement fine-tuning script can be found [here](docs/source_en/Instruction/Reinforced-Fine-tuning.md), and the large model API distillation sampling script is available [here](examples/sampler/distill/distill.sh).\n- \ud83d\udd25 2025.02.12: Support for the GRPO (Group Relative Policy Optimization) training algorithm has been added. Documentation is available [here](docs/source_en/Instruction/GRPO.md).\n- \ud83c\udf81 2024.12.04: Major update to **ms-swift 3.0**. Please refer to the [release notes and changes](docs/source_en/Instruction/ReleaseNote3.0.md).\n<details><summary>More</summary>\n\n- \ud83c\udf89 2024.08.12: The ms-swift paper has been published on arXiv and can be read [here](https://arxiv.org/abs/2408.05517).\n- \ud83d\udd25 2024.08.05: Support for using [evalscope](https://github.com/modelscope/evalscope/) as a backend for evaluating large models and multimodal models.\n- \ud83d\udd25 2024.07.29: Support for using [vllm](https://github.com/vllm-project/vllm) and [lmdeploy](https://github.com/InternLM/lmdeploy) to accelerate inference for large models and multimodal models. When performing infer/deploy/eval, you can specify `--infer_backend vllm/lmdeploy`.\n- \ud83d\udd25 2024.07.24: Support for human preference alignment training for multimodal large models, including DPO/ORPO/SimPO/CPO/KTO/RM/PPO.\n- \ud83d\udd25 2024.02.01: Support for Agent training! The training algorithm is derived from [this paper](https://arxiv.org/pdf/2309.00986.pdf).\n</details>\n\n## \ud83d\udee0\ufe0f Installation\nTo install using pip:\n```shell\npip install ms-swift -U\n```\n\nTo install from source:\n```shell\n# pip install git+https://github.com/modelscope/ms-swift.git\n\ngit clone https://github.com/modelscope/ms-swift.git\ncd ms-swift\npip install -e .\n```\n\nRunning Environment:\n\n|              | Range        | Recommended | Notes                                     |\n| ------------ |--------------| ----------- | ----------------------------------------- |\n| python       | >=3.9        | 3.10        |                                           |\n| cuda         |              | cuda12      | No need to install if using CPU, NPU, MPS |\n| torch        | >=2.0        |             |                                           |\n| transformers | >=4.33       | 4.51.3      |                                           |\n| modelscope   | >=1.23       |             |                                           |\n| peft | >=0.11,<0.16 | ||\n| trl | >=0.13,<0.19 | 0.18 |RLHF|\n| deepspeed    | >=0.14       | 0.16.9 | Training                                  |\n| vllm         | >=0.5.1      | 0.8.5.post1       | Inference/Deployment/Evaluation           |\n| sglang |     | 0.4.6.post5 | Inference/Deployment/Evaluation |\n| lmdeploy     | >=0.5        | 0.8       | Inference/Deployment/Evaluation           |\n| evalscope | >=0.11       |  | Evaluation |\n\nFor more optional dependencies, you can refer to [here](https://github.com/modelscope/ms-swift/blob/main/requirements/install_all.sh).\n\n\n## \ud83d\ude80 Quick Start\n\n10 minutes of self-cognition fine-tuning of Qwen2.5-7B-Instruct on a single 3090 GPU:\n\n### Command Line Interface\n\n```shell\n# 22GB\nCUDA_VISIBLE_DEVICES=0 \\\nswift sft \\\n    --model Qwen/Qwen2.5-7B-Instruct \\\n    --train_type lora \\\n    --dataset 'AI-ModelScope/alpaca-gpt4-data-zh#500' \\\n              'AI-ModelScope/alpaca-gpt4-data-en#500' \\\n              'swift/self-cognition#500' \\\n    --torch_dtype bfloat16 \\\n    --num_train_epochs 1 \\\n    --per_device_train_batch_size 1 \\\n    --per_device_eval_batch_size 1 \\\n    --learning_rate 1e-4 \\\n    --lora_rank 8 \\\n    --lora_alpha 32 \\\n    --target_modules all-linear \\\n    --gradient_accumulation_steps 16 \\\n    --eval_steps 50 \\\n    --save_steps 50 \\\n    --save_total_limit 2 \\\n    --logging_steps 5 \\\n    --max_length 2048 \\\n    --output_dir output \\\n    --system 'You are a helpful assistant.' \\\n    --warmup_ratio 0.05 \\\n    --dataloader_num_workers 4 \\\n    --model_author swift \\\n    --model_name swift-robot\n```\n\nTips:\n\n- If you want to train with a custom dataset, you can refer to [this guide](https://swift.readthedocs.io/en/latest/Customization/Custom-dataset.html) to organize your dataset format and specify `--dataset <dataset_path>`.\n- The `--model_author` and `--model_name` parameters are only effective when the dataset includes `swift/self-cognition`.\n- To train with a different model, simply modify `--model <model_id/model_path>`.\n- By default, ModelScope is used for downloading models and datasets. If you want to use HuggingFace, simply specify `--use_hf true`.\n\nAfter training is complete, use the following command to infer with the trained weights:\n\n- Here, `--adapters` should be replaced with the last checkpoint folder generated during training. Since the adapters folder contains the training parameter file `args.json`, there is no need to specify `--model`, `--system` separately; Swift will automatically read these parameters. To disable this behavior, you can set `--load_args false`.\n\n```shell\n# Using an interactive command line for inference.\nCUDA_VISIBLE_DEVICES=0 \\\nswift infer \\\n    --adapters output/vx-xxx/checkpoint-xxx \\\n    --stream true \\\n    --temperature 0 \\\n    --max_new_tokens 2048\n\n# merge-lora and use vLLM for inference acceleration\nCUDA_VISIBLE_DEVICES=0 \\\nswift infer \\\n    --adapters output/vx-xxx/checkpoint-xxx \\\n    --stream true \\\n    --merge_lora true \\\n    --infer_backend vllm \\\n    --max_model_len 8192 \\\n    --temperature 0 \\\n    --max_new_tokens 2048\n```\n\nFinally, use the following command to push the model to ModelScope:\n\n```shell\nCUDA_VISIBLE_DEVICES=0 \\\nswift export \\\n    --adapters output/vx-xxx/checkpoint-xxx \\\n    --push_to_hub true \\\n    --hub_model_id '<your-model-id>' \\\n    --hub_token '<your-sdk-token>' \\\n    --use_hf false\n```\n\n\n### Web-UI\nThe Web-UI is a **zero-threshold** training and deployment interface solution based on Gradio interface technology. For more details, you can check [here](https://swift.readthedocs.io/en/latest/GetStarted/Web-UI.html).\n\n```shell\nSWIFT_UI_LANG=en swift web-ui\n```\n\n![image.png](./docs/resources/web-ui-en.jpg)\n\n### Using Python\n\nms-swift also supports training and inference using Python. Below is pseudocode for training and inference. For more details, you can refer to [here](https://github.com/modelscope/ms-swift/blob/main/examples/notebook/qwen2_5-self-cognition/self-cognition-sft.ipynb).\n\nTraining:\n\n```python\n# Retrieve the model and template, and add a trainable LoRA module\nmodel, tokenizer = get_model_tokenizer(model_id_or_path, ...)\ntemplate = get_template(model.model_meta.template, tokenizer, ...)\nmodel = Swift.prepare_model(model, lora_config)\n\n# Download and load the dataset, and encode the text into tokens\ntrain_dataset, val_dataset = load_dataset(dataset_id_or_path, ...)\ntrain_dataset = EncodePreprocessor(template=template)(train_dataset, num_proc=num_proc)\nval_dataset = EncodePreprocessor(template=template)(val_dataset, num_proc=num_proc)\n\n# Train the model\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    data_collator=template.data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    template=template,\n)\ntrainer.train()\n```\nInference:\n\n```python\n# Perform inference using the native PyTorch engine\nengine = PtEngine(model_id_or_path, adapters=[lora_checkpoint])\ninfer_request = InferRequest(messages=[{'role': 'user', 'content': 'who are you?'}])\nrequest_config = RequestConfig(max_tokens=max_new_tokens, temperature=temperature)\n\nresp_list = engine.infer([infer_request], request_config)\nprint(f'response: {resp_list[0].choices[0].message.content}')\n```\n\n## \u2728 Usage\nHere is a minimal example of training to deployment using ms-swift. For more details, you can check the [examples](https://github.com/modelscope/ms-swift/tree/main/examples).\n\n- If you want to use other models or datasets (including multimodal models and datasets), you only need to modify `--model` to specify the corresponding model's ID or path, and modify `--dataset` to specify the corresponding dataset's ID or path.\n- By default, ModelScope is used for downloading models and datasets. If you want to use HuggingFace, simply specify `--use_hf true`.\n\n|   Useful Links |\n| ------ |\n|   [\ud83d\udd25Command Line Parameters](https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html)   |\n|   [Supported Models and Datasets](https://swift.readthedocs.io/en/latest/Instruction/Supported-models-and-datasets.html)   |\n|   [Custom Models](https://swift.readthedocs.io/en/latest/Customization/Custom-model.html), [\ud83d\udd25Custom Datasets](https://swift.readthedocs.io/en/latest/Customization/Custom-dataset.html)   |\n|   [LLM Tutorial](https://github.com/modelscope/modelscope-classroom/tree/main/LLM-tutorial)   |\n\n### Training\n\nSupported Training Methods:\n\n| Method                             | Full-Parameter                                               | LoRA                                                                                        | QLoRA                                                        | Deepspeed                                                    | Multi-Node                                                   | Multi-Modal                                                                                  |\n|------------------------------------|--------------------------------------------------------------|---------------------------------------------------------------------------------------------|--------------------------------------------------------------|--------------------------------------------------------------|--------------------------------------------------------------|----------------------------------------------------------------------------------------------|\n| Pre-training                       | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/pretrain/train.sh) | \u2705                                                                                           | \u2705                                                            | \u2705                                                            | \u2705                                                            | \u2705                                                                                            |\n| Instruction Supervised Fine-tuning | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/full/train.sh) | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/lora_sft.sh)            | [\u2705](https://github.com/modelscope/ms-swift/tree/main/examples/train/qlora) | [\u2705](https://github.com/modelscope/ms-swift/tree/main/examples/train/multi-gpu/deepspeed) | [\u2705](https://github.com/modelscope/ms-swift/tree/main/examples/train/multi-node)                                                            | [\u2705](https://github.com/modelscope/ms-swift/tree/main/examples/train/multimodal)              |\n| DPO Training                       | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/dpo)            | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/dpo) | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/multimodal/rlhf/dpo)  |\n| GRPO Training                      | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/grpo/internal) | \u2705                                                                                           | \u2705                                                            | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/tree/main/examples/train/grpo/external)                                    | \u2705                                                                                            |\n| Reward Model Training              | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/rm.sh)             | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/rm.sh) | \u2705                                                            | \u2705                                                                                            |\n| PPO Training                       | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/ppo)            | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/ppo) | \u2705                                                            | \u274c                                                                                            |\n| GKD Training                       | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/gkd)            | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/gkd) | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/multimodal/rlhf/gkd)  |\n| KTO Training                       | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/kto.sh)            | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/kto.sh) | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/multimodal/rlhf/kto.sh)  |\n| CPO Training                       | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/cpo.sh)            | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/cpo.sh) | \u2705                                                            | \u2705                                                                                            |\n| SimPO Training                     | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/simpo.sh)          | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/simpo.sh) | \u2705                                                            | \u2705                                                                                            |\n| ORPO Training                      | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/orpo.sh)           | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/orpo.sh) | \u2705                                                            | \u2705                                                                                            |\n| Classification Model Training      | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/seq_cls/qwen2_5/sft.sh) | \u2705                                                            | \u2705                                                            | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/seq_cls/qwen2_vl/sft.sh) |\n| Embedding Model Training           | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/embedding/train_gte.sh) | \u2705                                                            | \u2705                                                            | \u2705                                                            | [\u2705](https://github.com/modelscope/ms-swift/blob/main/examples/train/embedding/train_gme.sh)  |\n\n\n\nPre-training:\n```shell\n# 8*A100\nNPROC_PER_NODE=8 \\\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\nswift pt \\\n    --model Qwen/Qwen2.5-7B \\\n    --dataset swift/chinese-c4 \\\n    --streaming true \\\n    --train_type full \\\n    --deepspeed zero2 \\\n    --output_dir output \\\n    --max_steps 10000 \\\n    ...\n```\n\nFine-tuning:\n```shell\nCUDA_VISIBLE_DEVICES=0 swift sft \\\n    --model Qwen/Qwen2.5-7B-Instruct \\\n    --dataset AI-ModelScope/alpaca-gpt4-data-en \\\n    --train_type lora \\\n    --output_dir output \\\n    ...\n```\n\nRLHF:\n```shell\nCUDA_VISIBLE_DEVICES=0 swift rlhf \\\n    --rlhf_type dpo \\\n    --model Qwen/Qwen2.5-7B-Instruct \\\n    --dataset hjh0119/shareAI-Llama3-DPO-zh-en-emoji \\\n    --train_type lora \\\n    --output_dir output \\\n    ...\n```\n\n\n### Inference\n```shell\nCUDA_VISIBLE_DEVICES=0 swift infer \\\n    --model Qwen/Qwen2.5-7B-Instruct \\\n    --stream true \\\n    --infer_backend pt \\\n    --max_new_tokens 2048\n\n# LoRA\nCUDA_VISIBLE_DEVICES=0 swift infer \\\n    --model Qwen/Qwen2.5-7B-Instruct \\\n    --adapters swift/test_lora \\\n    --stream true \\\n    --infer_backend pt \\\n    --temperature 0 \\\n    --max_new_tokens 2048\n```\n\n### Interface Inference\n```shell\nCUDA_VISIBLE_DEVICES=0 swift app \\\n    --model Qwen/Qwen2.5-7B-Instruct \\\n    --stream true \\\n    --infer_backend pt \\\n    --max_new_tokens 2048\n```\n\n### Deployment\n```shell\nCUDA_VISIBLE_DEVICES=0 swift deploy \\\n    --model Qwen/Qwen2.5-7B-Instruct \\\n    --infer_backend vllm\n```\n\n### Sampling\n```shell\nCUDA_VISIBLE_DEVICES=0 swift sample \\\n    --model LLM-Research/Meta-Llama-3.1-8B-Instruct \\\n    --sampler_engine pt \\\n    --num_return_sequences 5 \\\n    --dataset AI-ModelScope/alpaca-gpt4-data-zh#5\n```\n\n### Evaluation\n```shell\nCUDA_VISIBLE_DEVICES=0 swift eval \\\n    --model Qwen/Qwen2.5-7B-Instruct \\\n    --infer_backend lmdeploy \\\n    --eval_backend OpenCompass \\\n    --eval_dataset ARC_c\n```\n\n### Quantization\n```shell\nCUDA_VISIBLE_DEVICES=0 swift export \\\n    --model Qwen/Qwen2.5-7B-Instruct \\\n    --quant_bits 4 --quant_method awq \\\n    --dataset AI-ModelScope/alpaca-gpt4-data-zh \\\n    --output_dir Qwen2.5-7B-Instruct-AWQ\n```\n\n### Push Model\n```shell\nswift export \\\n    --model <model-path> \\\n    --push_to_hub true \\\n    --hub_model_id '<model-id>' \\\n    --hub_token '<sdk-token>'\n```\n\n## \ud83c\udfdb License\n\nThis framework is licensed under the [Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE). For models and datasets, please refer to the original resource page and follow the corresponding License.\n\n## \ud83d\udcce Citation\n\n```bibtex\n@misc{zhao2024swiftascalablelightweightinfrastructure,\n      title={SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning},\n      author={Yuze Zhao and Jintao Huang and Jinghan Hu and Xingjun Wang and Yunlin Mao and Daoze Zhang and Zeyinzi Jiang and Zhikai Wu and Baole Ai and Ang Wang and Wenmeng Zhou and Yingda Chen},\n      year={2024},\n      eprint={2408.05517},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2408.05517},\n}\n```\n\n## Star History\n\n[![Star History Chart](https://api.star-history.com/svg?repos=modelscope/swift&type=Date)](https://star-history.com/#modelscope/ms-swift&Date)\n",
  "external_links_in_readme": [
    "https://img.shields.io/github/license/modelscope/ms-swift\"></a>",
    "https://trendshift.io/repositories/6427\"",
    "https://github.com/modelscope/ms-swift/tree/main/examples/train/grpo/external",
    "https://github.com/modelscope/ms-swift/blob/main/requirements/install_all.sh",
    "https://github.com/modelscope/ms-swift/blob/main/examples/notebook/qwen2_5-self-cognition/self-cognition-sft.ipynb",
    "https://img.shields.io/badge/PR-welcome-55EB99.svg\"></a>",
    "https://pypi.org/project/ms-swift/\"><img",
    "https://github.com/modelscope/modelscope/blob/master/LICENSE",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/multimodal/rlhf/dpo",
    "https://img.shields.io/badge/pytorch-%E2%89%A52.0-orange.svg\">",
    "https://github.com/modelscope/ms-swift/tree/main/examples/train/long_text",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/gkd",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/pretrain/train.sh",
    "https://ojs.aaai.org/index.php/AAAI/article/view/35383",
    "https://swift.readthedocs.io/en/latest/GetStarted/Web-UI.html",
    "https://github.com/vllm-project/vllm",
    "https://github.com/InternLM/lmdeploy",
    "https://github.com/modelscope/evalscope/",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/grpo/internal",
    "https://github.com/modelscope/ms-swift/blob/main/examples/infer/sglang",
    "https://pepy.tech/project/ms-swift\"><img",
    "https://github.com/modelscope/ms-swift/tree/main/examples/train/qlora",
    "https://github.com/modelscope/ms-swift/tree/main/examples/train/megatron/rlhf",
    "https://swift.readthedocs.io/en/latest/Customization/Custom-dataset.html",
    "https://github.com/modelscope/ms-swift.git",
    "https://swift.readthedocs.io/en/latest/Customization/Custom-model.html",
    "https://github.com/modelscope/modelscope/\"><img",
    "https://arxiv.org/abs/2408.05517},",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/full/train.sh",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/multimodal/rlhf/kto.sh",
    "https://star-history.com/#modelscope/ms-swift&Date",
    "https://discord.com/invite/D27yfEFVz5",
    "https://github.com/modelscope/ms-swift/tree/main/examples/train/multi-gpu/deepspeed",
    "https://trendshift.io/api/badge/repositories/6427\"",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/multimodal/rlhf/gkd",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/kto.sh",
    "https://github.com/modelscope/ms-swift/tree/main/examples/train/multimodal",
    "https://badge.fury.io/py/ms-swift.svg\"></a>",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/lora_sft.sh",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/embedding/train_gte.sh",
    "https://github.com/modelscope/modelscope-classroom/tree/main/LLM-tutorial",
    "https://github.com/modelscope/ms-swift/tree/main/examples",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/simpo.sh",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/rm.sh",
    "https://swift.readthedocs.io/en/latest/Instruction/Command-line-parameters.html",
    "https://arxiv.org/abs/2408.05517\">Paper</a>",
    "https://swift.readthedocs.io/en/latest/\">English",
    "https://swift.readthedocs.io/en/latest/Instruction/Megatron-SWIFT-Training.html",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/cpo.sh",
    "https://arxiv.org/abs/2408.05517",
    "https://github.com/modelscope/ms-swift/pulls\"><img",
    "https://swift.readthedocs.io/en/latest/Instruction/Supported-models-and-datasets.html",
    "https://arxiv.org/pdf/2309.00986.pdf",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/seq_cls/qwen2_5/sft.sh",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/seq_cls/qwen2_vl/sft.sh",
    "https://api.star-history.com/svg?repos=modelscope/swift&type=Date",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/ppo",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/orpo.sh",
    "https://swift.readthedocs.io/zh-cn/latest/\">\u4e2d\u6587\u6587\u6863</a>",
    "https://github.com/sgl-project/sglang",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/rlhf/dpo",
    "https://pepy.tech/badge/ms-swift\"></a>",
    "https://img.shields.io/badge/python-3.10-5be.svg\">",
    "https://wandb.ai/tastelikefeet/grpo_perf_test?nw=nwuseryuzezyz",
    "https://github.com/modelscope/ms-swift/blob/main/LICENSE\"><img",
    "https://github.com/modelscope/ms-swift/blob/main/examples/train/embedding/train_gme.sh",
    "https://github.com/modelscope/ms-swift/tree/main/examples/train/multi-node",
    "https://modelscope.cn/home\">ModelScope",
    "https://img.shields.io/badge/modelscope-%E2%89%A51.19-5D91D4.svg\"></a>"
  ]
}
```

</details>


---

