# GitHub Data for Qwen_Qwen2.5-VL-7B-Instruct

**Task Category:** Image-Text-to-Text

## Repository 1: dmlc/decord

# GitHub Repository Data

**Repository:** [dmlc/decord](https://github.com/dmlc/decord)

## Basic Information

- **Description:** An efficient video loader for deep learning with smart shuffling that's super easy to digest
- **Created:** 2019-01-18T23:07:31+00:00
- **Last Updated:** 2025-06-21T12:01:19+00:00
- **Last Pushed:** 2024-07-17T04:18:40+00:00
- **Default Branch:** master
- **Size:** 20424 KB

## Statistics

- **Stars:** 2,194
- **Forks:** 192
- **Watchers:** 2,194
- **Open Issues:** 197
- **Total Issues:** 0
- **Pull Requests:** 71

## License

- **Type:** Apache License 2.0
- **SPDX ID:** Apache-2.0
- **URL:** [License](https://github.com/dmlc/decord/blob/master/LICENSE)

## Languages

- **C++:** 428,260 bytes
- **Python:** 160,840 bytes
- **C:** 43,771 bytes
- **CMake:** 22,879 bytes
- **Cython:** 20,703 bytes
- **Shell:** 4,732 bytes
- **Cuda:** 3,371 bytes
- **Dockerfile:** 1,273 bytes
- **Batchfile:** 646 bytes

## Topics

- `video-loader`

## Top Contributors

1. **zhreshold** - 322 contributions
2. **innerlee** - 8 contributions
3. **leotac** - 7 contributions
4. **yinweisu** - 5 contributions
5. **bryant1410** - 3 contributions
6. **alesanfra** - 1 contributions
7. **AndreasCag** - 1 contributions
8. **levan92** - 1 contributions
9. **frankier** - 1 contributions
10. **JoannaLXY** - 1 contributions

## File Structure (Sample of 10 files)

Total files: 232

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/ccpp.yml` (blob)
- `.github/workflows/pypi.yml` (blob)
- `.gitignore` (blob)
- `.gitmodules` (blob)
- `3rdparty` (tree)
- `3rdparty/dlpack` (commit)
- `3rdparty/dmlc-core` (commit)
- `CMakeLists.txt` (blob)

## Recent Issues

- 🟢 **#347** Add support  linux aarch64 arch (open)
- 🟢 **#346** FFMPEG 7 + cuda 12.9 (open)
- 🔴 **#345** ffmpeg7 and cuda 12.9 with CI running (closed)
- 🔴 **#344** Updates to FFMPEG are no longer broken, compiles locally. (closed)
- 🔴 **#343** Incorrect timestamps for some videos (closed)

## Recent Pull Requests

- 🟢 **#346** FFMPEG 7 + cuda 12.9 (open)
- 🔴 **#345** ffmpeg7 and cuda 12.9 with CI running (closed)
- 🔴 **#344** Updates to FFMPEG are no longer broken, compiles locally. (closed)
- 🟢 **#342** fix(video): Prevent hang in SkipFramesImpl by adding pop retry limit (open)
- 🟢 **#341** Fix get_batch hang - use int64_t for NDArray PTS (open)

## Recent Commits

- **d2e56190** update readme (#211) - Steven (2022-07-19T08:17:19+00:00)
- **b63f61a2** Avoid creating extra mem in string join (#173) - Santiago Castro (2022-07-19T08:16:48+00:00)
- **48864b3c** Change the wording on the libnvcuvid issue (#172) - Santiago Castro (2022-07-19T08:15:53+00:00)
- **03924c56** Fix typo in device docstring (#171) - Santiago Castro (2022-07-19T08:15:26+00:00)
- **96b750c7** Docker support (#166) - Evan (2021-07-18T04:01:36+00:00)
- **35c87a6e** fix mem leak (#167) - Weisu Yin (2021-07-14T21:31:59+00:00)
- **b4526dd8** Fix VideoLoader code example (#164) - Andrew Anikin (2021-07-07T22:19:06+00:00)
- **6a3617ce** bump up to 0.6.0, adding audio reader to release - Joshua Z. Zhang (2021-06-14T20:40:53+00:00)
- **e0b7f7ca** fix assigning to 'AVCodec *' from incompatible type 'const AVCodec *' (#160) - Joshua Z. Zhang (2021-06-14T20:39:41+00:00)
- **b8fd6a53** remove --no-deps - Joshua Z. Zhang (2021-06-13T23:29:23+00:00)

## External Links Found in README

- https://brew.sh/
- https://user-images.githubusercontent.com/3307514/71223638-7199f300-2289-11ea-9e16-104038f94a55.png
- https://pypi.python.org/pypi/decord
- https://img.shields.io/pypi/v/decord.svg
- https://github.com/dmlc/decord/issues/102
- https://visualstudio.microsoft.com/
- https://github.com/dmlc/decord/workflows/C/C++%20CI/badge.svg?branch=master
- http://pepy.tech/badge/decord
- https://chocolatey.org/
- http://pepy.tech/project/decord
- https://github.com/dmlc/decord/workflows/Publish%20to%20PYPI/badge.svg?branch=master
- https://github.com/dmlc/decord

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 166483718,
  "name": "decord",
  "full_name": "dmlc/decord",
  "description": "An efficient video loader for deep learning with smart shuffling that's super easy to digest",
  "html_url": "https://github.com/dmlc/decord",
  "clone_url": "https://github.com/dmlc/decord.git",
  "ssh_url": "git@github.com:dmlc/decord.git",
  "homepage": "",
  "topics": [
    "video-loader"
  ],
  "default_branch": "master",
  "created_at": "2019-01-18T23:07:31+00:00",
  "updated_at": "2025-06-21T12:01:19+00:00",
  "pushed_at": "2024-07-17T04:18:40+00:00",
  "size_kb": 20424,
  "watchers_count": 2194,
  "stargazers_count": 2194,
  "forks_count": 192,
  "open_issues_count": 197,
  "license": {
    "key": "apache-2.0",
    "name": "Apache License 2.0",
    "spdx_id": "Apache-2.0",
    "url": "https://github.com/dmlc/decord/blob/master/LICENSE"
  },
  "languages": {
    "C++": 428260,
    "Python": 160840,
    "C": 43771,
    "CMake": 22879,
    "Cython": 20703,
    "Shell": 4732,
    "Cuda": 3371,
    "Dockerfile": 1273,
    "Batchfile": 646
  },
  "top_contributors": [
    {
      "login": "zhreshold",
      "contributions": 322
    },
    {
      "login": "innerlee",
      "contributions": 8
    },
    {
      "login": "leotac",
      "contributions": 7
    },
    {
      "login": "yinweisu",
      "contributions": 5
    },
    {
      "login": "bryant1410",
      "contributions": 3
    },
    {
      "login": "alesanfra",
      "contributions": 1
    },
    {
      "login": "AndreasCag",
      "contributions": 1
    },
    {
      "login": "levan92",
      "contributions": 1
    },
    {
      "login": "frankier",
      "contributions": 1
    },
    {
      "login": "JoannaLXY",
      "contributions": 1
    },
    {
      "login": "jenhaoyang",
      "contributions": 1
    },
    {
      "login": "leigh-plt",
      "contributions": 1
    }
  ],
  "file_tree_count": 232,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/ccpp.yml",
      "type": "blob"
    },
    {
      "path": ".github/workflows/pypi.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": ".gitmodules",
      "type": "blob"
    },
    {
      "path": "3rdparty",
      "type": "tree"
    },
    {
      "path": "3rdparty/dlpack",
      "type": "commit"
    },
    {
      "path": "3rdparty/dmlc-core",
      "type": "commit"
    },
    {
      "path": "CMakeLists.txt",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 71,
  "recent_issues": [
    {
      "number": 347,
      "title": "Add support  linux aarch64 arch",
      "state": "open"
    },
    {
      "number": 346,
      "title": "FFMPEG 7 + cuda 12.9",
      "state": "open"
    },
    {
      "number": 345,
      "title": "ffmpeg7 and cuda 12.9 with CI running",
      "state": "closed"
    },
    {
      "number": 344,
      "title": "Updates to FFMPEG are no longer broken, compiles locally.",
      "state": "closed"
    },
    {
      "number": 343,
      "title": "Incorrect timestamps for some videos",
      "state": "closed"
    }
  ],
  "recent_pulls": [
    {
      "number": 346,
      "title": "FFMPEG 7 + cuda 12.9",
      "state": "open"
    },
    {
      "number": 345,
      "title": "ffmpeg7 and cuda 12.9 with CI running",
      "state": "closed"
    },
    {
      "number": 344,
      "title": "Updates to FFMPEG are no longer broken, compiles locally.",
      "state": "closed"
    },
    {
      "number": 342,
      "title": "fix(video): Prevent hang in SkipFramesImpl by adding pop retry limit",
      "state": "open"
    },
    {
      "number": 341,
      "title": "Fix get_batch hang - use int64_t for NDArray PTS",
      "state": "open"
    }
  ],
  "recent_commits": [
    {
      "sha": "d2e56190286ae394032a8141885f76d5372bd44b",
      "author": "Steven",
      "date": "2022-07-19T08:17:19+00:00",
      "message": "update readme (#211)"
    },
    {
      "sha": "b63f61a267d9bf0b3ef1c034e5bd775c911b5be6",
      "author": "Santiago Castro",
      "date": "2022-07-19T08:16:48+00:00",
      "message": "Avoid creating extra mem in string join (#173)"
    },
    {
      "sha": "48864b3c829742d08e95c148e8ce869f9919cbfe",
      "author": "Santiago Castro",
      "date": "2022-07-19T08:15:53+00:00",
      "message": "Change the wording on the libnvcuvid issue (#172)"
    },
    {
      "sha": "03924c56f12c82b5820cb903dd7e782c4a975457",
      "author": "Santiago Castro",
      "date": "2022-07-19T08:15:26+00:00",
      "message": "Fix typo in device docstring (#171)"
    },
    {
      "sha": "96b750c7221322391969929e855b942d2fdcd06b",
      "author": "Evan",
      "date": "2021-07-18T04:01:36+00:00",
      "message": "Docker support (#166)"
    },
    {
      "sha": "35c87a6ecfb31e6561d3b133253841337b65b974",
      "author": "Weisu Yin",
      "date": "2021-07-14T21:31:59+00:00",
      "message": "fix mem leak (#167)"
    },
    {
      "sha": "b4526dd8b110cb8efad34af9e648c5f12056ac37",
      "author": "Andrew Anikin",
      "date": "2021-07-07T22:19:06+00:00",
      "message": "Fix VideoLoader code example (#164)"
    },
    {
      "sha": "6a3617cef035535193f390f86399dde139fa2a53",
      "author": "Joshua Z. Zhang",
      "date": "2021-06-14T20:40:53+00:00",
      "message": "bump up to 0.6.0, adding audio reader to release"
    },
    {
      "sha": "e0b7f7cabd8c7c224c2265453ee807ed19eb4d8a",
      "author": "Joshua Z. Zhang",
      "date": "2021-06-14T20:39:41+00:00",
      "message": "fix assigning to 'AVCodec *' from incompatible type 'const AVCodec *' (#160)"
    },
    {
      "sha": "b8fd6a53a1a6614409e2fbc5ddcb85da291de9c0",
      "author": "Joshua Z. Zhang",
      "date": "2021-06-13T23:29:23+00:00",
      "message": "remove --no-deps"
    },
    {
      "sha": "625e4b0d7894bee9f69d0813da1e380c882d92a4",
      "author": "Weisu Yin",
      "date": "2021-03-09T04:30:01+00:00",
      "message": "add support for old libavformat version (#144)"
    },
    {
      "sha": "f8e1706916c00d342a00e2201bc3672297527b03",
      "author": "Joshua Z. Zhang",
      "date": "2021-03-07T06:33:45+00:00",
      "message": "update 0.5.3"
    },
    {
      "sha": "371eff6bb6d0d5f6a8d4c39318182a434dd83bac",
      "author": "Joshua Z. Zhang",
      "date": "2021-03-07T06:27:59+00:00",
      "message": "more tolerant seek (#142)"
    },
    {
      "sha": "4593352ea140cebf81a8cf0eb2932ea653b5a0d3",
      "author": "Joshua Z. Zhang",
      "date": "2021-03-07T06:17:53+00:00",
      "message": "fix tensorflow dlpack context init (#141)"
    },
    {
      "sha": "0f6eeecdedf4c707338043e99d4c81fb76a04767",
      "author": "Joshua Z. Zhang",
      "date": "2021-03-07T05:24:12+00:00",
      "message": "add environment variable and instruction (#140)"
    },
    {
      "sha": "ed32ac420d8e08b9adeba88904516962b35db21a",
      "author": "Weisu Yin",
      "date": "2021-02-24T22:55:38+00:00",
      "message": "Update README to include decord audio (#136)"
    },
    {
      "sha": "c63d903e831b732b7fee004069cb06ae732a6642",
      "author": "Weisu Yin",
      "date": "2021-02-17T00:42:01+00:00",
      "message": "add cmath (#134)"
    },
    {
      "sha": "61dea3e4b3e6e4ae50c97101736ba08a52231939",
      "author": "Weisu Yin",
      "date": "2021-02-09T21:55:16+00:00",
      "message": "Decord Audio (#131)"
    },
    {
      "sha": "023d47715a61688662e1541665054296c6f0d7f0",
      "author": "Joshua Z. Zhang",
      "date": "2021-02-09T17:20:34+00:00",
      "message": "fix wheels con't"
    },
    {
      "sha": "fc8a158a7b8fe55a1d83c5e63305a6f3e805231f",
      "author": "Joshua Z. Zhang",
      "date": "2021-02-09T03:30:05+00:00",
      "message": "0.5.1"
    }
  ],
  "readme_text": "# Decord\n\n![CI Build](https://github.com/dmlc/decord/workflows/C/C++%20CI/badge.svg?branch=master)\n![Release Build](https://github.com/dmlc/decord/workflows/Publish%20to%20PYPI/badge.svg?branch=master)\n[![PyPI](https://img.shields.io/pypi/v/decord.svg)](https://pypi.python.org/pypi/decord)\n[![Downloads](http://pepy.tech/badge/decord)](http://pepy.tech/project/decord)\n\n![symbol](docs/symbol.png)\n\n`Decord` is a reverse procedure of `Record`. It provides convenient video slicing methods based on a thin wrapper on top of hardware accelerated video decoders, e.g.\n\n-   FFMPEG/LibAV(Done)\n-   Nvidia Codecs(Done)\n-   Intel Codecs\n\n`Decord` was designed to handle awkward video shuffling experience in order to provide smooth experiences similar to random image loader for deep learning.\n\n`Decord` is also able to decode audio from both video and audio files. One can slice video and audio together to get a synchronized result; hence providing a one-stop solution for both video and audio decoding.\n\nTable of contents\n=================\n\n- [Benchmark](#preliminary-benchmark)\n- [Installation](#installation)\n- [Usage](#usage)\n- [Bridge for Deep Learning frameworks](#bridges-for-deep-learning-frameworks)\n\n## Preliminary benchmark\n\nDecord is good at handling random access patterns, which is rather common during neural network training.\n\n![Speed up](https://user-images.githubusercontent.com/3307514/71223638-7199f300-2289-11ea-9e16-104038f94a55.png)\n\n## Installation\n\n### Install via pip\n\nSimply use\n\n```bash\npip install decord\n```\n\nSupported platforms:\n\n- [x] Linux\n- [x] Mac OS >= 10.12, python>=3.5\n- [x] Windows\n\n**Note that only CPU versions are provided with PYPI now. Please build from source to enable GPU acclerator.**\n\n\n### Install from source\n\n#### Linux\n\nInstall the system packages for building the shared library, for Debian/Ubuntu users, run:\n\n```bash\n# official PPA comes with ffmpeg 2.8, which lacks tons of features, we use ffmpeg 4.0 here\nsudo add-apt-repository ppa:jonathonf/ffmpeg-4 # for ubuntu20.04 official PPA is already version 4.2, you may skip this step\nsudo apt-get update\nsudo apt-get install -y build-essential python3-dev python3-setuptools make cmake\nsudo apt-get install -y ffmpeg libavcodec-dev libavfilter-dev libavformat-dev libavutil-dev\n# note: make sure you have cmake 3.8 or later, you can install from cmake official website if it's too old\n```\n\nClone the repo recursively(important)\n\n```bash\ngit clone --recursive https://github.com/dmlc/decord\n```\n\nBuild the shared library in source root directory:\n\n```bash\ncd decord\nmkdir build && cd build\ncmake .. -DUSE_CUDA=0 -DCMAKE_BUILD_TYPE=Release\nmake\n```\n\nyou can specify `-DUSE_CUDA=ON` or `-DUSE_CUDA=/path/to/cuda` or `-DUSE_CUDA=ON` `-DCMAKE_CUDA_COMPILER=/path/to/cuda/nvcc` to enable NVDEC hardware accelerated decoding:\n\n```bash\ncmake .. -DUSE_CUDA=ON -DCMAKE_BUILD_TYPE=Release\n```\n\nNote that if you encountered the an issue with `libnvcuvid.so` (e.g., see [#102](https://github.com/dmlc/decord/issues/102)), it's probably due to the missing link for\n`libnvcuvid.so`, you can manually find it (`ldconfig -p | grep libnvcuvid`) and link the library to `CUDA_TOOLKIT_ROOT_DIR\\lib64` to allow `decord` smoothly detect and link the correct library.\n\nTo specify a customized FFMPEG library path, use `-DFFMPEG_DIR=/path/to/ffmpeg\".\n\nInstall python bindings:\n\n```bash\ncd ../python\n# option 1: add python path to $PYTHONPATH, you will need to install numpy separately\npwd=$PWD\necho \"PYTHONPATH=$PYTHONPATH:$pwd\" >> ~/.bashrc\nsource ~/.bashrc\n# option 2: install with setuptools\npython3 setup.py install --user\n```\n\n#### Mac OS\n\nInstallation on macOS is similar to Linux. But macOS users need to install building tools like clang, GNU Make, cmake first.\n\nTools like clang and GNU Make are packaged in _Command Line Tools_ for macOS. To install:\n\n```bash\nxcode-select --install\n```\n\nTo install other needed packages like cmake, we recommend first installing Homebrew, which is a popular package manager for macOS. Detailed instructions can be found on its [homepage](https://brew.sh/).\n\nAfter installation of Homebrew, install cmake and ffmpeg by:\n\n```bash\nbrew install cmake ffmpeg\n# note: make sure you have cmake 3.8 or later, you can install from cmake official website if it's too old\n```\n\nClone the repo recursively(important)\n\n```bash\ngit clone --recursive https://github.com/dmlc/decord\n```\n\nThen go to root directory build shared library:\n\n```bash\ncd decord\nmkdir build && cd build\ncmake .. -DCMAKE_BUILD_TYPE=Release\nmake\n```\n\nInstall python bindings:\n\n```bash\ncd ../python\n# option 1: add python path to $PYTHONPATH, you will need to install numpy separately\npwd=$PWD\necho \"PYTHONPATH=$PYTHONPATH:$pwd\" >> ~/.bash_profile\nsource ~/.bash_profile\n# option 2: install with setuptools\npython3 setup.py install --user\n```\n\n#### Windows\n\nFor windows, you will need CMake and Visual Studio for C++ compilation.\n\n-   First, install `git`, `cmake`, `ffmpeg` and `python`. You can use [Chocolatey](https://chocolatey.org/) to manage packages similar to Linux/Mac OS.\n-   Second, install [`Visual Studio 2017 Community`](https://visualstudio.microsoft.com/), this my take some time.\n\nWhen dependencies are ready, open command line prompt:\n\n```bash\ncd your-workspace\ngit clone --recursive https://github.com/dmlc/decord\ncd decord\nmkdir build\ncd build\ncmake -DCMAKE_CXX_FLAGS=\"/DDECORD_EXPORTS\" -DCMAKE_CONFIGURATION_TYPES=\"Release\" -G \"Visual Studio 15 2017 Win64\" ..\n# open `decord.sln` and build project\n```\n\n## Usage\n\nDecord provides minimal API set for bootstraping. You can also check out jupyter notebook [examples](examples/).\n\n### VideoReader\n\nVideoReader is used to access frames directly from video files.\n\n```python\nfrom decord import VideoReader\nfrom decord import cpu, gpu\n\nvr = VideoReader('examples/flipping_a_pancake.mkv', ctx=cpu(0))\n# a file like object works as well, for in-memory decoding\nwith open('examples/flipping_a_pancake.mkv', 'rb') as f:\n  vr = VideoReader(f, ctx=cpu(0))\nprint('video frames:', len(vr))\n# 1. the simplest way is to directly access frames\nfor i in range(len(vr)):\n    # the video reader will handle seeking and skipping in the most efficient manner\n    frame = vr[i]\n    print(frame.shape)\n\n# To get multiple frames at once, use get_batch\n# this is the efficient way to obtain a long list of frames\nframes = vr.get_batch([1, 3, 5, 7, 9])\nprint(frames.shape)\n# (5, 240, 320, 3)\n# duplicate frame indices will be accepted and handled internally to avoid duplicate decoding\nframes2 = vr.get_batch([1, 2, 3, 2, 3, 4, 3, 4, 5]).asnumpy()\nprint(frames2.shape)\n# (9, 240, 320, 3)\n\n# 2. you can do cv2 style reading as well\n# skip 100 frames\nvr.skip_frames(100)\n# seek to start\nvr.seek(0)\nbatch = vr.next()\nprint('frame shape:', batch.shape)\nprint('numpy frames:', batch.asnumpy())\n\n```\n\n### VideoLoader\n\nVideoLoader is designed for training deep learning models with tons of video files.\nIt provides smart video shuffle techniques in order to provide high random access performance (We know that seeking in video is super slow and redundant).\nThe optimizations are underlying in the C++ code, which are invisible to user.\n\n```python\nfrom decord import VideoLoader\nfrom decord import cpu, gpu\n\nvl = VideoLoader(['1.mp4', '2.avi', '3.mpeg'], ctx=[cpu(0)], shape=(2, 320, 240, 3), interval=1, skip=5, shuffle=1)\nprint('Total batches:', len(vl))\n\nfor batch in vl:\n    print(batch[0].shape)\n```\n\nShuffling video can be tricky, thus we provide various modes:\n\n```python\nshuffle = -1  # smart shuffle mode, based on video properties, (not implemented yet)\nshuffle = 0  # all sequential, no seeking, following initial filename order\nshuffle = 1  # random filename order, no random access for each video, very efficient\nshuffle = 2  # random order\nshuffle = 3  # random frame access in each video only\n```\n\n### AudioReader\n\nAudioReader is used to access samples directly from both video(if there's an audio track) and audio files.\n\n```python\nfrom decord import AudioReader\nfrom decord import cpu, gpu\n\n# You can specify the desired sample rate and channel layout\n# For channels there are two options: default to the original layout or mono\nar = AudioReader('example.mp3', ctx=cpu(0), sample_rate=44100, mono=False)\nprint('Shape of audio samples: ', ar.shape())\n# To access the audio samples\nprint('The first sample: ', ar[0])\nprint('The first five samples: ', ar[0:5])\nprint('Get a batch of samples: ', ar.get_batch([1,3,5]))\n```\n\n### AVReader\n\nAVReader is a wraper for both AudioReader and VideoReader. It enables you to slice the video and audio simultaneously.\n\n```python\nfrom decord import AVReader\nfrom decord import cpu, gpu\n\nav = AVReader('example.mov', ctx=cpu(0))\n# To access both the video frames and corresponding audio samples\naudio, video = av[0:20]\n# Each element in audio will be a batch of samples corresponding to a frame of video\nprint('Frame #: ', len(audio))\nprint('Shape of the audio samples of the first frame: ', audio[0].shape)\nprint('Shape of the first frame: ', video.asnumpy()[0].shape)\n# Similarly, to get a batch\naudio2, video2 = av.get_batch([1,3,5])\n```\n\n\n\n## Bridges for deep learning frameworks:\n\nIt's important to have a bridge from decord to popular deep learning frameworks for training/inference\n\n-   Apache MXNet (Done)\n-   Pytorch (Done)\n-   TensorFlow (Done)\n\nUsing bridges for deep learning frameworks are simple, for example, one can set the default tensor output to `mxnet.ndarray`:\n\n```python\nimport decord\nvr = decord.VideoReader('examples/flipping_a_pancake.mkv')\nprint('native output:', type(vr[0]), vr[0].shape)\n# native output: <class 'decord.ndarray.NDArray'>, (240, 426, 3)\n# you only need to set the output type once\ndecord.bridge.set_bridge('mxnet')\nprint(type(vr[0], vr[0].shape))\n# <class 'mxnet.ndarray.ndarray.NDArray'> (240, 426, 3)\n# or pytorch and tensorflow(>=2.2.0)\ndecord.bridge.set_bridge('torch')\ndecord.bridge.set_bridge('tensorflow')\n# or back to decord native format\ndecord.bridge.set_bridge('native')\n```\n",
  "external_links_in_readme": [
    "https://brew.sh/",
    "https://user-images.githubusercontent.com/3307514/71223638-7199f300-2289-11ea-9e16-104038f94a55.png",
    "https://pypi.python.org/pypi/decord",
    "https://img.shields.io/pypi/v/decord.svg",
    "https://github.com/dmlc/decord/issues/102",
    "https://visualstudio.microsoft.com/",
    "https://github.com/dmlc/decord/workflows/C/C++%20CI/badge.svg?branch=master",
    "http://pepy.tech/badge/decord",
    "https://chocolatey.org/",
    "http://pepy.tech/project/decord",
    "https://github.com/dmlc/decord/workflows/Publish%20to%20PYPI/badge.svg?branch=master",
    "https://github.com/dmlc/decord"
  ]
}
```

</details>


---

## Repository 2: QwenLM/Qwen2.5-VL

# GitHub Repository Data

**Repository:** [QwenLM/Qwen2.5-VL](https://github.com/QwenLM/Qwen2.5-VL)

## Basic Information

- **Description:** Qwen2.5-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud.
- **Created:** 2024-08-29T08:30:38+00:00
- **Last Updated:** 2025-06-21T15:47:22+00:00
- **Last Pushed:** 2025-05-15T03:54:08+00:00
- **Default Branch:** main
- **Size:** 64335 KB

## Statistics

- **Stars:** 11,093
- **Forks:** 803
- **Watchers:** 11,093
- **Open Issues:** 813
- **Total Issues:** 0
- **Pull Requests:** 73

## License

- **Type:** Apache License 2.0
- **SPDX ID:** Apache-2.0
- **URL:** [License](https://github.com/QwenLM/Qwen2.5-VL/blob/main/LICENSE)

## Languages

- **Jupyter Notebook:** 28,745,081 bytes
- **Python:** 230,346 bytes
- **JavaScript:** 12,180 bytes
- **Shell:** 7,024 bytes

## Top Contributors

1. **ShuaiBai623** - 81 contributions
2. **kq-chen** - 41 contributions
3. **fyabc** - 8 contributions
4. **jinze1994** - 7 contributions
5. **LibertFan** - 7 contributions
6. **JustinLin610** - 5 contributions
7. **hiyouga** - 5 contributions
8. **tinytangent** - 3 contributions
9. **gluttony-10** - 3 contributions
10. **logicwong** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 119

- `.gitignore` (blob)
- `LICENSE` (blob)
- `README.md` (blob)
- `cookbooks` (tree)
- `cookbooks/assets` (tree)
- `cookbooks/assets/agent_function_call` (tree)
- `cookbooks/assets/agent_function_call/mobile_en_example.png` (blob)
- `cookbooks/assets/agent_function_call/mobile_zh_example.jpg` (blob)
- `cookbooks/assets/computer_use` (tree)
- `cookbooks/assets/computer_use/computer_use1.jpeg` (blob)

## Recent Issues

- 🟢 **#1291** The number of image tokens and effects of resize operation (open)
- 🟢 **#1290** Qwen2.5-VL's Performance on CharXiv (open)
- 🟢 **#1289** [BUG] Fine tune script `train_qwen.py:set_model()` bug (open)
- 🟢 **#1288** RuntimeError: probability tensor contains either `inf`, `nan` or element < 0 (open)
- 🔴 **#1287** close (closed)

## Recent Pull Requests

- 🟢 **#1286** fix: instantialize NousFnCallPrompt in mobile_agent (open)
- 🟢 **#1280** [validation] add assertions for data_args (open)
- 🟢 **#1243** Create devcontainer.json (open)
- 🔴 **#1201** Update README.md (closed)
- 🔴 **#1183** Update data_qwen.py (closed)

## Recent Commits

- **d2240f11** Merge pull request #1145 from Hui-design/pr_tch - ShuaiBai623 (2025-05-15T03:54:08+00:00)
- **dae28cb9** Merge pull request #1201 from creeponsky/patch-1 - ShuaiBai623 (2025-05-15T03:53:11+00:00)
- **712d3d01** Merge pull request #1183 from John-Ge/patch-1 - ShuaiBai623 (2025-05-15T03:52:17+00:00)
- **f675047c** Update data_qwen.py - Junyang Lin (2025-05-15T03:41:11+00:00)
- **7dae6d11** fix typo - Junyang Lin (2025-05-15T03:40:53+00:00)
- **6e98a0a6** Update README.md - CreepOnSky (2025-05-13T02:22:59+00:00)
- **4ed134c0** Update data_qwen.py - Chunjiang Ge (葛春江) (2025-05-07T14:22:36+00:00)
- **29a9da77** Merge pull request #1171 from John-Ge/main - ShuaiBai623 (2025-05-04T06:30:47+00:00)
- **dda09243** packing - John-Ge (2025-05-04T05:26:09+00:00)
- **a30e36fa** fix bug of smart_resize - Hui-design (2025-04-23T13:06:39+00:00)

## External Links Found in README

- https://github.com/QwenLM/Qwen2.5-VL/blob/main/cookbooks/spatial_understanding.ipynb
- https://hub.docker.com/r/qwenllm/qwenvl
- https://modelscope.cn/models/qwen/Qwen2.5-VL-7B-Instruct
- https://huggingface.co/Qwen/QVQ-72B-Preview
- https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/computer_use.ipynb
- https://arxiv.org/abs/2309.00071
- https://huggingface.co/Qwen
- https://github.com/QwenLM/Qwen2.5-VL/tree/main/cookbooks
- https://github.com/huggingface/transformers@f3f6c86582611976e72be054675e2bf0abb5f775
- https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png"}},
- https://arxiv.org/abs/2502.13923
- https://github.com/Dao-AILab/flash-attention
- http://path/to/your/image.jpg"},
- https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png",
- https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg"
- https://modelscope.cn/models/qwen/Qwen2.5-VL-72B-Instruct
- https://docs.vllm.ai/en/latest/serving/multimodal_inputs.html
- https://github.com/QwenLM/Qwen2.5-VL/blob/main/cookbooks/computer_use.ipynb
- https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL/qwen2.5vl_logo.png"
- https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 849239437,
  "name": "Qwen2.5-VL",
  "full_name": "QwenLM/Qwen2.5-VL",
  "description": "Qwen2.5-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud.",
  "html_url": "https://github.com/QwenLM/Qwen2.5-VL",
  "clone_url": "https://github.com/QwenLM/Qwen2.5-VL.git",
  "ssh_url": "git@github.com:QwenLM/Qwen2.5-VL.git",
  "homepage": "",
  "topics": [],
  "default_branch": "main",
  "created_at": "2024-08-29T08:30:38+00:00",
  "updated_at": "2025-06-21T15:47:22+00:00",
  "pushed_at": "2025-05-15T03:54:08+00:00",
  "size_kb": 64335,
  "watchers_count": 11093,
  "stargazers_count": 11093,
  "forks_count": 803,
  "open_issues_count": 813,
  "license": {
    "key": "apache-2.0",
    "name": "Apache License 2.0",
    "spdx_id": "Apache-2.0",
    "url": "https://github.com/QwenLM/Qwen2.5-VL/blob/main/LICENSE"
  },
  "languages": {
    "Jupyter Notebook": 28745081,
    "Python": 230346,
    "JavaScript": 12180,
    "Shell": 7024
  },
  "top_contributors": [
    {
      "login": "ShuaiBai623",
      "contributions": 81
    },
    {
      "login": "kq-chen",
      "contributions": 41
    },
    {
      "login": "fyabc",
      "contributions": 8
    },
    {
      "login": "jinze1994",
      "contributions": 7
    },
    {
      "login": "LibertFan",
      "contributions": 7
    },
    {
      "login": "JustinLin610",
      "contributions": 5
    },
    {
      "login": "hiyouga",
      "contributions": 5
    },
    {
      "login": "tinytangent",
      "contributions": 3
    },
    {
      "login": "gluttony-10",
      "contributions": 3
    },
    {
      "login": "logicwong",
      "contributions": 3
    },
    {
      "login": "John-Ge",
      "contributions": 2
    },
    {
      "login": "simonJJJ",
      "contributions": 2
    },
    {
      "login": "sibosutd",
      "contributions": 2
    },
    {
      "login": "ranpox",
      "contributions": 2
    },
    {
      "login": "qykong",
      "contributions": 1
    },
    {
      "login": "SulRash",
      "contributions": 1
    },
    {
      "login": "terryyz",
      "contributions": 1
    },
    {
      "login": "yangapku",
      "contributions": 1
    },
    {
      "login": "Zhaohai-Li",
      "contributions": 1
    },
    {
      "login": "c0g",
      "contributions": 1
    }
  ],
  "file_tree_count": 119,
  "file_tree_sample": [
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "cookbooks",
      "type": "tree"
    },
    {
      "path": "cookbooks/assets",
      "type": "tree"
    },
    {
      "path": "cookbooks/assets/agent_function_call",
      "type": "tree"
    },
    {
      "path": "cookbooks/assets/agent_function_call/mobile_en_example.png",
      "type": "blob"
    },
    {
      "path": "cookbooks/assets/agent_function_call/mobile_zh_example.jpg",
      "type": "blob"
    },
    {
      "path": "cookbooks/assets/computer_use",
      "type": "tree"
    },
    {
      "path": "cookbooks/assets/computer_use/computer_use1.jpeg",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 73,
  "recent_issues": [
    {
      "number": 1291,
      "title": "The number of image tokens and effects of resize operation",
      "state": "open"
    },
    {
      "number": 1290,
      "title": "Qwen2.5-VL's Performance on CharXiv",
      "state": "open"
    },
    {
      "number": 1289,
      "title": "[BUG] Fine tune script `train_qwen.py:set_model()` bug",
      "state": "open"
    },
    {
      "number": 1288,
      "title": "RuntimeError: probability tensor contains either `inf`, `nan` or element < 0",
      "state": "open"
    },
    {
      "number": 1287,
      "title": "close",
      "state": "closed"
    }
  ],
  "recent_pulls": [
    {
      "number": 1286,
      "title": "fix: instantialize NousFnCallPrompt in mobile_agent",
      "state": "open"
    },
    {
      "number": 1280,
      "title": "[validation] add assertions for data_args",
      "state": "open"
    },
    {
      "number": 1243,
      "title": "Create devcontainer.json",
      "state": "open"
    },
    {
      "number": 1201,
      "title": "Update README.md",
      "state": "closed"
    },
    {
      "number": 1183,
      "title": "Update data_qwen.py",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "d2240f11656bfe404b9ba56db4e51cd09f522ff1",
      "author": "ShuaiBai623",
      "date": "2025-05-15T03:54:08+00:00",
      "message": "Merge pull request #1145 from Hui-design/pr_tch"
    },
    {
      "sha": "dae28cb93a4b262df6f95550038ee436cce93e0e",
      "author": "ShuaiBai623",
      "date": "2025-05-15T03:53:11+00:00",
      "message": "Merge pull request #1201 from creeponsky/patch-1"
    },
    {
      "sha": "712d3d011e958caca4f27b3670a0d5ab6894ad08",
      "author": "ShuaiBai623",
      "date": "2025-05-15T03:52:17+00:00",
      "message": "Merge pull request #1183 from John-Ge/patch-1"
    },
    {
      "sha": "f675047c48d8033bcd576ce3a4ab23e469793ca9",
      "author": "Junyang Lin",
      "date": "2025-05-15T03:41:11+00:00",
      "message": "Update data_qwen.py"
    },
    {
      "sha": "7dae6d11d12cb75c7a8e133eabbeb8b6ee87ea04",
      "author": "Junyang Lin",
      "date": "2025-05-15T03:40:53+00:00",
      "message": "fix typo"
    },
    {
      "sha": "6e98a0a62bce167c5802ae6f5f95fcd97d2634cf",
      "author": "CreepOnSky",
      "date": "2025-05-13T02:22:59+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "4ed134c02e5b541e7eaaebbd48eee701137d1b48",
      "author": "Chunjiang Ge (\u845b\u6625\u6c5f)",
      "date": "2025-05-07T14:22:36+00:00",
      "message": "Update data_qwen.py"
    },
    {
      "sha": "29a9da773339adc8145302573f2f1571249aca47",
      "author": "ShuaiBai623",
      "date": "2025-05-04T06:30:47+00:00",
      "message": "Merge pull request #1171 from John-Ge/main"
    },
    {
      "sha": "dda092435b49d083cbadd8984ac0abdcfbfca09d",
      "author": "John-Ge",
      "date": "2025-05-04T05:26:09+00:00",
      "message": "packing"
    },
    {
      "sha": "a30e36facd0a5131d9ed59e93210c7ac5de75adb",
      "author": "Hui-design",
      "date": "2025-04-23T13:06:39+00:00",
      "message": "fix bug of smart_resize"
    },
    {
      "sha": "477fd9d4317266508705366ce36cac5b68d70936",
      "author": "ShuaiBai623",
      "date": "2025-04-23T08:32:08+00:00",
      "message": "Merge pull request #1127 from qykong/main"
    },
    {
      "sha": "5d0970c2ca9281ae45856c7560c1a491769ded74",
      "author": "ShuaiBai623",
      "date": "2025-04-23T08:31:17+00:00",
      "message": "Merge pull request #1124 from Zhaohai-Li/zhaohai_dev/add_mmmu_bench"
    },
    {
      "sha": "96ddf9594a9cbc4507015cd004a471e2399b1cbf",
      "author": "Zhaohai-Li",
      "date": "2025-04-18T17:56:11+00:00",
      "message": "add inference code and evaluation code for mmmu bench"
    },
    {
      "sha": "465b320d4bc365782513d500940ec76e8e06bfb9",
      "author": "Quyu Kong",
      "date": "2025-04-18T11:44:47+00:00",
      "message": "Fixed `draw_point` to wrong images in computer use notebook"
    },
    {
      "sha": "fe0d43a3b74d70b40d28062c8b44d05978a0ed98",
      "author": "ShuaiBai623",
      "date": "2025-04-18T06:20:39+00:00",
      "message": "Merge pull request #1121 from scutyuanzhi/main"
    },
    {
      "sha": "a6da42ee420057efeae207fb882936b3e04a9242",
      "author": "Kpillow",
      "date": "2025-04-18T04:00:46+00:00",
      "message": "fix typo"
    },
    {
      "sha": "739903ce74f6228c4e7c3a2e59cdf5ea1284b915",
      "author": "ShuaiBai623",
      "date": "2025-04-10T03:30:58+00:00",
      "message": "Merge pull request #1085 from sibosutd/main"
    },
    {
      "sha": "e8ec16d1597a4acd021272fc5c4ff102019a6203",
      "author": "ShuaiBai623",
      "date": "2025-04-10T03:30:17+00:00",
      "message": "Merge pull request #1080 from ygicp/patch-1"
    },
    {
      "sha": "9bbccac640d528129c31331b49f1c4010bcedfc1",
      "author": "sibo.ssb",
      "date": "2025-04-10T03:22:17+00:00",
      "message": "feat(qwen-vl-utils): add torchcodec support and enhance video processing"
    },
    {
      "sha": "e994ad452d36cbfaabccde0cb713c58d9d0d1c0e",
      "author": "ygicp",
      "date": "2025-04-09T10:04:10+00:00",
      "message": "Update video_understanding.ipynb - Making a calculation simpler to understand."
    }
  ],
  "readme_text": "# Qwen2.5-VL\n\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL/qwen2.5vl_logo.png\" width=\"400\"/>\n<p>\n\n<p align=\"center\">\n        \ud83d\udc9c <a href=\"https://chat.qwenlm.ai/\"><b>Qwen Chat</b></a>&nbsp&nbsp | &nbsp&nbsp\ud83e\udd17 <a href=\"https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5\">Hugging Face</a>&nbsp&nbsp | &nbsp&nbsp\ud83e\udd16 <a href=\"https://modelscope.cn/organization/qwen\">ModelScope</a>&nbsp&nbsp | &nbsp&nbsp\ud83d\udcd1 <a href=\"https://qwenlm.github.io/blog/qwen2.5-vl/\">Blog</a>&nbsp&nbsp | &nbsp&nbsp\ud83d\udcda <a href=\"https://github.com/QwenLM/Qwen2.5-VL/tree/main/cookbooks\">Cookbooks</a>&nbsp&nbsp | &nbsp&nbsp\ud83d\udcd1 <a href=\"https://arxiv.org/abs/2502.13923\">Paper</a>&nbsp&nbsp\n<br>\n\ud83d\udda5\ufe0f <a href=\"https://huggingface.co/spaces/Qwen/Qwen2.5-VL-72B-Instruct\">Demo</a>&nbsp&nbsp | &nbsp&nbsp\ud83d\udcac <a href=\"https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat (\u5fae\u4fe1)</a>&nbsp&nbsp | &nbsp&nbsp\ud83e\udee8 <a href=\"https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp | &nbsp&nbsp\ud83d\udcd1 <a href=\"https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api\">API</a>&nbsp&nbsp | &nbsp&nbsp\ud83d\udda5\ufe0f <a href=\"https://gallery.pai-ml.com/#/preview/deepLearning/cv/qwen2.5-vl\">PAI-DSW</a>\n</p>\n\n\n\n\n## Introduction\nIn the past five months since Qwen2-VL's release, numerous developers have built new models on the Qwen2-VL vision-language models, providing us with valuable feedback. During this period, we focused on building more useful vision-language models. Today, we are excited to introduce the latest addition to the Qwen family: Qwen2.5-VL.\n\n\n#### Key Enhancements:\n\n* **Powerful Document Parsing Capabilities**: Upgrade text recognition to omnidocument parsing, excelling in processing multi-scene, multilingual, and various built-in (handwriting, tables, charts, chemical formulas, and music sheets) documents.\n\n* **Precise Object Grounding Across Formats**: Unlock improved accuracy in detecting, pointing, and counting objects, accommodating absolute coordinate and JSON formats for advanced spatial reasoning.\n\n* **Ultra-long Video Understanding and Fine-grained Video Grounding**: Extend native dynamic resolution to the temporal dimension, enhancing the ability to understand videos lasting hours while extracting event segments in seconds.\n\n* **Enhanced Agent Functionality for Computer and Mobile Devices**: Leverage advanced grounding, reasoning, and decision-making abilities, boosting the model with superior agent functionality on smartphones and computers.\n\n\n#### Model Architecture Updates:\n\n* **Dynamic Resolution and Frame Rate Training for Video Understanding**:\n\nWe extend dynamic resolution to the temporal dimension by adopting dynamic FPS sampling, enabling the model to comprehend videos at various sampling rates. Accordingly, we update mRoPE in the time dimension with IDs and absolute time alignment, enabling the model to learn temporal sequence and speed, and ultimately acquire the ability to pinpoint specific moments.\n\n<p align=\"center\">\n    <img src=\"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL/qwen2.5vl_arc.jpeg\" width=\"80%\"/>\n<p>\n\n\n* **Streamlined and Efficient Vision Encoder**\n\nWe enhance both training and inference speeds by strategically implementing window attention into the ViT. The ViT architecture is further optimized with SwiGLU and RMSNorm, aligning it with the structure of the Qwen2.5 LLM.\n\n\n## News\n* 2025.04.08: We provide the [code](https://github.com/QwenLM/Qwen2.5-VL/tree/main/qwen-vl-finetune) for fine-tuning Qwen2-VL and Qwen2.5-VL.\n* 2025.03.25: We have released the [Qwen2.5-VL-32B](https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct). It is smarter and its responses align more closely with human preferences. For more details, please check our [blog](https://qwenlm.github.io/blog/qwen2.5-vl-32b/)!\n* 2025.02.20: we have released the [Qwen2.5-VL Technical Report](https://arxiv.org/abs/2502.13923). Alongside the report, we have also released AWQ-quantized models for Qwen2.5-VL in three different sizes: [3B](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct-AWQ), [7B](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct-AWQ) , and [72B](https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ) parameters.\n* 2025.01.28: We have released the [Qwen2.5-VL series](https://huggingface.co/Qwen). For more details, please check our [blog](https://qwenlm.github.io/blog/qwen2.5-vl/)!\n* 2024.12.25: We have released the [QvQ-72B-Preview](https://huggingface.co/Qwen/QVQ-72B-Preview). QvQ-72B-Preview is an experimental research model, focusing on enhancing visual reasoning capabilities. For more details, please check our [blog](https://qwenlm.github.io/blog/qvq-72b-preview/)!\n* 2024.09.19: The instruction-tuned [Qwen2-VL-72B model](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct) and its quantized version [[AWQ](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-AWQ), [GPTQ-Int4](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4), [GPTQ-Int8](https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8)] are now available. We have also released the [Qwen2-VL paper](https://arxiv.org/pdf/2409.12191) simultaneously.\n* 2024.08.30: We have released the [Qwen2-VL series](https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d). The 2B and 7B models are now available, and the 72B model for open source is coming soon. For more details, please check our [blog](https://qwenlm.github.io/blog/qwen2-vl/)!\n\n\n## Performance\n\n\n\n| Dataset            | Qwen2.5-VL-3B<br><sup>([\ud83e\udd17](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)[\ud83e\udd16](https://modelscope.cn/models/qwen/Qwen2.5-VL-3B-Instruct))     | Qwen2.5-VL-7B<br><sup>([\ud83e\udd17](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)[\ud83e\udd16](https://modelscope.cn/models/qwen/Qwen2.5-VL-7B-Instruct))    | Qwen2.5-VL-32B<br><sup>([\ud83e\udd17](https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct)[\ud83e\udd16](https://modelscope.cn/models/qwen/Qwen2.5-VL-32B-Instruct))    | Qwen2.5-VL-72B<br><sup>([\ud83e\udd17](https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct)[\ud83e\udd16](https://modelscope.cn/models/qwen/Qwen2.5-VL-72B-Instruct)) | Gemini-2 Flash | GPT-4o | Claude3.5 Sonnet | Qwen2-VL 72B | \n|--------------------|--------|--------|--------|----------------|----------------|--------|------------------|--------------|\n| MMMU               | 53.1  | 58.6 | 70.0  | **70.2**           | **70.7**           | 70.3   | 70.4             | 64.5         | \n| MMMU Pro           | 31.6  | 38.3 |49.5|**51.1**           | **57**             | 54.5   | 54.7             | 46.2         | \n| DocVQA             | 93.9 | 95.7 | 94.8 |**96.4**           | 92.1           | 91.1   | 95.2             | **96.5**         | \n| InfoVQA            | 77.1 | 82.6 | 83.4| **87.3**          | 77.8           | 80.7   | 74.3             | 84.5         | \n| CC-OCR             | 74.5 |  77.8 | 77.1 |**79.8**          | 73.0           | 66.6   | 62.7             | 68.7         | \n| OCRBenchV2         | 54.3/52.1 | 56.3/57.2 | 57.2/59.1| **61.5/63.7**      | -              | 46.5/32.3 | 45.2/39.6       | 47.8/46.1    | \n| MegaBench          | 28.9 |36.8 | - |**51.3**          | **55.2**           | 54.2   | 52.1             | 46.8         | \n| MMStar             | 55.8|63.9| 69.5 |**70.8**           | 69.4           | 64.7   | 65.1             | 68.3         | \n| MMBench1.1         |  81.5  | 84.3 | 84.6|**88.0**           | 83.0           | 82.1   | 83.4             | 86.6         | \n| MathVista          | 62.3  | 68.2  | 74.7 |**74.8**           | 73.1           | 63.8   | 65.4             | 70.5         | \n| MathVision         | 21.2  | 25.1 | **38.4** |38.1           | **41.3**           | 30.4   | 38.3             | 25.9         | \n| VideoMME           | 61.5/67.6 | 65.1/71.6 | 70.5/77.9 |**73.3/79.1**      | -/-            | 71.9/77.2 | 60/62.9         | 71.2/77.8    | \n| MMBench-Video      | 1.63           | 1.79    | 1.93 |**2.02**           | -              | 1.68   | 1.38             | 1.7          | \n| LVBench            | 43.3        | 45.3 | **49.0** | 47.3           | -              | 30.8   | -                | -            | \n| CharadesSTA        | 38.8        | 43.6    |   **54.2** | 50.9           | -              | 35.7   | -                | -            | \n| AITZ               | 76.9|81.9 | 83.1 |**83.2**           | -              | 35.3   | -                | -            | \n| Android Control    | 63.7/90.8 | 60.1/91.4 | 69.6/93.3 |**67.36/93.7**     | -              | -      | -                | 66.4/84.4    | \n| ScreenSpot         | 55.5|84.7| **88.5** | 87.1          | 84.0           | 18.1   | 83.0             | -            | \n| ScreenSpot Pro     | 23.9|29.0 |  39.4 |**43.6**           | -              | -      | 17.1             | -            | \n| AndroidWorld       | -  | -  | 22.0 |**35**             | -              | 34.5(SoM) | 27.9            | -            | \n| OSWorld            | -  | -  | 5.92 |**8.83**           | -              | 5.03   | **14.9**             | -            | \n\n\n\n\n\n## Quickstart\n\nBelow, we provide simple examples to show how to use Qwen2.5-VL with \ud83e\udd16 ModelScope and \ud83e\udd17 Transformers.\n\nThe code of Qwen2.5-VL has been in the latest Hugging face transformers and we advise you to build from source with command:\n```\npip install transformers==4.51.3 accelerate\n```\nor you might encounter the following error:\n```\nKeyError: 'qwen2_5_vl'\n```\n\n\nWe offer a toolkit to help you handle various types of visual input more conveniently, as if you were using an API. This includes base64, URLs, and interleaved images and videos. You can install it using the following command:\n\n```bash\n# It's highly recommended to use `[decord]` feature for faster video loading.\npip install qwen-vl-utils[decord]\n```\n\nCurrently, `qwen-vl-utils` supports three video decoding backends: `torchvision`, `decord`, and `torchcodec`. While `decord` and `torchcodec` generally offer significantly faster decoding speeds compared to `torchvision`, we recommend using `torchcodec`. This is because `decord` has known issues, such as decoding hangs, and its project is no longer actively maintained.\n\n- For `decord`, if you are not using Linux, you might not be able to install `decord` from PyPI. In that case, you can use `pip install qwen-vl-utils` which will fall back to using torchvision for video processing. However, you can still [install decord from source](https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source) to get decord used when loading video.\n\n- To use `torchcodec` as the backend for video decoding, follow the installation instructions provided in the official [torchcodec repository](https://github.com/pytorch/torchcodec/tree/main?tab=readme-ov-file#installing-torchcodec) and install it manually. Note that `torchcodec` depends on FFmpeg for decoding functionality.\n\n## Cookbooks\n\nWe are preparing [cookbooks](https://github.com/QwenLM/Qwen2.5-VL/tree/main/cookbooks) for many capabilities, including recognition, localization, document parsing, video understanding, key information extraction, and more. Welcome to learn more!\n\n| Cookbook | Description | Open |\n| -------- | ----------- | ---- |\n| [Universal Recognition](https://github.com/QwenLM/Qwen2.5-VL/blob/main/cookbooks/universal_recognition.ipynb) | Not only identify animals, plants, people, and scenic spots but also recognize various objects such as cars and merchandise. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/universal_recognition.ipynb) |\n| [Powerful Document Parsing Capabilities](https://github.com/QwenLM/Qwen2.5-VL/blob/main/cookbooks/document_parsing.ipynb) | The parsing of documents has reached a higher level, including not only text but also layout position information and our Qwen HTML format. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/document_parsing.ipynb) |\n| [Precise Object Grounding Across Formats](https://github.com/QwenLM/Qwen2.5-VL/blob/main/cookbooks/spatial_understanding.ipynb) | Using absolute position coordinates, it supports both boxes and points, allowing for diverse combinations of positioning and labeling tasks. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/spatial_understanding.ipynb) |\n| [General OCR and Key Information Extraction](https://github.com/QwenLM/Qwen2.5-VL/blob/main/cookbooks/ocr.ipynb) | Stronger text recognition capabilities in natural scenes and multiple languages, supporting diverse key information extraction needs. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/ocr.ipynb) |\n| [Video Understanding](https://github.com/QwenLM/Qwen2.5-VL/blob/main/cookbooks/video_understanding.ipynb) | Better video OCR, long video understanding, and video grounding. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/video_understanding.ipynb) |\n| [Mobile Agent](https://github.com/QwenLM/Qwen2.5-VL/blob/main/cookbooks/mobile_agent.ipynb) | Locate and think for mobile phone control. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/mobile_agent.ipynb) |\n| [Computer-Use Agent](https://github.com/QwenLM/Qwen2.5-VL/blob/main/cookbooks/computer_use.ipynb) | Locate and think for controlling computers and Web. | [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/computer_use.ipynb) |\n\n\n### Using \ud83e\udd17  Transformers to Chat\n\nHere we show a code snippet to show you how to use the chat model with `transformers` and `qwen_vl_utils`:\n\n```python\nfrom transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\nfrom qwen_vl_utils import process_vision_info\n\n# default: Load the model on the available device(s)\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-VL-7B-Instruct\", torch_dtype=\"auto\", device_map=\"auto\"\n)\n\n# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n#     \"Qwen/Qwen2.5-VL-7B-Instruct\",\n#     torch_dtype=torch.bfloat16,\n#     attn_implementation=\"flash_attention_2\",\n#     device_map=\"auto\",\n# )\n\n# default processor\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n\n# The default range for the number of visual tokens per image in the model is 4-16384.\n# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n# min_pixels = 256*28*28\n# max_pixels = 1280*28*28\n# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(model.device)\n\n# Inference: Generation of the output\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\n<details>\n<summary>Minimum VRAM requirements</summary>\n\n| Precision | Qwen2.5-VL-3B | Qwen2.5-VL-7B | Qwen2.5-VL-72B |\n|-----------|------------| --------- | -------- |\n| FP32      | 11.5 GB    | 26.34 GB  | 266.21 GB |\n| BF16      | 5.75 GB    | 13.17 GB  | 133.11 GB |\n| INT8      | 2.87 GB    | 6.59 GB   | 66.5 GB |\n| INT4      | 1.44 GB    | 3.29 GB   | 33.28 GB |\n\nNote: The table above presents the theoretical minimum video memory requirements for inference with `transformers`; however, in practice, the actual memory usage is typically at least 1.2 times higher. For more information, see the linked resource [here](https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator).\n</details>\n\n\n<details>\n<summary>Multi image inference</summary>\n\n```python\n# Messages containing multiple images and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n            {\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n            {\"type\": \"text\", \"text\": \"Identify the similarities between these images.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n</details>\n\n<details>\n<summary>Video inference</summary>\n\n```python\n# Messages containing a images list as a video and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": [\n                    \"file:///path/to/frame1.jpg\",\n                    \"file:///path/to/frame2.jpg\",\n                    \"file:///path/to/frame3.jpg\",\n                    \"file:///path/to/frame4.jpg\",\n                ],\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n# Messages containing a local video path and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": \"file:///path/to/video1.mp4\",\n                \"max_pixels\": 360 * 420,\n                \"fps\": 1.0,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n# Messages containing a video url and a text query\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"video\",\n                \"video\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",\n                \"min_pixels\": 4 * 28 * 28,\n                \"max_pixels\": 256 * 28 * 28,\n                \"total_pixels\": 20480 * 28 * 28,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this video.\"},\n        ],\n    }\n]\n\n# Preparation for inference\ntext = processor.apply_chat_template(\n    messages, tokenize=False, add_generation_prompt=True\n)\nimage_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\ninputs = processor(\n    text=[text],\n    images=image_inputs,\n    videos=video_inputs,\n    fps=fps,\n    padding=True,\n    return_tensors=\"pt\",\n    **video_kwargs,\n)\ninputs = inputs.to(\"cuda\")\n\n# Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_text = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_text)\n```\n\n#### Video URL compatibility\n\nVideo URL compatibility is primarily determined by the version of the third-party library being used. For more details, refer to the table below. If you prefer not to use the default backend, you can switch it by setting `FORCE_QWENVL_VIDEO_READER` to `torchvision`, `decord`, or `torchcodec`.\n\n| Backend     | HTTP | HTTPS |\n|-------------|------|-------|\n| torchvision >= 0.19.0 | \u2705  | \u2705   |\n| torchvision < 0.19.0  | \u274c  | \u274c   |\n| decord      | \u2705  | \u274c   |\n| torchcodec  | \u2705  | \u2705   |\n\n#### Configuration for adjusting video resolution\n\nWe recommend setting appropriate values for the `min_pixels` and `max_pixels` parameters based on available GPU memory and the specific application scenario to restrict the resolution of individual frames in the video. Alternatively, you can use the `total_pixels` parameter to limit the total number of tokens in the video (it is recommended to set this value below 24576 * 28 * 28 to avoid excessively long input sequences). For more details on parameter usage and processing logic, please refer to the `fetch_video` function in `qwen_vl_utils/vision_process.py`.\n\n</details>\n\n<details>\n<summary>Batch inference</summary>\n\n```python\n# Sample messages for batch inference\nmessages1 = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/image1.jpg\"},\n            {\"type\": \"image\", \"image\": \"file:///path/to/image2.jpg\"},\n            {\"type\": \"text\", \"text\": \"What are the common elements in these pictures?\"},\n        ],\n    }\n]\nmessages2 = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"Who are you?\"},\n]\n# Combine messages for batch processing\nmessages = [messages1, messages2]\n\n# Preparation for batch inference\ntexts = [\n    processor.apply_chat_template(msg, tokenize=False, add_generation_prompt=True)\n    for msg in messages\n]\nimage_inputs, video_inputs = process_vision_info(messages)\ninputs = processor(\n    text=texts,\n    images=image_inputs,\n    videos=video_inputs,\n    padding=True,\n    return_tensors=\"pt\",\n)\ninputs = inputs.to(\"cuda\")\n\n# Batch Inference\ngenerated_ids = model.generate(**inputs, max_new_tokens=128)\ngenerated_ids_trimmed = [\n    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n]\noutput_texts = processor.batch_decode(\n    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n)\nprint(output_texts)\n```\n</details>\n\n### \ud83e\udd16 ModelScope\nWe strongly advise users especially those in mainland China to use ModelScope. `snapshot_download` can help you solve issues concerning downloading checkpoints.\n\n### More Usage Tips\n\nFor input images, we support local files, base64, and URLs. For videos, we currently only support local files.\n\n```python\n# You can directly insert a local file path, a URL, or a base64-encoded image into the position where you want in the text.\n## Local file path\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"file:///path/to/your/image.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n## Image URL\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"http://path/to/your/image.jpg\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n## Base64 encoded image\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"image\", \"image\": \"data:image;base64,/9j/...\"},\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n```\n#### Image Resolution for performance boost\n\nThe model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs, such as a token count range of 256-1280, to balance speed and memory usage.\n\n```python\nmin_pixels = 256 * 28 * 28\nmax_pixels = 1280 * 28 * 28\nprocessor = AutoProcessor.from_pretrained(\n    \"Qwen/Qwen2.5-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels\n)\n```\n\nBesides, We provide two methods for fine-grained control over the image size input to the model:\n\n1. Specify exact dimensions: Directly set `resized_height` and `resized_width`. These values will be rounded to the nearest multiple of 28.\n\n2. Define min_pixels and max_pixels: Images will be resized to maintain their aspect ratio within the range of min_pixels and max_pixels.\n\n```python\n# resized_height and resized_width\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"file:///path/to/your/image.jpg\",\n                \"resized_height\": 280,\n                \"resized_width\": 420,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n# min_pixels and max_pixels\nmessages = [\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"file:///path/to/your/image.jpg\",\n                \"min_pixels\": 50176,\n                \"max_pixels\": 50176,\n            },\n            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n        ],\n    }\n]\n```\n\n#### Add ids for Multiple Image Inputs\nBy default, images and video content are directly included in the conversation. When handling multiple images, it's helpful to add labels to the images and videos for better reference. Users can control this behavior with the following settings:\n<details>\n<summary>Add vision ids</summary>\n\n```python\nconversation = [\n    {\n        \"role\": \"user\",\n        \"content\": [{\"type\": \"image\"}, {\"type\": \"text\", \"text\": \"Hello, how are you?\"}],\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"I'm doing well, thank you for asking. How can I assist you today?\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\"type\": \"text\", \"text\": \"Can you describe these images and video?\"},\n            {\"type\": \"image\"},\n            {\"type\": \"image\"},\n            {\"type\": \"video\"},\n            {\"type\": \"text\", \"text\": \"These are from my vacation.\"},\n        ],\n    },\n    {\n        \"role\": \"assistant\",\n        \"content\": \"I'd be happy to describe the images and video for you. Could you please provide more context about your vacation?\",\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"It was a trip to the mountains. Can you see the details in the images and video?\",\n    },\n]\n\n# default:\nprompt_without_id = processor.apply_chat_template(\n    conversation, add_generation_prompt=True\n)\n# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Hello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI'm doing well, thank you for asking. How can I assist you today?<|im_end|>\\n<|im_start|>user\\nCan you describe these images and video?<|vision_start|><|image_pad|><|vision_end|><|vision_start|><|image_pad|><|vision_end|><|vision_start|><|video_pad|><|vision_end|>These are from my vacation.<|im_end|>\\n<|im_start|>assistant\\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?<|im_end|>\\n<|im_start|>user\\nIt was a trip to the mountains. Can you see the details in the images and video?<|im_end|>\\n<|im_start|>assistant\\n'\n\n\n# add ids\nprompt_with_id = processor.apply_chat_template(\n    conversation, add_generation_prompt=True, add_vision_id=True\n)\n# Excepted output: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nPicture 1: <|vision_start|><|image_pad|><|vision_end|>Hello, how are you?<|im_end|>\\n<|im_start|>assistant\\nI'm doing well, thank you for asking. How can I assist you today?<|im_end|>\\n<|im_start|>user\\nCan you describe these images and video?Picture 2: <|vision_start|><|image_pad|><|vision_end|>Picture 3: <|vision_start|><|image_pad|><|vision_end|>Video 1: <|vision_start|><|video_pad|><|vision_end|>These are from my vacation.<|im_end|>\\n<|im_start|>assistant\\nI'd be happy to describe the images and video for you. Could you please provide more context about your vacation?<|im_end|>\\n<|im_start|>user\\nIt was a trip to the mountains. Can you see the details in the images and video?<|im_end|>\\n<|im_start|>assistant\\n'\n```\n</details>\n\n#### Flash-Attention 2 to speed up generation\n\nFirst, make sure to install the latest version of Flash Attention 2:\n\n```bash\npip install -U flash-attn --no-build-isolation\n```\n\nAlso, you should have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of the [flash attention repository](https://github.com/Dao-AILab/flash-attention). FlashAttention-2 can only be used when a model is loaded in `torch.float16` or `torch.bfloat16`.\n\nTo load and run a model using Flash Attention-2, simply add `attn_implementation=\"flash_attention_2\"` when loading the model as follows:\n\n```python\nfrom transformers import Qwen2_5_VLForConditionalGeneration\n\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    \"Qwen/Qwen2.5-VL-7B-Instruct\", \n    torch_dtype=torch.bfloat16, \n    attn_implementation=\"flash_attention_2\",\n)\n```\n\n### Processing Long Texts\n\nThe current `config.json` is set for context length up to 32,768 tokens.\nTo handle extensive inputs exceeding 32,768 tokens, we utilize [YaRN](https://arxiv.org/abs/2309.00071), a technique for enhancing model length extrapolation, ensuring optimal performance on lengthy texts.\n\nFor supported frameworks, you could add the following to `config.json` to enable YaRN:\n\n```\n{\n\t...,\n    \"type\": \"yarn\",\n    \"mrope_section\": [\n        16,\n        24,\n        24\n    ],\n    \"factor\": 4,\n    \"original_max_position_embeddings\": 32768\n}\n```\n\nHowever, it should be noted that this method has a significant impact on the performance of temporal and spatial localization tasks, and is therefore not recommended for use.\n\nAt the same time, for long video inputs, since MRoPE itself is more economical with ids, the max_position_embeddings can be directly modified to a larger value, such as 64k.\n\n\n### Try Qwen2.5-VL-72B with API!\n\nTo explore Qwen2.5-VL-72B, a more fascinating multimodal model, we encourage you to test our cutting-edge API service. Let's start the exciting journey right now!\n\n#### Installation\n```bash\npip install dashscope\n```\n\n#### Examples\n```python\nimport dashscope\n\n\ndashscope.api_key = \"your_api_key\"\n\nmessages = [{\n    'role': 'user',\n    'content': [\n        {\n            'image': \"https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"\n        },\n        {\n            'text': 'What are in the image?'\n        },\n    ]\n}]\n\nresponse = dashscope.MultiModalConversation.call(model='qwen2.5-vl-72b-instruct', messages=messages)\nprint(response)\n```\n\nFor more usage, please refer to the tutorial at [aliyun](https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api).\n\n\n\n\n## Demo\n### Web UI Example\n\nIn this section, we provide instructions for users to build a web-based user interface (UI) demo. This UI demo allows users to interact with a predefined model or application through a web browser. Follow the steps below to get started.\n\n#### Installation\n\nBefore you begin, ensure that you have the required dependencies installed on your system. You can install them by running the following command:\n\n```bash\npip install -r requirements_web_demo.txt\n```\n\n#### Running the Demo with FlashAttention-2\n\nOnce the required packages are installed, you can launch the web demo using the following command. This command will start a web server and provide you with a link to access the UI in your web browser.\n\n**Recommended**: For enhanced performance and efficiency, especially in multi-image and video processing scenarios, we strongly recommend using [FlashAttention-2](https://github.com/Dao-AILab/flash-attention). FlashAttention-2 provides significant improvements in memory usage and speed, making it ideal for handling large-scale models and data processing.\n\nTo enable FlashAttention-2, use the following command:\n\n```bash\npython web_demo_mm.py --flash-attn2\n```\n\nThis will load the model with FlashAttention-2 enabled.\n\n**Default Usage**: If you prefer to run the demo without FlashAttention-2 or if you do not specify the `--flash-attn2` option, the demo will load the model using the standard attention implementation:\n\n```bash\npython web_demo_mm.py\n```\n\nAfter running the command, you\u2019ll see a link generated in the terminal similar to this:\n\n```\nRunning on local: http://127.0.0.1:7860/\n```\n\nCopy this link and paste it into your browser to access the web UI, where you can interact with the model by inputting text, uploading images, or using any other provided functionalities.\n\n##### Running the Streaming Video Chat Demo\nAn experimental streaming video chat demo is also available in the ``web_demo_streaming`` directory.\n\nTo run the streaming video chat demo, use the following command:\n\n```bash\ncd web_demo_streaming/\npython app.py --flash-attn2\n```\n\nIf you prefer to run the demo without FlashAttention-2, use the following command:\n```bash\ncd web_demo_streaming/\npython app.py\n```\n\nThis demo supports webcam/screen capture as its video input source. To support screen capture video input, we use code snippet from the following hugginface space: [gstaff/gradio-screen-recorder](https://huggingface.co/spaces/gstaff/gradio-screen-recorder/tree/main).\n\n\n## Deployment\n\nWe recommend using vLLM for fast Qwen2.5-VL deployment and inference. You need to install `vllm>0.7.2` to enable Qwen2.5-VL support. You can also use our [official docker image](#-docker).\n\nYou can also check [vLLM official documentation](https://docs.vllm.ai/en/latest/serving/multimodal_inputs.html) for more details about online serving and offline inference.\n\n### Installation\n```bash\npip install git+https://github.com/huggingface/transformers@f3f6c86582611976e72be054675e2bf0abb5f775\npip install accelerate\npip install qwen-vl-utils\npip install 'vllm>0.7.2'\n\n```\n### Start an OpenAI API Service\n\nRun the command below to start an OpenAI-compatible API service:\n\n```bash\nvllm serve Qwen/Qwen2.5-VL-7B-Instruct --port 8000 --host 0.0.0.0 --dtype bfloat16 --limit-mm-per-prompt image=5,video=5\n```\n\nThen you can use the chat API as below (via curl or Python API):\n\n```bash\ncurl http://localhost:8000/v1/chat/completions \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n    \"model\": \"Qwen/Qwen2.5-VL-7B-Instruct\",\n    \"messages\": [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png\"}},\n        {\"type\": \"text\", \"text\": \"What is the text in the illustrate?\"}\n    ]}\n    ]\n    }'\n```\n\n```python\nfrom openai import OpenAI\n\n# Set OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\nchat_response = client.chat.completions.create(\n    model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png\"\n                    },\n                },\n                {\"type\": \"text\", \"text\": \"What is the text in the illustrate?\"},\n            ],\n        },\n    ],\n)\nprint(\"Chat response:\", chat_response)\n```\n\nYou can also upload base64-encoded local images (see [OpenAI API protocol document](https://platform.openai.com/docs/guides/vision/uploading-base-64-encoded-images) for more details):\n```python\nimport base64\nfrom openai import OpenAI\n# Set OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\nimage_path = \"/path/to/local/image.png\"\nwith open(image_path, \"rb\") as f:\n    encoded_image = base64.b64encode(f.read())\nencoded_image_text = encoded_image.decode(\"utf-8\")\nbase64_qwen = f\"data:image;base64,{encoded_image_text}\"\nchat_response = client.chat.completions.create(\n    model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": base64_qwen\n                    },\n                },\n                {\"type\": \"text\", \"text\": \"What is the text in the illustrate?\"},\n            ],\n        },\n    ],\n)\nprint(\"Chat response:\", chat_response)\n```\n\nFor videos, you can use the chat API  as follows\uff1a\n```python\nimport base64\nimport numpy as np\nfrom PIL import Image\nfrom io import BytesIO\nfrom openai import OpenAI\nfrom qwen_vl_utils import process_vision_info\n\n\n# Set OpenAI's API key and API base to use vLLM's API server.\nopenai_api_key = \"EMPTY\"\nopenai_api_base = \"http://localhost:8000/v1\"\n\nclient = OpenAI(\n    api_key=openai_api_key,\n    base_url=openai_api_base,\n)\n\n\nvideo_messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": [\n        {\"type\": \"text\", \"text\": \"\u8bf7\u7528\u8868\u683c\u603b\u7ed3\u4e00\u4e0b\u89c6\u9891\u4e2d\u7684\u5546\u54c1\u7279\u70b9\"},\n        {\n            \"type\": \"video\",\n            \"video\": \"https://duguang-labelling.oss-cn-shanghai.aliyuncs.com/qiansun/video_ocr/videos/50221078283.mp4\",\n            \"total_pixels\": 20480 * 28 * 28, \"min_pixels\": 16 * 28 * 2, \n            'fps': 3.0  # The default value is 2.0, but for demonstration purposes, we set it to 3.0.\n        }]\n    },\n]\n\n\ndef prepare_message_for_vllm(content_messages):\n    \"\"\"\n    The frame extraction logic for videos in `vLLM` differs from that of `qwen_vl_utils`.\n    Here, we utilize `qwen_vl_utils` to extract video frames, with the `media_typ`e of the video explicitly set to `video/jpeg`.\n    By doing so, vLLM will no longer attempt to extract frames from the input base64-encoded images.\n    \"\"\"\n    vllm_messages, fps_list = [], []\n    for message in content_messages:\n        message_content_list = message[\"content\"]\n        if not isinstance(message_content_list, list):\n            vllm_messages.append(message)\n            continue\n\n        new_content_list = []\n        for part_message in message_content_list:\n            if 'video' in part_message:\n                video_message = [{'content': [part_message]}]\n                image_inputs, video_inputs, video_kwargs = process_vision_info(video_message, return_video_kwargs=True)\n                assert video_inputs is not None, \"video_inputs should not be None\"\n                video_input = (video_inputs.pop()).permute(0, 2, 3, 1).numpy().astype(np.uint8)\n                fps_list.extend(video_kwargs.get('fps', []))\n\n                # encode image with base64\n                base64_frames = []\n                for frame in video_input:\n                    img = Image.fromarray(frame)\n                    output_buffer = BytesIO()\n                    img.save(output_buffer, format=\"jpeg\")\n                    byte_data = output_buffer.getvalue()\n                    base64_str = base64.b64encode(byte_data).decode(\"utf-8\")\n                    base64_frames.append(base64_str)\n\n                part_message = {\n                    \"type\": \"video_url\",\n                    \"video_url\": {\"url\": f\"data:video/jpeg;base64,{','.join(base64_frames)}\"}\n                }\n            new_content_list.append(part_message)\n        message[\"content\"] = new_content_list\n        vllm_messages.append(message)\n    return vllm_messages, {'fps': fps_list}\n\n\nvideo_messages, video_kwargs = prepare_message_for_vllm(video_messages)\nchat_response = client.chat.completions.create(\n    model=\"Qwen/Qwen2.5-VL-7B-Instruct\",\n    messages=video_messages,\n    extra_body={\n        \"mm_processor_kwargs\": video_kwargs\n    }\n)\nprint(\"Chat response:\", chat_response)\n```\n\n### Inference Locally\n\nYou can also use vLLM to inference Qwen2.5-VL locally:\n\n```python\nfrom transformers import AutoProcessor\nfrom vllm import LLM, SamplingParams\nfrom qwen_vl_utils import process_vision_info\n\nMODEL_PATH = \"Qwen/Qwen2.5-VL-7B-Instruct\"\n\nllm = LLM(\n    model=MODEL_PATH,\n    limit_mm_per_prompt={\"image\": 10, \"video\": 10},\n)\n\nsampling_params = SamplingParams(\n    temperature=0.1,\n    top_p=0.001,\n    repetition_penalty=1.05,\n    max_tokens=256,\n    stop_token_ids=[],\n)\n\nimage_messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\n        \"role\": \"user\",\n        \"content\": [\n            {\n                \"type\": \"image\",\n                \"image\": \"https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png\",\n                \"min_pixels\": 224 * 224,\n                \"max_pixels\": 1280 * 28 * 28,\n            },\n            {\"type\": \"text\", \"text\": \"What is the text in the illustrate?\"},\n        ],\n    },\n]\n\n\n# For video input, you can pass following values instead:\n# \"type\": \"video\",\n# \"video\": \"<video URL>\",\nvideo_messages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": [\n            {\"type\": \"text\", \"text\": \"\u8bf7\u7528\u8868\u683c\u603b\u7ed3\u4e00\u4e0b\u89c6\u9891\u4e2d\u7684\u5546\u54c1\u7279\u70b9\"},\n            {\n                \"type\": \"video\", \n                \"video\": \"https://duguang-labelling.oss-cn-shanghai.aliyuncs.com/qiansun/video_ocr/videos/50221078283.mp4\",\n                \"total_pixels\": 20480 * 28 * 28, \"min_pixels\": 16 * 28 * 28\n            }\n        ]\n    },\n]\n\n# Here we use video messages as a demonstration\nmessages = video_messages\n\nprocessor = AutoProcessor.from_pretrained(MODEL_PATH)\nprompt = processor.apply_chat_template(\n    messages,\n    tokenize=False,\n    add_generation_prompt=True,\n)\nimage_inputs, video_inputs, video_kwargs = process_vision_info(messages, return_video_kwargs=True)\n\nmm_data = {}\nif image_inputs is not None:\n    mm_data[\"image\"] = image_inputs\nif video_inputs is not None:\n    mm_data[\"video\"] = video_inputs\n\nllm_inputs = {\n    \"prompt\": prompt,\n    \"multi_modal_data\": mm_data,\n\n    # FPS will be returned in video_kwargs\n    \"mm_processor_kwargs\": video_kwargs,\n}\n\noutputs = llm.generate([llm_inputs], sampling_params=sampling_params)\ngenerated_text = outputs[0].outputs[0].text\n\nprint(generated_text)\n```\n\n\n## \ud83d\udc33 Docker\n\nTo simplify the deploy process, we provide docker images with pre-build environments: [qwenllm/qwenvl](https://hub.docker.com/r/qwenllm/qwenvl). You only need to install the driver and download model files to launch demos.\n\n```bash\ndocker run --gpus all --ipc=host --network=host --rm --name qwen2.5 -it qwenllm/qwenvl:2.5-cu121 bash\n```\n\n## Citation\n\nIf you find our paper and code useful in your research, please consider giving a star :star: and citation :pencil: :)\n\n\n\n\n```BibTeX\n\n@article{Qwen2.5-VL,\n  title={Qwen2.5-VL Technical Report},\n  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and Zhong, Humen and Zhu, Yuanzhi and Yang, Mingkun and Li, Zhaohai and Wan, Jianqiang and Wang, Pengfei and Ding, Wei and Fu, Zheren and Xu, Yiheng and Ye, Jiabo and Zhang, Xi and Xie, Tianbao and Cheng, Zesen and Zhang, Hang and Yang, Zhibo and Xu, Haiyang and Lin, Junyang},\n  journal={arXiv preprint arXiv:2502.13923},\n  year={2025}\n}\n\n@article{Qwen2-VL,\n  title={Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution},\n  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Fan, Yang and Dang, Kai and Du, Mengfei and Ren, Xuancheng and Men, Rui and Liu, Dayiheng and Zhou, Chang and Zhou, Jingren and Lin, Junyang},\n  journal={arXiv preprint arXiv:2409.12191},\n  year={2024}\n}\n\n@article{Qwen-VL,\n  title={Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},\n  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},\n  journal={arXiv preprint arXiv:2308.12966},\n  year={2023}\n}\n```\n\n<br>\n",
  "external_links_in_readme": [
    "https://github.com/QwenLM/Qwen2.5-VL/blob/main/cookbooks/spatial_understanding.ipynb",
    "https://hub.docker.com/r/qwenllm/qwenvl",
    "https://modelscope.cn/models/qwen/Qwen2.5-VL-7B-Instruct",
    "https://huggingface.co/Qwen/QVQ-72B-Preview",
    "https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/computer_use.ipynb",
    "https://arxiv.org/abs/2309.00071",
    "https://huggingface.co/Qwen",
    "https://github.com/QwenLM/Qwen2.5-VL/tree/main/cookbooks",
    "https://github.com/huggingface/transformers@f3f6c86582611976e72be054675e2bf0abb5f775",
    "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png\"}},",
    "https://arxiv.org/abs/2502.13923",
    "https://github.com/Dao-AILab/flash-attention",
    "http://path/to/your/image.jpg\"},",
    "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png\",",
    "https://dashscope.oss-cn-beijing.aliyuncs.com/images/dog_and_girl.jpeg\"",
    "https://modelscope.cn/models/qwen/Qwen2.5-VL-72B-Instruct",
    "https://docs.vllm.ai/en/latest/serving/multimodal_inputs.html",
    "https://github.com/QwenLM/Qwen2.5-VL/blob/main/cookbooks/computer_use.ipynb",
    "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL/qwen2.5vl_logo.png\"",
    "https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8",
    "https://duguang-labelling.oss-cn-shanghai.aliyuncs.com/qiansun/video_ocr/videos/50221078283.mp4\",",
    "https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api\">API</a>&nbsp&nbsp",
    "https://chat.qwenlm.ai/\"><b>Qwen",
    "https://qwenlm.github.io/blog/qwen2.5-vl-32b/",
    "https://huggingface.co/collections/Qwen/qwen2-vl-66cee7455501d7126940800d",
    "https://github.com/QwenLM/Qwen2.5-VL/blob/main/cookbooks/universal_recognition.ipynb",
    "https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct",
    "https://github.com/dmlc/decord?tab=readme-ov-file#install-from-source",
    "https://arxiv.org/pdf/2409.12191",
    "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-VL/space_woaudio.mp4\",",
    "https://github.com/QwenLM/Qwen2.5-VL/blob/main/cookbooks/mobile_agent.ipynb",
    "https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int4",
    "https://github.com/QwenLM/Qwen2.5-VL/tree/main/cookbooks\">Cookbooks</a>&nbsp&nbsp",
    "https://gallery.pai-ml.com/#/preview/deepLearning/cv/qwen2.5-vl\">PAI-DSW</a>",
    "http://localhost:8000/v1\"",
    "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2.5-VL/qwen2.5vl_arc.jpeg\"",
    "https://huggingface.co/spaces/gstaff/gradio-screen-recorder/tree/main",
    "https://github.com/pytorch/torchcodec/tree/main?tab=readme-ov-file#installing-torchcodec",
    "https://huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct",
    "https://platform.openai.com/docs/guides/vision/uploading-base-64-encoded-images",
    "https://modelscope.cn/models/qwen/Qwen2.5-VL-3B-Instruct",
    "https://colab.research.google.com/assets/colab-badge.svg",
    "http://localhost:8000/v1/chat/completions",
    "https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct",
    "https://qwenlm.github.io/blog/qwen2.5-vl/\">Blog</a>&nbsp&nbsp",
    "https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct-AWQ",
    "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\",",
    "https://modelscope.cn/models/qwen/Qwen2.5-VL-32B-Instruct",
    "https://github.com/QwenLM/Qwen2.5-VL/blob/main/cookbooks/document_parsing.ipynb",
    "https://modelscope.oss-cn-beijing.aliyuncs.com/resource/qwen.png\"",
    "https://github.com/QwenLM/Qwen2.5-VL/blob/main/cookbooks/video_understanding.ipynb",
    "https://qwenlm.github.io/blog/qwen2.5-vl/",
    "https://help.aliyun.com/zh/model-studio/developer-reference/qwen-vl-api",
    "https://huggingface.co/collections/Qwen/qwen25-vl-6795ffac22b334a837c0f9a5\">Hugging",
    "https://modelscope.cn/organization/qwen\">ModelScope</a>&nbsp&nbsp",
    "https://qwenlm.github.io/blog/qvq-72b-preview/",
    "https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct",
    "https://github.com/QwenLM/Qwen2.5-VL/blob/main/cookbooks/ocr.ipynb",
    "https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/video_understanding.ipynb",
    "https://github.com/QwenLM/Qwen2.5-VL/tree/main/qwen-vl-finetune",
    "https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/mobile_agent.ipynb",
    "https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct",
    "https://qwenlm.github.io/blog/qwen2-vl/",
    "https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/spatial_understanding.ipynb",
    "https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/ocr.ipynb",
    "http://127.0.0.1:7860/",
    "https://github.com/QwenLM/Qwen/blob/main/assets/wechat.png\">WeChat",
    "https://huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct-AWQ",
    "https://huggingface.co/docs/accelerate/main/en/usage_guides/model_size_estimator",
    "https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/document_parsing.ipynb",
    "https://discord.gg/CV4E9rpNSD\">Discord</a>&nbsp&nbsp",
    "https://arxiv.org/abs/2502.13923\">Paper</a>&nbsp&nbsp",
    "https://colab.research.google.com/github/QwenLM/Qwen2.5-VL/blob/main/cookbooks/universal_recognition.ipynb",
    "https://huggingface.co/spaces/Qwen/Qwen2.5-VL-72B-Instruct\">Demo</a>&nbsp&nbsp",
    "https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct-AWQ",
    "https://huggingface.co/Qwen/Qwen2-VL-72B-Instruct-AWQ"
  ]
}
```

</details>


---

## Repository 3: huggingface/transformers

# GitHub Repository Data

**Repository:** [huggingface/transformers](https://github.com/huggingface/transformers)

## Basic Information

- **Description:** 🤗 Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. 
- **Created:** 2018-10-29T13:56:00+00:00
- **Last Updated:** 2025-06-22T02:35:25+00:00
- **Last Pushed:** 2025-06-21T15:54:35+00:00
- **Default Branch:** main
- **Size:** 322129 KB

## Statistics

- **Stars:** 145,916
- **Forks:** 29,424
- **Watchers:** 145,916
- **Open Issues:** 1,855
- **Total Issues:** 0
- **Pull Requests:** 20,964

## License

- **Type:** Apache License 2.0
- **SPDX ID:** Apache-2.0
- **URL:** [License](https://github.com/huggingface/transformers/blob/main/LICENSE)

## Languages

- **Python:** 65,553,510 bytes
- **Cuda:** 204,021 bytes
- **Dockerfile:** 42,475 bytes
- **C++:** 19,093 bytes
- **C:** 7,703 bytes
- **Makefile:** 4,377 bytes
- **Cython:** 3,635 bytes
- **Shell:** 1,838 bytes
- **Jsonnet:** 937 bytes

## Topics

- `nlp`
- `natural-language-processing`
- `pytorch`
- `pytorch-transformers`
- `transformer`
- `model-hub`
- `pretrained-models`
- `speech-recognition`
- `hacktoberfest`
- `python`
- `machine-learning`
- `deep-learning`
- `audio`
- `deepseek`
- `gemma`
- `glm`
- `llm`
- `qwen`
- `vlm`

## Top Contributors

1. **thomwolf** - 1384 contributions
2. **sgugger** - 1250 contributions
3. **ydshieh** - 1134 contributions
4. **LysandreJik** - 1053 contributions
5. **patrickvonplaten** - 789 contributions
6. **gante** - 525 contributions
7. **stas00** - 514 contributions
8. **julien-c** - 401 contributions
9. **ArthurZucker** - 381 contributions
10. **Rocketknight1** - 373 contributions

## File Structure (Sample of 10 files)

Total files: 5,854

- `.circleci` (tree)
- `.circleci/TROUBLESHOOT.md` (blob)
- `.circleci/config.yml` (blob)
- `.circleci/create_circleci_config.py` (blob)
- `.circleci/parse_test_outputs.py` (blob)
- `.gitattributes` (blob)
- `.github` (tree)
- `.github/ISSUE_TEMPLATE` (tree)
- `.github/ISSUE_TEMPLATE/bug-report.yml` (blob)
- `.github/ISSUE_TEMPLATE/config.yml` (blob)

## Recent Issues

- 🟢 **#38966** Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models (open)
- 🟢 **#38965** Modernbert implementation with Tensorflow (open)
- 🟢 **#38964** [docs] Typos - Single GPU efficient training features (open)
- 🟢 **#38963** RuntimeError: Index put requires the source and destination dtypes match, got Half for the destination and Float for the source. (open)
- 🟢 **#38962** Update test_candidate_generator.py (open)

## Recent Pull Requests

- 🟢 **#38964** [docs] Typos - Single GPU efficient training features (open)
- 🟢 **#38962** Update test_candidate_generator.py (open)
- 🟢 **#38960** feat: add number token loss implementation (open)
- 🟢 **#38959** Updated the model card for wav2vec2-phoneme (open)
- 🟢 **#38958** Updated model card for wav2vec2-conformer (open)

## Recent Commits

- **2166b6b4** Update blip model card (#38513) - DongKyu Kang (2025-06-20T20:46:19+00:00)
- **166e823f** Fix custom generate from local directory (#38916) - Manuel de Prada Corral (2025-06-20T16:36:57+00:00)
- **3d34b921** Switch to use A10 progressively (#38936) - Yih-Dar (2025-06-20T16:10:35+00:00)
- **b8059e1f** Fix more flaky `test_initialization` (#38932) - Yih-Dar (2025-06-20T15:28:32+00:00)
- **5ee60f97** Correctly raise error for awq quantization (#38945) - Cyril Vallez (2025-06-20T15:18:06+00:00)
- **8ac2d753** Pin PyTorch extras for AMD containers (#38941) - Ákos Hadnagy (2025-06-20T12:17:21+00:00)
- **9120567b** Add kwargs for timm.create_model in TimmWrapper (#38860) - Pavel Iakubovskii (2025-06-20T12:00:09+00:00)
- **ff95974b** [static cache] fix device map per layer in VLMs (#38488) - Raushan Turganbay (2025-06-20T11:49:29+00:00)
- **aa42987c** Remove `ALL_LAYERNORM_LAYERS` (#38922) - Cyril Vallez (2025-06-20T10:06:48+00:00)
- **38a9b707** add pytorch-xpu Dockerfile (#38875) - Yao Matrix (2025-06-20T09:42:44+00:00)

## External Links Found in README

- https://huggingface.com/models
- https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md"><img
- https://huggingface.co/Qwen/Qwen2.5-0.5B
- https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_as_a_model_definition.png"/>
- https://huggingface.co/llava-hf/llava-1.5-7b-hf
- https://docs.astral.sh/uv/
- https://flax.readthedocs.io/en/latest/
- https://huggingface.co/UsefulSensors/moonshine
- https://huggingface.co/meta-llama/Llama-3.2-1B
- https://github.com/huggingface/transformers/blob/main/i18n/README_es.md">Español</a>
- https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg">
- https://huggingface.co/magic-leap-community/superglue_outdoor
- https://github.com/huggingface/transformers/blob/main/LICENSE"><img
- https://huggingface.co/shi-labs/oneformer_ade20k_swin_large
- https://huggingface.com/
- https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png"
- https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md">日本語</a>
- https://huggingface.co/MCG-NJU/videomae-large
- https://huggingface.co/facebook/dinov2-base
- https://huggingface.co/BAAI/Emu3-Gen

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 155220641,
  "name": "transformers",
  "full_name": "huggingface/transformers",
  "description": "\ud83e\udd17 Transformers: the model-definition framework for state-of-the-art machine learning models in text, vision, audio, and multimodal models, for both inference and training. ",
  "html_url": "https://github.com/huggingface/transformers",
  "clone_url": "https://github.com/huggingface/transformers.git",
  "ssh_url": "git@github.com:huggingface/transformers.git",
  "homepage": "https://huggingface.co/transformers",
  "topics": [
    "nlp",
    "natural-language-processing",
    "pytorch",
    "pytorch-transformers",
    "transformer",
    "model-hub",
    "pretrained-models",
    "speech-recognition",
    "hacktoberfest",
    "python",
    "machine-learning",
    "deep-learning",
    "audio",
    "deepseek",
    "gemma",
    "glm",
    "llm",
    "qwen",
    "vlm"
  ],
  "default_branch": "main",
  "created_at": "2018-10-29T13:56:00+00:00",
  "updated_at": "2025-06-22T02:35:25+00:00",
  "pushed_at": "2025-06-21T15:54:35+00:00",
  "size_kb": 322129,
  "watchers_count": 145916,
  "stargazers_count": 145916,
  "forks_count": 29424,
  "open_issues_count": 1855,
  "license": {
    "key": "apache-2.0",
    "name": "Apache License 2.0",
    "spdx_id": "Apache-2.0",
    "url": "https://github.com/huggingface/transformers/blob/main/LICENSE"
  },
  "languages": {
    "Python": 65553510,
    "Cuda": 204021,
    "Dockerfile": 42475,
    "C++": 19093,
    "C": 7703,
    "Makefile": 4377,
    "Cython": 3635,
    "Shell": 1838,
    "Jsonnet": 937
  },
  "top_contributors": [
    {
      "login": "thomwolf",
      "contributions": 1384
    },
    {
      "login": "sgugger",
      "contributions": 1250
    },
    {
      "login": "ydshieh",
      "contributions": 1134
    },
    {
      "login": "LysandreJik",
      "contributions": 1053
    },
    {
      "login": "patrickvonplaten",
      "contributions": 789
    },
    {
      "login": "gante",
      "contributions": 525
    },
    {
      "login": "stas00",
      "contributions": 514
    },
    {
      "login": "julien-c",
      "contributions": 401
    },
    {
      "login": "ArthurZucker",
      "contributions": 381
    },
    {
      "login": "Rocketknight1",
      "contributions": 373
    },
    {
      "login": "younesbelkada",
      "contributions": 316
    },
    {
      "login": "sshleifer",
      "contributions": 281
    },
    {
      "login": "NielsRogge",
      "contributions": 276
    },
    {
      "login": "amyeroberts",
      "contributions": 234
    },
    {
      "login": "zucchini-nlp",
      "contributions": 233
    },
    {
      "login": "Narsil",
      "contributions": 230
    },
    {
      "login": "patil-suraj",
      "contributions": 201
    },
    {
      "login": "VictorSanh",
      "contributions": 195
    },
    {
      "login": "mrm8488",
      "contributions": 149
    },
    {
      "login": "sanchit-gandhi",
      "contributions": 149
    }
  ],
  "file_tree_count": 5854,
  "file_tree_sample": [
    {
      "path": ".circleci",
      "type": "tree"
    },
    {
      "path": ".circleci/TROUBLESHOOT.md",
      "type": "blob"
    },
    {
      "path": ".circleci/config.yml",
      "type": "blob"
    },
    {
      "path": ".circleci/create_circleci_config.py",
      "type": "blob"
    },
    {
      "path": ".circleci/parse_test_outputs.py",
      "type": "blob"
    },
    {
      "path": ".gitattributes",
      "type": "blob"
    },
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/bug-report.yml",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/config.yml",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 20964,
  "recent_issues": [
    {
      "number": 38966,
      "title": "Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models",
      "state": "open"
    },
    {
      "number": 38965,
      "title": "Modernbert implementation with Tensorflow",
      "state": "open"
    },
    {
      "number": 38964,
      "title": "[docs] Typos - Single GPU efficient training features",
      "state": "open"
    },
    {
      "number": 38963,
      "title": "RuntimeError: Index put requires the source and destination dtypes match, got Half for the destination and Float for the source.",
      "state": "open"
    },
    {
      "number": 38962,
      "title": "Update test_candidate_generator.py",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 38964,
      "title": "[docs] Typos - Single GPU efficient training features",
      "state": "open"
    },
    {
      "number": 38962,
      "title": "Update test_candidate_generator.py",
      "state": "open"
    },
    {
      "number": 38960,
      "title": "feat: add number token loss implementation",
      "state": "open"
    },
    {
      "number": 38959,
      "title": "Updated the model card for wav2vec2-phoneme",
      "state": "open"
    },
    {
      "number": 38958,
      "title": "Updated model card for wav2vec2-conformer",
      "state": "open"
    }
  ],
  "recent_commits": [
    {
      "sha": "2166b6b4ff09f6dd3867ab982f262f66482aa968",
      "author": "DongKyu Kang",
      "date": "2025-06-20T20:46:19+00:00",
      "message": "Update blip model card (#38513)"
    },
    {
      "sha": "166e823f770477b17988020b2476a796d49836a6",
      "author": "Manuel de Prada Corral",
      "date": "2025-06-20T16:36:57+00:00",
      "message": "Fix custom generate from local directory (#38916)"
    },
    {
      "sha": "3d34b92116c26518f476be8c40250c4d89de3cc3",
      "author": "Yih-Dar",
      "date": "2025-06-20T16:10:35+00:00",
      "message": "Switch to use A10 progressively (#38936)"
    },
    {
      "sha": "b8059e1f8f9ad245d71fbe2d18723d735ffccfec",
      "author": "Yih-Dar",
      "date": "2025-06-20T15:28:32+00:00",
      "message": "Fix more flaky `test_initialization` (#38932)"
    },
    {
      "sha": "5ee60f970a332b12458b113a18cc74999f8c5880",
      "author": "Cyril Vallez",
      "date": "2025-06-20T15:18:06+00:00",
      "message": "Correctly raise error for awq quantization (#38945)"
    },
    {
      "sha": "8ac2d75353f4c3eca90f5aba0ab65e31e3b20b1b",
      "author": "\u00c1kos Hadnagy",
      "date": "2025-06-20T12:17:21+00:00",
      "message": "Pin PyTorch extras for AMD containers (#38941)"
    },
    {
      "sha": "9120567b02a551d198337e21bee8c1465f389ab2",
      "author": "Pavel Iakubovskii",
      "date": "2025-06-20T12:00:09+00:00",
      "message": "Add kwargs for timm.create_model in TimmWrapper (#38860)"
    },
    {
      "sha": "ff95974bc67aa0a843d1045e4d2379352e334e2d",
      "author": "Raushan Turganbay",
      "date": "2025-06-20T11:49:29+00:00",
      "message": "[static cache] fix device map per layer in VLMs (#38488)"
    },
    {
      "sha": "aa42987c1e3a0cf1c18a5783274f0d8cc8409b53",
      "author": "Cyril Vallez",
      "date": "2025-06-20T10:06:48+00:00",
      "message": "Remove `ALL_LAYERNORM_LAYERS` (#38922)"
    },
    {
      "sha": "38a9b707862dc017f111eb02a2bba61e35a74104",
      "author": "Yao Matrix",
      "date": "2025-06-20T09:42:44+00:00",
      "message": "add pytorch-xpu Dockerfile (#38875)"
    },
    {
      "sha": "9bcdd5cde9411477cba66bc9e6d1c59e80b60b60",
      "author": "R\u00e9mi Ouazan",
      "date": "2025-06-20T09:22:32+00:00",
      "message": "Modernbert fixes (#38912)"
    },
    {
      "sha": "31d30b72245aacfdf70249165964b53790d9c4d8",
      "author": "Yih-Dar",
      "date": "2025-06-20T09:05:49+00:00",
      "message": "Skip some tests for now (#38931)"
    },
    {
      "sha": "0725cd6953803b8aacfc85288cbfb83dea30c469",
      "author": "Cyril Vallez",
      "date": "2025-06-19T17:25:20+00:00",
      "message": "Remove deprecated classes in modeling_utils.py (#38919)"
    },
    {
      "sha": "797860c68cfd8bd3ad38ce312540445073f76b30",
      "author": "Hamza Benchekroun",
      "date": "2025-06-19T15:54:08+00:00",
      "message": "feat: add flexible Liger Kernel configuration to TrainingArguments (#38911)"
    },
    {
      "sha": "89b35be618256d2a4a2458322a0653c57e8fa986",
      "author": "Matt",
      "date": "2025-06-19T14:22:59+00:00",
      "message": "Allow make-fixup on main branch, albeit slowly (#38892)"
    },
    {
      "sha": "9a02e7602d01c98946d755c38b51930bf8b43901",
      "author": "Gabe Goodhart",
      "date": "2025-06-19T14:20:42+00:00",
      "message": "feat: Add granite architectures to auto tokenizer name mappings (#38802)"
    },
    {
      "sha": "54a02160eb030da9be18231c77791f2eb3a52216",
      "author": "Matt",
      "date": "2025-06-19T13:53:52+00:00",
      "message": "Fix ReDOS in tokenizer digit substitution (#38844)"
    },
    {
      "sha": "af6120b3eb2470b994c21421bb6eaa76576128b0",
      "author": "ivarflakstad",
      "date": "2025-06-19T13:11:01+00:00",
      "message": "Skip sdpa tests if submodule does not support sdpa (#38907)"
    },
    {
      "sha": "5d26a387359d669d74f14effbdc859f907133647",
      "author": "Yih-Dar",
      "date": "2025-06-19T11:50:33+00:00",
      "message": "Fix `FalconMambaIntegrationTests` (#38566)"
    },
    {
      "sha": "a9ce8c69c946eae3431828871366bb4112d4ec1b",
      "author": "Yao Matrix",
      "date": "2025-06-19T11:48:23+00:00",
      "message": "align xpu's autocast behavior w/ cuda by using device agnostic torch APIs (#38284)"
    }
  ],
  "readme_text": "<!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\n  </picture>\n  <br/>\n  <br/>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://huggingface.com/models\"><img alt=\"Checkpoints on Hub\" src=\"https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen\"></a>\n    <a href=\"https://circleci.com/gh/huggingface/transformers\"><img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\"></a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\"><img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\"></a>\n    <a href=\"https://huggingface.co/docs/transformers/index\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\"></a>\n    <a href=\"https://github.com/huggingface/transformers/releases\"><img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\"></a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\"><img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\"></a>\n    <a href=\"https://zenodo.org/badge/latestdoi/155220641\"><img src=\"https://zenodo.org/badge/155220641.svg\" alt=\"DOI\"></a>\n</p>\n\n<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md\">\u7b80\u4f53\u4e2d\u6587</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md\">\u7e41\u9ad4\u4e2d\u6587</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md\">\ud55c\uad6d\uc5b4</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_es.md\">Espa\u00f1ol</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md\">\u65e5\u672c\u8a9e</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md\">\u0939\u093f\u0928\u094d\u0926\u0940</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md\">\u0420\u0443\u0441\u0441\u043a\u0438\u0439</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md\">\u0420ortugu\u00eas</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_te.md\">\u0c24\u0c46\u0c32\u0c41\u0c17\u0c41</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md\">Fran\u00e7ais</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_de.md\">Deutsch</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md\">Ti\u1ebfng Vi\u1ec7t</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md\">\u0627\u0644\u0639\u0631\u0628\u064a\u0629</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md\">\u0627\u0631\u062f\u0648</a> |\n    </p>\n</h4>\n\n<h3 align=\"center\">\n    <p>State-of-the-art pretrained models for inference and training</p>\n</h3>\n\n<h3 align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_as_a_model_definition.png\"/>\n</h3>\n\n\nTransformers acts as the model-definition framework for state-of-the-art machine learning models in text, computer \nvision, audio, video, and multimodal model, for both inference and training. \n\nIt centralizes the model definition so that this definition is agreed upon across the ecosystem. `transformers` is the \npivot across frameworks: if a model definition is supported, it will be compatible with the majority of training \nframeworks (Axolotl, Unsloth, DeepSpeed, FSDP, PyTorch-Lightning, ...), inference engines (vLLM, SGLang, TGI, ...),\nand adjacent modeling libraries (llama.cpp, mlx, ...) which leverage the model definition from `transformers`.\n\nWe pledge to help support new state-of-the-art models and democratize their usage by having their model definition be\nsimple, customizable, and efficient.\n\nThere are over 1M+ Transformers [model checkpoints](https://huggingface.co/models?library=transformers&sort=trending) on the [Hugging Face Hub](https://huggingface.com/models) you can use.\n\nExplore the [Hub](https://huggingface.com/) today to find a model and use Transformers to help you get started right away.\n\n## Installation\n\nTransformers works with Python 3.9+ [PyTorch](https://pytorch.org/get-started/locally/) 2.1+, [TensorFlow](https://www.tensorflow.org/install/pip) 2.6+, and [Flax](https://flax.readthedocs.io/en/latest/) 0.4.1+.\n\nCreate and activate a virtual environment with [venv](https://docs.python.org/3/library/venv.html) or [uv](https://docs.astral.sh/uv/), a fast Rust-based Python package and project manager.\n\n```py\n# venv\npython -m venv .my-env\nsource .my-env/bin/activate\n# uv\nuv venv .my-env\nsource .my-env/bin/activate\n```\n\nInstall Transformers in your virtual environment.\n\n```py\n# pip\npip install \"transformers[torch]\"\n\n# uv\nuv pip install \"transformers[torch]\"\n```\n\nInstall Transformers from source if you want the latest changes in the library or are interested in contributing. However, the *latest* version may not be stable. Feel free to open an [issue](https://github.com/huggingface/transformers/issues) if you encounter an error.\n\n```shell\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\n\n# pip\npip install .[torch]\n\n# uv\nuv pip install .[torch]\n```\n\n## Quickstart\n\nGet started with Transformers right away with the [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) API. The `Pipeline` is a high-level inference class that supports text, audio, vision, and multimodal tasks. It handles preprocessing the input and returns the appropriate output.\n\nInstantiate a pipeline and specify model to use for text generation. The model is downloaded and cached so you can easily reuse it again. Finally, pass some text to prompt the model.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"text-generation\", model=\"Qwen/Qwen2.5-1.5B\")\npipeline(\"the secret to baking a really good cake is \")\n[{'generated_text': 'the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.'}]\n```\n\nTo chat with a model, the usage pattern is the same. The only difference is you need to construct a chat history (the input to `Pipeline`) between you and the system.\n\n> [!TIP]\n> You can also chat with a model directly from the command line.\n> ```shell\n> transformers chat Qwen/Qwen2.5-0.5B-Instruct\n> ```\n\n```py\nimport torch\nfrom transformers import pipeline\n\nchat = [\n    {\"role\": \"system\", \"content\": \"You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.\"},\n    {\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"}\n]\n\npipeline = pipeline(task=\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", torch_dtype=torch.bfloat16, device_map=\"auto\")\nresponse = pipeline(chat, max_new_tokens=512)\nprint(response[0][\"generated_text\"][-1][\"content\"])\n```\n\nExpand the examples below to see how `Pipeline` works for different modalities and tasks.\n\n<details>\n<summary>Automatic speech recognition</summary>\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")\npipeline(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}\n```\n\n</details>\n\n<details>\n<summary>Image classification</summary>\n\n<h3 align=\"center\">\n    <a><img src=\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"></a>\n</h3>\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"image-classification\", model=\"facebook/dinov2-small-imagenet1k-1-layer\")\npipeline(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n[{'label': 'macaw', 'score': 0.997848391532898},\n {'label': 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n  'score': 0.0016551691805943847},\n {'label': 'lorikeet', 'score': 0.00018523589824326336},\n {'label': 'African grey, African gray, Psittacus erithacus',\n  'score': 7.85409429227002e-05},\n {'label': 'quail', 'score': 5.502637941390276e-05}]\n```\n\n</details>\n\n<details>\n<summary>Visual question answering</summary>\n\n\n<h3 align=\"center\">\n    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\"></a>\n</h3>\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"visual-question-answering\", model=\"Salesforce/blip-vqa-base\")\npipeline(\n    image=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\",\n    question=\"What is in the image?\",\n)\n[{'answer': 'statue of liberty'}]\n```\n\n</details>\n\n## Why should I use Transformers?\n\n1. Easy-to-use state-of-the-art models:\n    - High performance on natural language understanding & generation, computer vision, audio, video, and multimodal tasks.\n    - Low barrier to entry for researchers, engineers, and developers.\n    - Few user-facing abstractions with just three classes to learn.\n    - A unified API for using all our pretrained models.\n\n1. Lower compute costs, smaller carbon footprint:\n    - Share trained models instead of training from scratch.\n    - Reduce compute time and production costs.\n    - Dozens of model architectures with 1M+ pretrained checkpoints across all modalities.\n\n1. Choose the right framework for every part of a models lifetime:\n    - Train state-of-the-art models in 3 lines of code.\n    - Move a single model between PyTorch/JAX/TF2.0 frameworks at will.\n    - Pick the right framework for training, evaluation, and production.\n\n1. Easily customize a model or an example to your needs:\n    - We provide examples for each architecture to reproduce the results published by its original authors.\n    - Model internals are exposed as consistently as possible.\n    - Model files can be used independently of the library for quick experiments.\n\n<a target=\"_blank\" href=\"https://huggingface.co/enterprise\">\n    <img alt=\"Hugging Face Enterprise Hub\" src=\"https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925\">\n</a><br>\n\n## Why shouldn't I use Transformers?\n\n- This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.\n- The training API is optimized to work with PyTorch models provided by Transformers. For generic machine learning loops, you should use another library like [Accelerate](https://huggingface.co/docs/accelerate).\n- The [example scripts]((https://github.com/huggingface/transformers/tree/main/examples)) are only *examples*. They may not necessarily work out-of-the-box on your specific use case and you'll need to adapt the code for it to work.\n\n## 100 projects using Transformers\n\nTransformers is more than a toolkit to use pretrained models, it's a community of projects built around it and the\nHugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyone\nelse to build their dream projects.\n\nIn order to celebrate Transformers 100,000 stars, we wanted to put the spotlight on the\ncommunity with the [awesome-transformers](./awesome-transformers.md) page which lists 100\nincredible projects built with Transformers.\n\nIf you own or use a project that you believe should be part of the list, please open a PR to add it!\n\n## Example models\n\nYou can test most of our models directly on their [Hub model pages](https://huggingface.co/models).\n\nExpand each modality below to see a few example models for various use cases.\n\n<details>\n<summary>Audio</summary>\n\n- Audio classification with [Whisper](https://huggingface.co/openai/whisper-large-v3-turbo)\n- Automatic speech recognition with [Moonshine](https://huggingface.co/UsefulSensors/moonshine)\n- Keyword spotting with [Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)\n- Speech to speech generation with [Moshi](https://huggingface.co/kyutai/moshiko-pytorch-bf16)\n- Text to audio with [MusicGen](https://huggingface.co/facebook/musicgen-large)\n- Text to speech with [Bark](https://huggingface.co/suno/bark)\n\n</details>\n\n<details>\n<summary>Computer vision</summary>\n\n- Automatic mask generation with [SAM](https://huggingface.co/facebook/sam-vit-base)\n- Depth estimation with [DepthPro](https://huggingface.co/apple/DepthPro-hf)\n- Image classification with [DINO v2](https://huggingface.co/facebook/dinov2-base)\n- Keypoint detection with [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor)\n- Keypoint matching with [SuperGlue](https://huggingface.co/magic-leap-community/superglue)\n- Object detection with [RT-DETRv2](https://huggingface.co/PekingU/rtdetr_v2_r50vd)\n- Pose Estimation with [VitPose](https://huggingface.co/usyd-community/vitpose-base-simple)\n- Universal segmentation with [OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_swin_large)\n- Video classification with [VideoMAE](https://huggingface.co/MCG-NJU/videomae-large)\n\n</details>\n\n<details>\n<summary>Multimodal</summary>\n\n- Audio or text to text with [Qwen2-Audio](https://huggingface.co/Qwen/Qwen2-Audio-7B)\n- Document question answering with [LayoutLMv3](https://huggingface.co/microsoft/layoutlmv3-base)\n- Image or text to text with [Qwen-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)\n- Image captioning [BLIP-2](https://huggingface.co/Salesforce/blip2-opt-2.7b)\n- OCR-based document understanding with [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)\n- Table question answering with [TAPAS](https://huggingface.co/google/tapas-base)\n- Unified multimodal understanding and generation with [Emu3](https://huggingface.co/BAAI/Emu3-Gen)\n- Vision to text with [Llava-OneVision](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf)\n- Visual question answering with [Llava](https://huggingface.co/llava-hf/llava-1.5-7b-hf)\n- Visual referring expression segmentation with [Kosmos-2](https://huggingface.co/microsoft/kosmos-2-patch14-224)\n\n</details>\n\n<details>\n<summary>NLP</summary>\n\n- Masked word completion with [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base)\n- Named entity recognition with [Gemma](https://huggingface.co/google/gemma-2-2b)\n- Question answering with [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)\n- Summarization with [BART](https://huggingface.co/facebook/bart-large-cnn)\n- Translation with [T5](https://huggingface.co/google-t5/t5-base)\n- Text generation with [Llama](https://huggingface.co/meta-llama/Llama-3.2-1B)\n- Text classification with [Qwen](https://huggingface.co/Qwen/Qwen2.5-0.5B)\n\n</details>\n\n## Citation\n\nWe now have a [paper](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) you can cite for the \ud83e\udd17 Transformers library:\n```bibtex\n@inproceedings{wolf-etal-2020-transformers,\n    title = \"Transformers: State-of-the-Art Natural Language Processing\",\n    author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R\u00e9mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = oct,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",\n    pages = \"38--45\"\n}\n```\n",
  "external_links_in_readme": [
    "https://huggingface.com/models",
    "https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\"><img",
    "https://huggingface.co/Qwen/Qwen2.5-0.5B",
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_as_a_model_definition.png\"/>",
    "https://huggingface.co/llava-hf/llava-1.5-7b-hf",
    "https://docs.astral.sh/uv/",
    "https://flax.readthedocs.io/en/latest/",
    "https://huggingface.co/UsefulSensors/moonshine",
    "https://huggingface.co/meta-llama/Llama-3.2-1B",
    "https://github.com/huggingface/transformers/blob/main/i18n/README_es.md\">Espa\u00f1ol</a>",
    "https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">",
    "https://huggingface.co/magic-leap-community/superglue_outdoor",
    "https://github.com/huggingface/transformers/blob/main/LICENSE\"><img",
    "https://huggingface.co/shi-labs/oneformer_ade20k_swin_large",
    "https://huggingface.com/",
    "https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"",
    "https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md\">\u65e5\u672c\u8a9e</a>",
    "https://huggingface.co/MCG-NJU/videomae-large",
    "https://huggingface.co/facebook/dinov2-base",
    "https://huggingface.co/BAAI/Emu3-Gen",
    "https://github.com/huggingface/transformers/releases\"><img",
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\",",
    "https://github.com/huggingface/transformers/tree/main/examples",
    "https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf",
    "https://huggingface.co/facebook/sam-vit-base",
    "https://huggingface.co/suno/bark",
    "https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct",
    "https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\"></a>",
    "https://img.shields.io/circleci/build/github/huggingface/transformers/main\"></a>",
    "https://huggingface.co/docs/accelerate",
    "https://huggingface.co/apple/DepthPro-hf",
    "https://zenodo.org/badge/155220641.svg\"",
    "https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\"></a>",
    "https://huggingface.co/enterprise\">",
    "https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\"></a>",
    "https://huggingface.co/answerdotai/ModernBERT-base",
    "https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf",
    "http://www.apache.org/licenses/LICENSE-2.0",
    "https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"></a>",
    "https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen\"></a>",
    "https://zenodo.org/badge/latestdoi/155220641\"><img",
    "https://huggingface.co/usyd-community/vitpose-base-simple",
    "https://img.shields.io/github/release/huggingface/transformers.svg\"></a>",
    "https://huggingface.co/docs/transformers/index\"><img",
    "https://huggingface.co/superb/wav2vec2-base-superb-ks",
    "https://huggingface.co/docs/transformers/pipeline_tutorial",
    "https://huggingface.co/google-t5/t5-base",
    "https://huggingface.co/Salesforce/blip2-opt-2.7b",
    "https://huggingface.co/google/tapas-base",
    "https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\"",
    "https://huggingface.co/openai/whisper-large-v3-turbo",
    "https://huggingface.co/models",
    "https://github.com/huggingface/transformers/blob/main/i18n/README_de.md\">Deutsch</a>",
    "https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\"></a>",
    "https://huggingface.co/mistralai/Mixtral-8x7B-v0.1",
    "https://huggingface.co/microsoft/layoutlmv3-base",
    "https://circleci.com/gh/huggingface/transformers\"><img",
    "https://huggingface.com/models\"><img",
    "https://huggingface.co/models?library=transformers&sort=trending",
    "https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md\">\u0939\u093f\u0928\u094d\u0926\u0940</a>",
    "https://pytorch.org/get-started/locally/",
    "https://huggingface.co/Qwen/Qwen2-Audio-7B",
    "https://github.com/huggingface/transformers.git",
    "https://huggingface.co/google/gemma-2-2b",
    "https://docs.python.org/3/library/venv.html",
    "https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md\">\u0420\u0443\u0441\u0441\u043a\u0438\u0439</a>",
    "https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md\">\u7e41\u9ad4\u4e2d\u6587</a>",
    "https://huggingface.co/facebook/musicgen-large",
    "https://github.com/huggingface/transformers/blob/main/i18n/README_te.md\">\u0c24\u0c46\u0c32\u0c41\u0c17\u0c41</a>",
    "https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md\">\u0627\u0644\u0639\u0631\u0628\u064a\u0629</a>",
    "https://huggingface.co/PekingU/rtdetr_v2_r50vd",
    "https://www.tensorflow.org/install/pip",
    "https://github.com/huggingface/transformers/issues",
    "https://www.aclweb.org/anthology/2020.emnlp-demos.6/",
    "https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md\">\u7b80\u4f53\u4e2d\u6587</a>",
    "https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md\">\u0420ortugu\u00eas</a>",
    "https://www.aclweb.org/anthology/2020.emnlp-demos.6\",",
    "https://huggingface.co/facebook/bart-large-cnn",
    "https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md\">\u0627\u0631\u062f\u0648</a>",
    "https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\"",
    "https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md\">\ud55c\uad6d\uc5b4</a>",
    "https://huggingface.co/magic-leap-community/superglue",
    "https://huggingface.co/kyutai/moshiko-pytorch-bf16",
    "https://huggingface.co/microsoft/kosmos-2-patch14-224",
    "https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md\">Fran\u00e7ais</a>",
    "https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md\">Ti\u1ebfng",
    "https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">",
    "https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925\">"
  ]
}
```

</details>


---

