# GitHub Data for MIT_ast-finetuned-audioset-10-10-0.4593

**Task Category:** Audio Classification

## Repository 1: YuanGongND/ast

# GitHub Repository Data

**Repository:** [YuanGongND/ast](https://github.com/YuanGongND/ast)

## Basic Information

- **Description:** Code for the Interspeech 2021 paper "AST: Audio Spectrogram Transformer".
- **Created:** 2021-06-02T21:09:37+00:00
- **Last Updated:** 2025-06-19T07:33:00+00:00
- **Last Pushed:** 2023-05-21T21:12:01+00:00
- **Default Branch:** master
- **Size:** 2461 KB

## Statistics

- **Stars:** 1,303
- **Forks:** 232
- **Watchers:** 1,303
- **Open Issues:** 59
- **Total Issues:** 0
- **Pull Requests:** 4

## License

- **Type:** BSD 3-Clause "New" or "Revised" License
- **SPDX ID:** BSD-3-Clause
- **URL:** [License](https://github.com/YuanGongND/ast/blob/master/LICENSE)

## Languages

- **Jupyter Notebook:** 718,232 bytes
- **Python:** 87,215 bytes
- **Shell:** 7,500 bytes

## Topics

- `pytorch`
- `audio-classification`
- `deep-learning`
- `audio`
- `representation-learning`
- `keyword-spotting`
- `speech-commands`
- `speech-classification`

## Top Contributors

1. **YuanGongND** - 121 contributions
2. **JeffC0628** - 1 contributions

## File Structure (Sample of 10 files)

Total files: 121

- `.gitignore` (blob)
- `AST_ Audio Spectrogram Transformer _ Papers With Code.pdf` (blob)
- `LICENSE` (blob)
- `README.md` (blob)
- `ast.png` (blob)
- `colab` (tree)
- `colab/AST_Inference_Demo.ipynb` (blob)
- `colab/torchaudio_SpecMasking_1_1.ipynb` (blob)
- `egs` (tree)
- `egs/audioset` (tree)

## Recent Issues

- ðŸŸ¢ **#143** Speechcomand recepie (open)
- ðŸŸ¢ **#141** question about normalization  (open)
- ðŸ”´ **#140** Issue while trying to execute the Speechcommands V2 Recipe (closed)
- ðŸ”´ **#139** please remove this log (closed)
- ðŸŸ¢ **#138** Install requirement issue (open)

## Recent Pull Requests

- ðŸ”´ **#139** please remove this log (closed)
- ðŸ”´ **#69** require torchvision 0.9.1 (closed)
- ðŸ”´ **#20** Create inference.py (closed)
- ðŸ”´ **#8** Fixed errors in getting stat (closed)

## Recent Commits

- **31088be8** release demo - ygong (2023-05-21T21:11:58+00:00)
- **9e3bd994** add as-bal set training log, help reproduce - ygong (2022-12-02T05:54:15+00:00)
- **a3684723** add as-bal set training log, help reproduce - ygong (2022-12-02T05:53:29+00:00)
- **5192f0ff** add as-bal set training log, help reproduce - ygong (2022-12-02T05:46:39+00:00)
- **4d81c583** add as-bal set training log, help reproduce - ygong (2022-12-02T05:45:38+00:00)
- **5f50e009** decoupe datasets and hyperparameters - ygong (2022-11-30T19:28:37+00:00)
- **97e57e78** decoupe datasets and hyperparameters - ygong (2022-11-30T19:14:51+00:00)
- **412033a3** pin the torchvision version to avoid a bug - ygong (2022-11-25T00:59:37+00:00)
- **1e7cb6dc** update the colab script - ygong (2022-10-18T20:58:50+00:00)
- **837ba75c** update the colab script - ygong (2022-10-18T20:49:44+00:00)

## External Links Found in README

- https://share.weiyun.com/xRGK6zmg
- https://arxiv.org/abs/2203.06760
- https://www.dropbox.com/s/ca0b1v2nlxzyeb4/audioset_10_10_0.4593.pth?dl=1
- https://www.dropbox.com/s/kt6i0v9fvfm1mbq/audioset_10_10_0.4475.pth?dl=1
- https://www.dropbox.com/s/mdsa4t1xmcimia6/audioset_16_16_0.4422.pth?dl=1
- https://arxiv.org/abs/2110.09784
- https://mitprod-my.sharepoint.com/:f:/g/personal/yuangong_mit_edu/ErLKkiP-GwVMgdsCeGEjsmoBMtGvXMkX3tCj5_I0E7ikNA?e=JE9Om8
- https://mitprod-my.sharepoint.com/:u:/g/personal/yuangong_mit_edu/EWrY3raql55CqxZNV3cVSkABaoU7pXQxAeJXudE1PTNzQg?e=gwEICj
- https://github.com/YuanGongND/ast/issues/58
- https://arxiv.org/abs/2102.01243
- https://www.dropbox.com/s/q0tbqpwv44pquwy/speechcommands_10_10_0.9812.pth?dl=1
- https://github.com/YuanGongND/psla
- https://github.com/YuanGongND/ast/blob/102f0477099f83e04f6f2b30a498464b78bbaf46/src/dataloader.py#L191
- https://www.dropbox.com/s/z18s6pemtnxm4k7/audioset_14_14_0.4431.pth?dl=1
- https://github.com/YuanGongND/ssast
- https://www.dropbox.com/s/6u5sikl4b9wo4u5/audioset_10_10_0.4483.pth?dl=1
- https://github.com/YuanGongND/ltu
- https://arxiv.org/abs/2104.01778
- https://share.weiyun.com/kcmk2KHw
- https://colab.research.google.com/assets/colab-badge.svg

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 373303693,
  "name": "ast",
  "full_name": "YuanGongND/ast",
  "description": "Code for the Interspeech 2021 paper \"AST: Audio Spectrogram Transformer\".",
  "html_url": "https://github.com/YuanGongND/ast",
  "clone_url": "https://github.com/YuanGongND/ast.git",
  "ssh_url": "git@github.com:YuanGongND/ast.git",
  "homepage": "",
  "topics": [
    "pytorch",
    "audio-classification",
    "deep-learning",
    "audio",
    "representation-learning",
    "keyword-spotting",
    "speech-commands",
    "speech-classification"
  ],
  "default_branch": "master",
  "created_at": "2021-06-02T21:09:37+00:00",
  "updated_at": "2025-06-19T07:33:00+00:00",
  "pushed_at": "2023-05-21T21:12:01+00:00",
  "size_kb": 2461,
  "watchers_count": 1303,
  "stargazers_count": 1303,
  "forks_count": 232,
  "open_issues_count": 59,
  "license": {
    "key": "bsd-3-clause",
    "name": "BSD 3-Clause \"New\" or \"Revised\" License",
    "spdx_id": "BSD-3-Clause",
    "url": "https://github.com/YuanGongND/ast/blob/master/LICENSE"
  },
  "languages": {
    "Jupyter Notebook": 718232,
    "Python": 87215,
    "Shell": 7500
  },
  "top_contributors": [
    {
      "login": "YuanGongND",
      "contributions": 121
    },
    {
      "login": "JeffC0628",
      "contributions": 1
    }
  ],
  "file_tree_count": 121,
  "file_tree_sample": [
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "AST_ Audio Spectrogram Transformer _ Papers With Code.pdf",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "ast.png",
      "type": "blob"
    },
    {
      "path": "colab",
      "type": "tree"
    },
    {
      "path": "colab/AST_Inference_Demo.ipynb",
      "type": "blob"
    },
    {
      "path": "colab/torchaudio_SpecMasking_1_1.ipynb",
      "type": "blob"
    },
    {
      "path": "egs",
      "type": "tree"
    },
    {
      "path": "egs/audioset",
      "type": "tree"
    }
  ],
  "issues_count": 0,
  "pulls_count": 4,
  "recent_issues": [
    {
      "number": 143,
      "title": "Speechcomand recepie",
      "state": "open"
    },
    {
      "number": 141,
      "title": "question about normalization ",
      "state": "open"
    },
    {
      "number": 140,
      "title": "Issue while trying to execute the Speechcommands V2 Recipe",
      "state": "closed"
    },
    {
      "number": 139,
      "title": "please remove this log",
      "state": "closed"
    },
    {
      "number": 138,
      "title": "Install requirement issue",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 139,
      "title": "please remove this log",
      "state": "closed"
    },
    {
      "number": 69,
      "title": "require torchvision 0.9.1",
      "state": "closed"
    },
    {
      "number": 20,
      "title": "Create inference.py",
      "state": "closed"
    },
    {
      "number": 8,
      "title": "Fixed errors in getting stat",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "31088be8a3f6ef96416145c4b8d43c81f99eba7a",
      "author": "ygong",
      "date": "2023-05-21T21:11:58+00:00",
      "message": "release demo"
    },
    {
      "sha": "9e3bd9942210680b833b08c39d09f2284ddc4d1d",
      "author": "ygong",
      "date": "2022-12-02T05:54:15+00:00",
      "message": "add as-bal set training log, help reproduce"
    },
    {
      "sha": "a36847233d5774d8d077eb8fc992d96eaa717789",
      "author": "ygong",
      "date": "2022-12-02T05:53:29+00:00",
      "message": "add as-bal set training log, help reproduce"
    },
    {
      "sha": "5192f0ff779e5ac6aaaa30ca9373140098bced45",
      "author": "ygong",
      "date": "2022-12-02T05:46:39+00:00",
      "message": "add as-bal set training log, help reproduce"
    },
    {
      "sha": "4d81c58356f6c987d2891cb84f4ca9675ed0b6be",
      "author": "ygong",
      "date": "2022-12-02T05:45:38+00:00",
      "message": "add as-bal set training log, help reproduce"
    },
    {
      "sha": "5f50e009591748169172342303055bf88c282b8d",
      "author": "ygong",
      "date": "2022-11-30T19:28:37+00:00",
      "message": "decoupe datasets and hyperparameters"
    },
    {
      "sha": "97e57e7852809c6bc825c87a59f07635138cc43d",
      "author": "ygong",
      "date": "2022-11-30T19:14:51+00:00",
      "message": "decoupe datasets and hyperparameters"
    },
    {
      "sha": "412033a3af378dbf4523a1e90ac28823e5e266f2",
      "author": "ygong",
      "date": "2022-11-25T00:59:37+00:00",
      "message": "pin the torchvision version to avoid a bug"
    },
    {
      "sha": "1e7cb6dccf06b40199c342a6e16bf51f815ed8fe",
      "author": "ygong",
      "date": "2022-10-18T20:58:50+00:00",
      "message": "update the colab script"
    },
    {
      "sha": "837ba75cbfb7bdff35235c04e57167749b4f6bf3",
      "author": "ygong",
      "date": "2022-10-18T20:49:44+00:00",
      "message": "update the colab script"
    },
    {
      "sha": "be61de8723189791b4feca43875bfa51f12a2927",
      "author": "ygong",
      "date": "2022-10-18T20:43:33+00:00",
      "message": "update the colab script"
    },
    {
      "sha": "2dea1418b2fe5abfa6251cd74c6550793e07774d",
      "author": "Yuan Gong",
      "date": "2022-10-18T20:40:49+00:00",
      "message": "Created using Colaboratory"
    },
    {
      "sha": "2f5269821c22f053b7f022fb79f78e83b5057b7f",
      "author": "Yuan Gong",
      "date": "2022-10-18T20:37:11+00:00",
      "message": "Update the inference demo w/ attention visualization"
    },
    {
      "sha": "3f53567ed2aa9bf9305eb89f86dbbef17343695a",
      "author": "ygong",
      "date": "2022-09-06T05:41:49+00:00",
      "message": "add colab inference script"
    },
    {
      "sha": "1997d380890177b8fda87c7db8304fcd8aaba9ec",
      "author": "ygong",
      "date": "2022-09-04T21:49:45+00:00",
      "message": "add colab inference script"
    },
    {
      "sha": "388a5e06e75d0de4379e48f7e6b90a7599e1da20",
      "author": "ygong",
      "date": "2022-09-04T21:41:29+00:00",
      "message": "add colab inference script"
    },
    {
      "sha": "0901385c21873fee869cea65a19a7c79912d71b4",
      "author": "ygong",
      "date": "2022-09-03T06:27:33+00:00",
      "message": "add colab inference script"
    },
    {
      "sha": "0d8c96317fc1a6c3839db221249220ed20d0cafe",
      "author": "ygong",
      "date": "2022-08-31T19:31:46+00:00",
      "message": "add colab inference script"
    },
    {
      "sha": "4d18fdd159d3dda16a3e5e016ce2e4b408467c69",
      "author": "ygong",
      "date": "2022-08-31T06:44:45+00:00",
      "message": "add colab inference script"
    },
    {
      "sha": "9a1cb3e8147692091ef82c6f4b0f998e99fbbd38",
      "author": "ygong",
      "date": "2022-08-31T06:37:18+00:00",
      "message": "add colab inference script"
    }
  ],
  "readme_text": "\n# AST: Audio Spectrogram Transformer  \n - [News](#News)\n - [Introduction](#Introduction)\n - [Citing](#Citing)  \n - [Getting Started](#Getting-Started)\n - [ESC-50 Recipe](#ESC-50-Recipe)  \n - [Speechcommands Recipe](#Speechcommands-V2-Recipe)  \n - [AudioSet Recipe](#Audioset-Recipe)\n - [Pretrained Models](#Pretrained-Models)\n - [Use Pretrained Model For Downstream Tasks](#Use-Pretrained-Model-For-Downstream-Tasks)\n - [Contact](#Contact)\n\n## News\nMay, 2023: We have released demo for our audio large language model LTU (listen, think, and understand) that can do zero-shot audio classification and advanced reasoning. Try the online interactive demo **[[here]](https://github.com/YuanGongND/ltu)**.\n\nNovember, 2022: We decoupe `dataset` and hyper-parameters by moving hyper-parameters from `src/run.py` and `src/traintest.py` to `egs/{audioset,esc50,speechcommands}/run.sh`, so that it is easier to adapt the scripts to new datasets. This might cause a bug, please report if you have any issue running any recipe.\n\nOctober, 2022: We add an one-click, self-contained Google Colab script for (pretrained) AST inference with attention visualization. Please test the model with your own audio at [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YuanGongND/ast/blob/master/colab/AST_Inference_Demo.ipynb) by one click (no GPU needed). \n\nMay, 2022: It was found that newer `torchaudio` package has different behavior with older ones in SpecAugment and will cause a [bug](https://github.com/YuanGongND/ast/issues/58). We find a workaround and fixed it. If you are interested, see [here](https://colab.research.google.com/github/YuanGongND/ast/blob/master/colab/torchaudio_SpecMasking_1_1.ipynb).\n\nMarch, 2022: We released a new preprint [*CMKD: CNN/Transformer-Based Cross-Model Knowledge Distillation for Audio Classification*](https://arxiv.org/abs/2203.06760), where we proposed a knowledge distillation based method to further improve the AST model performance without changing its architecture.\n\nFeb, 2022: The [Self-Supervised AST (SSAST)](https://arxiv.org/abs/2110.09784) code is released [[**here**]](https://github.com/YuanGongND/ssast). SSAST use self-supervised pretraining instead of supervised ImageNet pretraining, so it supports arbitrary patch shape and size (e.g., a temperal frame and a square patch) with a good performance.\n\nNov, 2021: The [PSLA training pipeline](https://arxiv.org/abs/2102.01243) used to train AST and baseline efficientnet model code is released [[**here**]](https://github.com/YuanGongND/psla). It is a strong audio classification training pipeline that can be used for most deep learning models. Also, it has a one-click FSD50K recipe that achieves SOTA 0.567 mAP.\n\n## Introduction  \n\n<p align=\"center\"><img src=\"https://github.com/YuanGongND/ast/blob/master/ast.png?raw=true\" alt=\"Illustration of AST.\" width=\"300\"/></p>\n\nThis repository contains the official implementation (in PyTorch) of the **Audio Spectrogram Transformer (AST)** proposed in the Interspeech 2021 paper [AST: Audio Spectrogram Transformer](https://arxiv.org/abs/2104.01778) (Yuan Gong, Yu-An Chung, James Glass).  \n\nAST is the first **convolution-free, purely** attention-based model for audio classification which supports variable length input and can be applied to various tasks. We evaluate AST on various audio classification benchmarks, where it achieves new state-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50, and 98.1% accuracy on Speech Commands V2.  For details, please refer to the paper and the [ISCA SIGML talk](https://www.youtube.com/watch?v=CSRDbqGY0Vw).  \n  \nPlease have a try! AST can be used with a few lines of code, and we also provide recipes to reproduce the SOTA results on AudioSet, ESC-50, and Speechcommands with almost one click.  \n\nThe AST model file is in `src/models/ast_models.py`, the recipes are in `egs/[audioset,esc50,speechcommands]/run.sh`, when you run `run.sh`, it will call `/src/run.py`, which will then call `/src/dataloader.py` and `/src/traintest.py`, which will then call `/src/models/ast_models.py`.\n\nWe have an one-click, self-contained Google Colab script for (pretrained) AST inference and attention visualization. Please test the model with your own audio at [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YuanGongND/ast/blob/master/colab/AST_Inference_Demo.ipynb) by one click (no GPU needed).\n\n## Citing  \nPlease cite our paper(s) if you find this repository useful. The first paper proposes the Audio Spectrogram Transformer while the second paper describes the training pipeline that we applied on AST to achieve the new state-of-the-art on AudioSet.   \n```  \n@inproceedings{gong21b_interspeech,\n  author={Yuan Gong and Yu-An Chung and James Glass},\n  title={{AST: Audio Spectrogram Transformer}},\n  year=2021,\n  booktitle={Proc. Interspeech 2021},\n  pages={571--575},\n  doi={10.21437/Interspeech.2021-698}\n}\n```  \n```  \n@ARTICLE{gong_psla, \n    author={Gong, Yuan and Chung, Yu-An and Glass, James},  \n    journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   \n    title={PSLA: Improving Audio Tagging with Pretraining, Sampling, Labeling, and Aggregation},   \n    year={2021}, \n    doi={10.1109/TASLP.2021.3120633}\n}\n```  \n  \n## Getting Started  \n\nStep 1. Clone or download this repository and set it as the working directory, create a virtual environment and install the dependencies.\n\n```\ncd ast/ \npython3 -m venv venvast\nsource venvast/bin/activate\npip install -r requirements.txt \n```\n  \nStep 2. Test the AST model.\n\n```python\nASTModel(label_dim=527, \\\n         fstride=10, tstride=10, \\\n         input_fdim=128, input_tdim=1024, \\\n         imagenet_pretrain=True, audioset_pretrain=False, \\\n         model_size='base384')\n```  \n\n**Parameters:**\\\n`label_dim` : The number of classes (default:`527`).\\\n`fstride`:  The stride of patch spliting on the frequency dimension, for 16\\*16 patchs, fstride=16 means no overlap, fstride=10 means overlap of 6 (used in the paper). (default:`10`)\\\n`tstride`:  The stride of patch spliting on the time dimension, for 16*16 patchs, tstride=16 means no overlap, tstride=10 means overlap of 6 (used in the paper). (default:`10`)\\\n`input_fdim`: The number of frequency bins of the input spectrogram. (default:`128`)\\\n`input_tdim`: The number of time frames of the input spectrogram. (default:`1024`, i.e., 10.24s)\\\n`imagenet_pretrain`: If `True`, use ImageNet pretrained model. (default: `True`, we recommend to set it as `True` for all tasks.)\\\n`audioset_pretrain`: If`True`,  use full AudioSet And ImageNet pretrained model. Currently only support `base384` model with `fstride=tstride=10`. (default: `False`, we recommend to set it as `True` for all tasks except AudioSet.)\\\n`model_size`: The model size of AST, should be in `[tiny224, small224, base224, base384]` (default: `base384`).\n\n**Input:** Tensor in shape `[batch_size, temporal_frame_num, frequency_bin_num]`. Note: the input spectrogram should be normalized with dataset mean and std, see [here](https://github.com/YuanGongND/ast/blob/102f0477099f83e04f6f2b30a498464b78bbaf46/src/dataloader.py#L191). \\\n**Output:** Tensor of raw logits (i.e., without Sigmoid) in shape `[batch_size, label_dim]`.\n\n``` \ncd ast/src\npython\n```  \n\n```python\nimport os \nimport torch\nfrom models import ASTModel \n# download pretrained model in this directory\nos.environ['TORCH_HOME'] = '../pretrained_models'  \n# assume each input spectrogram has 100 time frames\ninput_tdim = 100\n# assume the task has 527 classes\nlabel_dim = 527\n# create a pseudo input: a batch of 10 spectrogram, each with 100 time frames and 128 frequency bins \ntest_input = torch.rand([10, input_tdim, 128]) \n# create an AST model\nast_mdl = ASTModel(label_dim=label_dim, input_tdim=input_tdim, imagenet_pretrain=True)\ntest_output = ast_mdl(test_input) \n# output should be in shape [10, 527], i.e., 10 samples, each with prediction of 527 classes. \nprint(test_output.shape)  \n```  \n\nWe have an one-click, self-contained Google Colab script for (pretrained) AST inference and attention visualization. Please test the model with your own audio at [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/YuanGongND/ast/blob/master/colab/AST_Inference_Demo.ipynb) by one click (no GPU needed).\n\n## ESC-50 Recipe  \nThe ESC-50 recipe is in `ast/egs/esc50/run_esc.sh`, the script will automatically download the ESC-50 dataset and resample it to 16kHz, then run standard 5-cross validation and report the result.\nThe recipe was tested on 4 GTX TITAN GPUs with 12GB memory. \nThe result is saved in `ast/egs/esc50/exp/yourexpname/acc_fold.csv` (the accuracy of fold 1-5 and the averaged accuracy), you can also check details in `result.csv` and `best_result.csv` (accuracy, AUC, loss, etc of each epoch / best epoch).\nWe attached our log file in `ast/egs/esc50/test-esc50-f10-t10-p-b48-lr1e-5`, the model achieves `95.75%` accuracy.\n\nTo run the recipe, simply comment out `. /data/sls/scratch/share-201907/slstoolchainrc` in `ast/egs/esc50/run_esc.sh`, adjust the path if needed, and run:\n``` \ncd ast/egs/esc50\n(slurm user) sbatch run_esc50.sh\n(local user) ./run_esc50.sh\n```  \n\n## Speechcommands V2 Recipe  \nThe Speechcommands recipe is in `ast/egs/speechcommands/run_sc.sh`, the script will automatically download the Speechcommands V2 dataset, train an AST model on the training set, validate it on the validation set, and evaluate it on the test set.\nThe recipe was tested on 4 GTX TITAN GPUs with 12GB memory. \nThe result is saved in `ast/egs/speechcommands/exp/yourexpname/eval_result.csv` in format `[val_acc, val_AUC, eval_acc, eval_AUC]`, you can also check details in `result.csv` (accuracy, AUC, loss, etc of each epoch).\nWe attached our log file in `ast/egs/speechcommends/test-speechcommands-f10-t10-p-b128-lr2.5e-4-0.5-false`, the model achieves `98.12%` accuracy.\n\nTo run the recipe, simply comment out `. /data/sls/scratch/share-201907/slstoolchainrc` in `ast/egs/esc50/run_sc.sh`, adjust the path if needed, and run:\n``` \ncd ast/egs/speechcommands\n(slurm user) sbatch run_sc.sh\n(local user) ./run_sc.sh\n```  \n\n## Audioset Recipe  \nAudioset is a little bit more complex, you will need to prepare your data json files (i.e., `train_data.json` and `eval_data.json`) by your self.\nThe reason is that the raw wavefiles of Audioset is not released and you need to download them by yourself. We have put a sample json file in `ast/egs/audioset/data/datafiles`, please generate files in the same format (You can also refer to `ast/egs/esc50/prep_esc50.py` and `ast/egs/speechcommands/prep_sc.py`.). Please keep the label code consistent with `ast/egs/audioset/data/class_labels_indices.csv`.\n\nOnce you have the json files, you will need to generate the sampling weight file of your training data (please check our [PSLA paper](https://arxiv.org/abs/2102.01243) to see why it is needed).\n```\ncd ast/egs/audioset\npython gen_weight_file.py ./data/datafiles/train_data.json\n```\n\nThen you just need to change the `tr_data` and `te_data` in `/ast/egs/audioset/run.sh` and then \n``` \ncd ast/egs/audioset\n(slurm user) sbatch run.sh\n(local user) ./run.sh\n```  \nYou should get a model achieves `0.448 mAP` (without weight averaging) and `0.459` (with weight averaging). This is the best **single** model reported in the paper. \nThe result of each epoch is saved in `ast/egs/audioset/exp/yourexpname/result.csv` in format `[mAP, mAUC, precision, recall, d_prime, train_loss, valid_loss, cum_mAP, cum_mAUC, lr]`\n, where `cum_` results are the checkpoint ensemble results (i.e., averaging the prediction of checkpoint models of each epoch, please check our [PSLA paper](https://arxiv.org/abs/2102.01243) for details). The result of weighted averaged model is saved in `wa_result.csv` in format `[mAP, AUC, precision, recall, d-prime]`. We attached our log file in `ast/egs/audioset/test-full-f10-t10-pTrue-b12-lr1e-5/`, the model achieves `0.459` mAP.\n\nIn order to reproduce ensembe results of `0.475 mAP` and `0.485 mAP`, please train 3 models use the same setting (i.e., repeat above three times) and train 6 models with different `tstride` and `fstride`, and average the output of the models. Please refer to `ast/egs/audioset/ensemble.py`. We attached our ensemble log in `/ast/egs/audioset/exp/ensemble-s.log` and `ensemble-m.log`. You can use our pretrained models (see below) to test ensemble result.\n\nWe use `16kHz` for our experiments. Note that you might get a slightly different result with us due to the YouTube videos are being removed with the time and your downloaded version might be different with us. We provide our evaluation audio ids in `ast/egs/audioset/data/sanity_check/our_as_eval_id.csv`. And please note that in order to compre with the PSLA paper, for the **balanced training set** experiments (with results of `0.347 mAP` and `0.378 mAP`), we use the enhanced label set (i.e., a label set that is modified by an algorithm, please see the PSLA paper for detail). So if you train with the original label set for the balanced training set, you will get a slightly worse result. However, we do not use enhanced label set for **full AudioSet experiments**, i.e., for the `0.459 mAP` (single) and `0.485 mAP` (ensemble) results, we use exactly same data and label with the official release, so you should be able to reproduce that. \n\n## Pretrained Models\nWe provide full AudioSet pretrained models and Speechcommands-V2-35 pretrained model.\n1. [Full AudioSet, 10 tstride, 10 fstride, with Weight Averaging (0.459 mAP)](https://www.dropbox.com/s/ca0b1v2nlxzyeb4/audioset_10_10_0.4593.pth?dl=1)\n2. [Full AudioSet, 10 tstride, 10 fstride, without Weight Averaging, Model 1 (0.450 mAP)](https://www.dropbox.com/s/1tv0hovue1bxupk/audioset_10_10_0.4495.pth?dl=1)\n3. [Full AudioSet, 10 tstride, 10 fstride, without Weight Averaging, Model 2  (0.448 mAP)](https://www.dropbox.com/s/6u5sikl4b9wo4u5/audioset_10_10_0.4483.pth?dl=1)\n4. [Full AudioSet, 10 tstride, 10 fstride, without Weight Averaging, Model 3  (0.448 mAP)](https://www.dropbox.com/s/kt6i0v9fvfm1mbq/audioset_10_10_0.4475.pth?dl=1)\n5. [Full AudioSet, 12 tstride, 12 fstride, without Weight Averaging, Model (0.447 mAP)](https://www.dropbox.com/s/snfhx3tizr4nuc8/audioset_12_12_0.4467.pth?dl=1)\n6. [Full AudioSet, 14 tstride, 14 fstride, without Weight Averaging, Model (0.443 mAP)](https://www.dropbox.com/s/z18s6pemtnxm4k7/audioset_14_14_0.4431.pth?dl=1)\n7. [Full AudioSet, 16 tstride, 16 fstride, without Weight Averaging, Model (0.442 mAP)](https://www.dropbox.com/s/mdsa4t1xmcimia6/audioset_16_16_0.4422.pth?dl=1)\n\n8. [Speechcommands V2-35, 10 tstride, 10 fstride, without Weight Averaging, Model (98.12% accuracy on evaluation set)](https://www.dropbox.com/s/q0tbqpwv44pquwy/speechcommands_10_10_0.9812.pth?dl=1)\n\nIf you want to finetune AudioSet-pretrained AST model on your task, you can simply set the `audioset_pretrain=True` when you create the AST model, it will automatically download model 1 (`0.459 mAP`). In our ESC-50 recipe, AudioSet pretraining is used.\n\nIf you want to reproduce ensemble experiments, you can download these models at one click using `ast/egs/audioset/download_models.sh`. Ensemble model 2-4 achieves `0.475 mAP`, Ensemble model 2-7 achieves and `0.485 mAP`. Once you download the model, you can try `ast/egs/audioset/ensemble.py`, you need to change the `eval_data_path` and `mdl_list` to run it. We attached our ensemble log in `/ast/egs/audioset/exp/ensemble-s.log` and `ensemble-m.log`.\n\nPlease  note that we use `16kHz` audios for training and test (for all AudioSet, SpeechCommands, and ESC-50), so if you want to use the pretrained model, please prepare your data in `16kHz`.\n\n(Note: the above links are Dropbox direct links (i.e., can be downloaded by wget) and should work for most users. For users having issue downloading with the above Dropbox links, it is recommended to use a VPN or use the [OneDrive links](https://mitprod-my.sharepoint.com/:f:/g/personal/yuangong_mit_edu/ErLKkiP-GwVMgdsCeGEjsmoBMtGvXMkX3tCj5_I0E7ikNA?e=JE9Om8) or [\u817e\u8baf\u5fae\u4e91\u94fe\u63a5\u4eec](https://share.weiyun.com/xRGK6zmg), however, OneDrive and \u817e\u8baf\u5fae\u4e91 links are not direct link, please manually download the `audioset_10_10_0.4593.pth`[[OneDrive]](https://mitprod-my.sharepoint.com/:u:/g/personal/yuangong_mit_edu/EWrY3raql55CqxZNV3cVSkABaoU7pXQxAeJXudE1PTNzQg?e=gwEICj) [[\u817e\u8baf\u5fae\u4e91]](https://share.weiyun.com/kcmk2KHw) and place it in `ast/pretrained_models` if you want to set `audioset_pretrain=True` because the wget link in the `ast/src/models/ast_models.py` would fail if you cannot connect to Dropbox.) \n\n## Use Pretrained Model For Downstream Tasks\n\nYou can use the pretrained AST model for your own dataset. There are two ways to doing so.\n\nYou can of course only take ``ast/src/models/ast_models.py``, set ``audioset_pretrain=True``, and use it with your training pipeline, the only thing need to take care of is the input normalization, we normalize our input to 0 mean and 0.5 std. To use the pretrained model, you should roughly normalize the input to this range. You can check ``ast/src/get_norm_stats.py`` to see how we compute the stats, or you can try using our AudioSet normalization ``input_spec = (input_spec + 4.26) / (4.57 * 2)``. Using your own training pipeline might be easier if you already have a good one.\nPlease note that AST needs smaller learning rate (we use 10 times smaller learning rate than our CNN model proposed in the [PSLA paper](https://arxiv.org/abs/2102.01243)) and converges faster, so please search the learning rate and learning rate scheduler for your task. \n\nIf you want to use our training pipeline, you would need to modify below for your new dataset.\n1. You need to create a json file, and a label index for your dataset, see ``ast/egs/audioset/data/`` for an example.\n2. In ``/your_dataset/run.sh``, you need to specify the data json file path. You need to set `dataset_mean` and `dataset_std`, if don't know, you can use our AudioSet stats (mean=-4.27, std=4.57); You need to set `audio_length`, which should be the number of frames (e.g., with a 10ms hop, 10-second audio=1000 frames); You need to set the `metrics` in [`acc`,`mAP`] and `loss` in [`CE`,`BCE`]; You need to set the inital learning rate `lr` and learning rate scheduler `lrscheduler_{start,step,decay}`;\nYou also need to set the SpecAug parameters (``freqm`` and ``timem``, we recommend to mask 48 frequency bins out of 128, and 20% of your time frames), the mixup rate (i.e., how many samples are mixup samples), batch size, etc. While it seems a lot, it is easy if you start with one of our recipe: ``ast/egs/[audioset,esc50,speechcommands]/run.sh]``.\n\n[comment]: <> (3. In ``ast/src/run.py``, line 60-65, you need to add the normalization stats, the input frame length, and if noise augmentation is needed for your dataset. Also take a look at line 101-127 if you have a seperate validation set. For normalization stats, you need to compute the mean and std of your dataset &#40;check ``ast/src/get_norm_stats.py``&#41; or you can try using our AudioSet normalization ``input_spec = &#40;input_spec + 4.26&#41; / &#40;4.57 * 2&#41;``.)\n\n[comment]: <> (4. In ``ast/src/traintest.`` line 55-82, you need to specify the learning rate scheduler, metrics, warmup setting and the optimizer for your task.)\n\nTo summarize, to use our training pipeline, you need to creat data files and modify the shell script. You can refer to our ESC-50 and Speechcommands recipes.\n\nAlso, please note that we use `16kHz` audios for the pretrained model, so if you want to use the pretrained model, please prepare your data in `16kHz`.\n\n\n ## Contact\nIf you have a question, please bring up an issue (preferred) or send me an email yuangong@mit.edu.\n\n",
  "external_links_in_readme": [
    "https://share.weiyun.com/xRGK6zmg",
    "https://arxiv.org/abs/2203.06760",
    "https://www.dropbox.com/s/ca0b1v2nlxzyeb4/audioset_10_10_0.4593.pth?dl=1",
    "https://www.dropbox.com/s/kt6i0v9fvfm1mbq/audioset_10_10_0.4475.pth?dl=1",
    "https://www.dropbox.com/s/mdsa4t1xmcimia6/audioset_16_16_0.4422.pth?dl=1",
    "https://arxiv.org/abs/2110.09784",
    "https://mitprod-my.sharepoint.com/:f:/g/personal/yuangong_mit_edu/ErLKkiP-GwVMgdsCeGEjsmoBMtGvXMkX3tCj5_I0E7ikNA?e=JE9Om8",
    "https://mitprod-my.sharepoint.com/:u:/g/personal/yuangong_mit_edu/EWrY3raql55CqxZNV3cVSkABaoU7pXQxAeJXudE1PTNzQg?e=gwEICj",
    "https://github.com/YuanGongND/ast/issues/58",
    "https://arxiv.org/abs/2102.01243",
    "https://www.dropbox.com/s/q0tbqpwv44pquwy/speechcommands_10_10_0.9812.pth?dl=1",
    "https://github.com/YuanGongND/psla",
    "https://github.com/YuanGongND/ast/blob/102f0477099f83e04f6f2b30a498464b78bbaf46/src/dataloader.py#L191",
    "https://www.dropbox.com/s/z18s6pemtnxm4k7/audioset_14_14_0.4431.pth?dl=1",
    "https://github.com/YuanGongND/ssast",
    "https://www.dropbox.com/s/6u5sikl4b9wo4u5/audioset_10_10_0.4483.pth?dl=1",
    "https://github.com/YuanGongND/ltu",
    "https://arxiv.org/abs/2104.01778",
    "https://share.weiyun.com/kcmk2KHw",
    "https://colab.research.google.com/assets/colab-badge.svg",
    "https://colab.research.google.com/github/YuanGongND/ast/blob/master/colab/AST_Inference_Demo.ipynb",
    "https://www.dropbox.com/s/1tv0hovue1bxupk/audioset_10_10_0.4495.pth?dl=1",
    "https://www.dropbox.com/s/snfhx3tizr4nuc8/audioset_12_12_0.4467.pth?dl=1",
    "https://www.youtube.com/watch?v=CSRDbqGY0Vw",
    "https://github.com/YuanGongND/ast/blob/master/ast.png?raw=true\"",
    "https://colab.research.google.com/github/YuanGongND/ast/blob/master/colab/torchaudio_SpecMasking_1_1.ipynb"
  ]
}
```

</details>


---

