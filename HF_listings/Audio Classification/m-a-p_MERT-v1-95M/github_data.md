# GitHub Data for m-a-p_MERT-v1-95M

**Task Category:** Audio Classification

## Repository 1: facebookresearch/encodec

# GitHub Repository Data

**Repository:** [facebookresearch/encodec](https://github.com/facebookresearch/encodec)

## Basic Information

- **Description:** State-of-the-art deep learning based audio codec supporting both mono 24 kHz audio and stereo 48 kHz audio.
- **Created:** 2022-10-20T12:35:53+00:00
- **Last Updated:** 2025-06-19T18:36:01+00:00
- **Last Pushed:** 2024-01-04T01:58:44+00:00
- **Default Branch:** main
- **Size:** 4294 KB

## Statistics

- **Stars:** 3,721
- **Forks:** 329
- **Watchers:** 3,721
- **Open Issues:** 58
- **Total Issues:** 0
- **Pull Requests:** 15

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/facebookresearch/encodec/blob/main/LICENSE)

## Languages

- **Python:** 117,023 bytes
- **Makefile:** 875 bytes

## Top Contributors

1. **adefossez** - 24 contributions
2. **0xflotus** - 1 contributions
3. **LWprogramming** - 1 contributions
4. **Vaibhavs10** - 1 contributions

## File Structure (Sample of 10 files)

Total files: 47

- `.github` (tree)
- `.github/ISSUE_TEMPLATE` (tree)
- `.github/ISSUE_TEMPLATE/bug.md` (blob)
- `.github/ISSUE_TEMPLATE/question.md` (blob)
- `.github/workflows` (tree)
- `.github/workflows/documentation.yml` (blob)
- `.github/workflows/linter.yml` (blob)
- `.github/workflows/tests.yml` (blob)
- `.gitignore` (blob)
- `CHANGELOG.md` (blob)

## Recent Issues

- ðŸŸ¢ **#91** Update README.md -- Small Typo (open)
- ðŸŸ¢ **#90** Feature/audio style transfer (open)
- ðŸŸ¢ **#89** Why concatenate the real and imaginary parts instead of using multiple channels in the MS-STFT discriminator? (open)
- ðŸŸ¢ **#88** add comment (open)
- ðŸŸ¢ **#87** channelsï¼Ÿ2 or 1? (open)

## Recent Pull Requests

- ðŸŸ¢ **#91** Update README.md -- Small Typo (open)
- ðŸŸ¢ **#90** Feature/audio style transfer (open)
- ðŸŸ¢ **#88** add comment (open)
- ðŸŸ¢ **#86** sync_buffer function in distrib.py has a typo? (open)
- ðŸ”´ **#78** fixed lstm autocast (closed)

## Recent Commits

- **0e2d0aed** Add ðŸ¤— Transformers usage and info (#60) - Vaibhav Srivastav (2023-06-20T15:15:57+00:00)
- **2d29d935** fix bug - Alexandre DÃ©fossez (2023-04-26T12:19:04+00:00)
- **349b7293** Releasing under the MIT license - Alexandre DÃ©fossez (2023-04-26T12:17:48+00:00)
- **6e8d7eda** fixing potential bug with dilations, although none of our models use it. - Alexandre DÃ©fossez (2023-03-31T13:22:36+00:00)
- **2a5c2f78** doc - Alexandre DÃ©fossez (2023-03-22T14:15:14+00:00)
- **27ff2772** fix sample_rate to frame_rate where appropriate (#40) - LWprogramming (2023-03-22T14:11:38+00:00)
- **f6a9f768** bump version - Alexandre DÃ©fossez (2023-03-03T09:49:59+00:00)
- **eff3a466** Fixing issue #35 - Alexandre DÃ©fossez (2023-03-03T09:48:28+00:00)
- **9dccd497** Merge pull request #31 from 0xflotus/patch-1 - Alexandre DÃ©fossez (2023-03-03T09:44:19+00:00)
- **f7aed5e8** Merge pull request #29 from eltociear/patch-1 - Alexandre DÃ©fossez (2023-02-21T22:50:10+00:00)

## External Links Found in README

- https://pytorch.org/docs/stable/hub.html#where-are-my-downloaded-models-saved
- https://open.spotify.com/artist/5eLv7rNfrf3IjMnK311ByP?si=X_zD9ackRRGjFP5Y6Q7Zng
- https://arxiv.org/pdf/2210.13438.pdf
- https://git@github.com/facebookresearch/encodec#egg=encodec
- https://huggingface.co/docs/transformers/main/en/model_doc/encodec
- https://open.spotify.com/artist/21HymveeIhDcM4KDKeNLz0?si=4zXF8VpeQpeKR9QUIuck9Q
- https://ai.honu.io/papers/encodec/final.mp4">
- https://github.com/huggingface/transformers.git@main
- https://ai.honu.io/papers/encodec/samples.html
- https://huggingface.co/facebook/encodec_24khz
- https://github.com/facebookresearch/encodec/workflows/linter/badge.svg
- https://github.com/facebookresearch/encodec/workflows/tests/badge.svg
- https://arxiv.org/abs/2210.13438
- https://huggingface.co/facebook/encodec_48khz

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 554807775,
  "name": "encodec",
  "full_name": "facebookresearch/encodec",
  "description": "State-of-the-art deep learning based audio codec supporting both mono 24 kHz audio and stereo 48 kHz audio.",
  "html_url": "https://github.com/facebookresearch/encodec",
  "clone_url": "https://github.com/facebookresearch/encodec.git",
  "ssh_url": "git@github.com:facebookresearch/encodec.git",
  "homepage": "",
  "topics": [],
  "default_branch": "main",
  "created_at": "2022-10-20T12:35:53+00:00",
  "updated_at": "2025-06-19T18:36:01+00:00",
  "pushed_at": "2024-01-04T01:58:44+00:00",
  "size_kb": 4294,
  "watchers_count": 3721,
  "stargazers_count": 3721,
  "forks_count": 329,
  "open_issues_count": 58,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/facebookresearch/encodec/blob/main/LICENSE"
  },
  "languages": {
    "Python": 117023,
    "Makefile": 875
  },
  "top_contributors": [
    {
      "login": "adefossez",
      "contributions": 24
    },
    {
      "login": "0xflotus",
      "contributions": 1
    },
    {
      "login": "LWprogramming",
      "contributions": 1
    },
    {
      "login": "Vaibhavs10",
      "contributions": 1
    }
  ],
  "file_tree_count": 47,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/bug.md",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/question.md",
      "type": "blob"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/documentation.yml",
      "type": "blob"
    },
    {
      "path": ".github/workflows/linter.yml",
      "type": "blob"
    },
    {
      "path": ".github/workflows/tests.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "CHANGELOG.md",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 15,
  "recent_issues": [
    {
      "number": 91,
      "title": "Update README.md -- Small Typo",
      "state": "open"
    },
    {
      "number": 90,
      "title": "Feature/audio style transfer",
      "state": "open"
    },
    {
      "number": 89,
      "title": "Why concatenate the real and imaginary parts instead of using multiple channels in the MS-STFT discriminator?",
      "state": "open"
    },
    {
      "number": 88,
      "title": "add comment",
      "state": "open"
    },
    {
      "number": 87,
      "title": "channels\uff1f2 or 1?",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 91,
      "title": "Update README.md -- Small Typo",
      "state": "open"
    },
    {
      "number": 90,
      "title": "Feature/audio style transfer",
      "state": "open"
    },
    {
      "number": 88,
      "title": "add comment",
      "state": "open"
    },
    {
      "number": 86,
      "title": "sync_buffer function in distrib.py has a typo?",
      "state": "open"
    },
    {
      "number": 78,
      "title": "fixed lstm autocast",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "0e2d0aed29362c8e8f52494baf3e6f99056b214f",
      "author": "Vaibhav Srivastav",
      "date": "2023-06-20T15:15:57+00:00",
      "message": "Add \ud83e\udd17 Transformers usage and info (#60)"
    },
    {
      "sha": "2d29d9353c2ff0ab1aeadc6a3d439854ee77da3e",
      "author": "Alexandre D\u00e9fossez",
      "date": "2023-04-26T12:19:04+00:00",
      "message": "fix bug"
    },
    {
      "sha": "349b72939f57cb3bc7b60906c0ee8228c849485d",
      "author": "Alexandre D\u00e9fossez",
      "date": "2023-04-26T12:17:48+00:00",
      "message": "Releasing under the MIT license"
    },
    {
      "sha": "6e8d7eda6fff5b0d589d64f063610c7f6044963e",
      "author": "Alexandre D\u00e9fossez",
      "date": "2023-03-31T13:22:36+00:00",
      "message": "fixing potential bug with dilations, although none of our models use it."
    },
    {
      "sha": "2a5c2f78e6f06480c7dab20fe55d5a6ec1729127",
      "author": "Alexandre D\u00e9fossez",
      "date": "2023-03-22T14:15:14+00:00",
      "message": "doc"
    },
    {
      "sha": "27ff2772d365a52c76ab5af654bf7f616c71ad56",
      "author": "LWprogramming",
      "date": "2023-03-22T14:11:38+00:00",
      "message": "fix sample_rate to frame_rate where appropriate (#40)"
    },
    {
      "sha": "f6a9f768373ba351d0cd18b928769df40da1aeb5",
      "author": "Alexandre D\u00e9fossez",
      "date": "2023-03-03T09:49:59+00:00",
      "message": "bump version"
    },
    {
      "sha": "eff3a466347380482c03584ef6a78df7783b5104",
      "author": "Alexandre D\u00e9fossez",
      "date": "2023-03-03T09:48:28+00:00",
      "message": "Fixing issue #35"
    },
    {
      "sha": "9dccd4975eb3fe539e1081e8709ea3a2b8533b64",
      "author": "Alexandre D\u00e9fossez",
      "date": "2023-03-03T09:44:19+00:00",
      "message": "Merge pull request #31 from 0xflotus/patch-1"
    },
    {
      "sha": "f7aed5e8e99cb68bc7cd098392c28e8a9abbdbeb",
      "author": "Alexandre D\u00e9fossez",
      "date": "2023-02-21T22:50:10+00:00",
      "message": "Merge pull request #29 from eltociear/patch-1"
    },
    {
      "sha": "55e4265f4b65d0d5cb4a51eb7becfee4c7b15a54",
      "author": "0xflotus",
      "date": "2023-01-30T22:45:03+00:00",
      "message": "chore: fix small typo error"
    },
    {
      "sha": "c79ba28c9199494d106d2c7f56006260528d7b16",
      "author": "Alexandre D\u00e9fossez",
      "date": "2023-01-24T13:07:56+00:00",
      "message": "Add warning when using RVQ for training"
    },
    {
      "sha": "6aa35d20c716f7191f8b0aef86e17c3f5d487b6f",
      "author": "Alexandre D\u00e9fossez",
      "date": "2023-01-24T13:05:08+00:00",
      "message": "Merge branch 'main' of github.com:facebookresearch/encodec"
    },
    {
      "sha": "ebfee47d8d7b36f38a9c9089ef04c3f22587151d",
      "author": "Alexandre D\u00e9fossez",
      "date": "2023-01-24T13:04:45+00:00",
      "message": "fix convert_audio"
    },
    {
      "sha": "88b93a7b9ab2774e1ac6a4431244fabb74300b4b",
      "author": "Ikko Eltociear Ashimine",
      "date": "2023-01-16T18:26:58+00:00",
      "message": "fix typo in distrib.py"
    },
    {
      "sha": "e9264db8327a6f5f5a78556199a2277f2ecc21a8",
      "author": "Alexandre D\u00e9fossez",
      "date": "2022-12-30T10:13:33+00:00",
      "message": "Removing left over manual seed"
    },
    {
      "sha": "194329839fd812433992272fc5e7a889176e6fd1",
      "author": "Alexandre D\u00e9fossez",
      "date": "2022-12-01T11:04:36+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "bf04daec8fb969b4d9c07d26f71c41acecc1fd89",
      "author": "Alexandre D\u00e9fossez",
      "date": "2022-12-01T10:49:11+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "c1b7ae2e9fbdd0b29cdce499ddd255f4d756274b",
      "author": "Alexandre D\u00e9fossez",
      "date": "2022-11-17T16:26:56+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "88729bb47dde6063790b101a6697d1f6eb35b523",
      "author": "Alexandre D\u00e9fossez",
      "date": "2022-11-17T16:17:41+00:00",
      "message": "fix on windows paths"
    }
  ],
  "readme_text": "# EnCodec: High Fidelity Neural Audio Compression\n![linter badge](https://github.com/facebookresearch/encodec/workflows/linter/badge.svg)\n![tests badge](https://github.com/facebookresearch/encodec/workflows/tests/badge.svg)\n\nThis is the code for the EnCodec neural codec presented in the [High Fidelity Neural Audio Compression](https://arxiv.org/pdf/2210.13438.pdf) [[abs]](https://arxiv.org/abs/2210.13438).\npaper. We provide our two multi-bandwidth models:\n* A causal model operating at 24 kHz on monophonic audio trained on a variety of audio data.\n* A non-causal model operating at 48 kHz on stereophonic audio trained on music-only data.\n\nThe 24 kHz model can compress to 1.5, 3, 6, 12 or 24 kbps, while the 48 kHz model\nsupport 3, 6, 12 and 24 kbps. We also provide a pre-trained language model for each\nof the models, that can further compress the representation by up to 40% without\nany further loss of quality.\n\nFor reference, we also provide the code for our novel [MS-STFT discriminator](encodec/msstftd.py) and the [balancer](encodec/balancer.py).\n\n<p align=\"center\">\n<img src=\"./architecture.png\" alt=\"Schema representing the structure of Encodec,\n    with a convolutional+LSTM encoder, a Residual Vector Quantization in the middle,\n    followed by a convolutional+LSTM decoder. A multiscale complex spectrogram discriminator is applied to the output, along with objective reconstruction losses.\n    A small transformer model is trained to predict the RVQ output.\"\nwidth=\"800px\"></p>\n\n\n## Samples\n\nSamples including baselines are provided on [our sample page](https://ai.honu.io/papers/encodec/samples.html).\nYou can also have a quick demo of what we achieve for 48 kHz music with EnCodec, along with\nentropy coding, by clicking the thumbnail (original tracks provided by [Lucille Crew](https://open.spotify.com/artist/5eLv7rNfrf3IjMnK311ByP?si=X_zD9ackRRGjFP5Y6Q7Zng) and [Voyageur I](https://open.spotify.com/artist/21HymveeIhDcM4KDKeNLz0?si=4zXF8VpeQpeKR9QUIuck9Q)).\n\n<p align=\"center\">\n<a href=\"https://ai.honu.io/papers/encodec/final.mp4\">\n<img src=\"./thumbnail.png\" alt=\"Thumbnail for the sample video.\n\tYou will first here the ground truth, then ~3kbps, then 12kbps, for two songs.\"></a></p>\n\n## \ud83e\udd17 Transformers\n\nEncodec has now been added to Transformers. For more information, please refer to [Transformers' Encodec docs](https://huggingface.co/docs/transformers/main/en/model_doc/encodec).\n\nYou can find both the [24KHz](https://huggingface.co/facebook/encodec_24khz) and [48KHz](https://huggingface.co/facebook/encodec_48khz) checkpoints on the \ud83e\udd17 Hub.\n\nUsing \ud83e\udd17 Transformers, you can leverage Encodec at scale along with all the other supported models and datasets. \u26a1\ufe0f\nAlternatively you can also directly use the encodec package, as detailed in the Usage section. \n\nTo use first you'd need to set up your development environment!\n```\npip install -U datasets \npip install git+https://github.com/huggingface/transformers.git@main\n```\n\nThen, start embedding your audio datasets at scale!\n```python\nfrom datasets import load_dataset, Audio\nfrom transformers import EncodecModel, AutoProcessor\n\n# dummy dataset, however you can swap this with an dataset on the \ud83e\udd17 hub or bring your own\nlibrispeech_dummy = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n\n# load the model + processor (for pre-processing the audio)\nmodel = EncodecModel.from_pretrained(\"facebook/encodec_24khz\")\nprocessor = AutoProcessor.from_pretrained(\"facebook/encodec_24khz\")\n\n# cast the audio data to the correct sampling rate for the model\nlibrispeech_dummy = librispeech_dummy.cast_column(\"audio\", Audio(sampling_rate=processor.sampling_rate))\naudio_sample = librispeech_dummy[0][\"audio\"][\"array\"]\n\n# pre-process the inputs\ninputs = processor(raw_audio=audio_sample, sampling_rate=processor.sampling_rate, return_tensors=\"pt\")\n\n# explicitly encode then decode the audio inputs\nencoder_outputs = model.encode(inputs[\"input_values\"], inputs[\"padding_mask\"])\naudio_values = model.decode(encoder_outputs.audio_codes, encoder_outputs.audio_scales, inputs[\"padding_mask\"])[0]\n\n# or the equivalent with a forward pass\naudio_values = model(inputs[\"input_values\"], inputs[\"padding_mask\"]).audio_values\n\n# you can also extract the discrete codebook representation for LM tasks\n# output: concatenated tensor of all the representations\naudio_codes = model(inputs[\"input_values\"], inputs[\"padding_mask\"]).audio_codes\n\n```\n\n## What's up?\n\nSee [the changelog](CHANGELOG.md) for details on releases.\n\n## Installation\n\nEnCodec requires Python 3.8, and a reasonably recent version of PyTorch (1.11.0 ideally).\nTo install EnCodec, you can run from this repository:\n```bash\npip install -U encodec  # stable release\npip install -U git+https://git@github.com/facebookresearch/encodec#egg=encodec  # bleeding edge\n# of if you cloned the repo locally\npip install .\n```\n\n**Supported platforms:** we officially support only Mac OS X (you might need XCode installed if running on a non Intel Mac), and recent versions of mainstream Linux distributions. We will try to help out on Windows but cannot provide strong support. Any other platform (iOS / Android / onboard ARM) are not supported.\n\n## Usage\n\nYou can then use the EnCodec command, either as\n```bash\npython3 -m encodec [...]\n# or\nencodec [...]\n```\n\nIf you want to directly use the compression API, checkout `encodec.compress`\nand `encodec.model`. See hereafter for instructions on how to extract the discrete\nrepresentation.\n\n### Model storage\n\nThe models will be automatically downloaded on first use using Torch Hub.\nFor more information on where those models are stored, or how to customize\nthe storage location, [checkout their documentation.](https://pytorch.org/docs/stable/hub.html#where-are-my-downloaded-models-saved)\n\n### Compression\n\n```bash\nencodec [-b TARGET_BANDWIDTH] [-f] [--hq] [--lm] INPUT_FILE [OUTPUT_FILE]\n```\nGiven any audio file supported by torchaudio on your platform, compresses\nit with EnCodec to the target bandwidth (default is 6 kbps, can be either 1.5, 3, 6, 12 or 24).\nOUTPUT_FILE must end in `.ecdc`. If not provided it will be the same as `INPUT_FILE`,\nreplacing the extension with `.ecdc`.\nIn order to use the model operating at 48 kHz on stereophonic audio, use the `--hq` flag.\nThe `-f` flag is used to force overwrite an existing output file.\nUse the `--lm` flag to use the pretrained language model with entropy coding (expect it to\nbe much slower).\n\nIf the sample rate or number of channels of the input doesn't match that of the model,\nthe command will automatically resample / reduce channels as needed.\n\n### Decompression\n```bash\nencodec [-f] [-r] ENCODEC_FILE [OUTPUT_WAV_FILE]\n```\nGiven a `.ecdc` file previously generated, this will decode it to the given output wav file.\nIf not provided, the output will default to the input with the `.wav` extension.\nUse the `-f` file to force overwrite the output file (be carefull if compress then decompress,\nnot to overwrite your original file !). Use the `-r` flag if you experience clipping, this will\nrescale the output file to avoid it.\n\n### Compression + Decompression\n```bash\nencodec [-r] [-b TARGET_BANDWIDTH] [-f] [--hq] [--lm] INPUT_FILE OUTPUT_WAV_FILE\n```\nWhen `OUTPUT_WAV_FILE` has the `.wav` extension (as opposed to `.ecdc`), the `encodec`\ncommand will instead compress and immediately decompress without storing the intermediate\n`.ecdc` file.\n\n### Extracting discrete representations\n\nThe EnCodec model can also be used to extract discrete representations from the audio waveform.\n\n```python\nfrom encodec import EncodecModel\nfrom encodec.utils import convert_audio\n\nimport torchaudio\nimport torch\n\n# Instantiate a pretrained EnCodec model\nmodel = EncodecModel.encodec_model_24khz()\n# The number of codebooks used will be determined bythe bandwidth selected.\n# E.g. for a bandwidth of 6kbps, `n_q = 8` codebooks are used.\n# Supported bandwidths are 1.5kbps (n_q = 2), 3 kbps (n_q = 4), 6 kbps (n_q = 8) and 12 kbps (n_q =16) and 24kbps (n_q=32).\n# For the 48 kHz model, only 3, 6, 12, and 24 kbps are supported. The number\n# of codebooks for each is half that of the 24 kHz model as the frame rate is twice as much.\nmodel.set_target_bandwidth(6.0)\n\n# Load and pre-process the audio waveform\nwav, sr = torchaudio.load(\"<PATH_TO_AUDIO_FILE>\")\nwav = convert_audio(wav, sr, model.sample_rate, model.channels)\nwav = wav.unsqueeze(0)\n\n# Extract discrete codes from EnCodec\nwith torch.no_grad():\n    encoded_frames = model.encode(wav)\ncodes = torch.cat([encoded[0] for encoded in encoded_frames], dim=-1)  # [B, n_q, T]\n```\n\nNote that the 48 kHz model processes the audio by chunks of 1 seconds, with an overlap of 1%,\nand renormalizes the audio to have unit scale. For this model, the output of `model.encode(wav)`\nwould a list (for each frame of 1 second) of a tuple `(codes, scale)` with `scale` a scalar tensor.\n\n## Installation for development\n\nThis will install the dependencies and a `encodec` in developer mode (changes to the files\nwill directly reflect), along with the dependencies to run unit tests.\n```\npip install -e '.[dev]'\n```\n\n### Test\n\nYou can run the unit tests with\n```\nmake tests\n```\n\n## FAQ\n\nPlease check this section before opening an issue.\n\n### Out of memory errors with long files\n\nWe do not try to be smart about long files, and we apply the model at once on the entire file. This can lead to a large memory usage\nand result in the process being killed. At the moment we will not support this use case.\n\n### Bad interactions between DistributedDataParallel and the RVQ code\n\nWe do not use DDP, instead we recommend using the routines in `encodec/distrib.py`, in particular `encodec.distrib.sync_buffer` and `encodec.distrib.sync_grad`.\n\n## Citation\n\nIf you use this code or results in your paper, please cite our work as:\n\n```\n@article{defossez2022highfi,\n  title={High Fidelity Neural Audio Compression},\n  author={D\u00e9fossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi},\n  journal={arXiv preprint arXiv:2210.13438},\n  year={2022}\n}\n```\n\n## License\n\nThe code in this repository is released under the MIT license as found in the\n[LICENSE](LICENSE) file.\n",
  "external_links_in_readme": [
    "https://pytorch.org/docs/stable/hub.html#where-are-my-downloaded-models-saved",
    "https://open.spotify.com/artist/5eLv7rNfrf3IjMnK311ByP?si=X_zD9ackRRGjFP5Y6Q7Zng",
    "https://arxiv.org/pdf/2210.13438.pdf",
    "https://git@github.com/facebookresearch/encodec#egg=encodec",
    "https://huggingface.co/docs/transformers/main/en/model_doc/encodec",
    "https://open.spotify.com/artist/21HymveeIhDcM4KDKeNLz0?si=4zXF8VpeQpeKR9QUIuck9Q",
    "https://ai.honu.io/papers/encodec/final.mp4\">",
    "https://github.com/huggingface/transformers.git@main",
    "https://ai.honu.io/papers/encodec/samples.html",
    "https://huggingface.co/facebook/encodec_24khz",
    "https://github.com/facebookresearch/encodec/workflows/linter/badge.svg",
    "https://github.com/facebookresearch/encodec/workflows/tests/badge.svg",
    "https://arxiv.org/abs/2210.13438",
    "https://huggingface.co/facebook/encodec_48khz"
  ]
}
```

</details>


---

## Repository 2: yizhilll/MERT

# GitHub Repository Data

**Repository:** [yizhilll/MERT](https://github.com/yizhilll/MERT)

## Basic Information

- **Description:** Official implementation of the paper "Acoustic Music Understanding Model with Large-Scale Self-supervised Training".
- **Created:** 2023-05-29T21:25:55+00:00
- **Last Updated:** 2025-06-16T17:35:27+00:00
- **Last Pushed:** 2025-05-25T10:04:49+00:00
- **Default Branch:** main
- **Size:** 82 KB

## Statistics

- **Stars:** 378
- **Forks:** 25
- **Watchers:** 378
- **Open Issues:** 10
- **Total Issues:** 0
- **Pull Requests:** 0

## License

- **Type:** Apache License 2.0
- **SPDX ID:** Apache-2.0
- **URL:** [License](https://github.com/yizhilll/MERT/blob/main/LICENSE)

## Languages

- **Python:** 163,735 bytes
- **Shell:** 9,625 bytes

## Top Contributors

1. **yizhilll** - 21 contributions
2. **a43992899** - 1 contributions

## File Structure (Sample of 10 files)

Total files: 34

- `LICENSE` (blob)
- `README.md` (blob)
- `mert_fairseq` (tree)
- `mert_fairseq/config` (tree)
- `mert_fairseq/config/pretrain` (tree)
- `mert_fairseq/config/pretrain/MERT_RVQ-VAE_CQT_330M.yaml` (blob)
- `mert_fairseq/config/pretrain/MERT_RVQ-VAE_CQT_95M.yaml` (blob)
- `mert_fairseq/config/pretrain/run` (tree)
- `mert_fairseq/config/pretrain/run/submitit_reg.yaml` (blob)
- `mert_fairseq/data` (tree)

## Recent Issues

- ðŸŸ¢ **#20** MERT configuration in `fairseq` seems to differ from the configuration in `huggingface` (open)
- ðŸŸ¢ **#19** Why MERT-v1-330M was trained with `feat_extract_norm="group"` instead of "layer"? (open)
- ðŸŸ¢ **#18** Fine-tune MERT model on the custom dataset (open)
- ðŸŸ¢ **#17** HuBERT and data2vec retrained on music  (open)
- ðŸŸ¢ **#16** Music Descriptor prediction (especially for EMO task) (open)

## Recent Commits

- **391062e4** Update README.md - Yizhi Li (2025-05-25T10:04:49+00:00)
- **0c142bdf** [update] readme - yizhilll (2024-04-20T18:31:27+00:00)
- **157dcbf6** [add] HF conversion code - yizhilll (2024-04-20T18:26:56+00:00)
- **6632135f** Merge branch 'main' of github.com:yizhilll/MERT - yizhilll (2024-04-20T17:53:06+00:00)
- **fba3d6cb** [add] encodec environment - yizhilll (2024-04-20T17:53:03+00:00)
- **b2fb3cca** Update README.md - Yizhi Li (2023-09-28T06:16:11+00:00)
- **056bc833** Create prepare_codecs_from_manifest.py - Yizhi Li (2023-09-28T06:10:12+00:00)
- **2c825cb1** Create prepare_manifest.py - Yizhi Li (2023-09-28T06:07:56+00:00)
- **53d5af88** Update environment_setup.sh - Yizhi Li (2023-09-26T18:15:41+00:00)
- **cf0a7bbc** Update environment_setup.sh - Yizhi Li (2023-09-22T17:14:25+00:00)

## External Links Found in README

- https://huggingface.co/docs/transformers/en/index
- https://marble-bm.shef.ac.uk/submit
- https://huggingface.co/m-a-p/MERT-v1-95M/blob/main/MERT-v1-95M_fairseq.pt
- https://huggingface.co/m-a-p/MERT-v1-330M
- https://github.com/NVIDIA/nccl
- https://github.com/NVIDIA/apex
- https://github.com/facebookresearch/fairseq/tree/main/examples/hubert/simple_kmeans
- https://marble-bm.shef.ac.uk
- https://github.com/KinWaiCheuk/nnAudio
- https://pytorch.org/
- https://github.com/facebookresearch/encodec
- https://github.com/facebookresearch/fairseq/tree/main/examples/hubert
- https://github.com/facebookresearch/fairseq
- https://github.com/facebookresearch/fairscale
- https://huggingface.co/m-a-p
- https://huggingface.co/m-a-p/MERT-v1-330M/blob/main/MERT-v1-330M_fairseq.pt
- https://huggingface.co/m-a-p/MERT-v1-95M
- https://marble-bm.shef.ac.uk/leaderboard

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 647008314,
  "name": "MERT",
  "full_name": "yizhilll/MERT",
  "description": "Official implementation of the paper \"Acoustic Music Understanding Model with Large-Scale Self-supervised Training\".",
  "html_url": "https://github.com/yizhilll/MERT",
  "clone_url": "https://github.com/yizhilll/MERT.git",
  "ssh_url": "git@github.com:yizhilll/MERT.git",
  "homepage": null,
  "topics": [],
  "default_branch": "main",
  "created_at": "2023-05-29T21:25:55+00:00",
  "updated_at": "2025-06-16T17:35:27+00:00",
  "pushed_at": "2025-05-25T10:04:49+00:00",
  "size_kb": 82,
  "watchers_count": 378,
  "stargazers_count": 378,
  "forks_count": 25,
  "open_issues_count": 10,
  "license": {
    "key": "apache-2.0",
    "name": "Apache License 2.0",
    "spdx_id": "Apache-2.0",
    "url": "https://github.com/yizhilll/MERT/blob/main/LICENSE"
  },
  "languages": {
    "Python": 163735,
    "Shell": 9625
  },
  "top_contributors": [
    {
      "login": "yizhilll",
      "contributions": 21
    },
    {
      "login": "a43992899",
      "contributions": 1
    }
  ],
  "file_tree_count": 34,
  "file_tree_sample": [
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "mert_fairseq",
      "type": "tree"
    },
    {
      "path": "mert_fairseq/config",
      "type": "tree"
    },
    {
      "path": "mert_fairseq/config/pretrain",
      "type": "tree"
    },
    {
      "path": "mert_fairseq/config/pretrain/MERT_RVQ-VAE_CQT_330M.yaml",
      "type": "blob"
    },
    {
      "path": "mert_fairseq/config/pretrain/MERT_RVQ-VAE_CQT_95M.yaml",
      "type": "blob"
    },
    {
      "path": "mert_fairseq/config/pretrain/run",
      "type": "tree"
    },
    {
      "path": "mert_fairseq/config/pretrain/run/submitit_reg.yaml",
      "type": "blob"
    },
    {
      "path": "mert_fairseq/data",
      "type": "tree"
    }
  ],
  "issues_count": 0,
  "pulls_count": 0,
  "recent_issues": [
    {
      "number": 20,
      "title": "MERT configuration in `fairseq` seems to differ from the configuration in `huggingface`",
      "state": "open"
    },
    {
      "number": 19,
      "title": "Why MERT-v1-330M was trained with `feat_extract_norm=\"group\"` instead of \"layer\"?",
      "state": "open"
    },
    {
      "number": 18,
      "title": "Fine-tune MERT model on the custom dataset",
      "state": "open"
    },
    {
      "number": 17,
      "title": "HuBERT and data2vec retrained on music ",
      "state": "open"
    },
    {
      "number": 16,
      "title": "Music Descriptor prediction (especially for EMO task)",
      "state": "open"
    }
  ],
  "recent_pulls": [],
  "recent_commits": [
    {
      "sha": "391062e4c384aaad4e5a992be339ef70769dbd6f",
      "author": "Yizhi Li",
      "date": "2025-05-25T10:04:49+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "0c142bdf88ae5f62adab677983a2b54bcdb18f93",
      "author": "yizhilll",
      "date": "2024-04-20T18:31:27+00:00",
      "message": "[update] readme"
    },
    {
      "sha": "157dcbf62084dddcae5ab5111a1d1ac5a24e7c15",
      "author": "yizhilll",
      "date": "2024-04-20T18:26:56+00:00",
      "message": "[add] HF conversion code"
    },
    {
      "sha": "6632135f59f58b7e4c9008c0791e569a75a5c8c4",
      "author": "yizhilll",
      "date": "2024-04-20T17:53:06+00:00",
      "message": "Merge branch 'main' of github.com:yizhilll/MERT"
    },
    {
      "sha": "fba3d6cb43574c52c6deeb43bb145c0ec884151a",
      "author": "yizhilll",
      "date": "2024-04-20T17:53:03+00:00",
      "message": "[add] encodec environment"
    },
    {
      "sha": "b2fb3cca5f3e2054617b879f55292fa21517e1c2",
      "author": "Yizhi Li",
      "date": "2023-09-28T06:16:11+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "056bc833fa53346b38cfbfa6b6ed4e75665472ab",
      "author": "Yizhi Li",
      "date": "2023-09-28T06:10:12+00:00",
      "message": "Create prepare_codecs_from_manifest.py"
    },
    {
      "sha": "2c825cb1c1e0b7b635ad2c262df9681ec957c0c8",
      "author": "Yizhi Li",
      "date": "2023-09-28T06:07:56+00:00",
      "message": "Create prepare_manifest.py"
    },
    {
      "sha": "53d5af883f84f8ce70e815bec47c51d774f6e0ab",
      "author": "Yizhi Li",
      "date": "2023-09-26T18:15:41+00:00",
      "message": "Update environment_setup.sh"
    },
    {
      "sha": "cf0a7bbc4cd8ee077c9d6e12f24f4affbed1e542",
      "author": "Yizhi Li",
      "date": "2023-09-22T17:14:25+00:00",
      "message": "Update environment_setup.sh"
    },
    {
      "sha": "77e66acb1496b5a1482817025cb29b3a67a39db9",
      "author": "Yizhi Li",
      "date": "2023-07-12T14:58:32+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "505df6514b844c1d71dd72efa5648389e189aba4",
      "author": "Yizhi Li",
      "date": "2023-07-12T14:55:42+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "5de1a839e11775d3512ee91b12f7449bf84cabcd",
      "author": "Yizhi Li",
      "date": "2023-07-11T20:28:37+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "5075454487b6ea980f3bfe48c78ec4d14c7b88b5",
      "author": "Ruibin Yuan",
      "date": "2023-06-05T22:55:51+00:00",
      "message": "Merge pull request #1 from eltociear/patch-1"
    },
    {
      "sha": "b63231a895d2e5b6aa5f7ab796d35dc0eeefa09e",
      "author": "Ikko Eltociear Ashimine",
      "date": "2023-06-05T14:35:26+00:00",
      "message": "Fix typo in mert/README.md"
    },
    {
      "sha": "4fe4c986225e5425383428a0594f7fe13086a88b",
      "author": "yizhilll",
      "date": "2023-06-03T20:17:22+00:00",
      "message": "update readme and scripts"
    },
    {
      "sha": "231469288699ee0efdd43a44161704ddb374bc75",
      "author": "yizhilll",
      "date": "2023-06-03T20:12:25+00:00",
      "message": "update READEME.md"
    },
    {
      "sha": "b48507fb2251c603b7e626740d7c906e867dfb75",
      "author": "yizhilll",
      "date": "2023-06-03T19:45:51+00:00",
      "message": "Merge branch 'main' of github.com:yizhilll/MERT"
    },
    {
      "sha": "30367a71b8cbdc8206cd69822ae1fdf6c1ff0126",
      "author": "yizhilll",
      "date": "2023-06-03T19:44:20+00:00",
      "message": "update README.md"
    },
    {
      "sha": "de25b90c38325162abc25e74f8c35df2f33d24a1",
      "author": "Yizhi Li",
      "date": "2023-06-02T13:53:35+00:00",
      "message": "Add citation"
    }
  ],
  "readme_text": "# MERT\n\nThis is the official implementation of the paper \"MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training\".\n\n\n**Evaluation, Benchmarking and Baselines**:\n* The codes for downstream task evaluation on MERT and baseline models can be referred to [MARBLE](https://marble-bm.shef.ac.uk) benchmark.\n* MERT is also evaluated with the [MARBLE protocol](https://marble-bm.shef.ac.uk/submit) and reported on the [music understanding leaderboard](https://marble-bm.shef.ac.uk/leaderboard). \n\n## Training\n\nThe MERT training is implemented with [fairseq](https://github.com/facebookresearch/fairseq). \nYou need to clone the fairseq repo inside our repo at `./src/fairseq` and MERT implementation codes as a fairseq example projcet. \n\n### Environment Setup\n\nThe training of MERT requires:\n* [fairseq](https://github.com/facebookresearch/fairseq) & [pytorch](https://pytorch.org/) for the training (must)\n* [nnAudio](https://github.com/KinWaiCheuk/nnAudio) for on-the-fly CQT inference (must)\n* [apex](https://github.com/NVIDIA/apex) for half-precision training (optaional)\n* [nccl](https://github.com/NVIDIA/nccl) for multiple device training (optional)\n* [fairscale](https://github.com/facebookresearch/fairscale) for FSDP and CPU offloading (optional)\n* [WARNING] the version of [transformers](https://huggingface.co/docs/transformers/en/index) requires to be `transformers==4.38` since a latter update cause incompatible.\n\n  \nYou could use the script `./scripts/environment_setup.sh` to set up the python environment from scarth, which could be easily modified to DOCKERFILE. \nAll the relevant folders will be placed at the customized MERT repo folder path `$MAP_PROJ_DIR`.\n\n### Data Preparation\n\nGenerally, there are 2 things you need to prepare:\n* `DATA_DIR=${MAP_PROJ_DIR}/data/audio_tsv`: a folder that contains a `train.tsv` and a `valid.tsv` file, which specify the root path to the audios at the first line and the relative paths at the rest lines.\n* `LABEL_ROOT_DIR=${MAP_PROJ_DIR}/data/labels`: a folder filled with all the discrete tokens that need to prepare before training. They could be K-means or RVQ-VAE tokens.\n\nThe two options for acoustic teacher peuso labels in MERT training can be constructed by:\n* K-means Labels from [HuBERT](https://github.com/facebookresearch/fairseq/tree/main/examples/hubert/simple_kmeans) (the vanilla MFCC version)\n* codecs from [EnCodec](https://github.com/facebookresearch/encodec)\n\nScripts for preparing the training data:\n```shell\n# First prepare the manifest file indexing the audios.\n# If needed the audios will be converted to 24K Hz.\npython scripts/prepare_manifest.py --root-dir /absolute/path/to/original/custom_audio_dataset \\\n      --target-rate 24000 --converted-root-dir /absolute/path/to/converted/custom_audio_dataset \\\n      --out-dir data/custom_audio_dataset_manifest --extension wav\n      \n# Prepare the codecs for audios in the manifest\npython scripts/prepare_codecs_from_manifest.py  \\\n      --manifest_path data/custom_audio_dataset_manifest --manifest_file_name train.tsv \\\n      --out_root data/encodec_labels/custom_audio_dataset --codebook_size 1024 --n_codebook 8\n```\n\nThe data preparation and format can be referred to [HuBERT](https://github.com/facebookresearch/fairseq/tree/main/examples/hubert) for more details.\n\n### Start Training\n\nNoted that we follow the fariseq development protocol to put our codes as an example project. \nWhen running the fairseq program, you can specify the MERT customized codes by `common.user_dir=${MAP_PROJ_DIR}/mert_faiseq`.\n\n\nAfter the environment is set up, you could use the following scripts:\n```shell\n# for MERT95M\nbash scripts/run_training.sh 0 dummy MERT_RVQ-VAE_CQT_95M\n\n# for MERT 330M\nbash scripts/run_training.sh 0 dummy MERT_RVQ-VAE_CQT_330M\n```\n\n## Inference\n\nWe use the huggingface models for interface and evaluation. Using the example of RVQ-VAE 95M MERT as example, the following codes show how to load and extract representations with MERT.\n\n```shell\npython MERT/scripts/MERT_demo_inference.py\n```\n\n## Checkpoints\n\n### Huggingface Checkpoint\n\nOur Huggingface Transformers checkpoints for convenient inference are uploaded to the [m-a-p](https://huggingface.co/m-a-p) project page.\n* [MERT-v0](https://huggingface.co/m-a-p/MERT-v1-95M): The base (95M) model trained with K-means acoustic teacher and musical teacher.\n* [MERT-v0-public](https://huggingface.co/m-a-p/MERT-v1-95M): The base (95M) model trained with K-means acoustic teacher and musical teacher using the public music4all training data.\n* [MERT-v1-95M](https://huggingface.co/m-a-p/MERT-v1-95M): The base (95M) model trained with RVQ-VAE acoustic teacher and musical teacher.\n* [MERT-v1-330M](https://huggingface.co/m-a-p/MERT-v1-330M): The large (330M) model trained with RVQ-VAE acoustic teacher and musical teacher.\n\nTo convert your self-trained models, check the scripts:\n```shell\nbash scripts/convert_HF_script.sh default mert config_mert_base [/absolute/path/to/a/fairseq/checkpoint.pt]\n```\n\n### Fairseq Checkpoint\n\nWe also provide the corresponding fairseq checkpoint for continual training or further modification hosted at the corresponding HF repos:\n* [MERT-v1-95M](https://huggingface.co/m-a-p/MERT-v1-95M/blob/main/MERT-v1-95M_fairseq.pt) \n* [MERT-v1-330M](https://huggingface.co/m-a-p/MERT-v1-330M/blob/main/MERT-v1-330M_fairseq.pt) \n\n\n## Citation\n\n```shell\n@misc{li2023mert,\n      title={MERT: Acoustic Music Understanding Model with Large-Scale Self-supervised Training}, \n      author={Yizhi Li and Ruibin Yuan and Ge Zhang and Yinghao Ma and Xingran Chen and Hanzhi Yin and Chenghua Lin and Anton Ragni and Emmanouil Benetos and Norbert Gyenge and Roger Dannenberg and Ruibo Liu and Wenhu Chen and Gus Xia and Yemin Shi and Wenhao Huang and Yike Guo and Jie Fu},\n      year={2023},\n      eprint={2306.00107},\n      archivePrefix={arXiv},\n      primaryClass={cs.SD}\n}\n```\n",
  "external_links_in_readme": [
    "https://huggingface.co/docs/transformers/en/index",
    "https://marble-bm.shef.ac.uk/submit",
    "https://huggingface.co/m-a-p/MERT-v1-95M/blob/main/MERT-v1-95M_fairseq.pt",
    "https://huggingface.co/m-a-p/MERT-v1-330M",
    "https://github.com/NVIDIA/nccl",
    "https://github.com/NVIDIA/apex",
    "https://github.com/facebookresearch/fairseq/tree/main/examples/hubert/simple_kmeans",
    "https://marble-bm.shef.ac.uk",
    "https://github.com/KinWaiCheuk/nnAudio",
    "https://pytorch.org/",
    "https://github.com/facebookresearch/encodec",
    "https://github.com/facebookresearch/fairseq/tree/main/examples/hubert",
    "https://github.com/facebookresearch/fairseq",
    "https://github.com/facebookresearch/fairscale",
    "https://huggingface.co/m-a-p",
    "https://huggingface.co/m-a-p/MERT-v1-330M/blob/main/MERT-v1-330M_fairseq.pt",
    "https://huggingface.co/m-a-p/MERT-v1-95M",
    "https://marble-bm.shef.ac.uk/leaderboard"
  ]
}
```

</details>


---

