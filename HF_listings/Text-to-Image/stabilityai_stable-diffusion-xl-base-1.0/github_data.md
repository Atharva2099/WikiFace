# GitHub Data for stabilityai_stable-diffusion-xl-base-1.0

**Task Category:** Text-to-Image

## Repository 1: huggingface/optimum

# GitHub Repository Data

**Repository:** [huggingface/optimum](https://github.com/huggingface/optimum)

## Basic Information

- **Description:** 游 Accelerate inference and training of 游뱅 Transformers, Diffusers, TIMM and Sentence Transformers with easy to use hardware optimization tools
- **Created:** 2021-07-20T12:36:40+00:00
- **Last Updated:** 2025-06-21T02:49:16+00:00
- **Last Pushed:** 2025-06-20T16:13:52+00:00
- **Default Branch:** main
- **Size:** 5561 KB

## Statistics

- **Stars:** 2,947
- **Forks:** 552
- **Watchers:** 2,947
- **Open Issues:** 314
- **Total Issues:** 0
- **Pull Requests:** 1,456

## License

- **Type:** Apache License 2.0
- **SPDX ID:** Apache-2.0
- **URL:** [License](https://github.com/huggingface/optimum/blob/main/LICENSE)

## Languages

- **Python:** 2,412,043 bytes
- **Makefile:** 1,777 bytes
- **Shell:** 379 bytes

## Topics

- `onnx`
- `pytorch`
- `inference`
- `training`
- `intel`
- `graphcore`
- `onnxruntime`
- `transformers`
- `quantization`
- `habana`
- `optimization`
- `tflite`

## Top Contributors

1. **fxmarty** - 336 contributions
2. **echarlaix** - 277 contributions
3. **JingyaHuang** - 97 contributions
4. **regisss** - 73 contributions
5. **michaelbenayoun** - 53 contributions
6. **IlyasMoutawwakil** - 44 contributions
7. **mht-sharma** - 34 contributions
8. **lewtun** - 22 contributions
9. **younesbelkada** - 20 contributions
10. **madlag** - 20 contributions

## File Structure (Sample of 10 files)

Total files: 427

- `.github` (tree)
- `.github/ISSUE_TEMPLATE` (tree)
- `.github/ISSUE_TEMPLATE/bug-report.yml` (blob)
- `.github/ISSUE_TEMPLATE/config.yml` (blob)
- `.github/ISSUE_TEMPLATE/feature-request.yml` (blob)
- `.github/PULL_REQUEST_TEMPLATE.md` (blob)
- `.github/workflows` (tree)
- `.github/workflows/build_main_documentation.yml` (blob)
- `.github/workflows/build_pr_documentation.yml` (blob)
- `.github/workflows/quality.yml` (blob)

## Recent Issues

- 游릭 **#2300** Support for EuroBERT models (open)
- 游댮 **#2299** remove timm from exporters extra (closed)
- 游릭 **#2298**  Move onnx and ort related code to optimum-onnx  (open)
- 游릭 **#2297** Add task tflite register (open)
- 游댮 **#2296** Fix taskmanager (closed)

## Recent Pull Requests

- 游댮 **#2299** remove timm from exporters extra (closed)
- 游릭 **#2298**  Move onnx and ort related code to optimum-onnx  (open)
- 游릭 **#2297** Add task tflite register (open)
- 游댮 **#2296** Fix taskmanager (closed)
- 游댮 **#2295** Fix 'Block pattern could not be match. Pass `block_name_to_quantize` argument in `quantize_model`' while loading Qwen VL GPTQ model (closed)

## Recent Commits

- **cf6d4cc1** Remove timm from exporters extra (#2299) - Ella Charlaix (2025-06-20T13:09:35+00:00)
- **ed70d27e** Fix 'Block pattern could not be match. Pass `block_name_to_quantize` argument in `quantize_model`' while loading Qwen VL GPTQ model (#2295) - Arun Madhusudhanan (2025-06-19T16:17:33+00:00)
- **824e3683** Remove flatten_inputs from taskmanager (#2296) - Ella Charlaix (2025-06-19T16:08:14+00:00)
- **be76dde5** ExporterConfig refactorization (#2157) - Ella Charlaix (2025-06-18T13:17:28+00:00)
- **05aef419** Upgrade windows runner image (#2294) - Ella Charlaix (2025-06-17T14:41:26+00:00)
- **5246fc84** No more forcing separators (#2279) - Ilyas Moutawwakil (2025-06-17T14:05:02+00:00)
- **de103475** Add task onnx register (#2291) - Ella Charlaix (2025-06-17T11:35:35+00:00)
- **0c6dbd48** Bump protobuf for onnxruntime training example (#2292) - dependabot[bot] (2025-06-17T08:14:11+00:00)
- **dba0386a** Add back from_transformers for base model (#2288) - Ella Charlaix (2025-06-13T10:10:45+00:00)
- **0b252b3d** Dev version (#2287) - Ella Charlaix (2025-06-13T08:40:54+00:00)

## External Links Found in README

- https://onnxruntime.ai/
- https://huggingface.co/docs/optimum/main/en/intel/installation
- https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model
- https://img.shields.io/pypi/l/optimum"/></a>
- https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_bert
- https://huggingface.co/docs/optimum/exporters/overview
- https://huggingface.co/docs/optimum/amd/index
- https://img.shields.io/pypi/pyversions/optimum"/></a>
- https://github.com/huggingface/optimum-habana/tree/main/examples
- https://github.com/pytorch/executorch
- https://github.com/huggingface/optimum-executorch.git
- https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models
- https://github.com/huggingface/optimum-quanto
- https://huggingface.co/docs/optimum/onnxruntime/overview
- https://huggingface.co/docs/optimum-neuron/training_tutorials/sft_lora_finetune_llm
- https://img.shields.io/pypi/v/optimum"/></a>
- https://huggingface.co/docs/optimum/furiosa/index
- https://huggingface.co/docs/optimum/main/exporters/tflite/usage_guides/export_a_model
- https://huggingface.co/docs/optimum/intel/inference
- https://huggingface.co/docs/optimum/habana/quickstart

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 387786881,
  "name": "optimum",
  "full_name": "huggingface/optimum",
  "description": "\ud83d\ude80 Accelerate inference and training of \ud83e\udd17 Transformers, Diffusers, TIMM and Sentence Transformers with easy to use hardware optimization tools",
  "html_url": "https://github.com/huggingface/optimum",
  "clone_url": "https://github.com/huggingface/optimum.git",
  "ssh_url": "git@github.com:huggingface/optimum.git",
  "homepage": "https://huggingface.co/docs/optimum/main/",
  "topics": [
    "onnx",
    "pytorch",
    "inference",
    "training",
    "intel",
    "graphcore",
    "onnxruntime",
    "transformers",
    "quantization",
    "habana",
    "optimization",
    "tflite"
  ],
  "default_branch": "main",
  "created_at": "2021-07-20T12:36:40+00:00",
  "updated_at": "2025-06-21T02:49:16+00:00",
  "pushed_at": "2025-06-20T16:13:52+00:00",
  "size_kb": 5561,
  "watchers_count": 2947,
  "stargazers_count": 2947,
  "forks_count": 552,
  "open_issues_count": 314,
  "license": {
    "key": "apache-2.0",
    "name": "Apache License 2.0",
    "spdx_id": "Apache-2.0",
    "url": "https://github.com/huggingface/optimum/blob/main/LICENSE"
  },
  "languages": {
    "Python": 2412043,
    "Makefile": 1777,
    "Shell": 379
  },
  "top_contributors": [
    {
      "login": "fxmarty",
      "contributions": 336
    },
    {
      "login": "echarlaix",
      "contributions": 277
    },
    {
      "login": "JingyaHuang",
      "contributions": 97
    },
    {
      "login": "regisss",
      "contributions": 73
    },
    {
      "login": "michaelbenayoun",
      "contributions": 53
    },
    {
      "login": "IlyasMoutawwakil",
      "contributions": 44
    },
    {
      "login": "mht-sharma",
      "contributions": 34
    },
    {
      "login": "lewtun",
      "contributions": 22
    },
    {
      "login": "younesbelkada",
      "contributions": 20
    },
    {
      "login": "madlag",
      "contributions": 20
    },
    {
      "login": "xenova",
      "contributions": 16
    },
    {
      "login": "SunMarc",
      "contributions": 16
    },
    {
      "login": "mfuntowicz",
      "contributions": 16
    },
    {
      "login": "baskrahmer",
      "contributions": 12
    },
    {
      "login": "philschmid",
      "contributions": 10
    },
    {
      "login": "eaidova",
      "contributions": 9
    },
    {
      "login": "prathikr",
      "contributions": 8
    },
    {
      "login": "AdamLouly",
      "contributions": 8
    },
    {
      "login": "zhenglongjiepheonix",
      "contributions": 4
    },
    {
      "login": "changwangss",
      "contributions": 4
    }
  ],
  "file_tree_count": 427,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/bug-report.yml",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/config.yml",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/feature-request.yml",
      "type": "blob"
    },
    {
      "path": ".github/PULL_REQUEST_TEMPLATE.md",
      "type": "blob"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/build_main_documentation.yml",
      "type": "blob"
    },
    {
      "path": ".github/workflows/build_pr_documentation.yml",
      "type": "blob"
    },
    {
      "path": ".github/workflows/quality.yml",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 1456,
  "recent_issues": [
    {
      "number": 2300,
      "title": "Support for EuroBERT models",
      "state": "open"
    },
    {
      "number": 2299,
      "title": "remove timm from exporters extra",
      "state": "closed"
    },
    {
      "number": 2298,
      "title": " Move onnx and ort related code to optimum-onnx ",
      "state": "open"
    },
    {
      "number": 2297,
      "title": "Add task tflite register",
      "state": "open"
    },
    {
      "number": 2296,
      "title": "Fix taskmanager",
      "state": "closed"
    }
  ],
  "recent_pulls": [
    {
      "number": 2299,
      "title": "remove timm from exporters extra",
      "state": "closed"
    },
    {
      "number": 2298,
      "title": " Move onnx and ort related code to optimum-onnx ",
      "state": "open"
    },
    {
      "number": 2297,
      "title": "Add task tflite register",
      "state": "open"
    },
    {
      "number": 2296,
      "title": "Fix taskmanager",
      "state": "closed"
    },
    {
      "number": 2295,
      "title": "Fix 'Block pattern could not be match. Pass `block_name_to_quantize` argument in `quantize_model`' while loading Qwen VL GPTQ model",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "cf6d4cc1dbad8f79ea28b9ed6b9faeb537d2da7b",
      "author": "Ella Charlaix",
      "date": "2025-06-20T13:09:35+00:00",
      "message": "Remove timm from exporters extra (#2299)"
    },
    {
      "sha": "ed70d27e634fa285f807451d75c9bdcdce6f28c5",
      "author": "Arun Madhusudhanan",
      "date": "2025-06-19T16:17:33+00:00",
      "message": "Fix 'Block pattern could not be match. Pass `block_name_to_quantize` argument in `quantize_model`' while loading Qwen VL GPTQ model (#2295)"
    },
    {
      "sha": "824e3683f149432c1d49e6e0a5c90184ecb72720",
      "author": "Ella Charlaix",
      "date": "2025-06-19T16:08:14+00:00",
      "message": "Remove flatten_inputs from taskmanager (#2296)"
    },
    {
      "sha": "be76dde503d125b6a21cf1d3bec99e1185f86ba8",
      "author": "Ella Charlaix",
      "date": "2025-06-18T13:17:28+00:00",
      "message": "ExporterConfig refactorization (#2157)"
    },
    {
      "sha": "05aef4199a4fcb9bd4ba004653f568f7639f2dcc",
      "author": "Ella Charlaix",
      "date": "2025-06-17T14:41:26+00:00",
      "message": "Upgrade windows runner image (#2294)"
    },
    {
      "sha": "5246fc84f2c846cc17292733ac2b52db3a6f65ed",
      "author": "Ilyas Moutawwakil",
      "date": "2025-06-17T14:05:02+00:00",
      "message": "No more forcing separators (#2279)"
    },
    {
      "sha": "de1034759d808a33a71ad388dd14efcbce59a36e",
      "author": "Ella Charlaix",
      "date": "2025-06-17T11:35:35+00:00",
      "message": "Add task onnx register (#2291)"
    },
    {
      "sha": "0c6dbd48bbf394905613fdba03dd931679cd62dc",
      "author": "dependabot[bot]",
      "date": "2025-06-17T08:14:11+00:00",
      "message": "Bump protobuf for onnxruntime training example (#2292)"
    },
    {
      "sha": "dba0386a1d196714e4198d0d88e057e43abfa361",
      "author": "Ella Charlaix",
      "date": "2025-06-13T10:10:45+00:00",
      "message": "Add back from_transformers for base model (#2288)"
    },
    {
      "sha": "0b252b3dc5941023d25998e9f493b07abe183008",
      "author": "Ella Charlaix",
      "date": "2025-06-13T08:40:54+00:00",
      "message": "Dev version (#2287)"
    },
    {
      "sha": "d4ad6ec3c921803cf5578a253875d719626ec53b",
      "author": "Ella Charlaix",
      "date": "2025-06-11T14:37:23+00:00",
      "message": "Add style bot (#2286)"
    },
    {
      "sha": "e7c807bc2f20cf7c15a98aa931953b182aa0c0b9",
      "author": "amas0",
      "date": "2025-06-11T13:43:48+00:00",
      "message": "Add ONNX export optimization support for ModernBERT (#2208)"
    },
    {
      "sha": "74f2345bef8a3466f717eb05607704a4ca990db2",
      "author": "Joshua Lochner",
      "date": "2025-06-11T13:39:15+00:00",
      "message": "Add ONNX export support for Chinese CLIP (#1591)"
    },
    {
      "sha": "76ddcba844a75d2e9ccb4c693cf9688bfeaa9690",
      "author": "Ella Charlaix",
      "date": "2025-06-10T08:00:39+00:00",
      "message": "Furiosa documentation (#2285)"
    },
    {
      "sha": "297a6a07383c2e24e71d4118c828b346c767442d",
      "author": "Gerard",
      "date": "2025-06-06T18:25:29+00:00",
      "message": "Add ONNX support for InternLM2 (#2244)"
    },
    {
      "sha": "1c7012f1818115e5b0c7be9034942d94486ae72f",
      "author": "Gangin Park",
      "date": "2025-06-06T18:21:57+00:00",
      "message": "Add ONNX exporter support for ColPali model (#2251)"
    },
    {
      "sha": "85376e337681b1db6aa9752cc4f592b56eedb85e",
      "author": "Ilyas Moutawwakil",
      "date": "2025-05-28T08:51:40+00:00",
      "message": "Distribute and complete onnxruntime tests (decoder models) (#2278)"
    },
    {
      "sha": "93c284a8bf9ddf3acb4eba6b04e9f67353eb8408",
      "author": "Joshua Lochner",
      "date": "2025-05-27T15:41:26+00:00",
      "message": "Add ONNX export support for D-FINE (#2249)"
    },
    {
      "sha": "da5200b6f931a7f31e674e18df348d98a37a9d57",
      "author": "Ella Charlaix",
      "date": "2025-05-27T14:42:23+00:00",
      "message": "Add compatibility with transformers 4.52 (#2270)"
    },
    {
      "sha": "e15053d33e60f42bb87389a869c3a9d823ea972f",
      "author": "Ilyas Moutawwakil",
      "date": "2025-05-20T09:08:14+00:00",
      "message": "Fix and uniformize hub kwargs (#2276)"
    }
  ],
  "readme_text": "<!---\nCopyright 2025 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<h1 align=\"center\"><p>\ud83e\udd17 Optimum</p></h1>\n\n<p align=\"center\">\n<a href=\"https://pypi.org/project/optimum/\"><img alt=\"PyPI - License\" src=\"https://img.shields.io/pypi/l/optimum\"/></a>\n<a href=\"https://pypi.org/project/optimum/\"><img alt=\"PyPI - Python Version\" src=\"https://img.shields.io/pypi/pyversions/optimum\"/></a>\n<a href=\"https://pypi.org/project/optimum/\"><img alt=\"PyPI - Version\" src=\"https://img.shields.io/pypi/v/optimum\"/></a>\n<a href=\"https://pypi.org/project/optimum/\"><img alt=\"PyPI - Downloads\" src=\"https://img.shields.io/pypi/dm/optimum\"/></a>\n<a href=\"https://huggingface.co/docs/optimum/index\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/optimum/index.svg?down_color=red&down_message=offline&up_message=online\"/></a>\n</p>\n\n<p align=\"center\">\nOptimum is an extension of Transformers \ud83e\udd16 Diffusers \ud83e\udde8 TIMM \ud83d\uddbc\ufe0f and Sentence-Transformers \ud83e\udd17, providing a set of optimization tools and enabling maximum efficiency to train and run models on targeted hardware, while keeping things easy to use.\n</p>\n\n## Installation\n\nOptimum can be installed using `pip` as follows:\n\n```bash\npython -m pip install optimum\n```\n\nIf you'd like to use the accelerator-specific features of Optimum, you can check the documentation and install the required dependencies according to the table below:\n\n| Accelerator                                                                         | Installation                                                                |\n| :---------------------------------------------------------------------------------- | :-------------------------------------------------------------------------- |\n| [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/overview)            | `pip install --upgrade --upgrade-strategy eager optimum[onnxruntime]`       |\n| [Intel Neural Compressor](https://huggingface.co/docs/optimum/intel/index)          | `pip install --upgrade --upgrade-strategy eager optimum[neural-compressor]` |\n| [OpenVINO](https://huggingface.co/docs/optimum/intel/index)                         | `pip install --upgrade --upgrade-strategy eager optimum[openvino]`          |\n| [IPEX](https://huggingface.co/docs/optimum/intel/ipex/inference)                    | `pip install --upgrade --upgrade-strategy eager optimum[ipex]`              |\n| [NVIDIA TensorRT-LLM](https://huggingface.co/docs/optimum/main/en/nvidia_overview)  | `docker run -it --gpus all --ipc host huggingface/optimum-nvidia`           |\n| [AMD Instinct GPUs and Ryzen AI NPU](https://huggingface.co/docs/optimum/amd/index) | `pip install --upgrade --upgrade-strategy eager optimum[amd]`               |\n| [AWS Trainum & Inferentia](https://huggingface.co/docs/optimum-neuron/index)        | `pip install --upgrade --upgrade-strategy eager optimum[neuronx]`           |\n| [Intel Gaudi Accelerators (HPU)](https://huggingface.co/docs/optimum/habana/index)  | `pip install --upgrade --upgrade-strategy eager optimum[habana]`            |\n| [FuriosaAI](https://huggingface.co/docs/optimum/furiosa/index)                      | `pip install --upgrade --upgrade-strategy eager optimum[furiosa]`           |\n\nThe `--upgrade --upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version.\n\nTo install from source:\n\n```bash\npython -m pip install git+https://github.com/huggingface/optimum.git\n```\n\nFor the accelerator-specific features, append `optimum[accelerator_type]` to the above command:\n\n```bash\npython -m pip install optimum[onnxruntime]@git+https://github.com/huggingface/optimum.git\n```\n\n## Accelerated Inference\n\nOptimum provides multiple tools to export and run optimized models on various ecosystems:\n\n- [ONNX](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model) / [ONNX Runtime](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models), one of the most popular open formats for model export, and a high-performance inference engine for deployment.\n- [OpenVINO](https://huggingface.co/docs/optimum/intel/inference), a toolkit for optimizing, quantizing and deploying deep learning models on Intel hardware.\n- [ExecuTorch](https://huggingface.co/docs/optimum-executorch/guides/export), PyTorch\u2019s native solution for on-device inference across mobile and edge devices.\n- [TensorFlow Lite](https://huggingface.co/docs/optimum/exporters/tflite/usage_guides/export_a_model), a lightweight solution for running TensorFlow models on mobile and edge.\n- [Intel Gaudi Accelerators](https://huggingface.co/docs/optimum/main/en/habana/usage_guides/accelerate_inference) enabling optimal performance on first-gen Gaudi, Gaudi2 and Gaudi3.\n- [AWS Inferentia](https://huggingface.co/docs/optimum-neuron/en/guides/models) for accelerated inference on Inf2 and Inf1 instances.\n- [NVIDIA TensorRT-LLM](https://huggingface.co/blog/optimum-nvidia).\n\nThe [export](https://huggingface.co/docs/optimum/exporters/overview) and optimizations can be done both programmatically and with a command line.\n\n### ONNX + ONNX Runtime\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install optimum[exporters,onnxruntime]\n```\n\nIt is possible to export Transformers and Diffusers models to the [ONNX](https://onnx.ai/) format and perform graph optimization as well as quantization easily.\n\nFor more information on the ONNX export, please check the [documentation](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model).\n\nOnce the model is exported to the ONNX format, we provide Python classes enabling you to run the exported ONNX model in a seemless manner using [ONNX Runtime](https://onnxruntime.ai/) in the backend.\n\nMore details on how to run ONNX models with `ORTModelForXXX` classes [here](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/models).\n\n### Intel (OpenVINO + Neural Compressor + IPEX)\n\nBefore you begin, make sure you have all the necessary [libraries installed](https://huggingface.co/docs/optimum/main/en/intel/installation).\n\nYou can find more information on the different integration in our [documentation](https://huggingface.co/docs/optimum/main/en/intel/index) and in the examples of [`optimum-intel`](https://github.com/huggingface/optimum-intel).\n\n### ExecuTorch\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install optimum-executorch@git+https://github.com/huggingface/optimum-executorch.git\n```\n\nUsers can export Transformers models to [ExecuTorch](https://github.com/pytorch/executorch) and run inference on edge devices within PyTorch's ecosystem.\n\nFor more information about export Transformers to ExecuTorch, please check the doc for [Optimum-ExecuTorch](https://huggingface.co/docs/optimum-executorch/guides/export).\n\n### TensorFlow Lite\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install optimum[exporters-tf]\n```\n\nJust as for ONNX, it is possible to export models to [TensorFlow Lite](https://www.tensorflow.org/lite) and quantize them.\nYou can find more information in our [documentation](https://huggingface.co/docs/optimum/main/exporters/tflite/usage_guides/export_a_model).\n\n### Quanto\n\n[Quanto](https://github.com/huggingface/optimum-quanto) is a pytorch quantization backend which allows you to quantize a model either using the python API or the `optimum-cli`.\n\nYou can see more details and [examples](https://github.com/huggingface/optimum-quanto/tree/main/examples) in the [Quanto](https://github.com/huggingface/optimum-quanto) repository.\n\n## Accelerated training\n\nOptimum provides wrappers around the original Transformers [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) to enable training on powerful hardware easily.\nWe support many providers:\n\n- [Intel Gaudi Accelerators (HPU)](https://huggingface.co/docs/optimum/main/en/habana/usage_guides/accelerate_training) enabling optimal performance on first-gen Gaudi, Gaudi2 and Gaudi3.\n- [AWS Trainium](https://huggingface.co/docs/optimum-neuron/training_tutorials/sft_lora_finetune_llm) for accelerated training on Trn1 and Trn1n instances.\n- ONNX Runtime (optimized for GPUs).\n\n### Intel Gaudi Accelerators\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install --upgrade --upgrade-strategy eager optimum[habana]\n```\n\nYou can find examples in the [documentation](https://huggingface.co/docs/optimum/habana/quickstart) and in the [examples](https://github.com/huggingface/optimum-habana/tree/main/examples).\n\n### AWS Trainium\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install --upgrade --upgrade-strategy eager optimum[neuronx]\n```\n\nYou can find examples in the [documentation](https://huggingface.co/docs/optimum-neuron/index) and in the [tutorials](https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_bert).\n\n### ONNX Runtime\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\n```bash\npip install optimum[onnxruntime-training]\n```\n\nYou can find examples in the [documentation](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/trainer) and in the [examples](https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training).\n",
  "external_links_in_readme": [
    "https://onnxruntime.ai/",
    "https://huggingface.co/docs/optimum/main/en/intel/installation",
    "https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model",
    "https://img.shields.io/pypi/l/optimum\"/></a>",
    "https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_bert",
    "https://huggingface.co/docs/optimum/exporters/overview",
    "https://huggingface.co/docs/optimum/amd/index",
    "https://img.shields.io/pypi/pyversions/optimum\"/></a>",
    "https://github.com/huggingface/optimum-habana/tree/main/examples",
    "https://github.com/pytorch/executorch",
    "https://github.com/huggingface/optimum-executorch.git",
    "https://huggingface.co/docs/optimum/onnxruntime/usage_guides/models",
    "https://github.com/huggingface/optimum-quanto",
    "https://huggingface.co/docs/optimum/onnxruntime/overview",
    "https://huggingface.co/docs/optimum-neuron/training_tutorials/sft_lora_finetune_llm",
    "https://img.shields.io/pypi/v/optimum\"/></a>",
    "https://huggingface.co/docs/optimum/furiosa/index",
    "https://huggingface.co/docs/optimum/main/exporters/tflite/usage_guides/export_a_model",
    "https://huggingface.co/docs/optimum/intel/inference",
    "https://huggingface.co/docs/optimum/habana/quickstart",
    "https://huggingface.co/docs/optimum/main/en/nvidia_overview",
    "https://huggingface.co/blog/optimum-nvidia",
    "https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/models",
    "https://huggingface.co/docs/transformers/main_classes/trainer",
    "https://github.com/huggingface/optimum-quanto/tree/main/examples",
    "https://huggingface.co/docs/optimum/onnxruntime/usage_guides/trainer",
    "https://img.shields.io/website/http/huggingface.co/docs/optimum/index.svg?down_color=red&down_message=offline&up_message=online\"/></a>",
    "https://huggingface.co/docs/optimum-executorch/guides/export",
    "https://huggingface.co/docs/optimum/main/en/intel/index",
    "https://huggingface.co/docs/optimum/index\"><img",
    "https://www.tensorflow.org/lite",
    "https://github.com/huggingface/optimum/tree/main/examples/onnxruntime/training",
    "https://github.com/huggingface/optimum-intel",
    "https://huggingface.co/docs/optimum-neuron/en/guides/models",
    "https://huggingface.co/docs/optimum/intel/ipex/inference",
    "https://onnx.ai/",
    "https://img.shields.io/pypi/dm/optimum\"/></a>",
    "https://pypi.org/project/optimum/\"><img",
    "https://github.com/huggingface/optimum.git",
    "https://huggingface.co/docs/optimum/main/en/habana/usage_guides/accelerate_training",
    "http://www.apache.org/licenses/LICENSE-2.0",
    "https://huggingface.co/docs/optimum/habana/index",
    "https://huggingface.co/docs/optimum/intel/index",
    "https://huggingface.co/docs/optimum-neuron/index",
    "https://huggingface.co/docs/optimum/main/en/habana/usage_guides/accelerate_inference",
    "https://huggingface.co/docs/optimum/exporters/tflite/usage_guides/export_a_model"
  ]
}
```

</details>


---

## Repository 2: mlfoundations/open_clip

# GitHub Repository Data

**Repository:** [mlfoundations/open_clip](https://github.com/mlfoundations/open_clip)

## Basic Information

- **Description:** An open source implementation of CLIP.
- **Created:** 2021-07-28T23:24:39+00:00
- **Last Updated:** 2025-06-21T21:40:03+00:00
- **Last Pushed:** 2025-06-10T00:04:08+00:00
- **Default Branch:** main
- **Size:** 15777 KB

## Statistics

- **Stars:** 11,994
- **Forks:** 1,120
- **Watchers:** 11,994
- **Open Issues:** 83
- **Total Issues:** 0
- **Pull Requests:** 389

## License

- **Type:** Other
- **SPDX ID:** NOASSERTION
- **URL:** [License](https://github.com/mlfoundations/open_clip/blob/main/LICENSE)

## Languages

- **Python:** 456,132 bytes
- **Shell:** 2,457 bytes
- **Makefile:** 403 bytes

## Topics

- `deep-learning`
- `pytorch`
- `computer-vision`
- `language-model`
- `multi-modal-learning`
- `contrastive-loss`
- `zero-shot-classification`
- `pretrained-models`

## Top Contributors

1. **rwightman** - 255 contributions
2. **rom1504** - 64 contributions
3. **gabrielilharco** - 55 contributions
4. **mitchellnw** - 41 contributions
5. **Zasder3** - 31 contributions
6. **iejMac** - 25 contributions
7. **bryant1410** - 12 contributions
8. **gpucce** - 11 contributions
9. **EIFY** - 8 contributions
10. **lopho** - 7 contributions

## File Structure (Sample of 10 files)

Total files: 245

- `.gitattributes` (blob)
- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/ci.yml` (blob)
- `.github/workflows/clear-cache.yml` (blob)
- `.github/workflows/python-publish.yml` (blob)
- `.gitignore` (blob)
- `CITATION.cff` (blob)
- `HISTORY.md` (blob)
- `LICENSE` (blob)

## Recent Issues

- 游댮 **#1087** RuntimeError: : NCCL communicator was aborted on rank 3 (closed)
- 游릭 **#1086** Wire up custom attention block via config (open)
- 游댮 **#1085** Fix an issue where CustomText models not gettin proper pos embed interpolation (closed)
- 游릭 **#1084** An alternative text masking helper fn (open)
- 游댮 **#1083** Bug Report: KeyError: 'filepath' when using --csv-img-key filepath (closed)

## Recent Pull Requests

- 游릭 **#1086** Wire up custom attention block via config (open)
- 游댮 **#1085** Fix an issue where CustomText models not gettin proper pos embed interpolation (closed)
- 游릭 **#1084** An alternative text masking helper fn (open)
- 游댮 **#1082** fix CustomTextCLIP.no_weight_decay (closed)
- 游릭 **#1081** Alternative tokenizer support for CLIPS (open)

## Recent Commits

- **a87f11ea** Fix an issue where CustomText models not gettin proper pos embed interpolation. Make errors on main load_checkpoint failure more severe local-dir refactor. - Ross Wightman (2025-06-03T00:33:53+00:00)
- **131f46f6** fix CustomTextCLIP.no_weight_decay - thelaao (2025-05-27T13:42:53+00:00)
- **204d53e7** Add fix for logit_bias not passed through to get_logits in ClipLoss - Ross Wightman (2025-05-21T18:07:28+00:00)
- **d14a57f6** Add --force-context-length argument, pass through model factory fns. Add docstrings to create_model_and_transforms and create_model_from_pretrained fns. Pass through new pretrained tower args. - Ross Wightman (2025-05-21T17:33:26+00:00)
- **1a058f79** Remove some logs - Ross Wightman (2025-04-22T21:53:12+00:00)
- **d673e9e6** Update default weight loading logic - Ross Wightman (2025-04-17T15:20:31+00:00)
- **3f58c8bd** Move preprocess_cfg attach after jit. Pass through some new args - Ross Wightman (2025-04-16T22:52:40+00:00)
- **4c1b5455** Initial work on adding local-dir: schema for model & tokenizer loading (load weights from local folder). - Ross Wightman (2025-04-16T20:37:50+00:00)
- **2eca151d** Fix two small profiler things - Ross Wightman (2025-04-23T17:49:16+00:00)
- **bf5d49c1** Release 2.32.0 - Ross Wightman (2025-04-05T21:54:47+00:00)

## External Links Found in README

- https://github.com/openai/CLIP
- https://github.com/lucidrains/CoCa-pytorch
- https://github.com/lucidrains
- https://www.rohantaori.com/
- https://github.com/LAION-AI/CLIP_benchmark#how-to-use
- https://github.com/rom1504/img2dataset/blob/main/dataset_examples/cc3m.md
- https://github.com/rom1504
- https://github.com/webdataset/webdataset](https://github.com/webdataset/webdataset
- https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/tree/main](https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/blob/main/open_clip_pytorch_model.bin
- https://github.com/openai/CLIP/blob/main/data/yfcc100m.md
- https://arxiv.org/abs/2304.14108
- https://openreview.net/forum?id=M3Y74vmsMcY}
- http://vaishaal.com/
- https://arxiv.org/abs/2210.08402
- https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/clip_zeroshot.png
- https://arxiv.org/abs/2212.00794">Recent
- https://arxiv.org/abs/2309.17425
- https://arxiv.org/abs/2205.01917
- https://arxiv.org/abs/2303.15343
- https://homes.cs.washington.edu/~ali/

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 390536799,
  "name": "open_clip",
  "full_name": "mlfoundations/open_clip",
  "description": "An open source implementation of CLIP.",
  "html_url": "https://github.com/mlfoundations/open_clip",
  "clone_url": "https://github.com/mlfoundations/open_clip.git",
  "ssh_url": "git@github.com:mlfoundations/open_clip.git",
  "homepage": "",
  "topics": [
    "deep-learning",
    "pytorch",
    "computer-vision",
    "language-model",
    "multi-modal-learning",
    "contrastive-loss",
    "zero-shot-classification",
    "pretrained-models"
  ],
  "default_branch": "main",
  "created_at": "2021-07-28T23:24:39+00:00",
  "updated_at": "2025-06-21T21:40:03+00:00",
  "pushed_at": "2025-06-10T00:04:08+00:00",
  "size_kb": 15777,
  "watchers_count": 11994,
  "stargazers_count": 11994,
  "forks_count": 1120,
  "open_issues_count": 83,
  "license": {
    "key": "other",
    "name": "Other",
    "spdx_id": "NOASSERTION",
    "url": "https://github.com/mlfoundations/open_clip/blob/main/LICENSE"
  },
  "languages": {
    "Python": 456132,
    "Shell": 2457,
    "Makefile": 403
  },
  "top_contributors": [
    {
      "login": "rwightman",
      "contributions": 255
    },
    {
      "login": "rom1504",
      "contributions": 64
    },
    {
      "login": "gabrielilharco",
      "contributions": 55
    },
    {
      "login": "mitchellnw",
      "contributions": 41
    },
    {
      "login": "Zasder3",
      "contributions": 31
    },
    {
      "login": "iejMac",
      "contributions": 25
    },
    {
      "login": "bryant1410",
      "contributions": 12
    },
    {
      "login": "gpucce",
      "contributions": 11
    },
    {
      "login": "EIFY",
      "contributions": 8
    },
    {
      "login": "lopho",
      "contributions": 7
    },
    {
      "login": "visheratin",
      "contributions": 6
    },
    {
      "login": "zw615",
      "contributions": 5
    },
    {
      "login": "carlini",
      "contributions": 5
    },
    {
      "login": "JeniaJitsev",
      "contributions": 5
    },
    {
      "login": "mehdidc",
      "contributions": 4
    },
    {
      "login": "lucidrains",
      "contributions": 4
    },
    {
      "login": "MengqingCao",
      "contributions": 4
    },
    {
      "login": "Vaishaal",
      "contributions": 4
    },
    {
      "login": "ludwigschmidt",
      "contributions": 3
    },
    {
      "login": "xwen99",
      "contributions": 2
    }
  ],
  "file_tree_count": 245,
  "file_tree_sample": [
    {
      "path": ".gitattributes",
      "type": "blob"
    },
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/ci.yml",
      "type": "blob"
    },
    {
      "path": ".github/workflows/clear-cache.yml",
      "type": "blob"
    },
    {
      "path": ".github/workflows/python-publish.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "CITATION.cff",
      "type": "blob"
    },
    {
      "path": "HISTORY.md",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 389,
  "recent_issues": [
    {
      "number": 1087,
      "title": "RuntimeError: : NCCL communicator was aborted on rank 3",
      "state": "closed"
    },
    {
      "number": 1086,
      "title": "Wire up custom attention block via config",
      "state": "open"
    },
    {
      "number": 1085,
      "title": "Fix an issue where CustomText models not gettin proper pos embed interpolation",
      "state": "closed"
    },
    {
      "number": 1084,
      "title": "An alternative text masking helper fn",
      "state": "open"
    },
    {
      "number": 1083,
      "title": "Bug Report: KeyError: 'filepath' when using --csv-img-key filepath",
      "state": "closed"
    }
  ],
  "recent_pulls": [
    {
      "number": 1086,
      "title": "Wire up custom attention block via config",
      "state": "open"
    },
    {
      "number": 1085,
      "title": "Fix an issue where CustomText models not gettin proper pos embed interpolation",
      "state": "closed"
    },
    {
      "number": 1084,
      "title": "An alternative text masking helper fn",
      "state": "open"
    },
    {
      "number": 1082,
      "title": "fix CustomTextCLIP.no_weight_decay",
      "state": "closed"
    },
    {
      "number": 1081,
      "title": "Alternative tokenizer support for CLIPS",
      "state": "open"
    }
  ],
  "recent_commits": [
    {
      "sha": "a87f11eaf354000d2736580855ae0d9b76ad2a22",
      "author": "Ross Wightman",
      "date": "2025-06-03T00:33:53+00:00",
      "message": "Fix an issue where CustomText models not gettin proper pos embed interpolation. Make errors on main load_checkpoint failure more severe local-dir refactor."
    },
    {
      "sha": "131f46f6fc1c5caebdc6e7d6c487d33de5b85d2a",
      "author": "thelaao",
      "date": "2025-05-27T13:42:53+00:00",
      "message": "fix CustomTextCLIP.no_weight_decay"
    },
    {
      "sha": "204d53e76847cd34257ca927fd1c40cadbd70d50",
      "author": "Ross Wightman",
      "date": "2025-05-21T18:07:28+00:00",
      "message": "Add fix for logit_bias not passed through to get_logits in ClipLoss"
    },
    {
      "sha": "d14a57f6593fbec46fe0be8382a44bf7cfe64ba1",
      "author": "Ross Wightman",
      "date": "2025-05-21T17:33:26+00:00",
      "message": "Add --force-context-length argument, pass through model factory fns. Add docstrings to create_model_and_transforms and create_model_from_pretrained fns. Pass through new pretrained tower args."
    },
    {
      "sha": "1a058f7919bd59097dcd4753999fb8ec72bf6b27",
      "author": "Ross Wightman",
      "date": "2025-04-22T21:53:12+00:00",
      "message": "Remove some logs"
    },
    {
      "sha": "d673e9e64ed5bcb5137ce7983604084374237193",
      "author": "Ross Wightman",
      "date": "2025-04-17T15:20:31+00:00",
      "message": "Update default weight loading logic"
    },
    {
      "sha": "3f58c8bd6cc7acaa762400f705730d2e9bfca1bd",
      "author": "Ross Wightman",
      "date": "2025-04-16T22:52:40+00:00",
      "message": "Move preprocess_cfg attach after jit. Pass through some new args"
    },
    {
      "sha": "4c1b5455e363c924fc30d245e8cdc674c55ed71a",
      "author": "Ross Wightman",
      "date": "2025-04-16T20:37:50+00:00",
      "message": "Initial work on adding local-dir: schema for model & tokenizer loading (load weights from local folder)."
    },
    {
      "sha": "2eca151d2825075be52a3fe97a9da7939e657752",
      "author": "Ross Wightman",
      "date": "2025-04-23T17:49:16+00:00",
      "message": "Fix two small profiler things"
    },
    {
      "sha": "bf5d49c112c82c738f7b34bde6e154760a711790",
      "author": "Ross Wightman",
      "date": "2025-04-05T21:54:47+00:00",
      "message": "Release 2.32.0"
    },
    {
      "sha": "597ba80401e60f90ef3cddd78eff15c1b4dffa56",
      "author": "Ross Wightman",
      "date": "2025-04-05T20:59:53+00:00",
      "message": "Allow CLIP loss to be used with models that have logit bias"
    },
    {
      "sha": "7260a46e7b4bcf518f5200fea06da5bc85aae025",
      "author": "Jack T",
      "date": "2025-03-17T22:18:30+00:00",
      "message": "Updated README (#1050)"
    },
    {
      "sha": "c4fba838acbf4dc94736489164868c3c172a7433",
      "author": "Ross Wightman",
      "date": "2025-03-01T00:30:29+00:00",
      "message": "API for getting intermediate image and text features, forward_intermediates() (#1035)"
    },
    {
      "sha": "14b5c09c74060b23de6bf67ec0b2a5bfe79fce61",
      "author": "Ross Wightman",
      "date": "2025-02-24T06:06:39+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "614b75087cd3a435b31881d5de7fed1b562cdd58",
      "author": "Ross Wightman",
      "date": "2025-02-23T16:24:48+00:00",
      "message": "Release 2.31.0"
    },
    {
      "sha": "5eb34737ac6f38340eee4a91fe899d06a4339643",
      "author": "Ross Wightman",
      "date": "2025-02-23T05:08:07+00:00",
      "message": "Update requirements-training.txt"
    },
    {
      "sha": "71255462697a81d11ed51a81868817b797787823",
      "author": "Ross Wightman",
      "date": "2025-02-21T18:28:31+00:00",
      "message": "Update SigLIP2 pretrained and tokenizer entries for uploaded weights"
    },
    {
      "sha": "f53b44bff560b6d169a1139e949020117ca79f4a",
      "author": "Ross Wightman",
      "date": "2025-02-20T20:16:01+00:00",
      "message": "Change deprecated upload-artifact@v3 -> v4"
    },
    {
      "sha": "008046c6b9c96e664f164efcfc75c352fe0f7d56",
      "author": "Ross Wightman",
      "date": "2025-02-20T16:52:11+00:00",
      "message": "Add SigLIP2 models"
    },
    {
      "sha": "63fbfd857c83af619e6a0a8344635ebb4a151a96",
      "author": "Ross Wightman",
      "date": "2025-01-04T16:56:32+00:00",
      "message": "Update README.md"
    }
  ],
  "readme_text": "# OpenCLIP\n\n[[Paper]](https://arxiv.org/abs/2212.07143) [[Citations]](#citing) [[Clip Colab]](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb) [[Coca Colab]](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb)\n[![pypi](https://img.shields.io/pypi/v/open_clip_torch.svg)](https://pypi.python.org/pypi/open_clip_torch)\n\nWelcome to an open source implementation of OpenAI's [CLIP](https://arxiv.org/abs/2103.00020) (Contrastive Language-Image Pre-training).\n\nUsing this codebase, we have trained several models on a variety of data sources and compute budgets, ranging from [small-scale experiments](docs/LOW_ACC.md) to larger runs including models trained on datasets such as [LAION-400M](https://arxiv.org/abs/2111.02114), [LAION-2B](https://arxiv.org/abs/2210.08402) and [DataComp-1B](https://arxiv.org/abs/2304.14108).\nMany of our models and their scaling properties are studied in detail in the paper [reproducible scaling laws for contrastive language-image learning](https://arxiv.org/abs/2212.07143).\nSome of the best models we've trained and their zero-shot ImageNet-1k accuracy are shown below, along with the ViT-L model trained by OpenAI and other state-of-the-art open source alternatives (all can be loaded via OpenCLIP).\nWe provide more details about our full collection of pretrained models [here](docs/PRETRAINED.md), and zero-shot results for 38 datasets [here](docs/openclip_results.csv).\n\n\n\n| Model    | Training data | Resolution | # of samples seen | ImageNet zero-shot acc. | \n| -------- | ------- |  ------- |  ------- |  ------- |  \n| ConvNext-Base | LAION-2B  | 256px | 13B | 71.5% |\n| ConvNext-Large | LAION-2B  | 320px | 29B | 76.9% |\n| ConvNext-XXLarge | LAION-2B | 256px | 34B | 79.5% |\n| ViT-B/32  | DataComp-1B  | 256px | 34B | 72.8% |\n| ViT-B/16  | DataComp-1B  | 224px | 13B | 73.5% |\n| ViT-L/14  | LAION-2B  | 224px | 32B | 75.3% |\n| ViT-H/14  | LAION-2B  | 224px | 32B | 78.0% |\n| ViT-L/14  | DataComp-1B  | 224px | 13B | 79.2% |\n| ViT-G/14  | LAION-2B  | 224px | 34B | 80.1% |\n|  |  |   |   |  |\n| ViT-L/14-quickgelu [(Original CLIP)](https://arxiv.org/abs/2103.00020) | WIT | 224px | 13B | 75.5% | \n| ViT-SO400M/14 [(SigLIP)](https://arxiv.org/abs/2303.15343) | WebLI | 224px | 45B | 82.0% | \n| ViT-L/14 [(DFN)](https://arxiv.org/abs/2309.17425) | DFN-2B | 224px | 39B | 82.2% | \n| ViT-SO400M-14-SigLIP-384 [(SigLIP)](https://arxiv.org/abs/2303.15343) |  WebLI | 384px | 45B | 83.1% |\n| ViT-H/14-quickgelu [(DFN)](https://arxiv.org/abs/2309.17425) | DFN-5B | 224px | 39B | 83.4% | \n| ViT-H-14-378-quickgelu [(DFN)](https://arxiv.org/abs/2309.17425) | DFN-5B | 378px | 44B | 84.4% |\n\nModel cards with additional model specific details can be found on the Hugging Face Hub under the OpenCLIP library tag: https://huggingface.co/models?library=open_clip. \n\nIf you found this repository useful, please consider [citing](#citing).\nWe welcome anyone to submit an issue or send an email if you have any other requests or suggestions.\n\nNote that portions of `src/open_clip/` modelling and tokenizer code are adaptations of OpenAI's official [repository](https://github.com/openai/CLIP).\n\n## Approach\n\n| ![CLIP](https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/CLIP.png) |\n|:--:|\n| Image Credit: https://github.com/openai/CLIP |\n\n## Usage\n\n```\npip install open_clip_torch\n```\n\n```python\nimport torch\nfrom PIL import Image\nimport open_clip\n\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\nmodel.eval()  # model in train mode by default, impacts some models with BatchNorm or stochastic depth active\ntokenizer = open_clip.get_tokenizer('ViT-B-32')\n\nimage = preprocess(Image.open(\"docs/CLIP.png\")).unsqueeze(0)\ntext = tokenizer([\"a diagram\", \"a dog\", \"a cat\"])\n\nwith torch.no_grad(), torch.autocast(\"cuda\"):\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n\n    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n\nprint(\"Label probs:\", text_probs)  # prints: [[1., 0., 0.]]\n```\n\nIf model uses `timm` image encoders (convnext, siglip, eva, etc) ensure the latest timm is installed. Upgrade `timm` if you see 'Unknown model' errors for the image encoder.\n\nIf model uses transformers tokenizers, ensure `transformers` is installed.\n\nSee also this [[Clip Colab]](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb).\n\nTo compute billions of embeddings efficiently, you can use [clip-retrieval](https://github.com/rom1504/clip-retrieval) which has openclip support.\n\n### Pretrained models\n\nWe offer a simple model interface to instantiate both pre-trained and untrained models.\nTo see which pretrained models are available, use the following code snippet.\nMore details about our pretrained models are available [here](docs/PRETRAINED.md).\n\n```python\n>>> import open_clip\n>>> open_clip.list_pretrained()\n```\n\nYou can find more about the models we support (e.g. number of parameters, FLOPs) in [this table](docs/model_profile.csv).\n\nNOTE: Many existing checkpoints use the QuickGELU activation from the original OpenAI models. This activation is actually less efficient than native torch.nn.GELU in recent versions of PyTorch. The model defaults are now nn.GELU, so one should use model definitions with `-quickgelu` postfix for the OpenCLIP pretrained weights. All OpenAI pretrained weights will always default to QuickGELU. One can also use the non `-quickgelu` model definitions with pretrained weights using QuickGELU but there will be an accuracy drop, for fine-tune that will likely vanish for longer runs.\nFuture trained models will use nn.GELU.\n\n### Loading models\n\nModels can be loaded with `open_clip.create_model_and_transforms`, as shown in the example below. The model name and corresponding `pretrained` keys are compatible with the outputs of `open_clip.list_pretrained()`. \n\nThe `pretrained` argument also accepts local paths, for example `/path/to/my/b32.pt`.\nYou can also load checkpoints from huggingface this way. To do so, download the `open_clip_pytorch_model.bin` file (for example, [https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/tree/main](https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/blob/main/open_clip_pytorch_model.bin)), and use `pretrained=/path/to/open_clip_pytorch_model.bin`.\n\n```python\n# pretrained also accepts local paths\nmodel, _, preprocess = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k') \n```\n\n## Fine-tuning on classification tasks\n\nThis repository is focused on training CLIP models. To fine-tune a *trained* zero-shot model on a downstream classification task such as ImageNet, please see [our other repository: WiSE-FT](https://github.com/mlfoundations/wise-ft). The [WiSE-FT repository](https://github.com/mlfoundations/wise-ft) contains code for our paper on [Robust Fine-tuning of Zero-shot Models](https://arxiv.org/abs/2109.01903), in which we introduce a technique for fine-tuning zero-shot models while preserving robustness under distribution shift.\n\n## Data\n\nTo download datasets as webdataset, we recommend [img2dataset](https://github.com/rom1504/img2dataset).\n\n### Conceptual Captions\n\nSee [cc3m img2dataset example](https://github.com/rom1504/img2dataset/blob/main/dataset_examples/cc3m.md).\n\n### YFCC and other datasets\n\nIn addition to specifying the training data via CSV files as mentioned above, our codebase also supports [webdataset](https://github.com/webdataset/webdataset), which is recommended for larger scale datasets. The expected format is a series of `.tar` files. Each of these `.tar` files should contain two files for each training example, one for the image and one for the corresponding text. Both files should have the same name but different extensions. For instance, `shard_001.tar` could contain files such as `abc.jpg` and `abc.txt`. You can learn more about `webdataset` at [https://github.com/webdataset/webdataset](https://github.com/webdataset/webdataset). We use `.tar` files with 1,000 data points each, which we create using [tarp](https://github.com/webdataset/tarp).\n\nYou can download the YFCC dataset from [Multimedia Commons](http://mmcommons.org/).\nSimilar to OpenAI, we used a subset of YFCC to reach the aforementioned accuracy numbers.\nThe indices of images in this subset are in [OpenAI's CLIP repository](https://github.com/openai/CLIP/blob/main/data/yfcc100m.md).\n\n\n## Training CLIP\n\n### Install\n\nWe advise you first create a virtual environment with:\n\n```\npython3 -m venv .env\nsource .env/bin/activate\npip install -U pip\n```\n\nYou can then install openclip for training with `pip install 'open_clip_torch[training]'`.\n\n#### Development\n\nIf you want to make changes to contribute code, you can clone openclip then run `make install` in openclip folder (after creating a virtualenv)\n\nInstall pip PyTorch as per https://pytorch.org/get-started/locally/\n\nYou may run `make install-training` to install training deps\n\n#### Testing\n\nTest can be run with `make install-test` then `make test`\n\n`python -m pytest -x -s -v tests -k \"training\"` to run a specific test\n\nRunning regression tests against a specific git revision or tag:\n1. Generate testing data\n    ```sh\n    python tests/util_test.py --model RN50 RN101 --save_model_list models.txt --git_revision 9d31b2ec4df6d8228f370ff20c8267ec6ba39383\n    ```\n    **_WARNING_: This will invoke git and modify your working tree, but will reset it to the current state after data has been generated! \\\n    Don't modify your working tree while test data is being generated this way.**\n\n2. Run regression tests\n    ```sh\n    OPEN_CLIP_TEST_REG_MODELS=models.txt python -m pytest -x -s -v -m regression_test\n    ```\n\n### Sample single-process running code:\n\n```bash\npython -m open_clip_train.main \\\n    --save-frequency 1 \\\n    --zeroshot-frequency 1 \\\n    --report-to tensorboard \\\n    --train-data=\"/path/to/train_data.csv\"  \\\n    --val-data=\"/path/to/validation_data.csv\"  \\\n    --csv-img-key filepath \\\n    --csv-caption-key title \\\n    --imagenet-val=/path/to/imagenet/root/val/ \\\n    --warmup 10000 \\\n    --batch-size=128 \\\n    --lr=1e-3 \\\n    --wd=0.1 \\\n    --epochs=30 \\\n    --workers=8 \\\n    --model RN50\n```\n\nNote: `imagenet-val` is the path to the *validation* set of ImageNet for zero-shot evaluation, not the training set!\nYou can remove this argument if you do not want to perform zero-shot evaluation on ImageNet throughout training. Note that the `val` folder should contain subfolders. If it does not, please use [this script](https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh).\n\n### Multi-GPU and Beyond\n\nThis code has been battle tested up to 1024 A100s and offers a variety of solutions\nfor distributed training. We include native support for SLURM clusters.\n\nAs the number of devices used to train increases, so does the space complexity of\nthe the logit matrix. Using a na\u00efve all-gather scheme, space complexity will be\n`O(n^2)`. Instead, complexity may become effectively linear if the flags\n`--gather-with-grad` and `--local-loss` are used. This alteration results in one-to-one\nnumerical results as the na\u00efve method.\n\n#### Epochs\n\nFor larger datasets (eg Laion2B), we recommend setting `--train-num-samples` to a lower value than the full epoch, for example `--train-num-samples 135646078` to 1/16 of an epoch in conjunction with `--dataset-resampled` to do sampling with replacement. This allows having frequent checkpoints to evaluate more often.\n\n#### Patch Dropout\n\n<a href=\"https://arxiv.org/abs/2212.00794\">Recent research</a> has shown that one can dropout half to three-quarters of the visual tokens, leading to up to 2-3x training speeds without loss of accuracy.\n\nYou can set this on your visual transformer config with the key `patch_dropout`.\n\nIn the paper, they also finetuned without the patch dropout at the end. You can do this with the command-line argument `--force-patch-dropout 0.`\n\n#### Multiple data sources\n\nOpenCLIP supports using multiple data sources, by separating different data paths with `::`.\nFor instance, to train on CC12M and on LAION, one might use `--train-data \"/data/cc12m/cc12m-train-{0000..2175}.tar::/data/LAION-400M/{00000..41455}.tar\"`.\nUsing `--dataset-resampled` is recommended for these cases.\n\nBy default, on expectation the amount of times the model will see a sample from each source is proportional to the size of the source.\nFor instance, when training on one data source with size 400M and one with size 10M, samples from the first source are 40x more likely to be seen in expectation.\n\nWe also support different weighting of the data sources, by using the `--train-data-upsampling-factors` flag.\nFor instance, using `--train-data-upsampling-factors=1::1` in the above scenario is equivalent to not using the flag, and `--train-data-upsampling-factors=1::2` is equivalent to upsampling the second data source twice.\nIf you want to sample from data sources with the same frequency, the upsampling factors should be inversely proportional to the sizes of the data sources.\nFor instance, if dataset `A` has 1000 samples and dataset `B` has 100 samples, you can use `--train-data-upsampling-factors=0.001::0.01` (or analogously, `--train-data-upsampling-factors=1::10`).\n\n#### Single-Node\n\nWe make use of `torchrun` to launch distributed jobs. The following launches a\na job on a node of 4 GPUs:\n\n```bash\ncd open_clip/src\ntorchrun --nproc_per_node 4 -m open_clip_train.main \\\n    --train-data '/data/cc12m/cc12m-train-{0000..2175}.tar' \\\n    --train-num-samples 10968539 \\\n    --dataset-type webdataset \\\n    --batch-size 320 \\\n    --precision amp \\\n    --workers 4 \\\n    --imagenet-val /data/imagenet/validation/\n```\n\n#### Multi-Node\n\nThe same script above works, so long as users include information about the number\nof nodes and host node.\n\n```bash\ncd open_clip/src\ntorchrun --nproc_per_node=4 \\\n    --rdzv_endpoint=$HOSTE_NODE_ADDR \\\n    -m open_clip_train.main \\\n    --train-data '/data/cc12m/cc12m-train-{0000..2175}.tar' \\\n    --train-num-samples 10968539 \\\n    --dataset-type webdataset \\\n    --batch-size 320 \\\n    --precision amp \\\n    --workers 4 \\\n    --imagenet-val /data/imagenet/validation/\n```\n\n#### SLURM\n\nThis is likely the easiest solution to utilize. The following script was used to\ntrain our largest models:\n\n```bash\n#!/bin/bash -x\n#SBATCH --nodes=32\n#SBATCH --gres=gpu:4\n#SBATCH --ntasks-per-node=4\n#SBATCH --cpus-per-task=6\n#SBATCH --wait-all-nodes=1\n#SBATCH --job-name=open_clip\n#SBATCH --account=ACCOUNT_NAME\n#SBATCH --partition PARTITION_NAME\n\neval \"$(/path/to/conda/bin/conda shell.bash hook)\" # init conda\nconda activate open_clip\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\nexport MASTER_PORT=12802\n\nmaster_addr=$(scontrol show hostnames \"$SLURM_JOB_NODELIST\" | head -n 1)\nexport MASTER_ADDR=$master_addr\n\ncd /shared/open_clip\nexport PYTHONPATH=\"$PYTHONPATH:$PWD/src\"\nsrun --cpu_bind=v --accel-bind=gn python -u src/open_clip_train/main.py \\\n    --save-frequency 1 \\\n    --report-to tensorboard \\\n    --train-data=\"/data/LAION-400M/{00000..41455}.tar\" \\\n    --warmup 2000 \\\n    --batch-size=256 \\\n    --epochs=32 \\\n    --workers=8 \\\n    --model ViT-B-32 \\\n    --name \"ViT-B-32-Vanilla\" \\\n    --seed 0 \\\n    --local-loss \\\n    --gather-with-grad\n```\n\n### Resuming from a checkpoint:\n\n```bash\npython -m open_clip_train.main \\\n    --train-data=\"/path/to/train_data.csv\" \\\n    --val-data=\"/path/to/validation_data.csv\"  \\\n    --resume /path/to/checkpoints/epoch_K.pt\n```\n\n### Training CoCa:\nTraining [CoCa](https://arxiv.org/abs/2205.01917) models is enabled through specifying a CoCa config using the ```--model``` parameter of the training script. Currently available configs are \"coca_base\", \"coca_ViT-B-32\", and \"coca_roberta-ViT-B-32\" (which uses RoBERTa as the text encoder). CoCa configs are different from CLIP configs because they have an additional \"multimodal_cfg\" component which specifies parameters for the multimodal text decoder. Here's an example from the coca_ViT-B-32 config:\n```json\n\"multimodal_cfg\": {\n\t\"context_length\": 76,\n\t\"vocab_size\": 49408,\n\t\"width\": 512,\n\t\"heads\": 8,\n\t\"layers\": 12,\n\t\"latent_dim\": 512,\n\t\"attn_pooler_heads\": 8\n}\n```\nCredit to [lucidrains](https://github.com/lucidrains) for [initial code](https://github.com/lucidrains/CoCa-pytorch), [gpucce](https://github.com/gpucce) for adapting the code to open_clip, and [iejMac](https://github.com/iejMac) for training the models.\n\n### Generating text with CoCa\n\n```python\nimport open_clip\nimport torch\nfrom PIL import Image\n\nmodel, _, transform = open_clip.create_model_and_transforms(\n  model_name=\"coca_ViT-L-14\",\n  pretrained=\"mscoco_finetuned_laion2B-s13B-b90k\"\n)\n\nim = Image.open(\"cat.jpg\").convert(\"RGB\")\nim = transform(im).unsqueeze(0)\n\nwith torch.no_grad(), torch.cuda.amp.autocast():\n  generated = model.generate(im)\n\nprint(open_clip.decode(generated[0]).split(\"<end_of_text>\")[0].replace(\"<start_of_text>\", \"\"))\n```\n\nSee also this [[Coca Colab]](https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb)\n\n### Fine Tuning CoCa\n\nTo fine-tune coca on mscoco, first create the dataset, one way is using a csvdataset and perhaps the simplest way to do it is using [CLIP_benchmark](https://github.com/LAION-AI/CLIP_benchmark) which in turn uses [pycocotools](https://github.com/cocodataset/cocoapi) (that can be used also by itself).\n\n```python\nfrom clip_benchmark.datasets.builder import build_dataset\nimport pandas as pd\nimport os\n\nroot_path = \"path/to/data/dir\" # set this to smth meaningful\nds = build_dataset(\"mscoco_captions\", root=root_path, split=\"train\", task=\"captioning\") # this downloads the dataset if it is not there already\ncoco = ds.coco\nimgs = coco.loadImgs(coco.getImgIds())\nfuture_df = {\"filepath\":[], \"title\":[]}\nfor img in imgs:\n    caps = coco.imgToAnns[img[\"id\"]]\n    for cap in caps:\n        future_df[\"filepath\"].append(img[\"file_name\"])\n        future_df[\"title\"].append(cap[\"caption\"])\npd.DataFrame.from_dict(future_df).to_csv(\n  os.path.join(root_path, \"train2014.csv\"), index=False, sep=\"\\t\"\n)\n```\nThis should create a csv dataset that one can use to fine-tune coca with open_clip\n```bash\npython -m open_clip_train.main \\\n    --dataset-type \"csv\" \\\n    --train-data \"path/to/data/dir/train2014.csv\" \\\n    --warmup 1000 \\\n    --batch-size 128 \\\n    --lr 1e-5 \\\n    --wd 0.1 \\\n    --epochs 1 \\\n    --workers 3 \\\n    --model \"coca_ViT-L-14\" \\\n    --report-to \"wandb\" \\\n    --coca-contrastive-loss-weight 0 \\\n    --coca-caption-loss-weight 1 \\\n    --log-every-n-steps 100\n```\n\nThis is a general setting, open_clip has very parameters that can be set, ```python -m open_clip_train.main --help``` should show them. The only relevant change compared to pre-training are the two arguments\n\n```bash\n--coca-contrastive-loss-weight 0\n--coca-caption-loss-weight 1\n```\nwhich make the model only train the generative side.\n\n### Training with pre-trained language models as text encoder:\n\nIf you wish to use different language models as the text encoder for CLIP you can do so by using one of the Hugging Face model configs in ```src/open_clip/model_configs``` and passing in it's tokenizer as the ```--model``` and ```--hf-tokenizer-name``` parameters respectively. Currently we only support RoBERTa (\"test-roberta\" config), however adding new models should be trivial. You can also determine how many layers, from the end, to leave unfrozen with the ```--lock-text-unlocked-layers``` parameter. Here's an example command to train CLIP with the RoBERTa LM that has it's last 10 layers unfrozen:\n```bash\npython -m open_clip_train.main \\\n         --train-data=\"pipe:aws s3 cp s3://s-mas/cc3m/{00000..00329}.tar -\" \\\n         --train-num-samples 3000000 \\\n         --val-data=\"pipe:aws s3 cp s3://s-mas/cc3m/{00330..00331}.tar -\" \\\n         --val-num-samples 10000 \\\n         --dataset-type webdataset \\\n         --batch-size 256 \\\n         --warmup 2000 \\\n         --epochs 10 \\\n         --lr 5e-4 \\\n         --precision amp \\\n         --workers 6 \\\n         --model \"roberta-ViT-B-32\" \\\n         --lock-text \\\n         --lock-text-unlocked-layers 10 \\\n         --name \"10_unfrozen\" \\\n         --report-to \"tensorboard\" \\\n```\n\n### Loss Curves\n\nWhen run on a machine with 8 GPUs the command should produce the following training curve for Conceptual Captions:\n\n![CLIP zero shot training curve](https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/clip_zeroshot.png)\n\nMore detailed curves for Conceptual Captions are given at [/docs/clip_conceptual_captions.md](/docs/clip_conceptual_captions.md).\n\nWhen training a RN50 on YFCC the same hyperparameters as above are used, with the exception of `lr=5e-4` and `epochs=32`.\n\nNote that to use another model, like `ViT-B/32` or `RN50x4` or `RN50x16` or `ViT-B/16`, specify with `--model RN50x4`.\n\n### Logging\n\nFor tensorboard logging, run:\n```bash\ntensorboard --logdir=logs/tensorboard/ --port=7777\n```\n\nFor wandb logging, we recommend looking at the `step` variable instead of `Step`, since the later was not properly set in earlier versions of this codebase.\nFor older runs with models trained before https://github.com/mlfoundations/open_clip/pull/613, the `Step` variable should be ignored.\nFor newer runs, after that PR, the two variables are the same.\n\n## Evaluation / Zero-Shot\n\nWe recommend https://github.com/LAION-AI/CLIP_benchmark#how-to-use for systematic evaluation on 40 datasets.\n\n### Evaluating local checkpoint:\n\n```bash\npython -m open_clip_train.main \\\n    --val-data=\"/path/to/validation_data.csv\"  \\\n    --model RN101 \\\n    --pretrained /path/to/checkpoints/epoch_K.pt\n```\n\n### Evaluating hosted pretrained checkpoint on ImageNet zero-shot prediction:\n\n```bash\npython -m open_clip_train.main \\\n    --imagenet-val /path/to/imagenet/validation \\\n    --model ViT-B-32-quickgelu \\\n    --pretrained laion400m_e32\n```\n\n### Model distillation\n\nYou can distill from a pre-trained by using `--distill-model` and `--distill-pretrained` to specify the model you'd like to distill from.\nFor instance, to distill from OpenAI ViT-L/14 use `--distill-model ViT-L-14 --distill-pretrained openai`.\n\n### Gradient accumulation\n\nTo simulate larger batches use `--accum-freq k`. If per gpu batch size, `--batch-size`, is `m`, then the effective batch size will be `k * m * num_gpus`.\n\nWhen increasing `--accum-freq` from its default of 1, samples/s will remain approximately constant (batch size will double, as will time-per-batch). It is recommended to use other features to reduce batch size such as `--grad-checkpointing --local-loss --gather-with-grad` before increasing `--accum-freq`. `--accum-freq` can be used in addition to these features.\n\nInstead of 1 forward pass per example, there are now 2 forward passes per-example. However, the first is done with `torch.no_grad`.\n\nThere is some additional GPU memory required --- the features and data from all `m` batches are stored in memory.\n\nThere are also `m` loss computations instead of the usual 1.\n\nFor more information see Cui et al. (https://arxiv.org/abs/2112.09331) or Pham et al. (https://arxiv.org/abs/2111.10050).\n\n### Int8 Support\n\nWe have beta support for int8 training and inference.\nYou can enable int8 training with `--use-bnb-linear SwitchBackLinearGlobal` or `--use-bnb-linear SwitchBackLinearGlobalMemEfficient`.\nPlease see the bitsandbytes library for definitions for these layers.\nFor CLIP VIT-Huge this should currently correspond to a 10% training speedup with no accuracy loss.\nMore speedups comin when the attention layer is refactored so that linear layers man be replaced there, too.\n\nSee the tutorial https://github.com/mlfoundations/open_clip/blob/main/tutorials/int8_tutorial.ipynb or [paper](https://arxiv.org/abs/2304.13013).\n\n### Support for remote loading/training\n\nIt is always possible to resume directly from a remote file, e.g., a file in an s3 bucket. Just set `--resume s3://<path-to-checkpoint> `.\nThis will work with any filesystem supported by `fsspec`.\n\nIt is also possible to train `open_clip` models while continuously backing up to s3. This can help to avoid slow local file systems.\n\nSay that your node has a local ssd `/scratch`, an s3 bucket `s3://<path-to-bucket>`.\n\nIn that case, set `--logs /scratch` and `--remote-sync s3://<path-to-bucket>`. Then, a background process will sync `/scratch/<run-name>` to `s3://<path-to-bucket>/<run-name>`. After syncing, the background process will sleep for `--remote-sync-frequency` seconds, which defaults to 5 minutes.\n\nThere is also experimental support for syncing to other remote file systems, not just s3. To do so, specify `--remote-sync-protocol fsspec`. However, this is currently very slow and not recommended.\n\nAlso, to optionally avoid saving too many checkpoints locally when using these features, you can use `--delete-previous-checkpoint` which deletes the previous checkpoint after saving a new one.\n\nNote: if you are using this feature with `--resume latest`, there are a few warnings. First, use with `--save-most-recent` is not supported. Second, only `s3` is supported. Finally, since the sync happens in the background, it is possible that the most recent checkpoint may not be finished syncing to the remote.\n\n### Pushing Models to Hugging Face Hub\n\nThe module `open_clip.push_to_hf_hub` includes helpers for pushing models /w weights and config to the HF Hub.\n\nThe tool can be run from command line, ex:\n`python -m open_clip.push_to_hf_hub --model convnext_large_d_320 --pretrained /train/checkpoints/epoch_12.pt --repo-id laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft`\n\n\n\n## Acknowledgments\n\nWe gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding this part of work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at J\u00fclich Supercomputing Centre (JSC).\n\n## The Team\n\nCurrent development of this repository is led by [Ross Wightman](https://rwightman.com/), [Romain Beaumont](https://github.com/rom1504), [Cade Gordon](http://cadegordon.io/), and [Vaishaal Shankar](http://vaishaal.com/).\n\nThe original version of this repository is from a group of researchers at UW, Google, Stanford, Amazon, Columbia, and Berkeley.\n\n[Gabriel Ilharco*](http://gabrielilharco.com/), [Mitchell Wortsman*](https://mitchellnw.github.io/), [Nicholas Carlini](https://nicholas.carlini.com/), [Rohan Taori](https://www.rohantaori.com/), [Achal Dave](http://www.achaldave.com/), [Vaishaal Shankar](http://vaishaal.com/), [John Miller](https://people.eecs.berkeley.edu/~miller_john/), [Hongseok Namkoong](https://hsnamkoong.github.io/), [Hannaneh Hajishirzi](https://homes.cs.washington.edu/~hannaneh/), [Ali Farhadi](https://homes.cs.washington.edu/~ali/), [Ludwig Schmidt](https://people.csail.mit.edu/ludwigs/)\n\nSpecial thanks to [Jong Wook Kim](https://jongwook.kim/) and [Alec Radford](https://github.com/Newmu) for help with reproducing CLIP!\n\n## Citing\n\nIf you found this repository useful, please consider citing:\n```bibtex\n@software{ilharco_gabriel_2021_5143773,\n  author       = {Ilharco, Gabriel and\n                  Wortsman, Mitchell and\n                  Wightman, Ross and\n                  Gordon, Cade and\n                  Carlini, Nicholas and\n                  Taori, Rohan and\n                  Dave, Achal and\n                  Shankar, Vaishaal and\n                  Namkoong, Hongseok and\n                  Miller, John and\n                  Hajishirzi, Hannaneh and\n                  Farhadi, Ali and\n                  Schmidt, Ludwig},\n  title        = {OpenCLIP},\n  month        = jul,\n  year         = 2021,\n  note         = {If you use this software, please cite it as below.},\n  publisher    = {Zenodo},\n  version      = {0.1},\n  doi          = {10.5281/zenodo.5143773},\n  url          = {https://doi.org/10.5281/zenodo.5143773}\n}\n```\n\n```bibtex\n@inproceedings{cherti2023reproducible,\n  title={Reproducible scaling laws for contrastive language-image learning},\n  author={Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},\n  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},\n  pages={2818--2829},\n  year={2023}\n}\n```\n\n```bibtex\n@inproceedings{Radford2021LearningTV,\n  title={Learning Transferable Visual Models From Natural Language Supervision},\n  author={Alec Radford and Jong Wook Kim and Chris Hallacy and A. Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},\n  booktitle={ICML},\n  year={2021}\n}\n```\n\n```bibtex\n@inproceedings{schuhmann2022laionb,\n  title={{LAION}-5B: An open large-scale dataset for training next generation image-text models},\n  author={Christoph Schuhmann and\n          Romain Beaumont and\n          Richard Vencu and\n          Cade W Gordon and\n          Ross Wightman and\n          Mehdi Cherti and\n          Theo Coombes and\n          Aarush Katta and\n          Clayton Mullis and\n          Mitchell Wortsman and\n          Patrick Schramowski and\n          Srivatsa R Kundurthy and\n          Katherine Crowson and\n          Ludwig Schmidt and\n          Robert Kaczmarczyk and\n          Jenia Jitsev},\n  booktitle={Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track},\n  year={2022},\n  url={https://openreview.net/forum?id=M3Y74vmsMcY}\n}\n```\n\n[![DOI](https://zenodo.org/badge/390536799.svg)](https://zenodo.org/badge/latestdoi/390536799)\n",
  "external_links_in_readme": [
    "https://github.com/openai/CLIP",
    "https://github.com/lucidrains/CoCa-pytorch",
    "https://github.com/lucidrains",
    "https://www.rohantaori.com/",
    "https://github.com/LAION-AI/CLIP_benchmark#how-to-use",
    "https://github.com/rom1504/img2dataset/blob/main/dataset_examples/cc3m.md",
    "https://github.com/rom1504",
    "https://github.com/webdataset/webdataset](https://github.com/webdataset/webdataset",
    "https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/tree/main](https://huggingface.co/laion/CLIP-ViT-L-14-DataComp.XL-s13B-b90K/blob/main/open_clip_pytorch_model.bin",
    "https://github.com/openai/CLIP/blob/main/data/yfcc100m.md",
    "https://arxiv.org/abs/2304.14108",
    "https://openreview.net/forum?id=M3Y74vmsMcY}",
    "http://vaishaal.com/",
    "https://arxiv.org/abs/2210.08402",
    "https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/clip_zeroshot.png",
    "https://arxiv.org/abs/2212.00794\">Recent",
    "https://arxiv.org/abs/2309.17425",
    "https://arxiv.org/abs/2205.01917",
    "https://arxiv.org/abs/2303.15343",
    "https://homes.cs.washington.edu/~ali/",
    "https://arxiv.org/abs/2111.02114",
    "https://people.eecs.berkeley.edu/~miller_john/",
    "https://github.com/webdataset/tarp",
    "https://github.com/rom1504/clip-retrieval",
    "https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_clip.ipynb",
    "https://arxiv.org/abs/2109.01903",
    "https://colab.research.google.com/github/mlfoundations/open_clip/blob/master/docs/Interacting_with_open_coca.ipynb",
    "https://github.com/webdataset/webdataset",
    "https://hsnamkoong.github.io/",
    "http://mmcommons.org/",
    "https://rwightman.com/",
    "https://pypi.python.org/pypi/open_clip_torch",
    "https://arxiv.org/abs/2103.00020",
    "https://arxiv.org/abs/2112.09331",
    "https://github.com/rom1504/img2dataset",
    "http://gabrielilharco.com/",
    "https://github.com/mlfoundations/open_clip/pull/613,",
    "https://github.com/LAION-AI/CLIP_benchmark",
    "https://raw.githubusercontent.com/mlfoundations/open_clip/main/docs/CLIP.png",
    "https://github.com/mlfoundations/open_clip/blob/main/tutorials/int8_tutorial.ipynb",
    "https://homes.cs.washington.edu/~hannaneh/",
    "https://github.com/cocodataset/cocoapi",
    "https://people.csail.mit.edu/ludwigs/",
    "https://zenodo.org/badge/latestdoi/390536799",
    "https://nicholas.carlini.com/",
    "https://pytorch.org/get-started/locally/",
    "https://huggingface.co/models?library=open_clip.",
    "https://github.com/mlfoundations/wise-ft",
    "https://github.com/Newmu",
    "https://doi.org/10.5281/zenodo.5143773}",
    "https://zenodo.org/badge/390536799.svg",
    "https://arxiv.org/abs/2304.13013",
    "http://cadegordon.io/",
    "https://github.com/iejMac",
    "https://arxiv.org/abs/2111.10050",
    "https://arxiv.org/abs/2212.07143",
    "https://github.com/gpucce",
    "https://mitchellnw.github.io/",
    "http://www.achaldave.com/",
    "https://raw.githubusercontent.com/soumith/imagenetloader.torch/master/valprep.sh",
    "https://img.shields.io/pypi/v/open_clip_torch.svg",
    "https://jongwook.kim/"
  ]
}
```

</details>


---

## Repository 3: openai/CLIP

# GitHub Repository Data

**Repository:** [openai/CLIP](https://github.com/openai/CLIP)

## Basic Information

- **Description:** CLIP (Contrastive Language-Image Pretraining),  Predict the most relevant text snippet given an image
- **Created:** 2020-12-16T11:24:42+00:00
- **Last Updated:** 2025-06-21T19:43:56+00:00
- **Last Pushed:** 2024-07-23T01:30:44+00:00
- **Default Branch:** main
- **Size:** 9140 KB

## Statistics

- **Stars:** 29,475
- **Forks:** 3,643
- **Watchers:** 29,475
- **Open Issues:** 254
- **Total Issues:** 0
- **Pull Requests:** 69

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/openai/CLIP/blob/main/LICENSE)

## Languages

- **Jupyter Notebook:** 3,658,307 bytes
- **Python:** 34,355 bytes

## Topics

- `deep-learning`
- `machine-learning`

## Top Contributors

1. **jongwook** - 32 contributions
2. **jiahuei** - 2 contributions
3. **or-toledano** - 2 contributions
4. **sarveshwar-s** - 2 contributions
5. **bryant1410** - 2 contributions
6. **JamieMartin** - 1 contributions
7. **neverix** - 1 contributions
8. **liuxingbaoyu** - 1 contributions
9. **boba-and-beer** - 1 contributions
10. **syvb** - 1 contributions

## File Structure (Sample of 10 files)

Total files: 28

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/test.yml` (blob)
- `.gitignore` (blob)
- `CLIP.png` (blob)
- `LICENSE` (blob)
- `MANIFEST.in` (blob)
- `README.md` (blob)
- `clip` (tree)
- `clip/__init__.py` (blob)

## Recent Issues

- 游릭 **#504** Unable to Download EEG Dataset from OSF Link (open)
- 游댮 **#503** Unable to Download EEG Dataset from OSF Link (closed)
- 游릭 **#502** Recall (R@k) way lower than the one obtained in papers (open)
- 游릭 **#501** Can someone call it in C #? (open)
- 游댮 **#500** error during training (closed)

## Recent Pull Requests

- 游릭 **#497** [New Feature] Loading HuggingFace .safetensors and .bin variants for CLIP models (open)
- 游댮 **#493** initial commit (closed)
- 游댮 **#487** first pr (closed)
- 游릭 **#484** Add Intel춽 Gaudi춽 HPU Support (open)
- 游릭 **#462** fix torch.load after torch.jit.load (open)

## Recent Commits

- **dcba3cb2** import packaging to be compatible with setuptools==70.0.0 (#449) - tminer (2024-06-04T19:47:22+00:00)
- **a1d07173** Fix torch._C.Node attribute access (#372) - James Thewlis (2023-07-08T09:26:30+00:00)
- **a9b1bf59** Update README.md (#326) - Jong Wook Kim (2023-02-20T19:49:43+00:00)
- **37028498** np.float is removed in numpy 1.24 (#315) - Harry Wang (2023-01-20T19:31:18+00:00)
- **d50d76da** Removed another unused f-string (#276) - sarveshwar-s (2022-07-27T19:42:37+00:00)
- **c5478aac** Removed unused f-string (#273) - sarveshwar-s (2022-07-27T07:30:08+00:00)
- **f69a9bc2** Remove inefficient computation from `AttentionPool2d` Module (#271) - Penn (2022-07-21T20:04:35+00:00)
- **4d120f3e** Add PyTorch Hub configuration file (#259) - John Sutor (2022-07-14T12:40:02+00:00)
- **b46f5ac7** Don't reuse nn.ReLU modules (#239) - ProGamerGov (2022-05-01T21:31:46+00:00)
- **b4ae4492** ViT-L/14@336px (#234) - Jong Wook Kim (2022-04-21T23:45:46+00:00)

## External Links Found in README

- https://pytorch.org/get-started/locally/
- https://www.cs.toronto.edu/~kriz/cifar.html
- https://arxiv.org/abs/2103.00020
- https://openai.com/blog/clip/
- https://github.com/openai/CLIP.git
- https://huggingface.co/docs/transformers/model_doc/clip
- https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb
- https://scikit-learn.org/
- https://github.com/mlfoundations/open_clip

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 321960447,
  "name": "CLIP",
  "full_name": "openai/CLIP",
  "description": "CLIP (Contrastive Language-Image Pretraining),  Predict the most relevant text snippet given an image",
  "html_url": "https://github.com/openai/CLIP",
  "clone_url": "https://github.com/openai/CLIP.git",
  "ssh_url": "org-14957082@github.com:openai/CLIP.git",
  "homepage": "",
  "topics": [
    "deep-learning",
    "machine-learning"
  ],
  "default_branch": "main",
  "created_at": "2020-12-16T11:24:42+00:00",
  "updated_at": "2025-06-21T19:43:56+00:00",
  "pushed_at": "2024-07-23T01:30:44+00:00",
  "size_kb": 9140,
  "watchers_count": 29475,
  "stargazers_count": 29475,
  "forks_count": 3643,
  "open_issues_count": 254,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/openai/CLIP/blob/main/LICENSE"
  },
  "languages": {
    "Jupyter Notebook": 3658307,
    "Python": 34355
  },
  "top_contributors": [
    {
      "login": "jongwook",
      "contributions": 32
    },
    {
      "login": "jiahuei",
      "contributions": 2
    },
    {
      "login": "or-toledano",
      "contributions": 2
    },
    {
      "login": "sarveshwar-s",
      "contributions": 2
    },
    {
      "login": "bryant1410",
      "contributions": 2
    },
    {
      "login": "JamieMartin",
      "contributions": 1
    },
    {
      "login": "neverix",
      "contributions": 1
    },
    {
      "login": "liuxingbaoyu",
      "contributions": 1
    },
    {
      "login": "boba-and-beer",
      "contributions": 1
    },
    {
      "login": "syvb",
      "contributions": 1
    },
    {
      "login": "sebastianberns",
      "contributions": 1
    },
    {
      "login": "rom1504",
      "contributions": 1
    },
    {
      "login": "ProGamerGov",
      "contributions": 1
    },
    {
      "login": "jenkspt",
      "contributions": 1
    },
    {
      "login": "kcosta42",
      "contributions": 1
    },
    {
      "login": "johnsutor",
      "contributions": 1
    },
    {
      "login": "jamt9000",
      "contributions": 1
    },
    {
      "login": "chajath",
      "contributions": 1
    },
    {
      "login": "harrywang",
      "contributions": 1
    },
    {
      "login": "haofanwang",
      "contributions": 1
    }
  ],
  "file_tree_count": 28,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/test.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "CLIP.png",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "MANIFEST.in",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "clip",
      "type": "tree"
    },
    {
      "path": "clip/__init__.py",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 69,
  "recent_issues": [
    {
      "number": 504,
      "title": "Unable to Download EEG Dataset from OSF Link",
      "state": "open"
    },
    {
      "number": 503,
      "title": "Unable to Download EEG Dataset from OSF Link",
      "state": "closed"
    },
    {
      "number": 502,
      "title": "Recall (R@k) way lower than the one obtained in papers",
      "state": "open"
    },
    {
      "number": 501,
      "title": "Can someone call it in C #?",
      "state": "open"
    },
    {
      "number": 500,
      "title": "error during training",
      "state": "closed"
    }
  ],
  "recent_pulls": [
    {
      "number": 497,
      "title": "[New Feature] Loading HuggingFace .safetensors and .bin variants for CLIP models",
      "state": "open"
    },
    {
      "number": 493,
      "title": "initial commit",
      "state": "closed"
    },
    {
      "number": 487,
      "title": "first pr",
      "state": "closed"
    },
    {
      "number": 484,
      "title": "Add Intel\u00ae Gaudi\u00ae HPU Support",
      "state": "open"
    },
    {
      "number": 462,
      "title": "fix torch.load after torch.jit.load",
      "state": "open"
    }
  ],
  "recent_commits": [
    {
      "sha": "dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1",
      "author": "tminer",
      "date": "2024-06-04T19:47:22+00:00",
      "message": "import packaging to be compatible with setuptools==70.0.0 (#449)"
    },
    {
      "sha": "a1d071733d7111c9c014f024669f959182114e33",
      "author": "James Thewlis",
      "date": "2023-07-08T09:26:30+00:00",
      "message": "Fix torch._C.Node attribute access (#372)"
    },
    {
      "sha": "a9b1bf5920416aaeaec965c25dd9e8f98c864f16",
      "author": "Jong Wook Kim",
      "date": "2023-02-20T19:49:43+00:00",
      "message": "Update README.md (#326)"
    },
    {
      "sha": "3702849800aa56e2223035bccd1c6ef91c704ca8",
      "author": "Harry Wang",
      "date": "2023-01-20T19:31:18+00:00",
      "message": "np.float is removed in numpy 1.24 (#315)"
    },
    {
      "sha": "d50d76daa670286dd6cacf3bcd80b5e4823fc8e1",
      "author": "sarveshwar-s",
      "date": "2022-07-27T19:42:37+00:00",
      "message": "Removed another unused f-string (#276)"
    },
    {
      "sha": "c5478aac7b9e007a2659d36b57ebe148849e542a",
      "author": "sarveshwar-s",
      "date": "2022-07-27T07:30:08+00:00",
      "message": "Removed unused f-string (#273)"
    },
    {
      "sha": "f69a9bc217f6df9213628848b3f9b0b6fc542401",
      "author": "Penn",
      "date": "2022-07-21T20:04:35+00:00",
      "message": "Remove inefficient computation from `AttentionPool2d` Module (#271)"
    },
    {
      "sha": "4d120f3ec35b30bd0f992f5d8af2d793aad98d2a",
      "author": "John Sutor",
      "date": "2022-07-14T12:40:02+00:00",
      "message": "Add PyTorch Hub configuration file (#259)"
    },
    {
      "sha": "b46f5ac7587d2e1862f8b7b1573179d80dcdd620",
      "author": "ProGamerGov",
      "date": "2022-05-01T21:31:46+00:00",
      "message": "Don't reuse nn.ReLU modules (#239)"
    },
    {
      "sha": "b4ae44927b78d0093b556e3ce43cbdcff422017a",
      "author": "Jong Wook Kim",
      "date": "2022-04-21T23:45:46+00:00",
      "message": "ViT-L/14@336px (#234)"
    },
    {
      "sha": "e58d49454c92986a1d2a6a48add2333bbfbeaf51",
      "author": "neverix",
      "date": "2022-04-10T21:08:16+00:00",
      "message": "use pkg_resources for PyTorch version checks in notebooks (#191)"
    },
    {
      "sha": "3482bb6ed319f70542094d1ed224c0db0b88c3a5",
      "author": "liuxingbaoyu",
      "date": "2022-04-10T21:07:46+00:00",
      "message": "fix utf8 username on windows (#227)"
    },
    {
      "sha": "c0065a27ad170a67d9572f693292b594982ab9b2",
      "author": "Jong Wook Kim",
      "date": "2022-04-10T20:39:41+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "7ef63f265bc1d167c19088f3272259fc69014308",
      "author": "In-Ho Yi",
      "date": "2022-04-10T20:35:32+00:00",
      "message": "Patch clip model for ONNX compatibility (#219)"
    },
    {
      "sha": "40f5484c1c74edd83cb9cf687c6ab92b28d8b656",
      "author": "Jong Wook Kim",
      "date": "2022-01-26T01:23:09+00:00",
      "message": "update torch versions in workflow"
    },
    {
      "sha": "67fc250eb6aa84ef9ad19a020e3f8eb4e698feb4",
      "author": "Jong Wook Kim",
      "date": "2022-01-26T01:04:00+00:00",
      "message": "add RN50x64 and ViT-L/14 models"
    },
    {
      "sha": "573315e83f07b53a61ff5098757e8fc885f1703e",
      "author": "Tan Jia Huei",
      "date": "2021-11-09T06:57:26+00:00",
      "message": "use `pkg_resources` for PyTorch version comparison (#176)"
    },
    {
      "sha": "1a8b4b28997910a35757e1cd55b8ce3d26e44d3c",
      "author": "Tan Jia Huei",
      "date": "2021-11-05T05:17:26+00:00",
      "message": "Fix PyTorch version check for nightly builds (#173)"
    },
    {
      "sha": "2867559c5fe0b02a2d3167aeacd333b3c4276847",
      "author": "Santiago Castro",
      "date": "2021-11-04T21:32:10+00:00",
      "message": "Fix PyTorch version check (#160)"
    },
    {
      "sha": "c7ba4f390785bf19620f685f96b215563ba00e35",
      "author": "Jong Wook Kim",
      "date": "2021-11-04T21:20:40+00:00",
      "message": "Add 1.10.0 in tests"
    }
  ],
  "readme_text": "# CLIP\n\n[[Blog]](https://openai.com/blog/clip/) [[Paper]](https://arxiv.org/abs/2103.00020) [[Model Card]](model-card.md) [[Colab]](https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb)\n\nCLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs. It can be instructed in natural language to predict the most relevant text snippet, given an image, without directly optimizing for the task, similarly to the zero-shot capabilities of GPT-2 and 3. We found CLIP matches the performance of the original ResNet50 on ImageNet \u201czero-shot\u201d without using any of the original 1.28M labeled examples, overcoming several major challenges in computer vision.\n\n\n\n## Approach\n\n![CLIP](CLIP.png)\n\n\n\n## Usage\n\nFirst, [install PyTorch 1.7.1](https://pytorch.org/get-started/locally/) (or later) and torchvision, as well as small additional dependencies, and then install this repo as a Python package. On a CUDA GPU machine, the following will do the trick:\n\n```bash\n$ conda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\n$ pip install ftfy regex tqdm\n$ pip install git+https://github.com/openai/CLIP.git\n```\n\nReplace `cudatoolkit=11.0` above with the appropriate CUDA version on your machine or `cpuonly` when installing on a machine without a GPU.\n\n```python\nimport torch\nimport clip\nfrom PIL import Image\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\nimage = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\ntext = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\"]).to(device)\n\nwith torch.no_grad():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    \n    logits_per_image, logits_per_text = model(image, text)\n    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n\nprint(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]\n```\n\n\n## API\n\nThe CLIP module `clip` provides the following methods:\n\n#### `clip.available_models()`\n\nReturns the names of the available CLIP models.\n\n#### `clip.load(name, device=..., jit=False)`\n\nReturns the model and the TorchVision transform needed by the model, specified by the model name returned by `clip.available_models()`. It will download the model as necessary. The `name` argument can also be a path to a local checkpoint.\n\nThe device to run the model can be optionally specified, and the default is to use the first CUDA device if there is any, otherwise the CPU. When `jit` is `False`, a non-JIT version of the model will be loaded.\n\n#### `clip.tokenize(text: Union[str, List[str]], context_length=77)`\n\nReturns a LongTensor containing tokenized sequences of given text input(s). This can be used as the input to the model\n\n---\n\nThe model returned by `clip.load()` supports the following methods:\n\n#### `model.encode_image(image: Tensor)`\n\nGiven a batch of images, returns the image features encoded by the vision portion of the CLIP model.\n\n#### `model.encode_text(text: Tensor)`\n\nGiven a batch of text tokens, returns the text features encoded by the language portion of the CLIP model.\n\n#### `model(image: Tensor, text: Tensor)`\n\nGiven a batch of images and a batch of text tokens, returns two Tensors, containing the logit scores corresponding to each image and text input. The values are cosine similarities between the corresponding image and text features, times 100.\n\n\n\n## More Examples\n\n### Zero-Shot Prediction\n\nThe code below performs zero-shot prediction using CLIP, as shown in Appendix B in the paper. This example takes an image from the [CIFAR-100 dataset](https://www.cs.toronto.edu/~kriz/cifar.html), and predicts the most likely labels among the 100 textual labels from the dataset.\n\n```python\nimport os\nimport clip\nimport torch\nfrom torchvision.datasets import CIFAR100\n\n# Load the model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load('ViT-B/32', device)\n\n# Download the dataset\ncifar100 = CIFAR100(root=os.path.expanduser(\"~/.cache\"), download=True, train=False)\n\n# Prepare the inputs\nimage, class_id = cifar100[3637]\nimage_input = preprocess(image).unsqueeze(0).to(device)\ntext_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in cifar100.classes]).to(device)\n\n# Calculate features\nwith torch.no_grad():\n    image_features = model.encode_image(image_input)\n    text_features = model.encode_text(text_inputs)\n\n# Pick the top 5 most similar labels for the image\nimage_features /= image_features.norm(dim=-1, keepdim=True)\ntext_features /= text_features.norm(dim=-1, keepdim=True)\nsimilarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\nvalues, indices = similarity[0].topk(5)\n\n# Print the result\nprint(\"\\nTop predictions:\\n\")\nfor value, index in zip(values, indices):\n    print(f\"{cifar100.classes[index]:>16s}: {100 * value.item():.2f}%\")\n```\n\nThe output will look like the following (the exact numbers may be slightly different depending on the compute device):\n\n```\nTop predictions:\n\n           snake: 65.31%\n          turtle: 12.29%\n    sweet_pepper: 3.83%\n          lizard: 1.88%\n       crocodile: 1.75%\n```\n\nNote that this example uses the `encode_image()` and `encode_text()` methods that return the encoded features of given inputs.\n\n\n### Linear-probe evaluation\n\nThe example below uses [scikit-learn](https://scikit-learn.org/) to perform logistic regression on image features.\n\n```python\nimport os\nimport clip\nimport torch\n\nimport numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import CIFAR100\nfrom tqdm import tqdm\n\n# Load the model\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load('ViT-B/32', device)\n\n# Load the dataset\nroot = os.path.expanduser(\"~/.cache\")\ntrain = CIFAR100(root, download=True, train=True, transform=preprocess)\ntest = CIFAR100(root, download=True, train=False, transform=preprocess)\n\n\ndef get_features(dataset):\n    all_features = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for images, labels in tqdm(DataLoader(dataset, batch_size=100)):\n            features = model.encode_image(images.to(device))\n\n            all_features.append(features)\n            all_labels.append(labels)\n\n    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()\n\n# Calculate the image features\ntrain_features, train_labels = get_features(train)\ntest_features, test_labels = get_features(test)\n\n# Perform logistic regression\nclassifier = LogisticRegression(random_state=0, C=0.316, max_iter=1000, verbose=1)\nclassifier.fit(train_features, train_labels)\n\n# Evaluate using the logistic regression classifier\npredictions = classifier.predict(test_features)\naccuracy = np.mean((test_labels == predictions).astype(float)) * 100.\nprint(f\"Accuracy = {accuracy:.3f}\")\n```\n\nNote that the `C` value should be determined via a hyperparameter sweep using a validation split.\n\n\n## See Also\n\n* [OpenCLIP](https://github.com/mlfoundations/open_clip): includes larger and independently trained CLIP models up to ViT-G/14\n* [Hugging Face implementation of CLIP](https://huggingface.co/docs/transformers/model_doc/clip): for easier integration with the HF ecosystem\n",
  "external_links_in_readme": [
    "https://pytorch.org/get-started/locally/",
    "https://www.cs.toronto.edu/~kriz/cifar.html",
    "https://arxiv.org/abs/2103.00020",
    "https://openai.com/blog/clip/",
    "https://github.com/openai/CLIP.git",
    "https://huggingface.co/docs/transformers/model_doc/clip",
    "https://colab.research.google.com/github/openai/clip/blob/master/notebooks/Interacting_with_CLIP.ipynb",
    "https://scikit-learn.org/",
    "https://github.com/mlfoundations/open_clip"
  ]
}
```

</details>


---

## Repository 4: Stability-AI/generative-models

# GitHub Repository Data

**Repository:** [Stability-AI/generative-models](https://github.com/Stability-AI/generative-models)

## Basic Information

- **Description:** Generative Models by Stability AI
- **Created:** 2023-06-22T00:36:35+00:00
- **Last Updated:** 2025-06-22T02:11:32+00:00
- **Last Pushed:** 2025-05-20T14:53:33+00:00
- **Default Branch:** main
- **Size:** 88629 KB

## Statistics

- **Stars:** 26,056
- **Forks:** 2,898
- **Watchers:** 26,056
- **Open Issues:** 317
- **Total Issues:** 0
- **Pull Requests:** 112

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/Stability-AI/generative-models/blob/main/LICENSE-CODE)

## Languages

- **Python:** 612,143 bytes
- **Roff:** 11,651 bytes

## Top Contributors

1. **timudk** - 12 contributions
2. **voletiv** - 9 contributions
3. **akx** - 8 contributions
4. **ymxie97** - 7 contributions
5. **chunhanyao-stable** - 5 contributions
6. **rromb** - 4 contributions
7. **palp** - 3 contributions
8. **pharmapsychotic** - 2 contributions
9. **jenuk** - 1 contributions
10. **yvrjsharma** - 1 contributions

## File Structure (Sample of 10 files)

Total files: 184

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/black.yml` (blob)
- `.github/workflows/test-build.yaml` (blob)
- `.github/workflows/test-inference.yml` (blob)
- `.gitignore` (blob)
- `CODEOWNERS` (blob)
- `LICENSE-CODE` (blob)
- `README.md` (blob)
- `assets` (tree)

## Recent Issues

- 游릭 **#452** How to train lora for svd? Please help  (open)
- 游릭 **#451** Inquiry about the Release Timeline for SV4D 2.0 4D Optimization from Multi-view Videos Code (open)
- 游릭 **#450** Some questions about image editing (open)
- 游릭 **#449** Are there other quantized versions of the model (open)
- 游댮 **#448** Update README.md (closed)

## Recent Pull Requests

- 游댮 **#448** Update README.md (closed)
- 游릭 **#446** Create 3 Fakta gila luar angkasa (open)
- 游댮 **#440** add SV4D 2.0 (closed)
- 游릭 **#438** Update README.md (open)
- 游릭 **#434** Update README.md (open)

## Recent Commits

- **0ad7de9a** Update README.md (#448) - chunhanyao-stable (2025-05-20T14:53:31+00:00)
- **c3147b86** add SV4D 2.0 (#440) - chunhanyao-stable (2025-05-20T14:38:11+00:00)
- **1659a1c0** Merge pull request #394 from Stability-AI/yiming/sv4d - chunhanyao-stable (2024-08-03T05:57:15+00:00)
- **37ab71e2** sv4d: fixed readme - ymxie97 (2024-08-02T17:26:04+00:00)
- **e90e9533** sv4d: fix readme; - ymxie97 (2024-08-02T17:19:03+00:00)
- **da40ebad** sv4d: fix readme - ymxie97 (2024-08-02T06:24:58+00:00)
- **50364a7d** sv4d_gradio_demo comments minor fix - ymxie97 (2024-08-02T05:53:54+00:00)
- **2cea114c** SV4D: remove unused comments - ymxie97 (2024-08-02T05:17:14+00:00)
- **734195d1** SV4D: add gradio demo - ymxie97 (2024-08-02T05:01:57+00:00)
- **854bd4f0** SV4D: reduce the memory consumption and speed up - ymxie97 (2024-08-02T04:59:37+00:00)

## External Links Found in README

- https://hatch.pypa.io/latest/
- https://huggingface.co/stabilityai/sv3d
- https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt
- https://github.com/ShieldMnt/invisible-watermark/
- https://github.com/Stability-AI/datapipelines
- https://github.com/Stability-AI/generative-models.git
- https://sv3d.github.io/static/paper.pdf
- https://clipdrop.co/
- https://www.youtube.com/watch?v=dtqj-s50ynU
- https://www.youtube.com/watch?v=RBP8vdAWTgk
- https://github.com/danielgatis/rembg
- https://stability.ai/research/adversarial-diffusion-distillation
- https://github.com/Stability-AI/ModelSpec
- https://download.pytorch.org/whl/cu118
- https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/
- https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0
- https://huggingface.co/stabilityai/sv4d
- https://streamlit.io/
- https://huggingface.co/stabilityai/sdxl-turbo
- https://sv4d.github.io

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 656933936,
  "name": "generative-models",
  "full_name": "Stability-AI/generative-models",
  "description": "Generative Models by Stability AI",
  "html_url": "https://github.com/Stability-AI/generative-models",
  "clone_url": "https://github.com/Stability-AI/generative-models.git",
  "ssh_url": "git@github.com:Stability-AI/generative-models.git",
  "homepage": "",
  "topics": [],
  "default_branch": "main",
  "created_at": "2023-06-22T00:36:35+00:00",
  "updated_at": "2025-06-22T02:11:32+00:00",
  "pushed_at": "2025-05-20T14:53:33+00:00",
  "size_kb": 88629,
  "watchers_count": 26056,
  "stargazers_count": 26056,
  "forks_count": 2898,
  "open_issues_count": 317,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/Stability-AI/generative-models/blob/main/LICENSE-CODE"
  },
  "languages": {
    "Python": 612143,
    "Roff": 11651
  },
  "top_contributors": [
    {
      "login": "timudk",
      "contributions": 12
    },
    {
      "login": "voletiv",
      "contributions": 9
    },
    {
      "login": "akx",
      "contributions": 8
    },
    {
      "login": "ymxie97",
      "contributions": 7
    },
    {
      "login": "chunhanyao-stable",
      "contributions": 5
    },
    {
      "login": "rromb",
      "contributions": 4
    },
    {
      "login": "palp",
      "contributions": 3
    },
    {
      "login": "pharmapsychotic",
      "contributions": 2
    },
    {
      "login": "jenuk",
      "contributions": 1
    },
    {
      "login": "yvrjsharma",
      "contributions": 1
    },
    {
      "login": "johngull",
      "contributions": 1
    },
    {
      "login": "patrickvonplaten",
      "contributions": 1
    },
    {
      "login": "lantiga",
      "contributions": 1
    },
    {
      "login": "qp-qp",
      "contributions": 1
    },
    {
      "login": "brycedrennan",
      "contributions": 1
    },
    {
      "login": "benjaminaubin",
      "contributions": 1
    },
    {
      "login": "gen-ai-experts",
      "contributions": 1
    }
  ],
  "file_tree_count": 184,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/black.yml",
      "type": "blob"
    },
    {
      "path": ".github/workflows/test-build.yaml",
      "type": "blob"
    },
    {
      "path": ".github/workflows/test-inference.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "CODEOWNERS",
      "type": "blob"
    },
    {
      "path": "LICENSE-CODE",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "assets",
      "type": "tree"
    }
  ],
  "issues_count": 0,
  "pulls_count": 112,
  "recent_issues": [
    {
      "number": 452,
      "title": "How to train lora for svd? Please help ",
      "state": "open"
    },
    {
      "number": 451,
      "title": "Inquiry about the Release Timeline for SV4D 2.0 4D Optimization from Multi-view Videos Code",
      "state": "open"
    },
    {
      "number": 450,
      "title": "Some questions about image editing",
      "state": "open"
    },
    {
      "number": 449,
      "title": "Are there other quantized versions of the model",
      "state": "open"
    },
    {
      "number": 448,
      "title": "Update README.md",
      "state": "closed"
    }
  ],
  "recent_pulls": [
    {
      "number": 448,
      "title": "Update README.md",
      "state": "closed"
    },
    {
      "number": 446,
      "title": "Create 3 Fakta gila luar angkasa",
      "state": "open"
    },
    {
      "number": 440,
      "title": "add SV4D 2.0",
      "state": "closed"
    },
    {
      "number": 438,
      "title": "Update README.md",
      "state": "open"
    },
    {
      "number": 434,
      "title": "Update README.md",
      "state": "open"
    }
  ],
  "recent_commits": [
    {
      "sha": "0ad7de9a5cb53fd63d6d30a4f385485e72e08597",
      "author": "chunhanyao-stable",
      "date": "2025-05-20T14:53:31+00:00",
      "message": "Update README.md (#448)"
    },
    {
      "sha": "c3147b86db11da9330e1b17fb230a7c2ecdb9cf2",
      "author": "chunhanyao-stable",
      "date": "2025-05-20T14:38:11+00:00",
      "message": "add SV4D 2.0 (#440)"
    },
    {
      "sha": "1659a1c09b0953ad9cc0d480f42e4526c5575b37",
      "author": "chunhanyao-stable",
      "date": "2024-08-03T05:57:15+00:00",
      "message": "Merge pull request #394 from Stability-AI/yiming/sv4d"
    },
    {
      "sha": "37ab71e234f54100f5e42c700aa16a37cf7d65c5",
      "author": "ymxie97",
      "date": "2024-08-02T17:26:04+00:00",
      "message": "sv4d: fixed readme"
    },
    {
      "sha": "e90e953330c1bebe823101c98616bfaeeaae9d7b",
      "author": "ymxie97",
      "date": "2024-08-02T17:19:03+00:00",
      "message": "sv4d: fix readme;"
    },
    {
      "sha": "da40ebad4ea5b07ed89486ed0413db99ea6e166c",
      "author": "ymxie97",
      "date": "2024-08-02T06:24:58+00:00",
      "message": "sv4d: fix readme"
    },
    {
      "sha": "50364a7d2f00795638300bfd2ae0eb8aa421ba8d",
      "author": "ymxie97",
      "date": "2024-08-02T05:53:54+00:00",
      "message": "sv4d_gradio_demo comments minor fix"
    },
    {
      "sha": "2cea114cc19318006cf6827f7979e5bd4c0cd77b",
      "author": "ymxie97",
      "date": "2024-08-02T05:17:14+00:00",
      "message": "SV4D: remove unused comments"
    },
    {
      "sha": "734195d1c9ec49c33107799b09228ef458bceb57",
      "author": "ymxie97",
      "date": "2024-08-02T05:01:57+00:00",
      "message": "SV4D: add gradio demo"
    },
    {
      "sha": "854bd4f0dfde535221e9feaa6d5e253bd45122d0",
      "author": "ymxie97",
      "date": "2024-08-02T04:59:37+00:00",
      "message": "SV4D: reduce the memory consumption and speed up"
    },
    {
      "sha": "e0596f1aca2a9e3bb2db1c3e4e63066892d3c50d",
      "author": "Vikram Voleti",
      "date": "2024-08-02T04:10:50+00:00",
      "message": "Merge pull request #392 from Stability-AI/chunhan/sv4d"
    },
    {
      "sha": "ce1576bfca97f0c3aa84029c7d1ae973c2deb557",
      "author": "Chun-Han Yao",
      "date": "2024-08-01T19:39:54+00:00",
      "message": "update sv4d readme and scripts"
    },
    {
      "sha": "1cd0cbaff43401693f3dbd80bca9b3bd8fb069e3",
      "author": "Chun-Han Yao",
      "date": "2024-07-31T18:42:28+00:00",
      "message": "update sv4d sampling script and readme"
    },
    {
      "sha": "863665548f95ff827273948766a3f732ab01bc49",
      "author": "Vikram Voleti",
      "date": "2024-07-24T15:04:54+00:00",
      "message": "Merge pull request #386 from Stability-AI/vikram/sv4d"
    },
    {
      "sha": "e3e4b9d263ac764bee8edf133c176c3e7269ad2d",
      "author": "Vikram Voleti",
      "date": "2024-07-24T15:02:41+00:00",
      "message": "Fix to SV3D link"
    },
    {
      "sha": "1aa06e5995166aee0e95f250a63c99171d039c21",
      "author": "Vikram Voleti",
      "date": "2024-07-24T15:00:06+00:00",
      "message": "Merge pull request #385 from Stability-AI/vikram/sv4d"
    },
    {
      "sha": "998cb122d3d87d6fa31eb7a06c37ebd75d2a9ec6",
      "author": "Vikram Voleti",
      "date": "2024-07-24T14:57:39+00:00",
      "message": "Fixes links"
    },
    {
      "sha": "31fe459a85687b0588cb240c4cfba28a2ed22eb4",
      "author": "Vikram Voleti",
      "date": "2024-07-24T14:44:37+00:00",
      "message": "Merge pull request #384 from Stability-AI/vikram/sv4d"
    },
    {
      "sha": "abe9ed3d40b3ac0b1f15c652ded5a50856674f20",
      "author": "Vikram Voleti",
      "date": "2024-07-23T20:17:16+00:00",
      "message": "Adds SV4D code"
    },
    {
      "sha": "fbdc58cab9f4ee2be7a5e1f2e2787ecd9311942f",
      "author": "Vikram Voleti",
      "date": "2024-03-18T21:07:11+00:00",
      "message": "Fixes typos (#308)"
    }
  ],
  "readme_text": "# Generative Models by Stability AI\n\n![sample1](assets/000.jpg)\n\n## News\n\n\n**May 20, 2025**\n- We are releasing **[Stable Video 4D 2.0 (SV4D 2.0)](https://huggingface.co/stabilityai/sv4d2.0)**, an enhanced video-to-4D diffusion model for high-fidelity novel-view video synthesis and 4D asset generation. For research purposes:\n    - **SV4D 2.0** was trained to generate 48 frames (12 video frames x 4 camera views) at 576x576 resolution, given a 12-frame input video of the same size, ideally consisting of white-background images of a moving object.\n    - Compared to our previous 4D model [SV4D](https://huggingface.co/stabilityai/sv4d), **SV4D 2.0** can generate videos with higher fidelity, sharper details during motion, and better spatio-temporal consistency. It also generalizes much better to real-world videos. Moreover, it does not rely on refernce multi-view of the first frame generated by SV3D, making it more robust to self-occlusions.\n    - To generate longer novel-view videos, we autoregressively generate 12 frames at a time and use the previous generation as conditioning views for the remaining frames.\n    - Please check our [project page](https://sv4d20.github.io), [arxiv paper](https://arxiv.org/pdf/2503.16396) and [video summary](https://www.youtube.com/watch?v=dtqj-s50ynU) for more details.\n\n**QUICKSTART** :\n- `python scripts/sampling/simple_video_sample_4d2.py --input_path assets/sv4d_videos/camel.gif --output_folder outputs` (after downloading [sv4d2.safetensors](https://huggingface.co/stabilityai/sv4d2.0) from HuggingFace into `checkpoints/`)\n\nTo run **SV4D 2.0** on a single input video of 21 frames:\n- Download SV4D 2.0 model (`sv4d2.safetensors`) from [here](https://huggingface.co/stabilityai/sv4d2.0) to `checkpoints/`: `huggingface-cli download stabilityai/sv4d2.0 sv4d2.safetensors --local-dir checkpoints`\n- Run inference: `python scripts/sampling/simple_video_sample_4d2.py --input_path <path/to/video>`\n    - `input_path` : The input video `<path/to/video>` can be\n      - a single video file in `gif` or `mp4` format, such as `assets/sv4d_videos/camel.gif`, or\n      - a folder containing images of video frames in `.jpg`, `.jpeg`, or `.png` format, or\n      - a file name pattern matching images of video frames.\n    - `num_steps` : default is 50, can decrease to it to shorten sampling time.\n    - `elevations_deg` : specified elevations (reletive to input view), default is 0.0 (same as input view).\n    - **Background removal** : For input videos with plain background, (optionally) use [rembg](https://github.com/danielgatis/rembg) to remove background and crop video frames by setting `--remove_bg=True`. To obtain higher quality outputs on real-world input videos with noisy background, try segmenting the foreground object using [Clipdrop](https://clipdrop.co/) or [SAM2](https://github.com/facebookresearch/segment-anything-2) before running SV4D.\n    - **Low VRAM environment** : To run on GPUs with low VRAM, try setting `--encoding_t=1` (of frames encoded at a time) and `--decoding_t=1` (of frames decoded at a time) or lower video resolution like `--img_size=512`.\n\nNotes:\n- We also train a 8-view model that generates 5 frames x 8 views at a time (same as SV4D).\n  - Download the model from huggingface: `huggingface-cli download stabilityai/sv4d2.0 sv4d2_8views.safetensors --local-dir checkpoints`\n  - Run inference: `python scripts/sampling/simple_video_sample_4d2.py --model_path checkpoints/sv4d2_8views.safetensors --input_path assets/sv4d_videos/chest.gif --output_folder outputs`\n  - The 5x8 model takes 5 frames of input at a time. But the inference scripts for both model take 21-frame video as input by default (same as SV3D and SV4D), we run the model autoregressively until we generate 21 frames.\n- Install dependencies before running:\n```\npython3.10 -m venv .generativemodels\nsource .generativemodels/bin/activate\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 # check CUDA version\npip3 install -r requirements/pt2.txt\npip3 install .\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n  ![tile](assets/sv4d2.gif)\n\n\n**July 24, 2024**\n- We are releasing **[Stable Video 4D (SV4D)](https://huggingface.co/stabilityai/sv4d)**, a video-to-4D diffusion model for novel-view video synthesis. For research purposes:\n    - **SV4D** was trained to generate 40 frames (5 video frames x 8 camera views) at 576x576 resolution, given 5 context frames (the input video), and 8 reference views (synthesised from the first frame of the input video, using a multi-view diffusion model like SV3D) of the same size, ideally white-background images with one object.\n    - To generate longer novel-view videos (21 frames), we propose a novel sampling method using SV4D, by first sampling 5 anchor frames and then densely sampling the remaining frames while maintaining temporal consistency.\n    - To run the community-build gradio demo locally, run `python -m scripts.demo.gradio_app_sv4d`.\n    - Please check our [project page](https://sv4d.github.io), [tech report](https://sv4d.github.io/static/sv4d_technical_report.pdf) and [video summary](https://www.youtube.com/watch?v=RBP8vdAWTgk) for more details.\n\n**QUICKSTART** : `python scripts/sampling/simple_video_sample_4d.py --input_path assets/sv4d_videos/test_video1.mp4 --output_folder outputs/sv4d` (after downloading [sv4d.safetensors](https://huggingface.co/stabilityai/sv4d) and [sv3d_u.safetensors](https://huggingface.co/stabilityai/sv3d) from HuggingFace into `checkpoints/`)\n\nTo run **SV4D** on a single input video of 21 frames:\n- Download SV3D models (`sv3d_u.safetensors` and `sv3d_p.safetensors`) from [here](https://huggingface.co/stabilityai/sv3d) and SV4D model (`sv4d.safetensors`) from [here](https://huggingface.co/stabilityai/sv4d) to `checkpoints/`\n- Run `python scripts/sampling/simple_video_sample_4d.py --input_path <path/to/video>`\n    - `input_path` : The input video `<path/to/video>` can be\n      - a single video file in `gif` or `mp4` format, such as `assets/sv4d_videos/test_video1.mp4`, or\n      - a folder containing images of video frames in `.jpg`, `.jpeg`, or `.png` format, or\n      - a file name pattern matching images of video frames.\n    - `num_steps` : default is 20, can increase to 50 for better quality but longer sampling time.\n    - `sv3d_version` : To specify the SV3D model to generate reference multi-views, set `--sv3d_version=sv3d_u` for SV3D_u or `--sv3d_version=sv3d_p` for SV3D_p.\n    - `elevations_deg` : To generate novel-view videos at a specified elevation (default elevation is 10) using SV3D_p (default is SV3D_u), run `python scripts/sampling/simple_video_sample_4d.py --input_path assets/sv4d_videos/test_video1.mp4 --sv3d_version sv3d_p --elevations_deg 30.0`\n    - **Background removal** : For input videos with plain background, (optionally) use [rembg](https://github.com/danielgatis/rembg) to remove background and crop video frames by setting `--remove_bg=True`. To obtain higher quality outputs on real-world input videos with noisy background, try segmenting the foreground object using [Clipdrop](https://clipdrop.co/) or [SAM2](https://github.com/facebookresearch/segment-anything-2) before running SV4D.\n    - **Low VRAM environment** : To run on GPUs with low VRAM, try setting `--encoding_t=1` (of frames encoded at a time) and `--decoding_t=1` (of frames decoded at a time) or lower video resolution like `--img_size=512`.\n\n  ![tile](assets/sv4d.gif)\n\n\n**March 18, 2024**\n- We are releasing **[SV3D](https://huggingface.co/stabilityai/sv3d)**, an image-to-video model for novel multi-view synthesis, for research purposes:\n    - **SV3D** was trained to generate 21 frames at resolution 576x576, given 1 context frame of the same size, ideally a white-background image with one object.\n    - **SV3D_u**: This variant generates orbital videos based on single image inputs without camera conditioning..\n    - **SV3D_p**: Extending the capability of **SVD3_u**, this variant accommodates both single images and orbital views allowing for the creation of 3D video along specified camera paths.\n    - We extend the streamlit demo `scripts/demo/video_sampling.py` and the standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Please check our [project page](https://sv3d.github.io), [tech report](https://sv3d.github.io/static/paper.pdf) and [video summary](https://youtu.be/Zqw4-1LcfWg) for more details.\n\nTo run **SV3D_u** on a single image:\n- Download `sv3d_u.safetensors` from https://huggingface.co/stabilityai/sv3d to `checkpoints/sv3d_u.safetensors`\n- Run `python scripts/sampling/simple_video_sample.py --input_path <path/to/image.png> --version sv3d_u`\n\nTo run **SV3D_p** on a single image:\n- Download `sv3d_p.safetensors` from https://huggingface.co/stabilityai/sv3d to `checkpoints/sv3d_p.safetensors`\n1. Generate static orbit at a specified elevation eg. 10.0 : `python scripts/sampling/simple_video_sample.py --input_path <path/to/image.png> --version sv3d_p --elevations_deg 10.0`\n2. Generate dynamic orbit at a specified elevations and azimuths: specify sequences of 21 elevations (in degrees) to `elevations_deg` ([-90, 90]), and 21 azimuths (in degrees) to `azimuths_deg` [0, 360] in sorted order from 0 to 360. For example: `python scripts/sampling/simple_video_sample.py --input_path <path/to/image.png> --version sv3d_p --elevations_deg [<list of 21 elevations in degrees>] --azimuths_deg [<list of 21 azimuths in degrees>]`\n\nTo run SVD or SV3D on a streamlit server:\n`streamlit run scripts/demo/video_sampling.py`\n\n  ![tile](assets/sv3d.gif)\n\n\n**November 30, 2023**\n- Following the launch of SDXL-Turbo, we are releasing [SD-Turbo](https://huggingface.co/stabilityai/sd-turbo).\n\n**November 28, 2023**\n- We are releasing SDXL-Turbo, a lightning fast text-to image model.\n  Alongside the model, we release a [technical report](https://stability.ai/research/adversarial-diffusion-distillation)\n    - Usage:\n        - Follow the installation instructions or update the existing environment with `pip install streamlit-keyup`.\n        - Download the [weights](https://huggingface.co/stabilityai/sdxl-turbo) and place them in the `checkpoints/` directory.\n        - Run `streamlit run scripts/demo/turbo.py`.\n\n  ![tile](assets/turbo_tile.png)\n\n\n**November 21, 2023**\n- We are releasing Stable Video Diffusion, an image-to-video model, for research purposes:\n    - [SVD](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid): This model was trained to generate 14\n      frames at resolution 576x1024 given a context frame of the same size.\n      We use the standard image encoder from SD 2.1, but replace the decoder with a temporally-aware `deflickering decoder`.\n    - [SVD-XT](https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt): Same architecture as `SVD` but finetuned\n      for 25 frame generation.\n    - You can run the community-build gradio demo locally by running `python -m scripts.demo.gradio_app`.\n    - We provide a streamlit demo `scripts/demo/video_sampling.py` and a standalone python script `scripts/sampling/simple_video_sample.py` for inference of both models.\n    - Alongside the model, we release a [technical report](https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets).\n\n  ![tile](assets/tile.gif)\n\n**July 26, 2023**\n\n- We are releasing two new open models with a\n  permissive [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0) (see [Inference](#inference) for file\n  hashes):\n    - [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0): An improved version\n      over `SDXL-base-0.9`.\n    - [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0): An improved version\n      over `SDXL-refiner-0.9`.\n\n![sample2](assets/001_with_eval.png)\n\n**July 4, 2023**\n\n- A technical report on SDXL is now available [here](https://arxiv.org/abs/2307.01952).\n\n**June 22, 2023**\n\n- We are releasing two new diffusion models for research purposes:\n    - `SDXL-base-0.9`: The base model was trained on a variety of aspect ratios on images with resolution 1024^2. The\n      base model uses [OpenCLIP-ViT/G](https://github.com/mlfoundations/open_clip)\n      and [CLIP-ViT/L](https://github.com/openai/CLIP/tree/main) for text encoding whereas the refiner model only uses\n      the OpenCLIP model.\n    - `SDXL-refiner-0.9`: The refiner has been trained to denoise small noise levels of high quality data and as such is\n      not expected to work as a text-to-image model; instead, it should only be used as an image-to-image model.\n\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-0.9-Base model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-0.9-Refiner](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n**We plan to do a full release soon (July).**\n\n## The codebase\n\n### General Philosophy\n\nModularity is king. This repo implements a config-driven approach where we build and combine submodules by\ncalling `instantiate_from_config()` on objects defined in yaml configs. See `configs/` for many examples.\n\n### Changelog from the old `ldm` codebase\n\nFor training, we use [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/), but it should be easy to use other\ntraining wrappers around the base modules. The core diffusion model class (formerly `LatentDiffusion`,\nnow `DiffusionEngine`) has been cleaned up:\n\n- No more extensive subclassing! We now handle all types of conditioning inputs (vectors, sequences and spatial\n  conditionings, and all combinations thereof) in a single class: `GeneralConditioner`,\n  see `sgm/modules/encoders/modules.py`.\n- We separate guiders (such as classifier-free guidance, see `sgm/modules/diffusionmodules/guiders.py`) from the\n  samplers (`sgm/modules/diffusionmodules/sampling.py`), and the samplers are independent of the model.\n- We adopt the [\"denoiser framework\"](https://arxiv.org/abs/2206.00364) for both training and inference (most notable\n  change is probably now the option to train continuous time models):\n    * Discrete times models (denoisers) are simply a special case of continuous time models (denoisers);\n      see `sgm/modules/diffusionmodules/denoiser.py`.\n    * The following features are now independent: weighting of the diffusion loss\n      function (`sgm/modules/diffusionmodules/denoiser_weighting.py`), preconditioning of the\n      network (`sgm/modules/diffusionmodules/denoiser_scaling.py`), and sampling of noise levels during\n      training (`sgm/modules/diffusionmodules/sigma_sampling.py`).\n- Autoencoding models have also been cleaned up.\n\n## Installation:\n\n<a name=\"installation\"></a>\n\n#### 1. Clone the repo\n\n```shell\ngit clone https://github.com/Stability-AI/generative-models.git\ncd generative-models\n```\n\n#### 2. Setting up the virtualenv\n\nThis is assuming you have navigated to the `generative-models` root after cloning it.\n\n**NOTE:** This is tested under `python3.10`. For other python versions, you might encounter version conflicts.\n\n**PyTorch 2.0**\n\n```shell\n# install required packages from pypi\npython3 -m venv .pt2\nsource .pt2/bin/activate\npip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\npip3 install -r requirements/pt2.txt\n```\n\n#### 3. Install `sgm`\n\n```shell\npip3 install .\n```\n\n#### 4. Install `sdata` for training\n\n```shell\npip3 install -e git+https://github.com/Stability-AI/datapipelines.git@main#egg=sdata\n```\n\n## Packaging\n\nThis repository uses PEP 517 compliant packaging using [Hatch](https://hatch.pypa.io/latest/).\n\nTo build a distributable wheel, install `hatch` and run `hatch build`\n(specifying `-t wheel` will skip building a sdist, which is not necessary).\n\n```\npip install hatch\nhatch build -t wheel\n```\n\nYou will find the built package in `dist/`. You can install the wheel with `pip install dist/*.whl`.\n\nNote that the package does **not** currently specify dependencies; you will need to install the required packages,\ndepending on your use case and PyTorch version, manually.\n\n## Inference\n\nWe provide a [streamlit](https://streamlit.io/) demo for text-to-image and image-to-image sampling\nin `scripts/demo/sampling.py`.\nWe provide file hashes for the complete file as well as for only the saved tensors in the file (\nsee [Model Spec](https://github.com/Stability-AI/ModelSpec) for a script to evaluate that).\nThe following models are currently supported:\n\n- [SDXL-base-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0)\n  ```\n  File Hash (sha256): 31e35c80fc4829d14f90153f4c74cd59c90b779f6afe05a74cd6120b893f7e5b\n  Tensordata Hash (sha256): 0xd7a9105a900fd52748f20725fe52fe52b507fd36bee4fc107b1550a26e6ee1d7\n  ```\n- [SDXL-refiner-1.0](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0)\n  ```\n  File Hash (sha256): 7440042bbdc8a24813002c09b6b69b64dc90fded4472613437b7f55f9b7d9c5f\n  Tensordata Hash (sha256): 0x1a77d21bebc4b4de78c474a90cb74dc0d2217caf4061971dbfa75ad406b75d81\n  ```\n- [SDXL-base-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9)\n- [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9)\n- [SD-2.1-512](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors)\n- [SD-2.1-768](https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors)\n\n**Weights for SDXL**:\n\n**SDXL-1.0:**\nThe weights of SDXL-1.0 are available (subject to\na [`CreativeML Open RAIL++-M` license](model_licenses/LICENSE-SDXL1.0)) here:\n\n- base model: https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/\n- refiner model: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/\n\n**SDXL-0.9:**\nThe weights of SDXL-0.9 are available and subject to a [research license](model_licenses/LICENSE-SDXL0.9).\nIf you would like to access these models for your research, please apply using one of the following links:\n[SDXL-base-0.9 model](https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9),\nand [SDXL-refiner-0.9](https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9).\nThis means that you can apply for any of the two links - and if you are granted - you can access both.\nPlease log in to your Hugging Face Account with your organization email to request access.\n\nAfter obtaining the weights, place them into `checkpoints/`.\nNext, start the demo using\n\n```\nstreamlit run scripts/demo/sampling.py --server.port <your_port>\n```\n\n### Invisible Watermark Detection\n\nImages generated with our code use the\n[invisible-watermark](https://github.com/ShieldMnt/invisible-watermark/)\nlibrary to embed an invisible watermark into the model output. We also provide\na script to easily detect that watermark. Please note that this watermark is\nnot the same as in previous Stable Diffusion 1.x/2.x versions.\n\nTo run the script you need to either have a working installation as above or\ntry an _experimental_ import using only a minimal amount of packages:\n\n```bash\npython -m venv .detect\nsource .detect/bin/activate\n\npip install \"numpy>=1.17\" \"PyWavelets>=1.1.1\" \"opencv-python>=4.1.0.25\"\npip install --no-deps invisible-watermark\n```\n\nTo run the script you need to have a working installation as above. The script\nis then useable in the following ways (don't forget to activate your\nvirtual environment beforehand, e.g. `source .pt1/bin/activate`):\n\n```bash\n# test a single file\npython scripts/demo/detect.py <your filename here>\n# test multiple files at once\npython scripts/demo/detect.py <filename 1> <filename 2> ... <filename n>\n# test all files in a specific folder\npython scripts/demo/detect.py <your folder name here>/*\n```\n\n## Training:\n\nWe are providing example training configs in `configs/example_training`. To launch a training, run\n\n```\npython main.py --base configs/<config1.yaml> configs/<config2.yaml>\n```\n\nwhere configs are merged from left to right (later configs overwrite the same values).\nThis can be used to combine model, training and data configs. However, all of them can also be\ndefined in a single config. For example, to run a class-conditional pixel-based diffusion model training on MNIST,\nrun\n\n```bash\npython main.py --base configs/example_training/toy/mnist_cond.yaml\n```\n\n**NOTE 1:** Using the non-toy-dataset\nconfigs `configs/example_training/imagenet-f8_cond.yaml`, `configs/example_training/txt2img-clipl.yaml`\nand `configs/example_training/txt2img-clipl-legacy-ucg-training.yaml` for training will require edits depending on the\nused dataset (which is expected to stored in tar-file in\nthe [webdataset-format](https://github.com/webdataset/webdataset)). To find the parts which have to be adapted, search\nfor comments containing `USER:` in the respective config.\n\n**NOTE 2:** This repository supports both `pytorch1.13` and `pytorch2`for training generative models. However for\nautoencoder training as e.g. in `configs/example_training/autoencoder/kl-f4/imagenet-attnfree-logvar.yaml`,\nonly `pytorch1.13` is supported.\n\n**NOTE 3:** Training latent generative models (as e.g. in `configs/example_training/imagenet-f8_cond.yaml`) requires\nretrieving the checkpoint from [Hugging Face](https://huggingface.co/stabilityai/sdxl-vae/tree/main) and replacing\nthe `CKPT_PATH` placeholder in [this line](configs/example_training/imagenet-f8_cond.yaml#81). The same is to be done\nfor the provided text-to-image configs.\n\n### Building New Diffusion Models\n\n#### Conditioner\n\nThe `GeneralConditioner` is configured through the `conditioner_config`. Its only attribute is `emb_models`, a list of\ndifferent embedders (all inherited from `AbstractEmbModel`) that are used to condition the generative model.\nAll embedders should define whether or not they are trainable (`is_trainable`, default `False`), a classifier-free\nguidance dropout rate is used (`ucg_rate`, default `0`), and an input key (`input_key`), for example, `txt` for\ntext-conditioning or `cls` for class-conditioning.\nWhen computing conditionings, the embedder will get `batch[input_key]` as input.\nWe currently support two to four dimensional conditionings and conditionings of different embedders are concatenated\nappropriately.\nNote that the order of the embedders in the `conditioner_config` is important.\n\n#### Network\n\nThe neural network is set through the `network_config`. This used to be called `unet_config`, which is not general\nenough as we plan to experiment with transformer-based diffusion backbones.\n\n#### Loss\n\nThe loss is configured through `loss_config`. For standard diffusion model training, you will have to\nset `sigma_sampler_config`.\n\n#### Sampler config\n\nAs discussed above, the sampler is independent of the model. In the `sampler_config`, we set the type of numerical\nsolver, number of steps, type of discretization, as well as, for example, guidance wrappers for classifier-free\nguidance.\n\n### Dataset Handling\n\nFor large scale training we recommend using the data pipelines from\nour [data pipelines](https://github.com/Stability-AI/datapipelines) project. The project is contained in the requirement\nand automatically included when following the steps from the [Installation section](#installation).\nSmall map-style datasets should be defined here in the repository (e.g., MNIST, CIFAR-10, ...), and return a dict of\ndata keys/values,\ne.g.,\n\n```python\nexample = {\"jpg\": x,  # this is a tensor -1...1 chw\n           \"txt\": \"a beautiful image\"}\n```\n\nwhere we expect images in -1...1, channel-first format.\n",
  "external_links_in_readme": [
    "https://hatch.pypa.io/latest/",
    "https://huggingface.co/stabilityai/sv3d",
    "https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt",
    "https://github.com/ShieldMnt/invisible-watermark/",
    "https://github.com/Stability-AI/datapipelines",
    "https://github.com/Stability-AI/generative-models.git",
    "https://sv3d.github.io/static/paper.pdf",
    "https://clipdrop.co/",
    "https://www.youtube.com/watch?v=dtqj-s50ynU",
    "https://www.youtube.com/watch?v=RBP8vdAWTgk",
    "https://github.com/danielgatis/rembg",
    "https://stability.ai/research/adversarial-diffusion-distillation",
    "https://github.com/Stability-AI/ModelSpec",
    "https://download.pytorch.org/whl/cu118",
    "https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/",
    "https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0",
    "https://huggingface.co/stabilityai/sv4d",
    "https://streamlit.io/",
    "https://huggingface.co/stabilityai/sdxl-turbo",
    "https://sv4d.github.io",
    "https://github.com/webdataset/webdataset",
    "https://huggingface.co/stabilityai/stable-diffusion-2-1/blob/main/v2-1_768-ema-pruned.safetensors",
    "https://sv4d20.github.io",
    "https://arxiv.org/abs/2206.00364",
    "https://arxiv.org/abs/2307.01952",
    "https://arxiv.org/pdf/2503.16396",
    "https://github.com/facebookresearch/segment-anything-2",
    "https://huggingface.co/stabilityai/stable-video-diffusion-img2vid",
    "https://stability.ai/research/stable-video-diffusion-scaling-latent-video-diffusion-models-to-large-datasets",
    "https://huggingface.co/stabilityai/stable-diffusion-xl-base-0.9",
    "https://github.com/mlfoundations/open_clip",
    "https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.safetensors",
    "https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0/",
    "https://huggingface.co/stabilityai/sdxl-vae/tree/main",
    "https://huggingface.co/stabilityai/sv4d2.0",
    "https://youtu.be/Zqw4-1LcfWg",
    "https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0",
    "https://huggingface.co/stabilityai/sd-turbo",
    "https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-0.9",
    "https://sv4d.github.io/static/sv4d_technical_report.pdf",
    "https://sv3d.github.io",
    "https://github.com/openai/CLIP/tree/main",
    "https://github.com/Stability-AI/datapipelines.git@main#egg=sdata",
    "https://lightning.ai/docs/pytorch/stable/"
  ]
}
```

</details>


---

