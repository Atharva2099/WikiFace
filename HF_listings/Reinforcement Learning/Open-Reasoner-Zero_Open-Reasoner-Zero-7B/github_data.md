# GitHub Data for Open-Reasoner-Zero_Open-Reasoner-Zero-7B

**Task Category:** Reinforcement Learning

## Repository 1: deepspeedai/DeepSpeed

# GitHub Repository Data

**Repository:** [deepspeedai/DeepSpeed](https://github.com/deepspeedai/DeepSpeed)

## Basic Information

- **Description:** DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.
- **Created:** 2020-01-23T18:35:18+00:00
- **Last Updated:** 2025-06-22T01:58:40+00:00
- **Last Pushed:** 2025-06-21T21:34:57+00:00
- **Default Branch:** master
- **Size:** 227198 KB

## Statistics

- **Stars:** 39,048
- **Forks:** 4,426
- **Watchers:** 39,048
- **Open Issues:** 1,168
- **Total Issues:** 0
- **Pull Requests:** 3,342

## License

- **Type:** Apache License 2.0
- **SPDX ID:** Apache-2.0
- **URL:** [License](https://github.com/deepspeedai/DeepSpeed/blob/master/LICENSE)

## Languages

- **Python:** 5,659,821 bytes
- **C++:** 1,496,006 bytes
- **Cuda:** 713,389 bytes
- **Shell:** 28,837 bytes
- **C:** 25,866 bytes
- **Dockerfile:** 7,942 bytes
- **Makefile:** 713 bytes
- **Batchfile:** 388 bytes

## Topics

- `deep-learning`
- `pytorch`
- `gpu`
- `machine-learning`
- `billion-parameters`
- `data-parallelism`
- `model-parallelism`
- `inference`
- `pipeline-parallelism`
- `compression`
- `mixture-of-experts`
- `trillion-parameters`
- `zero`

## Top Contributors

1. **jeffra** - 443 contributions
2. **loadams** - 241 contributions
3. **tjruwase** - 181 contributions
4. **mrwyattii** - 168 contributions
5. **stas00** - 131 contributions
6. **RezaYazdaniAminabadi** - 87 contributions
7. **tohtana** - 69 contributions
8. **awan-10** - 46 contributions
9. **conglongli** - 44 contributions
10. **nelyahu** - 43 contributions

## File Structure (Sample of 10 files)

Total files: 2,166

- `.clang-format` (blob)
- `.flake8` (blob)
- `.github` (tree)
- `.github/ISSUE_TEMPLATE` (tree)
- `.github/ISSUE_TEMPLATE/ci_failure_report.md` (blob)
- `.github/ISSUE_TEMPLATE/compression_bug_report.md` (blob)
- `.github/ISSUE_TEMPLATE/deepspeed_chat_bug_report.md` (blob)
- `.github/ISSUE_TEMPLATE/feature_request.md` (blob)
- `.github/ISSUE_TEMPLATE/inference_bug_report.md` (blob)
- `.github/ISSUE_TEMPLATE/training_bug_report.md` (blob)

## Recent Issues

- 🟢 **#7377** Fix dtype mismatch in `TestParamPartitioningSkipInit` (open)
- 🟢 **#7376** Fix release of IPG buffer (open)
- 🔴 **#7375** Update latest news with DeepNVMe (closed)
- 🔴 **#7373** Relax tolerances for FP8 unit test only for ROCm + FP16  (closed)
- 🔴 **#7372** add Arctic Long Sequence Training paper reference (closed)

## Recent Pull Requests

- 🟢 **#7377** Fix dtype mismatch in `TestParamPartitioningSkipInit` (open)
- 🟢 **#7376** Fix release of IPG buffer (open)
- 🔴 **#7375** Update latest news with DeepNVMe (closed)
- 🔴 **#7373** Relax tolerances for FP8 unit test only for ROCm + FP16  (closed)
- 🔴 **#7372** add Arctic Long Sequence Training paper reference (closed)

## Recent Commits

- **9606f8f0** Update latest news with DeepNVMe (#7375) - Logan Adams (2025-06-20T17:05:59+00:00)
- **d33baf00** Relax tolerances for FP8 unit test only for ROCm + FP16  (#7373) - Ramya Ramineni (2025-06-20T15:24:43+00:00)
- **25da6fc1** Flops profiler support for F.interpolate (#7353) - Olatunji Ruwase (2025-06-20T15:02:56+00:00)
- **86097872** add ALST paper reference (#7372) - Stas Bekman (2025-06-20T00:25:11+00:00)
- **ed5f7375** Enable torch.autocast with ZeRO (#6993) - Masahiro Tanaka (2025-06-19T21:36:03+00:00)
- **d3b9cb8c** sequence parallel default dtype (#7364) - Stas Bekman (2025-06-19T18:32:14+00:00)
- **22cf1a44** Fix(scheduler): WarmupLR inherits optimizer lr when not specified (#7360) - Vensen (2025-06-19T16:32:54+00:00)
- **6f1a1c04** Restore real inputs for recompilation (#7356) - Masahiro Tanaka (2025-06-19T12:52:51+00:00)
- **f394e780** Fix tutorial title (#7365) - Stas Bekman (2025-06-17T17:10:16+00:00)
- **9ac94414** Fix 404s (#7363) - Olatunji Ruwase (2025-06-16T22:54:36+00:00)

## External Links Found in README

- https://arxiv.org/abs/2303.08374
- https://github.com/THUDM/GLM-130B
- https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepcompile/README.md
- https://www.deepspeed.ai/getting-started/
- https://www.deepspeed.ai/deepspeed4science/
- https://github.com/deepspeedai/DeepSpeed/actions/workflows/hpu-gaudi2.yml
- https://github.com/deepspeedai/DeepSpeed/actions/workflows/pages/pages-build-deployment
- https://github.com/Ascend/Ascend-CI/actions/workflows/deepspeed.yaml/badge.svg?branch=main
- https://dl.acm.org/doi/10.1145/3394486.3406703
- https://www.deepspeed.ai/tutorials/advanced-install/
- https://arxiv.org/abs/1910.02054
- https://arxiv.org/abs/2310.04610
- https://innovation.microsoft.com/en-us/exploring-ai-at-scale
- https://openreview.net/forum?id=Pgtn4l6eKjv
- https://arxiv.org/abs/2303.06318
- https://arxiv.org/abs/2312.08583
- https://www.deepspeed.ai/
- https://github.com/Ascend/Ascend-CI/actions/workflows/deepspeed.yaml
- https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch110-p40.yml/badge.svg?branch=master
- https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-torch-latest.yml

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 235860204,
  "name": "DeepSpeed",
  "full_name": "deepspeedai/DeepSpeed",
  "description": "DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.",
  "html_url": "https://github.com/deepspeedai/DeepSpeed",
  "clone_url": "https://github.com/deepspeedai/DeepSpeed.git",
  "ssh_url": "git@github.com:deepspeedai/DeepSpeed.git",
  "homepage": "https://www.deepspeed.ai/",
  "topics": [
    "deep-learning",
    "pytorch",
    "gpu",
    "machine-learning",
    "billion-parameters",
    "data-parallelism",
    "model-parallelism",
    "inference",
    "pipeline-parallelism",
    "compression",
    "mixture-of-experts",
    "trillion-parameters",
    "zero"
  ],
  "default_branch": "master",
  "created_at": "2020-01-23T18:35:18+00:00",
  "updated_at": "2025-06-22T01:58:40+00:00",
  "pushed_at": "2025-06-21T21:34:57+00:00",
  "size_kb": 227198,
  "watchers_count": 39048,
  "stargazers_count": 39048,
  "forks_count": 4426,
  "open_issues_count": 1168,
  "license": {
    "key": "apache-2.0",
    "name": "Apache License 2.0",
    "spdx_id": "Apache-2.0",
    "url": "https://github.com/deepspeedai/DeepSpeed/blob/master/LICENSE"
  },
  "languages": {
    "Python": 5659821,
    "C++": 1496006,
    "Cuda": 713389,
    "Shell": 28837,
    "C": 25866,
    "Dockerfile": 7942,
    "Makefile": 713,
    "Batchfile": 388
  },
  "top_contributors": [
    {
      "login": "jeffra",
      "contributions": 443
    },
    {
      "login": "loadams",
      "contributions": 241
    },
    {
      "login": "tjruwase",
      "contributions": 181
    },
    {
      "login": "mrwyattii",
      "contributions": 168
    },
    {
      "login": "stas00",
      "contributions": 131
    },
    {
      "login": "RezaYazdaniAminabadi",
      "contributions": 87
    },
    {
      "login": "tohtana",
      "contributions": 69
    },
    {
      "login": "awan-10",
      "contributions": 46
    },
    {
      "login": "conglongli",
      "contributions": 44
    },
    {
      "login": "nelyahu",
      "contributions": 43
    },
    {
      "login": "lekurile",
      "contributions": 38
    },
    {
      "login": "jomayeri",
      "contributions": 37
    },
    {
      "login": "delock",
      "contributions": 36
    },
    {
      "login": "samyam",
      "contributions": 33
    },
    {
      "login": "inkcherry",
      "contributions": 32
    },
    {
      "login": "Yejing-Lai",
      "contributions": 32
    },
    {
      "login": "cli99",
      "contributions": 31
    },
    {
      "login": "cmikeh2",
      "contributions": 31
    },
    {
      "login": "HeyangQin",
      "contributions": 25
    },
    {
      "login": "molly-smith",
      "contributions": 24
    }
  ],
  "file_tree_count": 2166,
  "file_tree_sample": [
    {
      "path": ".clang-format",
      "type": "blob"
    },
    {
      "path": ".flake8",
      "type": "blob"
    },
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE",
      "type": "tree"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/ci_failure_report.md",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/compression_bug_report.md",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/deepspeed_chat_bug_report.md",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/feature_request.md",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/inference_bug_report.md",
      "type": "blob"
    },
    {
      "path": ".github/ISSUE_TEMPLATE/training_bug_report.md",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 3342,
  "recent_issues": [
    {
      "number": 7377,
      "title": "Fix dtype mismatch in `TestParamPartitioningSkipInit`",
      "state": "open"
    },
    {
      "number": 7376,
      "title": "Fix release of IPG buffer",
      "state": "open"
    },
    {
      "number": 7375,
      "title": "Update latest news with DeepNVMe",
      "state": "closed"
    },
    {
      "number": 7373,
      "title": "Relax tolerances for FP8 unit test only for ROCm + FP16 ",
      "state": "closed"
    },
    {
      "number": 7372,
      "title": "add Arctic Long Sequence Training paper reference",
      "state": "closed"
    }
  ],
  "recent_pulls": [
    {
      "number": 7377,
      "title": "Fix dtype mismatch in `TestParamPartitioningSkipInit`",
      "state": "open"
    },
    {
      "number": 7376,
      "title": "Fix release of IPG buffer",
      "state": "open"
    },
    {
      "number": 7375,
      "title": "Update latest news with DeepNVMe",
      "state": "closed"
    },
    {
      "number": 7373,
      "title": "Relax tolerances for FP8 unit test only for ROCm + FP16 ",
      "state": "closed"
    },
    {
      "number": 7372,
      "title": "add Arctic Long Sequence Training paper reference",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "9606f8f0100700fab58ead3a98bece26085878bd",
      "author": "Logan Adams",
      "date": "2025-06-20T17:05:59+00:00",
      "message": "Update latest news with DeepNVMe (#7375)"
    },
    {
      "sha": "d33baf009bfc0d1a55b541c6131f3b47a5184330",
      "author": "Ramya Ramineni",
      "date": "2025-06-20T15:24:43+00:00",
      "message": "Relax tolerances for FP8 unit test only for ROCm + FP16  (#7373)"
    },
    {
      "sha": "25da6fc1ab30e21c7e7346c3e384fffc1ef030dd",
      "author": "Olatunji Ruwase",
      "date": "2025-06-20T15:02:56+00:00",
      "message": "Flops profiler support for F.interpolate (#7353)"
    },
    {
      "sha": "86097872c672961de659369c3a064e6911d61e10",
      "author": "Stas Bekman",
      "date": "2025-06-20T00:25:11+00:00",
      "message": "add ALST paper reference (#7372)"
    },
    {
      "sha": "ed5f737554bf30f04c209e6236852f92386318b6",
      "author": "Masahiro Tanaka",
      "date": "2025-06-19T21:36:03+00:00",
      "message": "Enable torch.autocast with ZeRO (#6993)"
    },
    {
      "sha": "d3b9cb8c4e785b44e2fe6516868c3ca4efcaa206",
      "author": "Stas Bekman",
      "date": "2025-06-19T18:32:14+00:00",
      "message": "sequence parallel default dtype (#7364)"
    },
    {
      "sha": "22cf1a44013cd026a91a4a048053934d1eb80ef4",
      "author": "Vensen",
      "date": "2025-06-19T16:32:54+00:00",
      "message": "Fix(scheduler): WarmupLR inherits optimizer lr when not specified (#7360)"
    },
    {
      "sha": "6f1a1c04c15dc598186e7c290860279f3a325df9",
      "author": "Masahiro Tanaka",
      "date": "2025-06-19T12:52:51+00:00",
      "message": "Restore real inputs for recompilation (#7356)"
    },
    {
      "sha": "f394e7803611f31157c5f449ba0c7f01859e936a",
      "author": "Stas Bekman",
      "date": "2025-06-17T17:10:16+00:00",
      "message": "Fix tutorial title (#7365)"
    },
    {
      "sha": "9ac94414000978054dd67b298d91b603ae794ce8",
      "author": "Olatunji Ruwase",
      "date": "2025-06-16T22:54:36+00:00",
      "message": "Fix 404s (#7363)"
    },
    {
      "sha": "600d280f211a1479cde6ff59790876b10c5f7003",
      "author": "Masahiro Tanaka",
      "date": "2025-06-14T23:58:56+00:00",
      "message": "Improve padding util for compile (#7355)"
    },
    {
      "sha": "766312154954bab3003193417da1a2f79d14cc34",
      "author": "wzy",
      "date": "2025-06-12T20:18:51+00:00",
      "message": "Fix error of <glog/logging.h> (#7351)"
    },
    {
      "sha": "10b106619a0da36e0fdd7b3c3a2cf8bd6eefa002",
      "author": "Olatunji Ruwase",
      "date": "2025-06-11T17:00:58+00:00",
      "message": "Don't break set_start_method (#7349)"
    },
    {
      "sha": "d7e60fd0f6f214f618b7af0396358f1e50161e8f",
      "author": "Stas Bekman",
      "date": "2025-06-11T00:10:54+00:00",
      "message": "s/UlyssesPlus/Arctic Long Sequence Training (ALST)/ (#7348)"
    },
    {
      "sha": "4686d5ef0b450a42f3807231f4e9b2544c14ae6d",
      "author": "Logan Adams",
      "date": "2025-06-10T04:03:16+00:00",
      "message": "Update version after 0.17.1 release (#7345)"
    },
    {
      "sha": "2ce55057999bdf1c9bf26cb5f000a8a9fb97e2d9",
      "author": "Logan Adams",
      "date": "2025-06-09T22:42:55+00:00",
      "message": "Move pytest pinning from individual tests to requirements-dev.txt until fixed. (#7327)"
    },
    {
      "sha": "4d0c159630efd7b36ae587c1c979af98cb301524",
      "author": "Felix Gondwe",
      "date": "2025-06-09T20:15:44+00:00",
      "message": "Fix docs that are rendering Incorrectly (#7344)"
    },
    {
      "sha": "e440506bee5f523691693a7fad6251202ec3dbcb",
      "author": "Olatunji Ruwase",
      "date": "2025-06-09T17:30:51+00:00",
      "message": "Improve overflow handling in ZeRO (#6976)"
    },
    {
      "sha": "bb293aea5d61198327f8d8956925ab754b747f44",
      "author": "Olatunji Ruwase",
      "date": "2025-06-09T16:15:18+00:00",
      "message": "Update folder name (#7343)"
    },
    {
      "sha": "05818e90d916734d45fb4a23a85d17574161d0e2",
      "author": "Emmanuel Ferdman",
      "date": "2025-06-07T18:09:01+00:00",
      "message": "Fix LoRA arxiv reference (#7340)"
    }
  ],
  "readme_text": "[![License Apache 2.0](https://badgen.net/badge/license/apache2.0/blue)](https://github.com/deepspeedai/DeepSpeed/blob/master/LICENSE)\n[![PyPI version](https://badge.fury.io/py/deepspeed.svg)](https://pypi.org/project/deepspeed/)\n[![Downloads](https://static.pepy.tech/badge/deepspeed)](https://pepy.tech/project/deepspeed)\n[![Build](https://badgen.net/badge/build/check-status/blue)](#build-pipeline-status)\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/9530/badge)](https://www.bestpractices.dev/projects/9530)\n[![Twitter](https://img.shields.io/twitter/follow/DeepSpeedAI)](https://twitter.com/intent/follow?screen_name=DeepSpeedAI)\n[![Japanese Twitter](https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9ETwitter-%40DeepSpeedAI_JP-blue)](https://twitter.com/DeepSpeedAI_JP)\n[![Chinese Zhihu](https://img.shields.io/badge/%E7%9F%A5%E4%B9%8E-%E5%BE%AE%E8%BD%AFDeepSpeed-blue)](https://www.zhihu.com/people/deepspeed)\n\n\n<div align=\"center\">\n <img src=\"docs/assets/images/DeepSpeed_light.svg#gh-light-mode-only\" width=\"400px\">\n <img src=\"docs/assets/images/DeepSpeed_dark_transparent.svg#gh-dark-mode-only\" width=\"400px\">\n</div>\n\n## Latest News\n<b> <span style=\"color:orange\" > DeepSpeed empowers ChatGPT-like model training with a single click, offering 15x speedup over SOTA RLHF systems with unprecedented cost reduction at all scales; [learn how](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/deepspeed-chat)</span>.</b>\n* [2025/04] [DeepCompile: Unlocking Compiler Optimization for Distributed Training](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepcompile/README.md)\n* [2025/03] [DeepSpeed-AutoTP: Automatic Tensor Parallel Training of Hugging Face models](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/huggingface-tp/README.md)\n* [2024/12] [Ulysses-Offload: Democratizing Long Context LLM Training ](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/ulysses-offload/README.md)\n* [2024/12] [DeepSpeed-Domino: Communication-Free LLM Training Engine](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-domino/README.md)\n* [2024/08] [DeepSpeed on Windows](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/windows/08-2024/README.md) [[\u65e5\u672c\u8a9e](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/windows/08-2024/japanese/README.md)]  [[\u4e2d\u6587](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/windows/08-2024/chinese/README.md)]\n\n<!-- NOTE: we must use html for news items otherwise links will be broken in the 'more news' section -->\n<details>\n <summary>More news</summary>\n <ul>\n   <li> [2024/08] <a href=\"https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-gds/README.md\"> DeepNVMe: Improving DL Applications through I/O Optimizations</a> [<a href=\"ttps://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-gds/japanese/README.md\"> \u65e5\u672c\u8a9e </a>] [<a href=\"https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-gds/japanese/README.md\"> \u4e2d\u6587 </a>]</li>\n\n    <li> [2024/07] <a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ucp/README.md\"> DeepSpeed Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training</a> [<a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ucp/japanese/README.md\"> \u65e5\u672c\u8a9e </a>] </li>\n\n   <li> [2024/03] <a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024/README.md\"> DeepSpeed-FP6: The Power of FP6-Centric Serving for Large Language Models</a> [<a href=\"https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024/README-Chinese.md\"> \u4e2d\u6587 </a>] </li>\n\n </ul>\n</details>\n\n---\n\n# Extreme Speed and Scale for DL Training and Inference\n\n***[DeepSpeed](https://www.deepspeed.ai/) enabled the world's most powerful language models (at the time of this writing) such as [MT-530B](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/) and [BLOOM](https://huggingface.co/blog/bloom-megatron-deepspeed)***. It is an easy-to-use deep learning optimization software suite that powers unprecedented scale and speed for both training and inference. With DeepSpeed you can:\n\n* Train/Inference dense or sparse models with billions or trillions of parameters\n* Achieve excellent system throughput and efficiently scale to thousands of GPUs\n* Train/Inference on resource constrained GPU systems\n* Achieve unprecedented low latency and high throughput for inference\n* Achieve extreme compression for an unparalleled inference latency and model size reduction with low costs\n\n---\n\n# DeepSpeed's four innovation pillars\n\n<img src=\"docs/assets/images/DeepSpeed-pillars.png\" width=\"800px\">\n\n\n## DeepSpeed-Training\n\nDeepSpeed offers a confluence of system innovations, that has made large scale DL training effective, and efficient, greatly improved ease of use, and redefined the DL training landscape in terms of scale that is possible. These innovations such as ZeRO, 3D-Parallelism, DeepSpeed-MoE, ZeRO-Infinity, etc. fall under the training pillar. Learn more: [DeepSpeed-Training](https://www.deepspeed.ai/training/)\n\n## DeepSpeed-Inference\n\nDeepSpeed brings together innovations in parallelism technology such as tensor, pipeline, expert and ZeRO-parallelism, and combines them with high performance custom inference kernels, communication optimizations and heterogeneous memory technologies to enable inference at an unprecedented scale, while achieving unparalleled latency, throughput and cost reduction. This systematic composition of system technologies for inference falls under the inference pillar. Learn more: [DeepSpeed-Inference](https://www.deepspeed.ai/inference)\n\n\n## DeepSpeed-Compression\n\nTo further increase the inference efficiency, DeepSpeed offers easy-to-use and flexible-to-compose compression techniques for researchers and practitioners to compress their models while delivering faster speed, smaller model size, and significantly reduced compression cost. Moreover, SoTA innovations on compression like ZeroQuant and XTC are included under the compression pillar. Learn more: [DeepSpeed-Compression](https://www.deepspeed.ai/compression)\n\n## DeepSpeed4Science\n\nIn line with Microsoft's mission to solve humanity's most pressing challenges, the DeepSpeed team at Microsoft is responding to this opportunity by launching a new initiative called *DeepSpeed4Science*, aiming to build unique capabilities through AI system technology innovations to help domain experts to unlock today's biggest science mysteries. Learn more: [tutorials](https://www.deepspeed.ai/deepspeed4science/)\n\n---\n\n# DeepSpeed Software Suite\n\n## DeepSpeed Library\n\n   The [DeepSpeed](https://github.com/deepspeedai/deepspeed) library (this repository) implements and packages the innovations and technologies in DeepSpeed Training, Inference and Compression Pillars into a single easy-to-use, open-sourced repository. It allows for easy composition of multitude of features within a single training, inference or compression pipeline. The DeepSpeed Library is heavily adopted by the DL community, and has been used to enable some of the most powerful models (see [DeepSpeed Adoption](#deepspeed-adoption)).\n\n## Model Implementations for Inference (MII)\n\n   [Model Implementations for Inference (MII)](https://github.com/deepspeedai/deepspeed-mii) is an open-sourced repository for making low-latency and high-throughput inference accessible to all data scientists by alleviating the need to apply complex system optimization techniques themselves. Out-of-box, MII offers support for thousands of widely used DL models, optimized using DeepSpeed-Inference, that can be deployed with a few lines of code, while achieving significant latency reduction compared to their vanilla open-sourced versions.\n\n## DeepSpeed on Azure\n\n   DeepSpeed users are diverse and have access to different environments. We recommend to try DeepSpeed on Azure as it is the simplest and easiest method. The recommended method to try DeepSpeed on Azure is through AzureML [recipes](https://github.com/Azure/azureml-examples/tree/main/v1/python-sdk/workflows/train/deepspeed). The job submission and data preparation scripts have been made available [here](https://github.com/deepspeedai/Megatron-DeepSpeed/tree/main/examples_deepspeed/azureml). For more details on how to use DeepSpeed on Azure, please follow the [Azure tutorial](https://www.deepspeed.ai/tutorials/azure/).\n\n---\n\n# DeepSpeed Adoption\n\nDeepSpeed was an important part of Microsoft\u2019s\n[AI at Scale](https://www.microsoft.com/en-us/research/project/ai-at-scale/)\ninitiative to enable next-generation AI capabilities at scale, where you can find more\ninformation [here](https://innovation.microsoft.com/en-us/exploring-ai-at-scale).\n\nDeepSpeed has been used to train many different large-scale models, below is a list of several examples that we are aware of (if you'd like to include your model please submit a PR):\n\n  * [Megatron-Turing NLG (530B)](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/)\n  * [Jurassic-1 (178B)](https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf)\n  * [BLOOM (176B)](https://huggingface.co/blog/bloom-megatron-deepspeed)\n  * [GLM (130B)](https://github.com/THUDM/GLM-130B)\n  * [xTrimoPGLM (100B)](https://www.biorxiv.org/content/10.1101/2023.07.05.547496v2)\n  * [YaLM (100B)](https://github.com/yandex/YaLM-100B)\n  * [GPT-NeoX (20B)](https://github.com/EleutherAI/gpt-neox)\n  * [AlexaTM (20B)](https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning)\n  * [Turing NLG (17B)](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)\n  * [METRO-LM (5.4B)](https://arxiv.org/pdf/2204.06644.pdf)\n\nDeepSpeed has been integrated with several different popular open-source DL frameworks such as:\n\n|                                                                                                | Documentation                                |\n| ---------------------------------------------------------------------------------------------- | -------------------------------------------- |\n<img src=\"docs/assets/images/transformers-light.png#gh-light-mode-only\" width=\"250px\"><img src=\"docs/assets/images/transformers-dark.png#gh-dark-mode-only\" width=\"250px\"> | [Transformers with DeepSpeed](https://huggingface.co/docs/transformers/deepspeed) |\n| <img src=\"docs/assets/images/accelerate-light.png#gh-light-mode-only\" width=\"250px\"><img src=\"docs/assets/images/accelerate-dark.png#gh-dark-mode-only\" width=\"250px\"> | [Accelerate with DeepSpeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed) |\n| <img src=\"docs/assets/images/lightning-light.svg#gh-light-mode-only\" width=\"200px\"><img src=\"docs/assets/images/lightning-dark.svg#gh-dark-mode-only\" width=\"200px\"> | [Lightning with DeepSpeed](https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html#deepspeed) |\n| <img src=\"docs/assets/images/mosaicml.svg\" width=\"200px\"> | [MosaicML with DeepSpeed](https://docs.mosaicml.com/projects/composer/en/latest/trainer/using_the_trainer.html?highlight=deepspeed#deepspeed-integration) |\n| <img src=\"docs/assets/images/determined.svg\" width=\"225px\"> | [Determined with DeepSpeed](https://docs.determined.ai/latest/training/apis-howto/deepspeed/overview.html) |\n| <img src=\"https://user-images.githubusercontent.com/58739961/187154444-fce76639-ac8d-429b-9354-c6fac64b7ef8.jpg\" width=150> | [MMEngine with DeepSpeed](https://mmengine.readthedocs.io/en/latest/common_usage/large_model_training.html#deepspeed) |\n\n---\n\n# Build Pipeline Status\n\n| Description | Status |\n| ----------- | ------ |\n| NVIDIA | [![nv-torch110-p40](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch110-p40.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch110-p40.yml) [![nv-torch110-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch110-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch110-v100.yml) [![nv-torch-latest-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml) [![nv-h100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-h100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-h100.yml) [![nv-inference](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-inference.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-inference.yml) [![nv-nightly](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-nightly.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-nightly.yml) |\n| AMD | [![amd-mi200](https://github.com/deepspeedai/DeepSpeed/actions/workflows/amd-mi200.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/amd-mi200.yml) |\n| CPU | [![torch-latest-cpu](https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-torch-latest.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-torch-latest.yml) [![cpu-inference](https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-inference.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-inference.yml) |\n| Intel Gaudi | [![hpu-gaudi2](https://github.com/deepspeedai/DeepSpeed/actions/workflows/hpu-gaudi2.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/hpu-gaudi2.yml) |\n| Intel XPU | [![xpu-max1100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/xpu-max1100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/xpu-max1100.yml) |\n| PyTorch Nightly | [![nv-torch-nightly-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml) |\n| Integrations | [![nv-transformers-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-transformers-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-transformers-v100.yml) [![nv-lightning-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-lightning-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-lightning-v100.yml) [![nv-accelerate-v100](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-accelerate-v100.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-accelerate-v100.yml) [![nv-mii](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-mii.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-mii.yml) [![nv-ds-chat](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-ds-chat.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-ds-chat.yml) [![nv-sd](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-sd.yml/badge.svg)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-sd.yml) |\n| Misc | [![Formatting](https://github.com/deepspeedai/DeepSpeed/actions/workflows/formatting.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/formatting.yml) [![pages-build-deployment](https://github.com/deepspeedai/DeepSpeed/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/pages/pages-build-deployment) [![Documentation Status](https://readthedocs.org/projects/deepspeed/badge/?version=latest)](https://deepspeed.readthedocs.io/en/latest/?badge=latest)[![python](https://github.com/deepspeedai/DeepSpeed/actions/workflows/python.yml/badge.svg?branch=master)](https://github.com/deepspeedai/DeepSpeed/actions/workflows/python.yml) |\n| Huawei Ascend NPU | [![Huawei Ascend NPU](https://github.com/Ascend/Ascend-CI/actions/workflows/deepspeed.yaml/badge.svg?branch=main)](https://github.com/Ascend/Ascend-CI/actions/workflows/deepspeed.yaml) |\n\n# Installation\n\nThe quickest way to get started with DeepSpeed is via pip, this will install\nthe latest release of DeepSpeed which is not tied to specific PyTorch or CUDA\nversions. DeepSpeed includes several C++/CUDA extensions that we commonly refer\nto as our 'ops'.  By default, all of these extensions/ops will be built\njust-in-time (JIT) using [torch's JIT C++ extension loader that relies on\nninja](https://pytorch.org/docs/stable/cpp_extension.html) to build and\ndynamically link them at runtime.\n\n## Requirements\n* [PyTorch](https://pytorch.org/) must be installed _before_ installing DeepSpeed.\n* For full feature support we recommend a version of PyTorch that is >= 1.9 and ideally the latest PyTorch stable release.\n* A CUDA or ROCm compiler such as [nvcc](https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#introduction) or [hipcc](https://github.com/ROCm-Developer-Tools/HIPCC) used to compile C++/CUDA/HIP extensions.\n* Specific GPUs we develop and test against are listed below, this doesn't mean your GPU will not work if it doesn't fall into this category it's just DeepSpeed is most well tested on the following:\n  * NVIDIA: Pascal, Volta, Ampere, and Hopper architectures\n  * AMD: MI100 and MI200\n\n## Contributed HW support\n* DeepSpeed now support various HW accelerators.\n\n| Contributor | Hardware                            | Accelerator Name | Contributor validated | Upstream validated |\n|-------------|-------------------------------------|------------------| --------------------- |--------------------|\n| Huawei      | Huawei Ascend NPU                   | npu              | Yes | No                 |\n| Intel       | Intel(R) Gaudi(R) 2 AI accelerator  | hpu              | Yes | Yes                |\n| Intel       | Intel(R) Xeon(R) Processors         | cpu              | Yes | Yes                |\n| Intel       | Intel(R) Data Center GPU Max series | xpu              | Yes | Yes                |\n| Tecorigin   | Scalable Data Analytics Accelerator | sdaa             | Yes | No                 |\n\n## PyPI\nWe regularly push releases to [PyPI](https://pypi.org/project/deepspeed/) and encourage users to install from there in most cases.\n\n```bash\npip install deepspeed\n```\n\nAfter installation, you can validate your install and see which extensions/ops\nyour machine is compatible with via the DeepSpeed environment report.\n\n```bash\nds_report\n```\n\nIf you would like to pre-install any of the DeepSpeed extensions/ops (instead\nof JIT compiling) or install pre-compiled ops via PyPI please see our [advanced\ninstallation instructions](https://www.deepspeed.ai/tutorials/advanced-install/).\n\n## Windows\nMany DeepSpeed features are supported on Windows for both training and inference. You can read more about this in the original blog post [here](https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/windows/08-2024/README.md). Among features that are currently not supported are async io (AIO) and GDS (which does not support Windows).\n1. Install PyTorch, such as pytorch 2.3+cu121.\n2. Install Visual C++ build tools, such as VS2022 C++ x64/x86 build tools.\n3. Launch Cmd console with Administrator permissions for creating required symlink folders and ensure MSVC tools are added to your PATH or launch the Developer Command Prompt for Visual Studio 2022 with administrator permissions.\n4. Run `build_win.bat` to build wheel in `dist` folder.\n\n# Features\n\nPlease checkout [DeepSpeed-Training](https://www.deepspeed.ai/training), [DeepSpeed-Inference](https://www.deepspeed.ai/inference) and [DeepSpeed-Compression](https://www.deepspeed.ai/compression) pages for full set of features offered along each of these three pillars.\n\n# Further Reading\n\nAll DeepSpeed documentation, tutorials, and blogs can be found on our website: [deepspeed.ai](https://www.deepspeed.ai/)\n\n\n|                                                                                                | Description                                  |\n| ---------------------------------------------------------------------------------------------- | -------------------------------------------- |\n| [Getting Started](https://www.deepspeed.ai/getting-started/)                                   |  First steps with DeepSpeed                  |\n| [DeepSpeed JSON Configuration](https://www.deepspeed.ai/docs/config-json/)                     |  Configuring DeepSpeed                       |\n| [API Documentation](https://deepspeed.readthedocs.io/en/latest/)                               |  Generated DeepSpeed API documentation       |\n| [Tutorials](https://www.deepspeed.ai/tutorials/)                                               |  Tutorials                                   |\n| [Blogs](https://www.deepspeed.ai/posts/)                                                       |  Blogs                                   |\n\n\n# Contributing\nDeepSpeed welcomes your contributions! Please see our\n[contributing](CONTRIBUTING.md) guide for more details on formatting, testing,\netc.<br/>\nThanks so much to all of our amazing contributors!\n\n<a href=\"https://github.com/deepspeedai/DeepSpeed/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=microsoft/DeepSpeed&r=\"  width=\"800px\"/>\n</a>\n\n## Contributor License Agreement\nThis project welcomes contributions and suggestions. Most contributions require you to\nagree to a Contributor License Agreement (CLA) declaring that you have the right to, and\nactually do, grant us the rights to use your contribution. For details, visit\nhttps://cla.opensource.microsoft.com.\n\nWhen you submit a pull request, a CLA bot will automatically determine whether you need\nto provide a CLA and decorate the PR appropriately (e.g., status check, comment). Simply\nfollow the instructions provided by the bot. You will only need to do this once across\nall repos using our CLA.\n\n## Code of Conduct\nThis project has adopted the [Microsoft Open Source Code of\nConduct](https://opensource.microsoft.com/codeofconduct/). For more information see the\n[Code of Conduct FAQ](https://opensource.microsoft.com/codeofconduct/faq/) or contact\n[opencode@microsoft.com](mailto:opencode@microsoft.com) with any additional questions or comments.\n\n# Publications\n1. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, Yuxiong He. (2019) ZeRO: memory optimizations toward training trillion parameter models. [arXiv:1910.02054](https://arxiv.org/abs/1910.02054) and [In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis (SC '20)](https://dl.acm.org/doi/10.5555/3433701.3433727).\n2. Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. (2020) DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. [In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD '20, Tutorial)](https://dl.acm.org/doi/10.1145/3394486.3406703).\n3. Minjia Zhang, Yuxiong He. (2020) Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping. [arXiv:2010.13369](https://arxiv.org/abs/2010.13369) and [NeurIPS 2020](https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html).\n4. Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, Yuxiong He. (2021) ZeRO-Offload: Democratizing Billion-Scale Model Training. [arXiv:2101.06840](https://arxiv.org/abs/2101.06840) and [USENIX ATC 2021](https://www.usenix.org/conference/atc21/presentation/ren-jie). [[paper]](https://arxiv.org/abs/2101.06840) [[slides]](https://www.usenix.org/system/files/atc21_slides_ren-jie.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/)\n5. Hanlin Tang, Shaoduo Gan, Ammar Ahmad Awan, Samyam Rajbhandari, Conglong Li, Xiangru Lian, Ji Liu, Ce Zhang, Yuxiong He. (2021) 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed. [arXiv:2102.02888](https://arxiv.org/abs/2102.02888) and [ICML 2021](http://proceedings.mlr.press/v139/tang21a.html).\n6. Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith, Yuxiong He. (2021) ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep Learning. [arXiv:2104.07857](https://arxiv.org/abs/2104.07857) and [SC 2021](https://dl.acm.org/doi/abs/10.1145/3458817.3476205). [[paper]](https://arxiv.org/abs/2104.07857) [[slides]](docs/assets/files/SC21-ZeRO-Infinity.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/)\n7. Conglong Li, Ammar Ahmad Awan, Hanlin Tang, Samyam Rajbhandari, Yuxiong He. (2021) 1-bit LAMB: Communication Efficient Large-Scale Large-Batch Training with LAMB's Convergence Speed. [arXiv:2104.06069](https://arxiv.org/abs/2104.06069) and [HiPC 2022](https://hipc.org/advance-program/).\n8. Conglong Li, Minjia Zhang, Yuxiong He. (2021) The Stability-Efficiency Dilemma: Investigating Sequence Length Warmup for Training GPT Models. [arXiv:2108.06084](https://arxiv.org/abs/2108.06084) and [NeurIPS 2022](https://openreview.net/forum?id=JpZ5du_Kdh).\n9. Yucheng Lu, Conglong Li, Minjia Zhang, Christopher De Sa, Yuxiong He. (2022) Maximizing Communication Efficiency for Large-scale Training via 0/1 Adam. [arXiv:2202.06009](https://arxiv.org/abs/2202.06009).\n10. Samyam Rajbhandari, Conglong Li, Zhewei Yao, Minjia Zhang, Reza Yazdani Aminabadi, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He. (2022) DeepSpeed-MoE: Advancing Mixture-of-Experts Inference and Training to Power Next-Generation AI Scale [arXiv:2201.05596](https://arxiv.org/abs/2201.05596) and [ICML 2022](https://proceedings.mlr.press/v162/rajbhandari22a.html). [[pdf]](https://arxiv.org/abs/2201.05596) [[slides]](docs/assets/files/ICML-5mins.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/)\n11. Shaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, Bryan Catanzaro. (2022) Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model [arXiv:2201.11990](https://arxiv.org/abs/2201.11990).\n12. Xiaoxia Wu, Zhewei Yao, Minjia Zhang, Conglong Li, Yuxiong He. (2022) Extreme Compression for Pre-trained Transformers Made Simple and Efficient. [arXiv:2206.01859](https://arxiv.org/abs/2206.01859) and [NeurIPS 2022](https://openreview.net/forum?id=xNeAhc2CNAl).\n13. Zhewei Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He. (2022) ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. [arXiv:2206.01861](https://arxiv.org/abs/2206.01861) and [NeurIPS 2022](https://openreview.net/forum?id=f-fVCElZ-G1) [[slides]](docs/assets/files/zeroquant_series.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-compression-a-composable-library-for-extreme-compression-and-zero-cost-quantization/)\n14. Reza Yazdani Aminabadi, Samyam Rajbhandari, Minjia Zhang, Ammar Ahmad Awan, Cheng Li, Du Li, Elton Zheng, Jeff Rasley, Shaden Smith, Olatunji Ruwase, Yuxiong He. (2022) DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale. [arXiv:2207.00032](https://arxiv.org/abs/2207.00032) and [SC 2022](https://dl.acm.org/doi/abs/10.5555/3571885.3571946). [[paper]](https://arxiv.org/abs/2207.00032) [[slides]](docs/assets/files/sc22-ds-inference.pdf) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/)\n15. Zhewei Yao, Xiaoxia Wu, Conglong Li, Connor Holmes, Minjia Zhang, Cheng Li, Yuxiong He. (2022) Random-LTD: Random and Layerwise Token Dropping Brings Efficient Training for Large-scale Transformers. [arXiv:2211.11586](https://arxiv.org/abs/2211.11586).\n16. Conglong Li, Zhewei Yao, Xiaoxia Wu, Minjia Zhang, Yuxiong He. (2022) DeepSpeed Data Efficiency: Improving Deep Learning Model Quality and Training Efficiency via Efficient Data Sampling and Routing. [arXiv:2212.03597](https://arxiv.org/abs/2212.03597) [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/)\n17. Xiaoxia Wu, Cheng Li, Reza Yazdani Aminabadi, Zhewei Yao, Yuxiong He. (2023) Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases. [arXiv:2301.12017](https://arxiv.org/abs/2301.12017) and [ICML2023](https://icml.cc/Conferences/2023).\n18. Syed Zawad, Cheng Li, Zhewei Yao, Elton Zheng, Yuxiong He, Feng Yan. (2023) DySR: Adaptive Super-Resolution via Algorithm and System Co-design. [ICLR:2023](https://openreview.net/forum?id=Pgtn4l6eKjv).\n19. Sheng Shen, Zhewei Yao, Chunyuan Li, Trevor Darrell, Kurt Keutzer, Yuxiong He. (2023) Scaling Vision-Language Models with Sparse Mixture of Experts. [arXiv:2303.07226](https://arxiv.org/abs/2303.07226) and [Finding at EMNLP2023](https://2023.emnlp.org/).\n20. Quentin Anthony, Ammar Ahmad Awan, Jeff Rasley, Yuxiong He, Aamir Shafi, Mustafa Abduljabbar, Hari Subramoni, Dhabaleswar Panda. (2023) MCR-DL: Mix-and-Match Communication Runtime for Deep Learning [arXiv:2303.08374](https://arxiv.org/abs/2303.08374) and will appear at IPDPS 2023.\n21. Siddharth Singh, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He, Abhinav Bhatele. (2023) A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-of-Experts Training [arXiv:2303.06318](https://arxiv.org/abs/2303.06318) and [ICS 2023](https://dl.acm.org/doi/10.1145/3577193.3593704).\n22. Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Xiaoxia Wu, Connor Holmes, Zhewei Yao, Samyam Rajbhandari, Olatunji Ruwase, Feng Yan, Lei Yang, Yuxiong He. (2023) ZeRO++: Extremely Efficient Collective Communication for Giant Model Training [arXiv:2306.10209](https://arxiv.org/abs/2306.10209) and [ML for Sys Workshop at NeurIPS2023](http://mlforsystems.org/) [[blog]](https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/)\n23. Zhewei Yao, Xiaoxia Wu, Cheng Li, Stephen Youn, Yuxiong He. (2023) ZeroQuant-V2: Exploring Post-training Quantization in LLMs from Comprehensive Study to Low Rank Compensation [arXiv:2303.08302](https://arxiv.org/abs/2303.08302) and [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/) [[slides]](docs/assets/files/zeroquant_series.pdf)\n24. Pareesa Ameneh Golnari, Zhewei Yao, Yuxiong He. (2023) Selective Guidance: Are All the Denoising Steps of Guided Diffusion Important? [arXiv:2305.09847](https://arxiv.org/abs/2305.09847)\n25. Zhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Ammar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, Yuxiong He. (2023) DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales [arXiv:2308.01320](https://arxiv.org/abs/2308.01320).\n26. Xiaoxia Wu, Zhewei Yao, Yuxiong He. (2023) ZeroQuant-FP: A Leap Forward in LLMs Post-Training W4A8 Quantization Using Floating-Point Formats [arXiv:2307.09782](https://arxiv.org/abs/2307.09782) and [ENLSP2023 Workshop at NeurIPS2023](https://neurips2023-enlsp.github.io/) [[slides]](docs/assets/files/zeroquant_series.pdf)\n27. Zhewei Yao, Xiaoxia Wu, Conglong Li, Minjia Zhang, Heyang Qin, Olatunji Ruwase, Ammar Ahmad Awan, Samyam Rajbhandari, Yuxiong He. (2023) DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention [arXiv:2309.14327](https://arxiv.org/pdf/2309.14327.pdf)\n28. Shuaiwen Leon Song, Bonnie Kruft, Minjia Zhang, Conglong Li, Shiyang Chen, Chengming Zhang, Masahiro Tanaka, Xiaoxia Wu, Jeff Rasley, Ammar Ahmad Awan, Connor Holmes, Martin Cai, Adam Ghanem, Zhongzhu Zhou, Yuxiong He, et al. (2023) DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies [arXiv:2310.04610](https://arxiv.org/abs/2310.04610) [[blog]](https://www.microsoft.com/en-us/research/blog/announcing-the-deepspeed4science-initiative-enabling-large-scale-scientific-discovery-through-sophisticated-ai-system-technologies/)\n29. Zhewei Yao, Reza Yazdani Aminabadi, Stephen Youn, Xiaoxia Wu, Elton Zheng, Yuxiong He. (2023) ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training Quantization Framework for W8A8 Transformers [arXiv:2310.17723](https://arxiv.org/abs/2310.17723)\n\n30. Xiaoxia Wu, Haojun Xia, Stephen Youn, Zhen Zheng, Shiyang Chen, Arash Bakhtiari, Michael Wyatt, Reza Yazdani Aminabadi, Yuxiong He, Olatunji Ruwase, Leon Song, Zhewei Yao (2023) ZeroQuant(4+2): Redefining LLMs Quantization with a New FP6-Centric Strategy for Diverse Generative Tasks [arXiv:2312.08583](https://arxiv.org/abs/2312.08583)\n\n31. Haojun Xia, Zhen Zheng, Xiaoxia Wu, Shiyang Chen, Zhewei Yao, Stephen Youn, Arash Bakhtiari, Michael Wyatt, Donglin Zhuang, Zhongzhu Zhou, Olatunji Ruwase, Yuxiong He, Shuaiwen Leon Song. (2024) FP6-LLM: Efficiently Serving Large Language Models Through FP6-Centric Algorithm-System Co-Design  [arXiv:2401.14112](https://arxiv.org/abs/2401.14112)\n32. Sam Ade Jacobs, Masahiro Tanaka, Chengming Zhang, Minjia Zhang, Reza Yazdani Aminadabi, Shuaiwen Leon Song, Samyam Rajbhandari, Yuxiong He. (2024) [System Optimizations for Enabling Training of Extreme Long Sequence Transformer Models](https://dl.acm.org/doi/10.1145/3662158.3662806)\n33. Xinyu Lian, Sam Ade Jacobs, Lev Kurilenko, Masahiro Tanaka, Stas Bekman, Olatunji Ruwase, Minjia Zhang. (2024) Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training [arXiv:2406.18820](https://arxiv.org/abs/2406.18820)\n34. Stas Bekman, Samyam Rajbhandari, Michael Wyatt, Jeff Rasley, Tunji Ruwase, Zhewei Yao, Aurick Qiao, Yuxiong He. (2025) Arctic Long Sequence Training: Scalable And Efficient Training For Multi-Million Token Sequences [arXiv:2506.13996](https://arxiv.org/abs/2506.13996)\n\n\n# Videos\n1. DeepSpeed KDD 2020 Tutorial\n    1. [Overview](https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29)\n    2. [ZeRO + large model training](https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28)\n    3. [17B T-NLG demo](https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27)\n    4. [Fastest BERT training + RScan tuning](https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26)\n    5. DeepSpeed hands on deep dive: [part 1](https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92), [part 2](https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94), [part 3](https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93)\n    6. [FAQ](https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24)\n2. Microsoft Research Webinar\n    * Registration is free and all videos are available on-demand.\n    * [ZeRO & Fastest BERT: Increasing the scale and speed of deep learning training in DeepSpeed](https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html).\n3. [DeepSpeed on AzureML](https://youtu.be/yBVXR8G8Bg8)\n4. [Large Model Training and Inference with DeepSpeed // Samyam Rajbhandari // LLMs in Prod Conference](https://www.youtube.com/watch?v=cntxC3g22oU) [[slides]](docs/assets/files/presentation-mlops.pdf)\n5. Community Tutorials\n    * [DeepSpeed: All the tricks to scale to gigantic models (Mark Saroufim)](https://www.youtube.com/watch?v=pDGI668pNg0)\n    * [Turing-NLG, DeepSpeed and the ZeRO optimizer (Yannic Kilcher)](https://www.youtube.com/watch?v=tC01FRB0M7w)\n    * [Ultimate Guide To Scaling ML Models (The AI Epiphany)](https://www.youtube.com/watch?v=hc0u4avAkuM)\n",
  "external_links_in_readme": [
    "https://arxiv.org/abs/2303.08374",
    "https://github.com/THUDM/GLM-130B",
    "https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepcompile/README.md",
    "https://www.deepspeed.ai/getting-started/",
    "https://www.deepspeed.ai/deepspeed4science/",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/hpu-gaudi2.yml",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/pages/pages-build-deployment",
    "https://github.com/Ascend/Ascend-CI/actions/workflows/deepspeed.yaml/badge.svg?branch=main",
    "https://dl.acm.org/doi/10.1145/3394486.3406703",
    "https://www.deepspeed.ai/tutorials/advanced-install/",
    "https://arxiv.org/abs/1910.02054",
    "https://arxiv.org/abs/2310.04610",
    "https://innovation.microsoft.com/en-us/exploring-ai-at-scale",
    "https://openreview.net/forum?id=Pgtn4l6eKjv",
    "https://arxiv.org/abs/2303.06318",
    "https://arxiv.org/abs/2312.08583",
    "https://www.deepspeed.ai/",
    "https://github.com/Ascend/Ascend-CI/actions/workflows/deepspeed.yaml",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch110-p40.yml/badge.svg?branch=master",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-torch-latest.yml",
    "https://www.youtube.com/watch?v=y4_bCiAsIAk&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=28",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/hpu-gaudi2.yml/badge.svg?branch=master",
    "https://pypi.org/project/deepspeed/",
    "https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ucp/README.md\">",
    "https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024/README-Chinese.md\">",
    "https://www.microsoft.com/en-us/research/project/ai-at-scale/",
    "https://github.com/deepspeedai/DeepSpeed/graphs/contributors\">",
    "https://arxiv.org/abs/2010.13369",
    "https://arxiv.org/abs/2101.06840",
    "http://mlforsystems.org/",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/formatting.yml",
    "https://www.microsoft.com/en-us/research/blog/deepspeed-accelerating-large-scale-model-inference-and-training-via-system-optimizations-and-compression/",
    "https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/huggingface-tp/README.md",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml",
    "https://www.microsoft.com/en-us/research/blog/deepspeed-advancing-moe-inference-and-training-to-power-next-generation-ai-scale/",
    "https://img.shields.io/badge/%E6%97%A5%E6%9C%AC%E8%AA%9ETwitter-%40DeepSpeedAI_JP-blue",
    "https://huggingface.co/blog/bloom-megatron-deepspeed",
    "https://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/#introduction",
    "https://arxiv.org/abs/2307.09782",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-ds-chat.yml",
    "https://www.usenix.org/system/files/atc21_slides_ren-jie.pdf",
    "https://arxiv.org/abs/2301.12017",
    "https://youtu.be/yBVXR8G8Bg8",
    "https://arxiv.org/abs/2310.17723",
    "https://www.usenix.org/conference/atc21/presentation/ren-jie",
    "https://uploads-ssl.webflow.com/60fd4503684b466578c0d307/61138924626a6981ee09caf6_jurassic_tech_paper.pdf",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/xpu-max1100.yml/badge.svg?branch=master",
    "https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-fp6/03-05-2024/README.md\">",
    "https://openreview.net/forum?id=f-fVCElZ-G1",
    "https://www.youtube.com/watch?v=k9yPkBTayos&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=93",
    "https://www.microsoft.com/en-us/research/blog/deepspeed-zero-a-leap-in-speed-for-llm-and-chat-model-training-with-4x-less-communication/",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/pages/pages-build-deployment/badge.svg",
    "https://arxiv.org/pdf/2309.14327.pdf",
    "https://arxiv.org/abs/2212.03597",
    "https://docs.determined.ai/latest/training/apis-howto/deepspeed/overview.html",
    "https://www.youtube.com/watch?v=pDGI668pNg0",
    "https://twitter.com/intent/follow?screen_name=DeepSpeedAI",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-nightly.yml",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-ds-chat.yml/badge.svg?branch=master",
    "https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-nightly-v100.yml/badge.svg?branch=master",
    "https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/ulysses-offload/README.md",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/python.yml",
    "https://opensource.microsoft.com/codeofconduct/faq/",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/formatting.yml/badge.svg?branch=master",
    "https://arxiv.org/pdf/2204.06644.pdf",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch110-v100.yml/badge.svg?branch=master",
    "https://www.youtube.com/watch?v=sG6_c4VXLww&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=94",
    "https://user-images.githubusercontent.com/58739961/187154444-fce76639-ac8d-429b-9354-c6fac64b7ef8.jpg\"",
    "https://www.deepspeed.ai/posts/",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch110-p40.yml",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-lightning-v100.yml",
    "https://lightning.ai/docs/pytorch/stable/advanced/model_parallel.html#deepspeed",
    "https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/deepspeed-chat",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-mii.yml",
    "https://badge.fury.io/py/deepspeed.svg",
    "https://www.youtube.com/watch?v=tC01FRB0M7w",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/amd-mi200.yml/badge.svg?branch=master",
    "https://readthedocs.org/projects/deepspeed/badge/?version=latest",
    "https://arxiv.org/abs/2206.01861",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-sd.yml/badge.svg",
    "https://arxiv.org/abs/2102.02888",
    "https://arxiv.org/abs/2506.13996",
    "https://arxiv.org/abs/2202.06009",
    "https://www.deepspeed.ai/tutorials/azure/",
    "https://arxiv.org/abs/2303.08302",
    "https://github.com/EleutherAI/gpt-neox",
    "https://openreview.net/forum?id=JpZ5du_Kdh",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-transformers-v100.yml/badge.svg?branch=master",
    "https://arxiv.org/abs/2211.11586",
    "https://neurips2023-enlsp.github.io/",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml",
    "https://arxiv.org/abs/2201.11990",
    "https://arxiv.org/abs/2306.10209",
    "https://arxiv.org/abs/2104.06069",
    "https://www.youtube.com/watch?v=CaseqC45DNc&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=29",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch110-v100.yml",
    "https://proceedings.mlr.press/v162/rajbhandari22a.html",
    "https://www.youtube.com/watch?v=hc0u4avAkuM",
    "https://www.deepspeed.ai/tutorials/",
    "https://www.youtube.com/watch?v=_NOk-mBwDYg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=92",
    "https://cla.opensource.microsoft.com.",
    "https://arxiv.org/abs/2303.07226",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-nightly.yml/badge.svg?branch=master",
    "https://github.com/yandex/YaLM-100B",
    "https://img.shields.io/twitter/follow/DeepSpeedAI",
    "https://pytorch.org/",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-accelerate-v100.yml/badge.svg?branch=master",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-lightning-v100.yml/badge.svg?branch=master",
    "https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-domino/README.md",
    "https://www.zhihu.com/people/deepspeed",
    "https://twitter.com/DeepSpeedAI_JP",
    "https://www.biorxiv.org/content/10.1101/2023.07.05.547496v2",
    "https://www.microsoft.com/en-us/research/blog/deepspeed-compression-a-composable-library-for-extreme-compression-and-zero-cost-quantization/",
    "https://github.com/deepspeedai/Megatron-DeepSpeed/tree/main/examples_deepspeed/azureml",
    "https://www.deepspeed.ai/docs/config-json/",
    "https://github.com/microsoft/DeepSpeed/tree/master/blogs/deepspeed-ucp/japanese/README.md\">",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-h100.yml/badge.svg?branch=master",
    "https://www.youtube.com/watch?v=nsHu6vEgPew&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=24",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-h100.yml",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-transformers-v100.yml",
    "https://note.microsoft.com/MSR-Webinar-DeepSpeed-Registration-On-Demand.html",
    "https://github.com/deepspeedai/DeepSpeed/blob/master/LICENSE",
    "https://2023.emnlp.org/",
    "https://github.com/deepspeedai/deepspeed",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-torch-latest.yml/badge.svg?branch=master",
    "https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/windows/08-2024/chinese/README.md",
    "https://deepspeed.readthedocs.io/en/latest/",
    "https://www.deepspeed.ai/training",
    "https://arxiv.org/abs/2104.07857",
    "https://arxiv.org/abs/2406.18820",
    "https://www.bestpractices.dev/projects/9530",
    "https://badgen.net/badge/build/check-status/blue",
    "https://arxiv.org/abs/2206.01859",
    "https://huggingface.co/docs/accelerate/usage_guides/deepspeed",
    "https://arxiv.org/abs/2207.00032",
    "https://www.youtube.com/watch?v=cntxC3g22oU",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/amd-mi200.yml",
    "https://dl.acm.org/doi/10.5555/3433701.3433727",
    "https://www.youtube.com/watch?v=9V-ZbP92drg&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=27",
    "https://mmengine.readthedocs.io/en/latest/common_usage/large_model_training.html#deepspeed",
    "https://www.microsoft.com/en-us/research/blog/deepspeed-extreme-scale-model-training-for-everyone/",
    "https://dl.acm.org/doi/10.1145/3577193.3593704",
    "https://deepspeed.readthedocs.io/en/latest/?badge=latest",
    "http://proceedings.mlr.press/v139/tang21a.html",
    "https://www.microsoft.com/en-us/research/blog/zero-infinity-and-deepspeed-unlocking-unprecedented-model-scale-for-deep-learning-training/",
    "https://arxiv.org/abs/2108.06084",
    "https://github.com/Azure/azureml-examples/tree/main/v1/python-sdk/workflows/train/deepspeed",
    "https://static.pepy.tech/badge/deepspeed",
    "https://pytorch.org/docs/stable/cpp_extension.html",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-mii.yml/badge.svg?branch=master",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-accelerate-v100.yml",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-inference.yml/badge.svg?branch=master",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-sd.yml",
    "https://dl.acm.org/doi/10.1145/3662158.3662806",
    "https://icml.cc/Conferences/2023",
    "https://contrib.rocks/image?repo=microsoft/DeepSpeed&r=\"",
    "https://pepy.tech/project/deepspeed",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-torch-latest-v100.yml/badge.svg?branch=master",
    "https://www.microsoft.com/en-us/research/blog/announcing-the-deepspeed4science-initiative-enabling-large-scale-scientific-discovery-through-sophisticated-ai-system-technologies/",
    "https://arxiv.org/abs/2308.01320",
    "https://dl.acm.org/doi/abs/10.1145/3458817.3476205",
    "https://arxiv.org/abs/2305.09847",
    "https://www.amazon.science/blog/20b-parameter-alexa-model-sets-new-marks-in-few-shot-learning",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-inference.yml/badge.svg?branch=master",
    "https://arxiv.org/abs/2201.05596",
    "https://github.com/deepspeedai/deepspeed-mii",
    "https://huggingface.co/docs/transformers/deepspeed",
    "https://docs.mosaicml.com/projects/composer/en/latest/trainer/using_the_trainer.html?highlight=deepspeed#deepspeed-integration",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/nv-inference.yml",
    "https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-gds/README.md\">",
    "https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/windows/08-2024/japanese/README.md",
    "https://img.shields.io/badge/%E7%9F%A5%E4%B9%8E-%E5%BE%AE%E8%BD%AFDeepSpeed-blue",
    "https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-gds/japanese/README.md\">",
    "https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/",
    "https://www.deepspeed.ai/inference",
    "https://github.com/deepspeedai/DeepSpeed/tree/master/blogs/windows/08-2024/README.md",
    "https://arxiv.org/abs/2401.14112",
    "https://www.bestpractices.dev/projects/9530/badge",
    "https://opensource.microsoft.com/codeofconduct/",
    "https://www.youtube.com/watch?v=o1K-ZG9F6u0&list=PLa85ZdUjfWS21mgibJ2vCvLziprjpKoW0&index=26",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/xpu-max1100.yml",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/python.yml/badge.svg?branch=master",
    "https://github.com/ROCm-Developer-Tools/HIPCC",
    "https://hipc.org/advance-program/",
    "https://proceedings.neurips.cc/paper/2020/hash/a1140a3d0df1c81e24ae954d935e8926-Abstract.html",
    "https://badgen.net/badge/license/apache2.0/blue",
    "https://www.deepspeed.ai/compression",
    "https://github.com/deepspeedai/DeepSpeed/actions/workflows/cpu-inference.yml",
    "https://www.deepspeed.ai/training/",
    "https://dl.acm.org/doi/abs/10.5555/3571885.3571946",
    "https://openreview.net/forum?id=xNeAhc2CNAl"
  ]
}
```

</details>


---

## Repository 2: Open-Reasoner-Zero/Open-Reasoner-Zero

# GitHub Repository Data

**Repository:** [Open-Reasoner-Zero/Open-Reasoner-Zero](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero)

## Basic Information

- **Description:** Official Repo for Open-Reasoner-Zero
- **Created:** 2025-02-19T17:28:25+00:00
- **Last Updated:** 2025-06-20T07:02:06+00:00
- **Last Pushed:** 2025-06-02T16:36:00+00:00
- **Default Branch:** main
- **Size:** 16502 KB

## Statistics

- **Stars:** 1,968
- **Forks:** 104
- **Watchers:** 1,968
- **Open Issues:** 19
- **Total Issues:** 0
- **Pull Requests:** 5

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE)

## Languages

- **Python:** 337,249 bytes
- **Dockerfile:** 1,739 bytes

## Top Contributors

1. **REIGN12** - 21 contributions
2. **vwxyzjn** - 5 contributions
3. **YinminZhang** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 71

- `.flake8` (blob)
- `.gitignore` (blob)
- `LICENSE` (blob)
- `ORZ_paper.pdf` (blob)
- `README.md` (blob)
- `data` (tree)
- `data/eval_data` (tree)
- `data/eval_data/aime2024.json` (blob)
- `data/eval_data/gpqa_diamond.json` (blob)
- `data/eval_data/math500.json` (blob)

## Recent Issues

- 🟢 **#73** DeepSpeed does not detect CUDA for some reason: (open)
- 🟢 **#72** In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map="meta"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device("meta")` (open)
- 🟢 **#71** In `orz/ppo/actors.py`: `with torch.device("meta"): AutoModel.from_pretrained(...)` throws (open)
- 🟢 **#70** `llm_engine.model_executor` seems not available on modern vllm (0.8.5) (open)
- 🟢 **#69** On newer vllm should use `llm_engine.vllm_config` and similar (open)

## Recent Pull Requests

- 🟢 **#66** 重构 PPOExpConfig 类，继承自 CommonPPOConfig，简化配置管理 (open)
- 🔴 **#62** Major Updates for Open-Reasoner-Zero (closed)
- 🟢 **#57** Support FSDP and vllm colocate with tp size > 1 (open)
- 🔴 **#32** Local ppo mini (closed)
- 🔴 **#31** Add a mini example (closed)

## Recent Commits

- **3fdd9a07** doc: update ORZ-R1-Distill-Qwen-14B results - REIGN12 (2025-06-02T16:35:57+00:00)
- **a2351927** chores: update qr code - REIGN12 (2025-06-02T16:35:25+00:00)
- **20e5bc83** README: update readme - REIGN12 (2025-04-08T12:58:51+00:00)
- **9c73e259** README: update readme - REIGN12 (2025-04-02T03:41:38+00:00)
- **a27ce5c1** README: update readme - REIGN12 (2025-04-02T03:38:35+00:00)
- **a288753d** README: update readme - REIGN12 (2025-03-31T15:53:39+00:00)
- **a7193f2c** Major Updates for Open-Reasoner-Zero (#62) - Jingcheng Hu (2025-03-31T15:45:50+00:00)
- **e008f6d9** Fix: bug fix for weight sync logic when colocate=False - REIGN12 (2025-03-05T08:08:42+00:00)
- **effe6fb8** Fix: fix typo in report - REIGN12 (2025-03-01T09:03:59+00:00)
- **9464093c** Fix: minor bugfix for dockerfile - REIGN12 (2025-03-01T08:54:56+00:00)

## External Links Found in README

- https://github.com/deepspeedai/DeepSpeed
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json
- https://huggingface.co/Qwen/Qwen2.5-0.5B
- https://arxiv.org/abs/2503.24290
- https://huggingface.co/Open-Reasoner-Zero"
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B
- https://www.stepfun.com/
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
- https://arxiv.org/abs/2503.24290},
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py
- https://github.com/vllm-project/vllm
- https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline
- https://huggingface.co/datasets/open-r1/OpenR1-Math-220k
- https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor"/></a>
- https://arxiv.org/abs/2503.24290"><b>Paper
- https://github.com/ray-project/ray
- https://projectnumina.ai/

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 935587127,
  "name": "Open-Reasoner-Zero",
  "full_name": "Open-Reasoner-Zero/Open-Reasoner-Zero",
  "description": "Official Repo for Open-Reasoner-Zero",
  "html_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero",
  "clone_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "ssh_url": "git@github.com:Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "homepage": "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903",
  "topics": [],
  "default_branch": "main",
  "created_at": "2025-02-19T17:28:25+00:00",
  "updated_at": "2025-06-20T07:02:06+00:00",
  "pushed_at": "2025-06-02T16:36:00+00:00",
  "size_kb": 16502,
  "watchers_count": 1968,
  "stargazers_count": 1968,
  "forks_count": 104,
  "open_issues_count": 19,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE"
  },
  "languages": {
    "Python": 337249,
    "Dockerfile": 1739
  },
  "top_contributors": [
    {
      "login": "REIGN12",
      "contributions": 21
    },
    {
      "login": "vwxyzjn",
      "contributions": 5
    },
    {
      "login": "YinminZhang",
      "contributions": 3
    }
  ],
  "file_tree_count": 71,
  "file_tree_sample": [
    {
      "path": ".flake8",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "ORZ_paper.pdf",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "data",
      "type": "tree"
    },
    {
      "path": "data/eval_data",
      "type": "tree"
    },
    {
      "path": "data/eval_data/aime2024.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/gpqa_diamond.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/math500.json",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 5,
  "recent_issues": [
    {
      "number": 73,
      "title": "DeepSpeed does not detect CUDA for some reason:",
      "state": "open"
    },
    {
      "number": 72,
      "title": "In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map=\"meta\"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device(\"meta\")`",
      "state": "open"
    },
    {
      "number": 71,
      "title": "In `orz/ppo/actors.py`: `with torch.device(\"meta\"): AutoModel.from_pretrained(...)` throws",
      "state": "open"
    },
    {
      "number": 70,
      "title": "`llm_engine.model_executor` seems not available on modern vllm (0.8.5)",
      "state": "open"
    },
    {
      "number": 69,
      "title": "On newer vllm should use `llm_engine.vllm_config` and similar",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 66,
      "title": "\u91cd\u6784 PPOExpConfig \u7c7b\uff0c\u7ee7\u627f\u81ea CommonPPOConfig\uff0c\u7b80\u5316\u914d\u7f6e\u7ba1\u7406",
      "state": "open"
    },
    {
      "number": 62,
      "title": "Major Updates for Open-Reasoner-Zero",
      "state": "closed"
    },
    {
      "number": 57,
      "title": "Support FSDP and vllm colocate with tp size > 1",
      "state": "open"
    },
    {
      "number": 32,
      "title": "Local ppo mini",
      "state": "closed"
    },
    {
      "number": 31,
      "title": "Add a mini example",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "3fdd9a07b4fb01e06005e6e74fc56690cde8a341",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:57+00:00",
      "message": "doc: update ORZ-R1-Distill-Qwen-14B results"
    },
    {
      "sha": "a23519277efdd71a063e1f2913d5c496a5f96180",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:25+00:00",
      "message": "chores: update qr code"
    },
    {
      "sha": "20e5bc83ae75695fded4e1a59a21e487063e24d9",
      "author": "REIGN12",
      "date": "2025-04-08T12:58:51+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "9c73e2590d23a7245a8d6341337952828110c631",
      "author": "REIGN12",
      "date": "2025-04-02T03:41:38+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a27ce5c1c5204d267c821542af4bdb47c70aa40a",
      "author": "REIGN12",
      "date": "2025-04-02T03:38:35+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a288753de916c0f3ba932fb0d1c218a49b8a403b",
      "author": "REIGN12",
      "date": "2025-03-31T15:53:39+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a7193f2c14aa90d055f9c161bf4566ec41ced30c",
      "author": "Jingcheng Hu",
      "date": "2025-03-31T15:45:50+00:00",
      "message": "Major Updates for Open-Reasoner-Zero (#62)"
    },
    {
      "sha": "e008f6d95f0b9a0e992f6b8bac912515b50a4634",
      "author": "REIGN12",
      "date": "2025-03-05T08:08:42+00:00",
      "message": "Fix: bug fix for weight sync logic when colocate=False"
    },
    {
      "sha": "effe6fb81570ccfde7d28d8d335c83127e95263f",
      "author": "REIGN12",
      "date": "2025-03-01T09:03:59+00:00",
      "message": "Fix: fix typo in report"
    },
    {
      "sha": "9464093c67c32a5fca69556b603f915e55fc00a5",
      "author": "REIGN12",
      "date": "2025-03-01T08:54:56+00:00",
      "message": "Fix: minor bugfix for dockerfile"
    },
    {
      "sha": "f594903dc0e953a8d0429ae0ecf339efcbe456f7",
      "author": "Jingcheng Hu",
      "date": "2025-02-25T15:03:35+00:00",
      "message": "Merge pull request #31 from vwxyzjn/ppo-mini"
    },
    {
      "sha": "ce912e108ca03d5ded42e151e3c857d44fc3f277",
      "author": "Costa Huang",
      "date": "2025-02-24T20:33:48+00:00",
      "message": "quick change"
    },
    {
      "sha": "34d8abc52ecba9c1c620350eb70110d05ea6b676",
      "author": "Costa Huang",
      "date": "2025-02-24T20:32:51+00:00",
      "message": "set eval to false"
    },
    {
      "sha": "55904eae7c35932e8ee2efb717c788ed3cd9b75e",
      "author": "Costa Huang",
      "date": "2025-02-24T20:31:44+00:00",
      "message": "quick change"
    },
    {
      "sha": "dbc40d8042c4981138b58773ca4936570e2339be",
      "author": "Costa Huang",
      "date": "2025-02-24T20:28:34+00:00",
      "message": "Add single GPU exampel"
    },
    {
      "sha": "ec2ce7e528f660f3b05c19369cdcff9d89cbb6b0",
      "author": "Costa Huang",
      "date": "2025-02-24T20:06:00+00:00",
      "message": "Add a mini example"
    },
    {
      "sha": "7b5dec17a8dda164e8a4aaba33393afc3e0e95d9",
      "author": "Jingcheng Hu",
      "date": "2025-02-24T09:18:55+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "3ccec9eec8717a5de0575918237bc81b6ba3e3a2",
      "author": "REIGN12",
      "date": "2025-02-24T07:24:11+00:00",
      "message": "Merge branch 'main' of https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
    },
    {
      "sha": "df77747d28c8580db768732f8da34f913e0411ca",
      "author": "REIGN12",
      "date": "2025-02-24T07:23:34+00:00",
      "message": "Fix: bug fix for readme and 32b exp"
    },
    {
      "sha": "ec6b01ef61f6b66a30bff0ea763b37fe4e9d00d1",
      "author": "hanqer",
      "date": "2025-02-24T05:52:43+00:00",
      "message": "fix: fix the package dependencies of pip and apt."
    }
  ],
  "readme_text": "<div align=\"center\">\n\n# Open Reasoner Zero\n\n<img src=\"figure/logo.jpg\" width=\"300\"/>\n\n<div>\n\nAn Open Source Approach to Scaling Up Reinforcement Learning on the Base Model\n</div>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n    <a href=\"https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\" style=\"margin: 2px;\"><img alt=\"Code\" src=\"https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/></a>\n  \n  <a href=\"https://huggingface.co/Open-Reasoner-Zero\" target=\"_blank\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>\n\n  <a href=\"https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\" target=\"_blank\">\n  <img alt=\"Notion Page\"\n    src=\"https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>\n\n  <br>\n  <a href=\"https://arxiv.org/abs/2503.24290\"><b>Paper Arxiv Link </b>\ud83d\udc41\ufe0f</a>\n</div>\n\n<div>\n<br>\n\n</div>\n\n## Overview \ud83c\udf0a\nWe introduce **Open-Reasoner-Zero**, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility.\nUsing the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency\u2014requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline.\n\nTo enable broader participation in this pivotal moment we witnessed and accelerate research towards artificial general intelligence (AGI), \nwe release our source code, parameter settings, training data, and model weights.\nPlease refer to our [paper](https://arxiv.org/abs/2503.24290) for more insights across various model sizes. \n\n**Let the Reasoner-Zero tide rise!**\n\n\n## Main Results \ud83c\udfc6\n\n![](figure/teaser.png)\n\n*Figure 1 | Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\}. Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\} on benchmarks (averaged on 16 responses) during training. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, Open-Reasoner-Zero-32B achieves superior performance on AIME2024, MATH500, and GPQA Diamond benchmark-requiring only a tenth of the training steps.*\n\n![](figure/train_curve.png)\n*Figure 2 | Train-time Scale up on Train Reward and Response Length of Open-Reasoner-Zero (ORZ) - \\{0.5B, 1.5B, 7B, 32B\\}. Train Reward and Response Length increase steadily, demonstrating consistent scalability across model sizes. Interestingly, the ORZ-32B Response Length exhibits fluctuations without negatively impacting training stability, highlighting the robustness of our minimalist recipe.*\n\n## Releases \ud83d\udce6\n\n<strong>[2025/06/03]</strong>\nWe release [ORZ-R1-Distill-Qwen-14B](https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B), obtained by applying ORZ recipe to reasoning-enhanced models like DeepSeek-R1-Distill-Qwen-14B. This ORZ-R1-Distill-Qwen-14B achieves strong results on reasoning benchmarks, even surpassing the larger DeepSeek-R1-Distill-Qwen-32B model.\n\n| Model                        | AIME 2024 | AIME 2025 | MATH500 | GPQA Dia. |\n| ---------------------------- | --------- | --------- | ------- | --------- |\n| DeepSeek-R1-Distill-Qwen-14B | 69.7      | 49.1      | 93.9    | 59.1      |\n| DeepSeek-R1-Distill-Qwen-32B | 72.6      | 60.0      | 94.3    | **62.1**     |\n| **ORZ-R1-Distill-Qwen-14B**  | **75.2**  | **60.0**  | **95.6** | 60.4  |\n\n\n<strong>[2025/03/31]</strong>\nWe announce a major milestone for `Open-Reasoner-Zero`:\n\n- \ud83c\udf0a [Updated Paper](https://arxiv.org/abs/2503.24290) with new results.\n- \ud83d\udd2d [Easy-to-use Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground):\n  - [ORZ-1.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py) and [ORZ-0.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py) (main results in Figure 2). \n  - [Minimal resource training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py): ORZ-0.5B can be run on a single A800/H800 gpu!\n- \ud83e\udd29 [Updated Curated Datasets](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data): \n  - 129k data in total:\n    - [original 57k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json).\n    - [extended 72k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json).\n  - [13k hard data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json) mined from the above 129k data. \n    - used in the \"annealing\" stage of ORZ-32B training: **AIME2024 from ~41% to ~48%**!\n- \ud83e\udd17 More HF Models: \n  - Updated HF Models: [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B).\n  - Released HF Models: [`Open-Reasoner-Zero-1.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B) and [`Open-Reasoner-Zero-0.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B).\n- \ud83d\ude80 Full Suite of Critic Models for in-depth research: `Open-Reasoner-Zero-Critic-`{[0.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B), [1.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B), [7B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B),  [32B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B)}.\n\n<strong>[2025/02/18]</strong>\nWe release `Open-Reasoner-Zero`. \n\nAs part of this release, we open-source:\n- \ud83c\udf0a [Paper(WIP)](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf) on our comprehensive analysis and insights in Reasoner-Zero training\n- \ud83e\udd17 HF Model [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B)\n- \ud83c\udf81 [`Our curated 57k training data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data)\n- \ud83d\udcc4 [Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground) to enjoy your own Reasoner-Zero journey!\n\n## Key Features in Codebase \ud83d\udd11\n\n- Adopt single controller trainer design, flexible and researcher-friendly.\n- Colocate training and generation in the same GPUs to maximize GPU utilization.\n\n## Getting Started \ud83d\ude80\n### Data\n\nWe release all of curated high-quality training data in the [`data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data) folder:\n* curated 129k data:\n  * [original 57k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json), collected from various sources, including AIME (up to 2023), MATH, Numina-Math collection and Tulu3 MATH.\n  * [extended 72k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json), mainly cleaned from OpenR1-Math-220k.\n* [hard 13k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json), mined from the first stage of ORZ-32B training.\n\nThe details for how to collect data are described in our [paper](https://arxiv.org/abs/2503.24290).\n\n### Installation & Training Scripts\nWe release our [Dockerfile](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile) in [docker](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker) folder to facilitate the reproducibility of our training.\n\nTo install the package, run:\n```bash\npip install -e .\n```\n\n#### Start ORZ-32B PPO Training\nHere are the starting commands in 16 nodes. \n\nFirst on master node, run:\n```bash\nray start --head\n# you will see logging like:\n# Next steps\n#  To add another node to this Ray cluster, run\n#    ray start --address='<master-node-ip>:<master-node-port>'\n```\n\nthen on all other nodes, run:\n```bash\nray start --address='<master-node-ip>:<master-node-port>' # <master-node-ip> and <master-node-port> are from above loggings!\n```\n\nfinally on master node, just run:\n```bash\npython -m playground.orz_32b_ppo\n```\nYour training log will be shown in the master node terminal.\n\n------\n\n#### Start ORZ-0.5B PPO Training\nYou can start the ORZ-0.5B PPO training in single A800/H800 node:\n```bash\npython -m playground.orz_0p5b_ppo\n```\n\nYou can even run in **a single A800/H800 gpu**: \n```bash\npython -m playground.orz_0p5b_ppo_1gpu\n```\n\nnote: since we are not in multi-node setting, no `ray start` like logics are needed.\n\n------\n\n#### Start ORZ-7B PPO Training\n\nMulti-node Training on 4 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n\n# then on master node, run:\npython -m playground.orz_7b_ppo\n```\n\nYour training log will be shown in the master node terminal.\n\n-----\n\n#### Start ORZ-1.5B PPO Training\n\nMulti-node Training on 2 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n# then on master node, run:\npython -m playground.orz_1p5b_ppo\n```\n\n----\n\n#### Debug Settings\nIn the code, we leave an environment variable `DEBUG_MODE` to run in debug setting for researcher to iterate. (Thought for now, we recommend using `python -m playground.orz_0p5b_ppo_1gpu` for debugging.)\n\nThe debug running command examples:\n```bash\n# NOTE: just for debug, not final setting!\n\n## Debug command in a single GPU with `EleutherAI/pythia-14m`\nDEBUG_MODE=True python -m playground.orz_14m_ppo_mini\n## Debug command in a single node (8 GPUs) with `Qwen/Qwen2.5-7B`\nDEBUG_MODE=True python -m playground.orz_7b_ppo\n```\n\n### How to Use the Model\n#### Policy Model\nPolicy models can be used in the same way as any chat model in transformers and vllm, since we have put the chat template jinja in the tokenizer.\n\n#### Critic Model\nCritic models can be loaded the same way like in the [training code](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738). \n\n\n## Acknowledgements \ud83d\udc96 \n\n- This work was supported by computing resources and valuable feedback provided by [StepFun](https://www.stepfun.com/) and Tsinghua University.\n- Our training framework is built on [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF), [vllm](https://github.com/vllm-project/vllm), [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) and [ray](https://github.com/ray-project/ray).\n- Our model is based on [Qwen2.5 Series](https://qwenlm.github.io/blog/qwen2.5-llm/) of **base models**, including [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B), [Qwen2.5-1.5B](https://huggingface.co/Qwen/Qwen2.5-1.5B), [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B) and [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B).\n- We thank [Project Numina](https://projectnumina.ai/), [Tulu3](https://allenai.org/blog/tulu-3-technical) and [OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k) for their collected open sourced data.\n\n## Advertisement Time \ud83d\udce3\n\nWe are hiring talented researchers and engineers to join our team. If you are interested in our project and would like to contribute to the reasoner scale-up all the way to AGI, please feel free to reach out to us at hanqer@stepfun.com\n\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline)](https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline)\n\n## Community Discussions \ud83c\udf7a\n\nWe have several wechat groups to help discussions and sharing, you can scan the QR code below to join the latest group.\n\n<img src=\"figure/WeChatGroup.png\" width=\"300\" style=\"display: block; margin: 0 auto;\"/>\n\n## Citation\n\n```bibtex\n@misc{hu2025openreasonerzeroopensourceapproach,\n      title={Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model}, \n      author={Jingcheng Hu and Yinmin Zhang and Qi Han and Daxin Jiang and Xiangyu Zhang and Heung-Yeung Shum},\n      year={2025},\n      eprint={2503.24290},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2503.24290}, \n}\n```\n",
  "external_links_in_readme": [
    "https://github.com/deepspeedai/DeepSpeed",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json",
    "https://huggingface.co/Qwen/Qwen2.5-0.5B",
    "https://arxiv.org/abs/2503.24290",
    "https://huggingface.co/Open-Reasoner-Zero\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B",
    "https://www.stepfun.com/",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\"",
    "https://arxiv.org/abs/2503.24290},",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py",
    "https://github.com/vllm-project/vllm",
    "https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline",
    "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k",
    "https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>",
    "https://arxiv.org/abs/2503.24290\"><b>Paper",
    "https://github.com/ray-project/ray",
    "https://projectnumina.ai/",
    "https://qwenlm.github.io/blog/qwen2.5-llm/",
    "https://huggingface.co/Qwen/Qwen2.5-7B",
    "https://huggingface.co/Qwen/Qwen2.5-1.5B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B",
    "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\"",
    "https://github.com/OpenRLHF/OpenRLHF",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json",
    "https://huggingface.co/Qwen/Qwen2.5-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py",
    "https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>",
    "https://allenai.org/blog/tulu-3-technical",
    "https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B",
    "https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline",
    "https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B"
  ]
}
```

</details>


---

## Repository 3: OpenRLHF/OpenRLHF

# GitHub Repository Data

**Repository:** [OpenRLHF/OpenRLHF](https://github.com/OpenRLHF/OpenRLHF)

## Basic Information

- **Description:** An Easy-to-use, Scalable and High-performance RLHF Framework based on Ray (PPO & GRPO & REINFORCE++ & vLLM & Ray & Dynamic Sampling & Async Agent RL)
- **Created:** 2023-07-30T02:20:13+00:00
- **Last Updated:** 2025-06-21T11:05:22+00:00
- **Last Pushed:** 2025-06-19T03:07:15+00:00
- **Default Branch:** main
- **Size:** 2955 KB

## Statistics

- **Stars:** 7,146
- **Forks:** 693
- **Watchers:** 7,146
- **Open Issues:** 261
- **Total Issues:** 0
- **Pull Requests:** 392

## License

- **Type:** Apache License 2.0
- **SPDX ID:** Apache-2.0
- **URL:** [License](https://github.com/OpenRLHF/OpenRLHF/blob/main/LICENSE)

## Languages

- **Python:** 473,284 bytes
- **Dockerfile:** 732 bytes
- **Shell:** 518 bytes

## Topics

- `transformers`
- `vllm`
- `large-language-models`
- `raylib`
- `reinforcement-learning-from-human-feedback`
- `reinforcement-learning`
- `openai-o1`
- `proximal-policy-optimization`

## Top Contributors

1. **hijkzzz** - 474 contributions
2. **xiaoxigua999** - 385 contributions
3. **openllmai0** - 239 contributions
4. **catqaq** - 38 contributions
5. **HollowMan6** - 33 contributions
6. **wuxibin89** - 23 contributions
7. **zhuzilin** - 22 contributions
8. **pre-commit-ci[bot]** - 12 contributions
9. **ifromeast** - 9 contributions
10. **pikaqqqqqq** - 8 contributions

## File Structure (Sample of 10 files)

Total files: 125

- `.github` (tree)
- `.github/workflows` (tree)
- `.github/workflows/python-package.yml` (blob)
- `.gitignore` (blob)
- `.pre-commit-config.yaml` (blob)
- `CONTRIBUTING.md` (blob)
- `LICENSE` (blob)
- `README.md` (blob)
- `README_ja.md` (blob)
- `README_zh.md` (blob)

## Recent Issues

- 🔴 **#1067** A Question about consumed_samples when I resume the training with different workers (closed)
- 🔴 **#1066** H800 GRPO qwen3-8B 12k seq-len 训练一直OOM。 (closed)
- 🟢 **#1065** Why CPU Memory Is Used More and More? (open)
- 🟢 **#1064** Training Failure due to NCCL P2P Access Not Supported Between Devices (open)
- 🟢 **#1063** Qwen-moe as actor model, and using more than one gpu, will hang during training  (open)

## Recent Pull Requests

- 🔴 **#1062** Fix kd evaluate (closed)
- 🔴 **#1060** RLHF for Vision using Dense Direct Performance Optimization. (closed)
- 🔴 **#1056** Update README_zh.md (closed)
- 🔴 **#1055** Update README.md (closed)
- 🔴 **#1041** Update README.md with MARTI (Multi-Agent Reinforced Training and Inference) (closed)

## Recent Commits

- **348e8b4e** Try to fix memory leak in save checkpoint - xiaoxigua999 (2025-06-19T03:07:13+00:00)
- **296db7ca** Fix kd evaluate (#1062) - AnbinX (2025-06-17T05:39:02+00:00)
- **5974dfe8** bump flash-attn to 2.8.0.post2 - xiaoxigua999 (2025-06-16T02:07:06+00:00)
- **29e898d5** fix args in kd and prm - xiaoxigua999 (2025-06-14T01:09:49+00:00)
- **1ce43e4c** bump deps version (vllm and transformers) - xiaoxigua999 (2025-06-13T10:50:23+00:00)
- **abb0475b** support custom scheduler - xiaoxigua999 (2025-06-12T09:40:19+00:00)
- **f92cfd4d** support constant lr_scheduler - xiaoxigua999 (2025-06-12T09:39:03+00:00)
- **1e206dac** update - xiaoxigua999 (2025-06-12T05:12:38+00:00)
- **a756e885** update README.md - xiaoxigua999 (2025-06-12T03:36:59+00:00)
- **f8c4c108** Update README.md - hijkzzz (2025-06-12T03:08:33+00:00)

## External Links Found in README

- https://github.com/microsoft/LMOps/tree/main/minillm
- https://github.com/deepspeedai/DeepSpeed
- https://openrlhf.readthedocs.io/en/latest/usage.html
- https://api.star-history.com/svg?repos=OpenRLHF/OpenRLHF&type=Date
- https://img.shields.io/github/discussions/OpenRLHF/OpenRLHF?color=0088ff"
- https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepcompile/README.md
- https://github.com/huggingface/transformers
- https://arxiv.org/abs/2502.01456
- https://github.com/OpenRLHF/OpenRLHF/graphs/contributors">
- https://huggingface.co/docs/transformers/main/en/chat_templating
- https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/sft_dataset.py#L9
- https://docs.vllm.ai/en/stable/automatic_prefix_caching/apc.html
- https://github.com/vllm-project/vllm
- https://blog.vllm.ai/2025/04/23/openrlhf-vllm.html
- https://github.com/OpenRLHF/OpenRLHF.git
- https://arxiv.org/pdf/2502.06773
- https://zhuanlan.zhihu.com/p/622134699
- https://github.com/microsoft/DeepSpeed
- https://cmu-l3.github.io/anlp-spring2025/
- https://arxiv.org/abs/2308.12050

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 672415139,
  "name": "OpenRLHF",
  "full_name": "OpenRLHF/OpenRLHF",
  "description": "An Easy-to-use, Scalable and High-performance RLHF Framework based on Ray (PPO & GRPO & REINFORCE++ & vLLM & Ray & Dynamic Sampling & Async Agent RL)",
  "html_url": "https://github.com/OpenRLHF/OpenRLHF",
  "clone_url": "https://github.com/OpenRLHF/OpenRLHF.git",
  "ssh_url": "git@github.com:OpenRLHF/OpenRLHF.git",
  "homepage": "https://openrlhf.readthedocs.io/",
  "topics": [
    "transformers",
    "vllm",
    "large-language-models",
    "raylib",
    "reinforcement-learning-from-human-feedback",
    "reinforcement-learning",
    "openai-o1",
    "proximal-policy-optimization"
  ],
  "default_branch": "main",
  "created_at": "2023-07-30T02:20:13+00:00",
  "updated_at": "2025-06-21T11:05:22+00:00",
  "pushed_at": "2025-06-19T03:07:15+00:00",
  "size_kb": 2955,
  "watchers_count": 7146,
  "stargazers_count": 7146,
  "forks_count": 693,
  "open_issues_count": 261,
  "license": {
    "key": "apache-2.0",
    "name": "Apache License 2.0",
    "spdx_id": "Apache-2.0",
    "url": "https://github.com/OpenRLHF/OpenRLHF/blob/main/LICENSE"
  },
  "languages": {
    "Python": 473284,
    "Dockerfile": 732,
    "Shell": 518
  },
  "top_contributors": [
    {
      "login": "hijkzzz",
      "contributions": 474
    },
    {
      "login": "xiaoxigua999",
      "contributions": 385
    },
    {
      "login": "openllmai0",
      "contributions": 239
    },
    {
      "login": "catqaq",
      "contributions": 38
    },
    {
      "login": "HollowMan6",
      "contributions": 33
    },
    {
      "login": "wuxibin89",
      "contributions": 23
    },
    {
      "login": "zhuzilin",
      "contributions": 22
    },
    {
      "login": "pre-commit-ci[bot]",
      "contributions": 12
    },
    {
      "login": "ifromeast",
      "contributions": 9
    },
    {
      "login": "pikaqqqqqq",
      "contributions": 8
    },
    {
      "login": "li-plus",
      "contributions": 5
    },
    {
      "login": "tongyx361",
      "contributions": 5
    },
    {
      "login": "richardodliu",
      "contributions": 5
    },
    {
      "login": "Freder-chen",
      "contributions": 4
    },
    {
      "login": "cemiu",
      "contributions": 4
    },
    {
      "login": "gzpan",
      "contributions": 4
    },
    {
      "login": "chuqi9527",
      "contributions": 4
    },
    {
      "login": "KLOSYX",
      "contributions": 3
    },
    {
      "login": "coding-famer",
      "contributions": 3
    },
    {
      "login": "UbeCc",
      "contributions": 3
    }
  ],
  "file_tree_count": 125,
  "file_tree_sample": [
    {
      "path": ".github",
      "type": "tree"
    },
    {
      "path": ".github/workflows",
      "type": "tree"
    },
    {
      "path": ".github/workflows/python-package.yml",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": ".pre-commit-config.yaml",
      "type": "blob"
    },
    {
      "path": "CONTRIBUTING.md",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "README_ja.md",
      "type": "blob"
    },
    {
      "path": "README_zh.md",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 392,
  "recent_issues": [
    {
      "number": 1067,
      "title": "A Question about consumed_samples when I resume the training with different workers",
      "state": "closed"
    },
    {
      "number": 1066,
      "title": "H800 GRPO qwen3-8B 12k seq-len \u8bad\u7ec3\u4e00\u76f4OOM\u3002",
      "state": "closed"
    },
    {
      "number": 1065,
      "title": "Why CPU Memory Is Used More and More?",
      "state": "open"
    },
    {
      "number": 1064,
      "title": "Training Failure due to NCCL P2P Access Not Supported Between Devices",
      "state": "open"
    },
    {
      "number": 1063,
      "title": "Qwen-moe as actor model, and using more than one gpu, will hang during training ",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 1062,
      "title": "Fix kd evaluate",
      "state": "closed"
    },
    {
      "number": 1060,
      "title": "RLHF for Vision using Dense Direct Performance Optimization.",
      "state": "closed"
    },
    {
      "number": 1056,
      "title": "Update README_zh.md",
      "state": "closed"
    },
    {
      "number": 1055,
      "title": "Update README.md",
      "state": "closed"
    },
    {
      "number": 1041,
      "title": "Update README.md with MARTI (Multi-Agent Reinforced Training and Inference)",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "348e8b4ee0e2309e549644b3b413eca0fe1367df",
      "author": "xiaoxigua999",
      "date": "2025-06-19T03:07:13+00:00",
      "message": "Try to fix memory leak in save checkpoint"
    },
    {
      "sha": "296db7ca515528969732066bbf6cb78f79aa494e",
      "author": "AnbinX",
      "date": "2025-06-17T05:39:02+00:00",
      "message": "Fix kd evaluate (#1062)"
    },
    {
      "sha": "5974dfe8bded77e90dcdb92f08f3bdba31972ff2",
      "author": "xiaoxigua999",
      "date": "2025-06-16T02:07:06+00:00",
      "message": "bump flash-attn to 2.8.0.post2"
    },
    {
      "sha": "29e898d5ec36f952b404a0427597bc5a11f198f0",
      "author": "xiaoxigua999",
      "date": "2025-06-14T01:09:49+00:00",
      "message": "fix args in kd and prm"
    },
    {
      "sha": "1ce43e4c5178803fa4ff16108b367ba126b6d215",
      "author": "xiaoxigua999",
      "date": "2025-06-13T10:50:23+00:00",
      "message": "bump deps version (vllm and transformers)"
    },
    {
      "sha": "abb0475b680ce62f4960b0a398381736aa6dee10",
      "author": "xiaoxigua999",
      "date": "2025-06-12T09:40:19+00:00",
      "message": "support custom scheduler"
    },
    {
      "sha": "f92cfd4d7cca6c83b18f81575d01736bf554d296",
      "author": "xiaoxigua999",
      "date": "2025-06-12T09:39:03+00:00",
      "message": "support constant lr_scheduler"
    },
    {
      "sha": "1e206dac6f0e3ca4a767e2f3b4187ba64abcb708",
      "author": "xiaoxigua999",
      "date": "2025-06-12T05:12:38+00:00",
      "message": "update"
    },
    {
      "sha": "a756e885e0ad5b1562c335d438df6d38d7b9ebb0",
      "author": "xiaoxigua999",
      "date": "2025-06-12T03:36:59+00:00",
      "message": "update README.md"
    },
    {
      "sha": "f8c4c10858ba9cbb9c4b8dd43e8e8a3fbf6c75b4",
      "author": "hijkzzz",
      "date": "2025-06-12T03:08:33+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "2b05a74b49ed241c5550dcd10346df6a2c8b23d9",
      "author": "hijkzzz",
      "date": "2025-06-12T02:05:59+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "e554a33783526762b44d9fac9e2185133dec0382",
      "author": "xiaoxigua999",
      "date": "2025-06-11T06:05:00+00:00",
      "message": "refactor adv norm"
    },
    {
      "sha": "9f168f43882aee34619f390e861645576706fc25",
      "author": "hijkzzz",
      "date": "2025-06-11T04:13:13+00:00",
      "message": "Update README.md (#1055)"
    },
    {
      "sha": "3a36ee2138d389076f099088f292033cd42f09f1",
      "author": "hijkzzz",
      "date": "2025-06-11T04:13:03+00:00",
      "message": "Update README_zh.md (#1056)"
    },
    {
      "sha": "986e5f884cbf382926fc418bd8b198b68b4326c6",
      "author": "hijkzzz",
      "date": "2025-06-11T02:20:42+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "a8404709c90a55dbafb971c12c7765a3487ee7eb",
      "author": "xiaoxigua999",
      "date": "2025-06-10T23:13:01+00:00",
      "message": "bump dev version after 0.8.4"
    },
    {
      "sha": "5d2b18e83ad3a99017b762819431bde76f382bb1",
      "author": "xiaoxigua999",
      "date": "2025-06-10T23:12:13+00:00",
      "message": "Fix https://github.com/OpenRLHF/OpenRLHF/issues/1053"
    },
    {
      "sha": "d2fd4b02a498be5b61a8e7edd2e8195a8a96d4e6",
      "author": "xiaoxigua999",
      "date": "2025-06-10T08:53:17+00:00",
      "message": "Fix python package.yml"
    },
    {
      "sha": "574d24de46383226ac45001737250ecb8652c0d0",
      "author": "xiaoxigua999",
      "date": "2025-06-10T08:43:31+00:00",
      "message": "Fix python package workflow: python 3.10.14"
    },
    {
      "sha": "c3faa20c13dc29a3e2522e5bd00507f844245944",
      "author": "xiaoxigua999",
      "date": "2025-06-10T07:24:18+00:00",
      "message": "update flash-attn 2.7.4.post1 builds link"
    }
  ],
  "readme_text": "<div align=\"center\">\n    <img alt=\"OpenRLHF logo\" src=\"./docs/logo.png\" style=\"height: 140px;\" />\n</div>\n<div align=\"center\">\n<p align=\"center\">\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/graphs/contributors\">\n        <img alt=\"GitHub Contributors\" src=\"https://img.shields.io/github/contributors/OpenRLHF/OpenRLHF\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/issues\">\n        <img alt=\"Issues\" src=\"https://img.shields.io/github/issues/OpenRLHF/OpenRLHF?color=0088ff\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/discussions\">\n        <img alt=\"Issues\" src=\"https://img.shields.io/github/discussions/OpenRLHF/OpenRLHF?color=0088ff\" />\n      </a>\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/pulls\">\n        <img alt=\"GitHub pull requests\" src=\"https://img.shields.io/github/issues-pr/OpenRLHF/OpenRLHF?color=0088ff\" />\n      <a href=\"https://github.com/OpenRLHF/OpenRLHF/stargazers\">\n        <img alt=\"GitHub stars\" src=\"https://img.shields.io/github/stars/OpenRLHF/OpenRLHF?color=ccf\" />\n      </a>\n      <a href=\"https://deepwiki.com/OpenRLHF/OpenRLHF\"><img src=\"https://deepwiki.com/badge.svg\" alt=\"Ask DeepWiki\"></a>\n      <br>\n      <em>Open-source / Comprehensive / Lightweight / Easy-to-use</em>\n    </p>\n</p>\n</div>\n\n<hr>\n\n<span>[ English | <a href=\"README_zh.md\">\u4e2d\u6587</a> | <a href=\"README_ja.md\">\u65e5\u672c\u8a9e</a> ]</span>\n\nOpenRLHF is the first easy-to-use, high-performance open-source RLHF framework built on Ray, vLLM, ZeRO-3 and HuggingFace Transformers, designed to make RLHF training simple and accessible:\n\n- **Distributed Architecture with Ray**  \n  OpenRLHF leverages [Ray](https://github.com/ray-project/ray) for efficient distributed scheduling. It separates the Actor, Reward, Reference, and Critic models across different GPUs, enabling scalable training for models up to 70B parameters.  \n  It also supports **Hybrid Engine** scheduling, allowing all models and vLLM engines to share GPU resources\u2014minimizing idle time and maximizing GPU utilization.\n- **vLLM Inference Acceleration + AutoTP**  \n  RLHF training spends 80% of the time on the sample generation stage. Powered by [vLLM](https://github.com/vllm-project/vllm) and Auto Tensor Parallelism (AutoTP), OpenRLHF delivers high-throughput, memory-efficient samples generation. Native integration with HuggingFace Transformers ensures seamless and fast generation, making it the fastest RLHF framework available.\n- **Memory-Efficient Training with ZeRO-3 / AutoTP**  \n  Built on [DeepSpeed's](https://github.com/deepspeedai/DeepSpeed) ZeRO-3, [deepcompile](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepcompile/README.md) and [AutoTP](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/huggingface-tp/README.md), OpenRLHF enables large model training without heavyweight frameworks. It works directly with HuggingFace for easy loading and fine-tuning of pretrained models.\n- **Optimized PPO Implementation**  \n  Incorporates advanced PPO tricks inspired by practical guides and community best practices, enhancing training stability and reward quality in RLHF workflows. Referencing [Zhihu](https://zhuanlan.zhihu.com/p/622134699) and [Advanced Tricks for Training Large Language Models with Proximal Policy Optimization](https://hijkzzz.notion.site/rlhf-implementation-tricks?v=158d9a33ecc98132bf9e000c39227361).\n\nMore details are in [Slides](https://docs.google.com/presentation/d/1JRhB1d7csofx0PIZBmfyBdMluxNd5JLPpUHrrvVhGnk/edit?usp=sharing) | [Technical Report](https://arxiv.org/abs/2405.11143) | [Documents](https://openrlhf.readthedocs.io/)\n\n## News\n- [2025/6] [Magistral](https://mistral.ai/static/research/magistral.pdf) uses the REINFORCE++-baseline to train the reasoning models.\n- [2025/5] [MARTI](https://github.com/TsinghuaC3I/MARTI) has been released as a fork of OpenRLHF. It is designed to train LLM-based multi-agent systems using RL, by integrating centralized multi-agent interactions with distributed policy training.\n- [2025/5] OpenRLHF 0.8.0 supports [Async Pipeline RLHF](./examples/scripts/train_reinforce_baseline_llama_ray_async.sh) (`--async_train`) and [Async Agent RLHF](./examples/scripts/train_reinforce_baseline_llama_ray_agent_async.sh)(`--agent_func_path`)\n- [2025/4] Post the blog [Accelerating RLHF with vLLM, Best Practice from OpenRLHF](https://blog.vllm.ai/2025/04/23/openrlhf-vllm.html)\n- [2025/4] Clean OpenRLHF: Refactored the source code based on Single Controller and Unified Packing Samples\n- [2025/3] The CMU [Advanced Natural Language Processing Spring 2025](https://cmu-l3.github.io/anlp-spring2025/) course uses OpenRLHF as the RLHF framework teaching case.\n- [2025/2] [Logic-RL](https://arxiv.org/abs/2502.14768) and [PRIME](https://arxiv.org/abs/2502.01456) demonstrate that REINFORCE++ is more stable in training compared to GRPO and faster than PPO.\n- [2025/2] [LMM-R1](https://github.com/TideDra/lmm-r1) is a fork of OpenRLHF, aimed at providing high-performance RL infrastructure for reproduction of DeepSeek-R1 on multimodal tasks.\n- [2025/2] MIT & Microsoft proposed the [On the Emergence of Thinking in LLMs I: Searching for the Right Intuition](https://arxiv.org/pdf/2502.06773) using OpenRLHF\n- [2025/1] HKUST reproduced the [DeepSeek-R1-Zero and DeepSeek-R1 training on small models using OpenRLHF](https://github.com/hkust-nlp/simpleRL-reason)\n- [2024/12] We \"proposed\" \ud83d\ude0a the [REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models](https://arxiv.org/abs/2501.03262).\n- [2024/12] We analyzed the PPO, REINFORCE++, GRPO and RLOO in the [Notion Blogpost](https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05).\n- [2023/8] OpenRLHF was open-sourced. \n\n\n## Features\n\n- Distributed [PPO](./examples/scripts/train_ppo_llama_ray.sh) and [REINFORCE++/REINFORCE++-baseline/GRPO/RLOO](./examples/scripts/train_reinforce_llama_ray_hybrid_engine.sh) implementations based on Ray.  \n- Support Ray-based [PPO](./examples/scripts/train_ppo_llama_ray_hybrid_engine.sh) and [REINFORCE++/REINFORCE++-baseline/GRPO/RLOO](./examples/scripts/train_reinforce_llama_ray_hybrid_engine.sh) using Hybrid Engine  (`--colocate_all_models`, `--vllm_enable_sleep` and `--vllm_gpu_memory_utilization 0.5`)\n- [Ray-based Reinforced Finetuning](./examples/scripts/train_ppo_llama_with_reward_fn.sh)\n- Integration with vLLM for accelerated generation in RLHF tasks (`--vllm_num_engines`).  \n- Support RL Dynamic Sampling from DAPO(`--dynamic_filtering` and `--dynamic_filtering_reward_range`)\n- Support [DeepSpeed AutoTP training](./examples/scripts/train_sft_llama_tensor_parallelism.sh) (`--ds_tensor_parallel_size`)\n- Implementation of [RingAttention](./examples/scripts/train_dpo_ring_llama.sh) (`--ring_attn_size`, `--ring_head_stride`).  \n- Implementation of [DPO (Direct Preference Optimization)/IPO/cDPO](./examples/scripts/train_dpo_llama.sh) and [Kahneman-Tversky Optimization (KTO)](./examples/scripts/train_kto_llama.sh).  \n- Support for [Iterative DPO](./examples/scripts/train_iterative_dpo_llama.sh) ([GitHub: Online-RLHF](https://github.com/RLHFlow/Online-RLHF)).  \n- Support for [Rejection Sampling](./examples/scripts/train_rejection_sampling_llama.sh).  \n- Implementation of [Conditional SFT](./examples/scripts/train_conditional_llama.sh) ([arXiv:2308.12050](https://arxiv.org/abs/2308.12050)).  \n- Support for [Knowledge Distillation](./examples/scripts/train_knowledge_distillation.sh) ([Microsoft: minillm](https://github.com/microsoft/LMOps/tree/main/minillm)).  \n- Integration of [Process Reward Model (PRM)](./examples/scripts/train_prm_mistral.sh).  \n- Packing of training samples for SFT, DPO, RM, PRM, and PPO (`--packing_samples`).  \n- Support for [Mixture of Experts (MoE)](./examples/test_scripts/train_sft_mixtral_lora.sh) (`--aux_loss_coef`).  \n- Integration of FlashAttention2 (`--flash_attn`).  \n- Support for QLoRA (`--load_in_4bit`) and [LoRA](./examples/scripts/train_sft_mixtral_lora.sh) (`--lora_rank`, `--target_modules`).  \n- Compatibility with HuggingFace's `tokenizer.apply_chat_template` for datasets (`--apply_chat_template` and `--input_key`).  \n- Logging support with Wandb (`--use_wandb`) and TensorBoard (`--use_tensorboard`).  \n- Checkpoint recovery functionality (`--load_checkpoint` and `--save_steps`).  \n- Provided multi-node training scripts, such as [DPO](./examples/scripts/train_llama_slurm.sh) and [Ray PPO](./examples/scripts/train_ppo_llama_ray_slurm.sh).\n\n## Quick Start\n\n### Installation\n\nTo use OpenRLHF, first launch the docker container (**Recommended**) and `pip install` openrlhf inside the docker container:\n\n```bash\n# Launch the docker container\ndocker run --runtime=nvidia -it --rm --shm-size=\"10g\" --cap-add=SYS_ADMIN -v $PWD:/openrlhf nvcr.io/nvidia/pytorch:25.02-py3 bash\nsudo pip uninstall xgboost transformer_engine flash_attn pynvml -y\n\n# pip install\npip install openrlhf\n\n# If you want to use vLLM acceleration (Install vLLM 0.9.1)\npip install openrlhf[vllm]\n# latest vLLM is also supported\npip install openrlhf[vllm_latest]\n# Install vLLM, ring-flash-attention and Liger-Kernel\npip install openrlhf[vllm,ring,liger]\n\n# pip install the latest version\npip install git+https://github.com/OpenRLHF/OpenRLHF.git\n\n# Or git clone\ngit clone https://github.com/OpenRLHF/OpenRLHF.git\ncd OpenRLHF\npip install -e .\n```\n\n> [!NOTE]\n>We recommend using vLLM 0.9.1 or higher.\n>We also provided the [Dockerfiles for vLLM](./dockerfile/) and [One-Click Installation Script of Nvidia-Docker](./examples/scripts/nvidia_docker_install.sh).\n\n### Prepare Datasets\nOpenRLHF provides multiple data processing methods in our dataset classes.\nSuch as in the [Prompt Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/prompts_dataset.py#L6):\n\n```python\ndef preprocess_data(data, input_template=None, input_key=\"input\", apply_chat_template=None) -> str:\n    if apply_chat_template:\n        chat = data[input_key]\n        if isinstance(chat, str):\n            chat = [{\"role\": \"user\", \"content\": chat}]\n        prompt = apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n    else:\n        prompt = data[input_key]\n        if input_template:\n            prompt = input_template.format(prompt)\n    return prompt\n```\n\n- We can use `--input_key` to specify the `JSON key name` of the input datasets `--prompt_data {name or path}` (PPO) or `--dataset {name or path}`, and use `--apply_chat_template` to utilize the `chat_template` from the [Huggingface Tokenizer](https://huggingface.co/docs/transformers/main/en/chat_templating).\n- If you don't want to use `--apply_chat_template`, you can use `--input_template` instead, or preprocess the datasets offline in advance.\n- OpenRLHF also support mixing multiple datasets using `--prompt_data_probs 0.1,0.4,0.5` (PPO) or `--dataset_probs 0.1,0.4,0.5`.\n\nHow Chat Templating Works:\n\n```python\ndataset = [{\"input_key\": [\n  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n  {\"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\"},\n]}]\n\ntokenizer.apply_chat_template(dataset[0][\"input_key\"], tokenize=False)\n\n\"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\"\n```\n\nHow to specify test datasets ?\n\nPlease set test datasets path using ``--eval_dataset {name or path}``.\n\n\n> [!NOTE]\n> The ``JSON key`` options depends on the specific datasets. See [Reward Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/reward_dataset.py#L10) and [SFT Dataset](https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/sft_dataset.py#L9)\n\n### Supervised Fine-tuning\n\nOpenRLHF's model checkpoint is fully compatible with HuggingFace models. You can specify the model name or path using `--pretrain  {name or path}`, `--reward_pretrain  {name or path}` and `--critic_pretrain  {name or path}`. We have provided some pre-trained checkpoints and datasets on [HuggingFace OpenRLHF](https://huggingface.co/OpenRLHF).\n\nThen you can use the startup scripts we provide in the [examples/scripts](./examples/scripts/) directory, or start the training using the following commands.\n\n```bash \ndeepspeed --module openrlhf.cli.train_sft \\\n   --max_len 4096 \\\n   --dataset Open-Orca/OpenOrca \\\n   --input_key question \\\n   --output_key response \\\n   --input_template $'User: {}\\nAssistant: ' \\\n   --train_batch_size 256 \\\n   --micro_train_batch_size 2 \\\n   --max_samples 500000 \\\n   --pretrain meta-llama/Meta-Llama-3-8B \\\n   --save_path ./checkpoint/llama3-8b-sft \\\n   --save_steps -1 \\\n   --logging_steps 1 \\\n   --eval_steps -1 \\\n   --zero_stage 2 \\\n   --max_epochs 1 \\\n   --packing_samples \\\n   --bf16 \\\n   --flash_attn \\\n   --learning_rate 5e-6 \\\n   --gradient_checkpointing \\\n   --use_wandb {wandb_token}\n\n# Support HF tokenizer.apply_chat_template\n# --apply_chat_template \n# --tokenizer_chat_template {HF Chat Template}\n\n# Support RingAttention\n# pip install ring_flash_attn\n#   --ring_attn_size 2 \\\n#   --ring_head_stride 2 \\\n\n# Multi-turn fine-tuning loss\n# --multiturn\n\n# Can also be used for continued pre-training\n# --pretrain_mode\n```\n\n> [!NOTE]\n> OpenRLHF SFT/DPO/RewardModel/PPO trainers support `--packing_samples` [based on `--flash_attn`](https://github.com/MeetKai/functionary/tree/main/functionary/train/packing)\n\n\n### Reward Model Training\n```bash\ndeepspeed --module openrlhf.cli.train_rm \\\n   --save_path ./checkpoint/llama3-8b-rm \\\n   --save_steps -1 \\\n   --logging_steps 1 \\\n   --eval_steps -1 \\\n   --train_batch_size 256 \\\n   --micro_train_batch_size 1 \\\n   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n   --bf16 \\\n   --max_epochs 1 \\\n   --max_len 8192 \\\n   --zero_stage 3 \\\n   --learning_rate 9e-6 \\\n   --dataset OpenRLHF/preference_dataset_mixture2_and_safe_pku \\\n   --apply_chat_template \\\n   --chosen_key chosen \\\n   --rejected_key rejected \\\n   --flash_attn \\\n   --packing_samples \\\n   --gradient_checkpointing \\\n   --use_wandb {wandb_token}\n\n```\n\nIt is recommended to set the `--value_prefix_head` option of the Reward Model to `score`, so that we can load the model using `AutoModelForSequenceClassification`:\n\n```python\nreward_model = AutoModelForSequenceClassification.from_pretrained(\n              reward_model_path,\n              num_labels=1,\n              torch_dtype=torch.bfloat16,\n              attn_implementation=\"flash_attention_2\",\n              use_cache=False,\n          )\ninputs = xxxx (Left Padding Input Tokens)\nreward = reward_model.model(*inputs).last_hidden_state\nreward = reward_model.score(reward)[:, -1]\n```\n\n### PPO/REINFORCE++ with Ray and vLLM\n\nTo improve RLHF training speed or support 70B models, we can use the PPO with Ray and vLLM acceleration (Hybrid Engine)\n\n```bash\n# launch the master node of ray in container\nray start --head --node-ip-address 0.0.0.0 --num-gpus 8\n\n# if you want to launch ray on more nodes, use\nray start --address {MASTER-NODE-ADDRESS}:6379  --num-gpus 8\n\nray job submit --address=\"http://127.0.0.1:8265\" \\\n   --runtime-env-json='{\"working_dir\": \"/openrlhf\"}' \\\n   -- python3 -m openrlhf.cli.train_ppo_ray \\\n   --ref_num_nodes 1 \\\n   --ref_num_gpus_per_node 8 \\\n   --reward_num_nodes 1 \\\n   --reward_num_gpus_per_node 8 \\\n   --critic_num_nodes 1 \\\n   --critic_num_gpus_per_node 8 \\\n   --actor_num_nodes 1 \\\n   --actor_num_gpus_per_node 8 \\\n   --vllm_num_engines 4 \\\n   --vllm_tensor_parallel_size 2 \\\n   --colocate_all_models \\\n   --vllm_gpu_memory_utilization 0.5 \\\n   --pretrain OpenRLHF/Llama-3-8b-sft-mixture \\\n   --reward_pretrain OpenRLHF/Llama-3-8b-rm-700k \\\n   --save_path /openrlhf/examples/test_scripts/final/llama3-8b-rlhf \\\n   --ckpt_path /openrlhf/examples/test_scripts/ckpt/llama3-8b-rlhf \\\n   --save_hf_ckpt \\\n   --micro_train_batch_size 8 \\\n   --train_batch_size 128 \\\n   --micro_rollout_batch_size 16 \\\n   --rollout_batch_size 1024 \\\n   --n_samples_per_prompt 1 \\\n   --max_epochs 1 \\\n   --prompt_max_len 1024 \\\n   --max_samples 100000 \\\n   --generate_max_len 1024 \\\n   --zero_stage 3 \\\n   --bf16 \\\n   --actor_learning_rate 5e-7 \\\n   --critic_learning_rate 9e-6 \\\n   --init_kl_coef 0.01 \\\n   --prompt_data OpenRLHF/prompt-collection-v0.1 \\\n   --input_key context_messages \\\n   --apply_chat_template \\\n   --normalize_reward \\\n   --gradient_checkpointing \\\n   --packing_samples \\\n   --vllm_sync_backend nccl \\\n   --enforce_eager \\\n   --vllm_enable_sleep \\\n   --deepspeed_enable_sleep\n   --use_wandb {wandb_token}\n\n# Support REINFORCE++  | RLOO | REINFORCE++-baseline | GRPO | Dr. GRPO\n# --advantage_estimator reinforce | rloo | reinforce_baseline | group_norm | dr_grpo\n\n# Set --init_kl_coef to 0 will not launch the reference model\n\n# Support remote reward model (HTTP)\n# --remote_rm_url http://localhost:5000/get_reward\n\n# Support N samples\n# --n_samples_per_prompt 4\n```\n> [!NOTE]\n> You can also use ``setup_commands`` to let Ray automatically deploy the environment, such as `--runtime-env-json='{\"setup_commands\": [\"pip install openrlhf[vllm]\"]}'`.\n\n> [!NOTE]\n> RLOO and REINFORCE++-baseline in OPENRLHF are a modification based on REINFORCE++:\n> - REINFORCE++ integrates key optimization techniques from PPO (such as advantage normalization and PPO-clip loss) into REINFORCE while eliminating the need for a critic network.\n> - REINFORCE++-baseline uses the `mean reward of multiple samples from the same prompt` as the baseline to reshape the rewards then apply the global advantage normalization in REINFORCE++.\n> - RLOO in OpenRLHF modifies the original version by incorporating the `per-token KL reward` and utilizing the `PPO-clip loss`.\n> - Dr. GRPO remove the local group normalization `/std` in GRPO.\n\n\n> [!NOTE]\n> If you you encounter an error related to index out of range when deepspeed sets up the GPU devices, you can try to set the environment variable [`RAY_EXPERIMENTAL_NOSET_*_VISIBLE_DEVICES`](openrlhf/trainer/ray/utils.py) as a workaround.\n>   ```bash\n>   # For NVIDIA GPUs:\n>   export RAY_EXPERIMENTAL_NOSET_CUDA_VISIBLE_DEVICES=1\n>   ```\n\nThe launch scripts and documents for supported algorithms are in [example/scripts](./examples/scripts/) and [Documents - Usage](https://openrlhf.readthedocs.io/en/latest/usage.html)\n\n## Reinforced Fine-tuning\n\nOpenRLHF supports convenient and efficient Reinforced Fine-tuning. You only need to implement a [file containing the custom `reward_func` function](./examples/scripts/reward_func.py) and pass its path to the `remote_rm_url` parameter. Such as\n\n```python\n# reward_func.py\nimport torch\n\ndef reward_func(queries, prompts, labels):\n    # queries is prompts + responses\n    # labels is answers\n    print(queries)\n\n    # Generate random rewards as an example\n    # In real applications, this should be replaced with actual reward calculation logic\n    reward = torch.randint(0, 2, (len(queries),)).float()\n\n    return {\n        \"rewards\": reward,  # Rewards for advantage calculation\n        \"scores\": reward,  # Scores for dynamic filtering (0-1 reward)\n        \"extra_logs\": {\"dummy_scores\": reward},  # Additional logging info for wandb\n    }\n```\n\nthen just set\n\n```shell \nray job submit --address=\"http://127.0.0.1:8265\" \\\n  --runtime-env-json='{\"working_dir\": \"/openrlhf\"}' \\\n  -- python3 -m openrlhf.cli.train_ppo_ray \\\n  ...\n  --remote_rm_url /path/to/reward_func.py \\\n  --label_key answer\n```\n\nwhere the `label_key` parameter is used to pass additional sample information such as answer to the reward function.\n\n## Async RLHF & Agent RLHF\n\nOpenRLHF provides comprehensive support for both Asynchronous RLHF and Agent-based RLHF implementations. To utilize these features, simply include the `--async_train` and `--agent_func_path` parameters in your training configuration. \n\n```python\n# agent_func.py\nstep_idx = 0\nmax_steps = 2\n\n# A n-step random environment\nasync def step(observation, action, label, **kwargs) -> Dict[str, Any]:\n    global step_idx, max_steps\n    print(f\"step_idx: {step_idx}, max_steps: {max_steps}\")\n\n    # End after verification\n    if step_idx >= max_steps:\n        done = True\n        # Generate a random reward using torch.rand\n        reward = torch.randint(0, 2, (1,)).float()\n        next_observation = (\n            observation\n            + action\n            + \"\\n\\nHuman: [VERIFICATION RESULT: CORRECT]\\nYour solution is valid and complete. The verification process is finished.\\n</s>\"\n        )\n    else:\n        done = False\n        reward = torch.tensor(0)\n        # Update observation\n        next_observation = (\n            observation\n            + action\n            + \"\\n\\nHuman: [VERIFICATION RESULT: INCORRECT]\\nLet's analyze what needs improvement:\\n1. What are the key issues in the current solution?\\n2. How can we make it more robust?\\n3. What additional considerations should we take into account?\\n\\nPlease provide your revised solution:\\n</s>\\n\\nAssistant: \"\n        )\n    step_idx += 1\n\n    return {\n        \"rewards\": reward,  # Rewards for advantage calculation\n        \"scores\": reward,  # Scores for dynamic filtering (0-1 reward)\n        \"next_observation\": next_observation,  # The updated observation for vLLM inference in next step\n        \"done\": done,  # Boolean indicating if the episode is complete\n        \"sampling_params\": kwargs.get(\"sampling_params\", None),  # Parameters for vLLM sampling in next step\n        \"extra_logs\": {\"dummy_scores\": reward},  # Additional logging information\n    }\n```\n\nYou can also configure the maximum number of concurrent agents per vLLM engine by setting `export OPENRLHF_ASYNC_NUM_TASKS=128`. \nAdditionally, you can control the degree of off-policy sampling by setting `export OPENRLHF_ASYNC_QUEUE_SIZE=1` (this parameter controls how many batches of data can be stored in the buffer at most) in your environment.\n\n> [!NOTE] \n> OpenRLHF's Agent RLHF also supports Hybrid Engine training. To enable this feature, please remove the `--async_train` flag and enable `--colocate_all_models`.\n\n> [!WARNING] \n> Asynchronous training may affect the training stability. It is recommended to prioritize using Hybrid Engine or synchronous training mode.\n\n### LoRA\nIf you use `LoRA (Low-Rank Adaptation)`, `OpenRLHF` will not save the full weights by default instead of `LoRA Adapter`. To continue in your task normally, you should combine the `Adapter` with weights of your base model\n\n```bash\npython -m openrlhf.cli.lora_combiner \\\n    --model_path meta-llama/Meta-Llama-3-8B \\\n    --lora_path ./checkpoint/llama3-8b-rm \\\n    --output_path ./checkpoint/llama-3-8b-rm-combined \\\n    --is_rm \\\n    --bf16\n```\n\n### Performance Tuning Guide\n\nTo achieve optimal performance, we recommend allocating nodes `vLLM:Actor:Critic = 1:1:1`. \n\n- For example, for a 70B model with 48 A100 GPUs, it is advised to allocate 16 A100 GPUs to the vLLM Engine, 16 GPUs to the Actor model, and the remaining 16 GPUs to the Critic model. \n- Enable asynchronous training `--async_train` when the convergence of the RL algorithm meets requirements.\n- Using hybrid engine `--colocate_all_models` and `--vllm_enable_sleep` and `--deepspeed_enable_sleep` rather than distributed RLHF when there are enough GPU memory.\n- Enable the `--colocate_critic_reward`, `--colocate_actor_ref` options to merge nodes.  \n- You should increase the `rollout_micro_batch_size` (and minimize the TP size of vLLM engine) as much as possible. During the training phase, a larger `--micro_train_batch_size` is better and enable `--packing_samples`.\n- When there are enough GPU memory, please disable `--adam_offload` and enable `--overlap_comm`.  Also enable ``--deepcompile`` to speed up the training.\n- For vLLM, please use `--vllm_sync_backend nccl`\n- Enable [enable_prefix_caching](https://docs.vllm.ai/en/stable/automatic_prefix_caching/apc.html) in vLLM generation when `n_samples_per_prompts` > 1.\n- For a large base model, if an OOM occurs, do not use any `--colocate_xxxx` options.\n\n\n## Companies and Organizations using OpenRLHF\n\n- Google\n- ByteDance\n- Tencent\n- Alibaba\n- Baidu\n- China Telecom\n- Vivo\n- Allen AI\n- NexusFlow\n- J\u00fclich Supercomputing Centre (JSC)\n- Berkeley Starling Team\n- M-A-P\n- ...\n\n## Join Us\n\n**How to Join?**\n\n1. Email us at janhu9527@gmail.com or join [GitHub Organization](https://github.com/OpenRLHF). Please include the following details:\n   - Your name\n   - Your GitHub username\n   - Your areas of interest\n   - Your skills and experience related to NLP and/or AI\n1. You can also join us through the official GitHub [OpenRLHF \u2197](https://github.com/OpenRLHF/OpenRLHF) project page. Just create an issue about your interest to contribute and we will get back to you.\n\n**What can you do?**\n\n1. Join the team and participate in the development of the OpenRLHF project.\n1. Contribute to the project by submitting pull requests.\n1. Help improve documentation, fix bugs, or create new features.\n1. Share the project and help us grow the community.\n\n## Sponsor Us\n\nYour sponsorship can help us maintain and improve OpenRLHF. If you find this project useful, please consider sponsoring us. You can sponsor us on [Open Collective \u2197](https://opencollective.com/OpenRLHF).\n\n## Starchart\n\n[![Star History Chart](https://api.star-history.com/svg?repos=OpenRLHF/OpenRLHF&type=Date)](https://star-history.com/#OpenRLHF/OpenRLHF&Date)\n\n## Contributors\n\nA big thank you to all our contributors! If you want to contribute, feel free to make a pull request or create an issue.\n\n<a href=\"https://github.com/OpenRLHF/OpenRLHF/graphs/contributors\">\n  <img src=\"https://contrib.rocks/image?repo=OpenRLHF/OpenRLHF\" />\n</a>\n\n## References & Acknowledgements\n\nWe would like to express our gratitude to the following projects and organizations for their contributions to the field of AI and NLP:\n\n- [Hugging Face Transformers \u2197](https://github.com/huggingface/transformers)\n- [OpenAI GPT \u2197](https://github.com/openai/gpt-3)\n- [LLaMA \u2197](https://llama.meta.com/)\n- [DeepSpeed \u2197](https://github.com/microsoft/DeepSpeed)\n- [Ray \u2197](https://github.com/ray-project/ray)\n\nOur project would also like to thank [ColossalChat](https://github.com/hpcaitech/ColossalAI/tree/main/applications/ColossalChat) and [DeepSpeedChat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat). In the early stages of the project, we referred to their code design. \nOur project would like to thank [Netmind.AI](https://www.netmind.ai/) for the GPU support of developing ring attention.\n\n(2024/7) Our GitHub organization has changed from OpenLLMAI to OpenRLHF.\n\n## Citation\nOpenRLHF\n```\n@article{hu2024openrlhf,\n  title={OpenRLHF: An Easy-to-use, Scalable and High-performance RLHF Framework},\n  author={Jian Hu and Xibin Wu and Zilin Zhu and Xianyu and Weixun Wang and Dehao Zhang and Yu Cao},\n  journal={arXiv preprint arXiv:2405.11143},\n  year={2024}\n}\n```\nREINFORCE++-baseline\n```\n@article{hu2025reinforce++,\n  title={Reinforce++: A simple and efficient approach for aligning large language models},\n  author={Hu, Jian},\n  journal={arXiv preprint arXiv:2501.03262},\n  year={2025}\n}\n```\n\n______________________________________________________________________\n\n*OpenRLHF \u00a9 2025 OpenRLHF. All Rights Reserved.*\n",
  "external_links_in_readme": [
    "https://github.com/microsoft/LMOps/tree/main/minillm",
    "https://github.com/deepspeedai/DeepSpeed",
    "https://openrlhf.readthedocs.io/en/latest/usage.html",
    "https://api.star-history.com/svg?repos=OpenRLHF/OpenRLHF&type=Date",
    "https://img.shields.io/github/discussions/OpenRLHF/OpenRLHF?color=0088ff\"",
    "https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepcompile/README.md",
    "https://github.com/huggingface/transformers",
    "https://arxiv.org/abs/2502.01456",
    "https://github.com/OpenRLHF/OpenRLHF/graphs/contributors\">",
    "https://huggingface.co/docs/transformers/main/en/chat_templating",
    "https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/sft_dataset.py#L9",
    "https://docs.vllm.ai/en/stable/automatic_prefix_caching/apc.html",
    "https://github.com/vllm-project/vllm",
    "https://blog.vllm.ai/2025/04/23/openrlhf-vllm.html",
    "https://github.com/OpenRLHF/OpenRLHF.git",
    "https://arxiv.org/pdf/2502.06773",
    "https://zhuanlan.zhihu.com/p/622134699",
    "https://github.com/microsoft/DeepSpeed",
    "https://cmu-l3.github.io/anlp-spring2025/",
    "https://arxiv.org/abs/2308.12050",
    "http://127.0.0.1:8265\"",
    "https://huggingface.co/OpenRLHF",
    "https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/reward_dataset.py#L10",
    "https://github.com/hpcaitech/ColossalAI/tree/main/applications/ColossalChat",
    "https://github.com/ray-project/ray",
    "https://github.com/OpenRLHF/OpenRLHF/pulls\">",
    "https://github.com/OpenRLHF/OpenRLHF/discussions\">",
    "https://github.com/TideDra/lmm-r1",
    "https://arxiv.org/abs/2502.14768",
    "https://llama.meta.com/",
    "https://docs.google.com/presentation/d/1JRhB1d7csofx0PIZBmfyBdMluxNd5JLPpUHrrvVhGnk/edit?usp=sharing",
    "https://github.com/OpenRLHF/OpenRLHF/blob/main/openrlhf/datasets/prompts_dataset.py#L6",
    "https://deepwiki.com/badge.svg\"",
    "https://arxiv.org/abs/2405.11143",
    "https://img.shields.io/github/issues-pr/OpenRLHF/OpenRLHF?color=0088ff\"",
    "https://hijkzzz.notion.site/unraveling-rlhf-and-its-variants-engineering-insights#147d9a33ecc9806090f3d5c749d31f05",
    "https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/huggingface-tp/README.md",
    "http://localhost:5000/get_reward",
    "https://github.com/OpenRLHF",
    "https://github.com/OpenRLHF/OpenRLHF",
    "https://contrib.rocks/image?repo=OpenRLHF/OpenRLHF\"",
    "https://github.com/openai/gpt-3",
    "https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat",
    "https://opencollective.com/OpenRLHF",
    "https://www.netmind.ai/",
    "https://deepwiki.com/OpenRLHF/OpenRLHF\"><img",
    "https://github.com/OpenRLHF/OpenRLHF/stargazers\">",
    "https://star-history.com/#OpenRLHF/OpenRLHF&Date",
    "https://github.com/OpenRLHF/OpenRLHF/issues\">",
    "https://mistral.ai/static/research/magistral.pdf",
    "https://github.com/RLHFlow/Online-RLHF",
    "https://github.com/TsinghuaC3I/MARTI",
    "https://img.shields.io/github/issues/OpenRLHF/OpenRLHF?color=0088ff\"",
    "https://hijkzzz.notion.site/rlhf-implementation-tricks?v=158d9a33ecc98132bf9e000c39227361",
    "https://img.shields.io/github/contributors/OpenRLHF/OpenRLHF\"",
    "https://openrlhf.readthedocs.io/",
    "https://img.shields.io/github/stars/OpenRLHF/OpenRLHF?color=ccf\"",
    "https://github.com/MeetKai/functionary/tree/main/functionary/train/packing",
    "https://github.com/hkust-nlp/simpleRL-reason",
    "https://arxiv.org/abs/2501.03262"
  ]
}
```

</details>


---

## Repository 4: Open-Reasoner-Zero/Open-Reasoner-Zero

# GitHub Repository Data

**Repository:** [Open-Reasoner-Zero/Open-Reasoner-Zero](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero)

## Basic Information

- **Description:** Official Repo for Open-Reasoner-Zero
- **Created:** 2025-02-19T17:28:25+00:00
- **Last Updated:** 2025-06-20T07:02:06+00:00
- **Last Pushed:** 2025-06-02T16:36:00+00:00
- **Default Branch:** main
- **Size:** 16502 KB

## Statistics

- **Stars:** 1,968
- **Forks:** 104
- **Watchers:** 1,968
- **Open Issues:** 19
- **Total Issues:** 0
- **Pull Requests:** 5

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE)

## Languages

- **Python:** 337,249 bytes
- **Dockerfile:** 1,739 bytes

## Top Contributors

1. **REIGN12** - 21 contributions
2. **vwxyzjn** - 5 contributions
3. **YinminZhang** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 71

- `.flake8` (blob)
- `.gitignore` (blob)
- `LICENSE` (blob)
- `ORZ_paper.pdf` (blob)
- `README.md` (blob)
- `data` (tree)
- `data/eval_data` (tree)
- `data/eval_data/aime2024.json` (blob)
- `data/eval_data/gpqa_diamond.json` (blob)
- `data/eval_data/math500.json` (blob)

## Recent Issues

- 🟢 **#73** DeepSpeed does not detect CUDA for some reason: (open)
- 🟢 **#72** In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map="meta"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device("meta")` (open)
- 🟢 **#71** In `orz/ppo/actors.py`: `with torch.device("meta"): AutoModel.from_pretrained(...)` throws (open)
- 🟢 **#70** `llm_engine.model_executor` seems not available on modern vllm (0.8.5) (open)
- 🟢 **#69** On newer vllm should use `llm_engine.vllm_config` and similar (open)

## Recent Pull Requests

- 🟢 **#66** 重构 PPOExpConfig 类，继承自 CommonPPOConfig，简化配置管理 (open)
- 🔴 **#62** Major Updates for Open-Reasoner-Zero (closed)
- 🟢 **#57** Support FSDP and vllm colocate with tp size > 1 (open)
- 🔴 **#32** Local ppo mini (closed)
- 🔴 **#31** Add a mini example (closed)

## Recent Commits

- **3fdd9a07** doc: update ORZ-R1-Distill-Qwen-14B results - REIGN12 (2025-06-02T16:35:57+00:00)
- **a2351927** chores: update qr code - REIGN12 (2025-06-02T16:35:25+00:00)
- **20e5bc83** README: update readme - REIGN12 (2025-04-08T12:58:51+00:00)
- **9c73e259** README: update readme - REIGN12 (2025-04-02T03:41:38+00:00)
- **a27ce5c1** README: update readme - REIGN12 (2025-04-02T03:38:35+00:00)
- **a288753d** README: update readme - REIGN12 (2025-03-31T15:53:39+00:00)
- **a7193f2c** Major Updates for Open-Reasoner-Zero (#62) - Jingcheng Hu (2025-03-31T15:45:50+00:00)
- **e008f6d9** Fix: bug fix for weight sync logic when colocate=False - REIGN12 (2025-03-05T08:08:42+00:00)
- **effe6fb8** Fix: fix typo in report - REIGN12 (2025-03-01T09:03:59+00:00)
- **9464093c** Fix: minor bugfix for dockerfile - REIGN12 (2025-03-01T08:54:56+00:00)

## External Links Found in README

- https://github.com/deepspeedai/DeepSpeed
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json
- https://huggingface.co/Qwen/Qwen2.5-0.5B
- https://arxiv.org/abs/2503.24290
- https://huggingface.co/Open-Reasoner-Zero"
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B
- https://www.stepfun.com/
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
- https://arxiv.org/abs/2503.24290},
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py
- https://github.com/vllm-project/vllm
- https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline
- https://huggingface.co/datasets/open-r1/OpenR1-Math-220k
- https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor"/></a>
- https://arxiv.org/abs/2503.24290"><b>Paper
- https://github.com/ray-project/ray
- https://projectnumina.ai/

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 935587127,
  "name": "Open-Reasoner-Zero",
  "full_name": "Open-Reasoner-Zero/Open-Reasoner-Zero",
  "description": "Official Repo for Open-Reasoner-Zero",
  "html_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero",
  "clone_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "ssh_url": "git@github.com:Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "homepage": "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903",
  "topics": [],
  "default_branch": "main",
  "created_at": "2025-02-19T17:28:25+00:00",
  "updated_at": "2025-06-20T07:02:06+00:00",
  "pushed_at": "2025-06-02T16:36:00+00:00",
  "size_kb": 16502,
  "watchers_count": 1968,
  "stargazers_count": 1968,
  "forks_count": 104,
  "open_issues_count": 19,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE"
  },
  "languages": {
    "Python": 337249,
    "Dockerfile": 1739
  },
  "top_contributors": [
    {
      "login": "REIGN12",
      "contributions": 21
    },
    {
      "login": "vwxyzjn",
      "contributions": 5
    },
    {
      "login": "YinminZhang",
      "contributions": 3
    }
  ],
  "file_tree_count": 71,
  "file_tree_sample": [
    {
      "path": ".flake8",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "ORZ_paper.pdf",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "data",
      "type": "tree"
    },
    {
      "path": "data/eval_data",
      "type": "tree"
    },
    {
      "path": "data/eval_data/aime2024.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/gpqa_diamond.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/math500.json",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 5,
  "recent_issues": [
    {
      "number": 73,
      "title": "DeepSpeed does not detect CUDA for some reason:",
      "state": "open"
    },
    {
      "number": 72,
      "title": "In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map=\"meta\"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device(\"meta\")`",
      "state": "open"
    },
    {
      "number": 71,
      "title": "In `orz/ppo/actors.py`: `with torch.device(\"meta\"): AutoModel.from_pretrained(...)` throws",
      "state": "open"
    },
    {
      "number": 70,
      "title": "`llm_engine.model_executor` seems not available on modern vllm (0.8.5)",
      "state": "open"
    },
    {
      "number": 69,
      "title": "On newer vllm should use `llm_engine.vllm_config` and similar",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 66,
      "title": "\u91cd\u6784 PPOExpConfig \u7c7b\uff0c\u7ee7\u627f\u81ea CommonPPOConfig\uff0c\u7b80\u5316\u914d\u7f6e\u7ba1\u7406",
      "state": "open"
    },
    {
      "number": 62,
      "title": "Major Updates for Open-Reasoner-Zero",
      "state": "closed"
    },
    {
      "number": 57,
      "title": "Support FSDP and vllm colocate with tp size > 1",
      "state": "open"
    },
    {
      "number": 32,
      "title": "Local ppo mini",
      "state": "closed"
    },
    {
      "number": 31,
      "title": "Add a mini example",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "3fdd9a07b4fb01e06005e6e74fc56690cde8a341",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:57+00:00",
      "message": "doc: update ORZ-R1-Distill-Qwen-14B results"
    },
    {
      "sha": "a23519277efdd71a063e1f2913d5c496a5f96180",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:25+00:00",
      "message": "chores: update qr code"
    },
    {
      "sha": "20e5bc83ae75695fded4e1a59a21e487063e24d9",
      "author": "REIGN12",
      "date": "2025-04-08T12:58:51+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "9c73e2590d23a7245a8d6341337952828110c631",
      "author": "REIGN12",
      "date": "2025-04-02T03:41:38+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a27ce5c1c5204d267c821542af4bdb47c70aa40a",
      "author": "REIGN12",
      "date": "2025-04-02T03:38:35+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a288753de916c0f3ba932fb0d1c218a49b8a403b",
      "author": "REIGN12",
      "date": "2025-03-31T15:53:39+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a7193f2c14aa90d055f9c161bf4566ec41ced30c",
      "author": "Jingcheng Hu",
      "date": "2025-03-31T15:45:50+00:00",
      "message": "Major Updates for Open-Reasoner-Zero (#62)"
    },
    {
      "sha": "e008f6d95f0b9a0e992f6b8bac912515b50a4634",
      "author": "REIGN12",
      "date": "2025-03-05T08:08:42+00:00",
      "message": "Fix: bug fix for weight sync logic when colocate=False"
    },
    {
      "sha": "effe6fb81570ccfde7d28d8d335c83127e95263f",
      "author": "REIGN12",
      "date": "2025-03-01T09:03:59+00:00",
      "message": "Fix: fix typo in report"
    },
    {
      "sha": "9464093c67c32a5fca69556b603f915e55fc00a5",
      "author": "REIGN12",
      "date": "2025-03-01T08:54:56+00:00",
      "message": "Fix: minor bugfix for dockerfile"
    },
    {
      "sha": "f594903dc0e953a8d0429ae0ecf339efcbe456f7",
      "author": "Jingcheng Hu",
      "date": "2025-02-25T15:03:35+00:00",
      "message": "Merge pull request #31 from vwxyzjn/ppo-mini"
    },
    {
      "sha": "ce912e108ca03d5ded42e151e3c857d44fc3f277",
      "author": "Costa Huang",
      "date": "2025-02-24T20:33:48+00:00",
      "message": "quick change"
    },
    {
      "sha": "34d8abc52ecba9c1c620350eb70110d05ea6b676",
      "author": "Costa Huang",
      "date": "2025-02-24T20:32:51+00:00",
      "message": "set eval to false"
    },
    {
      "sha": "55904eae7c35932e8ee2efb717c788ed3cd9b75e",
      "author": "Costa Huang",
      "date": "2025-02-24T20:31:44+00:00",
      "message": "quick change"
    },
    {
      "sha": "dbc40d8042c4981138b58773ca4936570e2339be",
      "author": "Costa Huang",
      "date": "2025-02-24T20:28:34+00:00",
      "message": "Add single GPU exampel"
    },
    {
      "sha": "ec2ce7e528f660f3b05c19369cdcff9d89cbb6b0",
      "author": "Costa Huang",
      "date": "2025-02-24T20:06:00+00:00",
      "message": "Add a mini example"
    },
    {
      "sha": "7b5dec17a8dda164e8a4aaba33393afc3e0e95d9",
      "author": "Jingcheng Hu",
      "date": "2025-02-24T09:18:55+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "3ccec9eec8717a5de0575918237bc81b6ba3e3a2",
      "author": "REIGN12",
      "date": "2025-02-24T07:24:11+00:00",
      "message": "Merge branch 'main' of https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
    },
    {
      "sha": "df77747d28c8580db768732f8da34f913e0411ca",
      "author": "REIGN12",
      "date": "2025-02-24T07:23:34+00:00",
      "message": "Fix: bug fix for readme and 32b exp"
    },
    {
      "sha": "ec6b01ef61f6b66a30bff0ea763b37fe4e9d00d1",
      "author": "hanqer",
      "date": "2025-02-24T05:52:43+00:00",
      "message": "fix: fix the package dependencies of pip and apt."
    }
  ],
  "readme_text": "<div align=\"center\">\n\n# Open Reasoner Zero\n\n<img src=\"figure/logo.jpg\" width=\"300\"/>\n\n<div>\n\nAn Open Source Approach to Scaling Up Reinforcement Learning on the Base Model\n</div>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n    <a href=\"https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\" style=\"margin: 2px;\"><img alt=\"Code\" src=\"https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/></a>\n  \n  <a href=\"https://huggingface.co/Open-Reasoner-Zero\" target=\"_blank\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>\n\n  <a href=\"https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\" target=\"_blank\">\n  <img alt=\"Notion Page\"\n    src=\"https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>\n\n  <br>\n  <a href=\"https://arxiv.org/abs/2503.24290\"><b>Paper Arxiv Link </b>\ud83d\udc41\ufe0f</a>\n</div>\n\n<div>\n<br>\n\n</div>\n\n## Overview \ud83c\udf0a\nWe introduce **Open-Reasoner-Zero**, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility.\nUsing the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency\u2014requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline.\n\nTo enable broader participation in this pivotal moment we witnessed and accelerate research towards artificial general intelligence (AGI), \nwe release our source code, parameter settings, training data, and model weights.\nPlease refer to our [paper](https://arxiv.org/abs/2503.24290) for more insights across various model sizes. \n\n**Let the Reasoner-Zero tide rise!**\n\n\n## Main Results \ud83c\udfc6\n\n![](figure/teaser.png)\n\n*Figure 1 | Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\}. Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\} on benchmarks (averaged on 16 responses) during training. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, Open-Reasoner-Zero-32B achieves superior performance on AIME2024, MATH500, and GPQA Diamond benchmark-requiring only a tenth of the training steps.*\n\n![](figure/train_curve.png)\n*Figure 2 | Train-time Scale up on Train Reward and Response Length of Open-Reasoner-Zero (ORZ) - \\{0.5B, 1.5B, 7B, 32B\\}. Train Reward and Response Length increase steadily, demonstrating consistent scalability across model sizes. Interestingly, the ORZ-32B Response Length exhibits fluctuations without negatively impacting training stability, highlighting the robustness of our minimalist recipe.*\n\n## Releases \ud83d\udce6\n\n<strong>[2025/06/03]</strong>\nWe release [ORZ-R1-Distill-Qwen-14B](https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B), obtained by applying ORZ recipe to reasoning-enhanced models like DeepSeek-R1-Distill-Qwen-14B. This ORZ-R1-Distill-Qwen-14B achieves strong results on reasoning benchmarks, even surpassing the larger DeepSeek-R1-Distill-Qwen-32B model.\n\n| Model                        | AIME 2024 | AIME 2025 | MATH500 | GPQA Dia. |\n| ---------------------------- | --------- | --------- | ------- | --------- |\n| DeepSeek-R1-Distill-Qwen-14B | 69.7      | 49.1      | 93.9    | 59.1      |\n| DeepSeek-R1-Distill-Qwen-32B | 72.6      | 60.0      | 94.3    | **62.1**     |\n| **ORZ-R1-Distill-Qwen-14B**  | **75.2**  | **60.0**  | **95.6** | 60.4  |\n\n\n<strong>[2025/03/31]</strong>\nWe announce a major milestone for `Open-Reasoner-Zero`:\n\n- \ud83c\udf0a [Updated Paper](https://arxiv.org/abs/2503.24290) with new results.\n- \ud83d\udd2d [Easy-to-use Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground):\n  - [ORZ-1.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py) and [ORZ-0.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py) (main results in Figure 2). \n  - [Minimal resource training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py): ORZ-0.5B can be run on a single A800/H800 gpu!\n- \ud83e\udd29 [Updated Curated Datasets](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data): \n  - 129k data in total:\n    - [original 57k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json).\n    - [extended 72k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json).\n  - [13k hard data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json) mined from the above 129k data. \n    - used in the \"annealing\" stage of ORZ-32B training: **AIME2024 from ~41% to ~48%**!\n- \ud83e\udd17 More HF Models: \n  - Updated HF Models: [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B).\n  - Released HF Models: [`Open-Reasoner-Zero-1.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B) and [`Open-Reasoner-Zero-0.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B).\n- \ud83d\ude80 Full Suite of Critic Models for in-depth research: `Open-Reasoner-Zero-Critic-`{[0.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B), [1.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B), [7B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B),  [32B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B)}.\n\n<strong>[2025/02/18]</strong>\nWe release `Open-Reasoner-Zero`. \n\nAs part of this release, we open-source:\n- \ud83c\udf0a [Paper(WIP)](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf) on our comprehensive analysis and insights in Reasoner-Zero training\n- \ud83e\udd17 HF Model [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B)\n- \ud83c\udf81 [`Our curated 57k training data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data)\n- \ud83d\udcc4 [Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground) to enjoy your own Reasoner-Zero journey!\n\n## Key Features in Codebase \ud83d\udd11\n\n- Adopt single controller trainer design, flexible and researcher-friendly.\n- Colocate training and generation in the same GPUs to maximize GPU utilization.\n\n## Getting Started \ud83d\ude80\n### Data\n\nWe release all of curated high-quality training data in the [`data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data) folder:\n* curated 129k data:\n  * [original 57k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json), collected from various sources, including AIME (up to 2023), MATH, Numina-Math collection and Tulu3 MATH.\n  * [extended 72k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json), mainly cleaned from OpenR1-Math-220k.\n* [hard 13k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json), mined from the first stage of ORZ-32B training.\n\nThe details for how to collect data are described in our [paper](https://arxiv.org/abs/2503.24290).\n\n### Installation & Training Scripts\nWe release our [Dockerfile](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile) in [docker](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker) folder to facilitate the reproducibility of our training.\n\nTo install the package, run:\n```bash\npip install -e .\n```\n\n#### Start ORZ-32B PPO Training\nHere are the starting commands in 16 nodes. \n\nFirst on master node, run:\n```bash\nray start --head\n# you will see logging like:\n# Next steps\n#  To add another node to this Ray cluster, run\n#    ray start --address='<master-node-ip>:<master-node-port>'\n```\n\nthen on all other nodes, run:\n```bash\nray start --address='<master-node-ip>:<master-node-port>' # <master-node-ip> and <master-node-port> are from above loggings!\n```\n\nfinally on master node, just run:\n```bash\npython -m playground.orz_32b_ppo\n```\nYour training log will be shown in the master node terminal.\n\n------\n\n#### Start ORZ-0.5B PPO Training\nYou can start the ORZ-0.5B PPO training in single A800/H800 node:\n```bash\npython -m playground.orz_0p5b_ppo\n```\n\nYou can even run in **a single A800/H800 gpu**: \n```bash\npython -m playground.orz_0p5b_ppo_1gpu\n```\n\nnote: since we are not in multi-node setting, no `ray start` like logics are needed.\n\n------\n\n#### Start ORZ-7B PPO Training\n\nMulti-node Training on 4 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n\n# then on master node, run:\npython -m playground.orz_7b_ppo\n```\n\nYour training log will be shown in the master node terminal.\n\n-----\n\n#### Start ORZ-1.5B PPO Training\n\nMulti-node Training on 2 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n# then on master node, run:\npython -m playground.orz_1p5b_ppo\n```\n\n----\n\n#### Debug Settings\nIn the code, we leave an environment variable `DEBUG_MODE` to run in debug setting for researcher to iterate. (Thought for now, we recommend using `python -m playground.orz_0p5b_ppo_1gpu` for debugging.)\n\nThe debug running command examples:\n```bash\n# NOTE: just for debug, not final setting!\n\n## Debug command in a single GPU with `EleutherAI/pythia-14m`\nDEBUG_MODE=True python -m playground.orz_14m_ppo_mini\n## Debug command in a single node (8 GPUs) with `Qwen/Qwen2.5-7B`\nDEBUG_MODE=True python -m playground.orz_7b_ppo\n```\n\n### How to Use the Model\n#### Policy Model\nPolicy models can be used in the same way as any chat model in transformers and vllm, since we have put the chat template jinja in the tokenizer.\n\n#### Critic Model\nCritic models can be loaded the same way like in the [training code](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738). \n\n\n## Acknowledgements \ud83d\udc96 \n\n- This work was supported by computing resources and valuable feedback provided by [StepFun](https://www.stepfun.com/) and Tsinghua University.\n- Our training framework is built on [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF), [vllm](https://github.com/vllm-project/vllm), [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) and [ray](https://github.com/ray-project/ray).\n- Our model is based on [Qwen2.5 Series](https://qwenlm.github.io/blog/qwen2.5-llm/) of **base models**, including [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B), [Qwen2.5-1.5B](https://huggingface.co/Qwen/Qwen2.5-1.5B), [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B) and [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B).\n- We thank [Project Numina](https://projectnumina.ai/), [Tulu3](https://allenai.org/blog/tulu-3-technical) and [OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k) for their collected open sourced data.\n\n## Advertisement Time \ud83d\udce3\n\nWe are hiring talented researchers and engineers to join our team. If you are interested in our project and would like to contribute to the reasoner scale-up all the way to AGI, please feel free to reach out to us at hanqer@stepfun.com\n\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline)](https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline)\n\n## Community Discussions \ud83c\udf7a\n\nWe have several wechat groups to help discussions and sharing, you can scan the QR code below to join the latest group.\n\n<img src=\"figure/WeChatGroup.png\" width=\"300\" style=\"display: block; margin: 0 auto;\"/>\n\n## Citation\n\n```bibtex\n@misc{hu2025openreasonerzeroopensourceapproach,\n      title={Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model}, \n      author={Jingcheng Hu and Yinmin Zhang and Qi Han and Daxin Jiang and Xiangyu Zhang and Heung-Yeung Shum},\n      year={2025},\n      eprint={2503.24290},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2503.24290}, \n}\n```\n",
  "external_links_in_readme": [
    "https://github.com/deepspeedai/DeepSpeed",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json",
    "https://huggingface.co/Qwen/Qwen2.5-0.5B",
    "https://arxiv.org/abs/2503.24290",
    "https://huggingface.co/Open-Reasoner-Zero\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B",
    "https://www.stepfun.com/",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\"",
    "https://arxiv.org/abs/2503.24290},",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py",
    "https://github.com/vllm-project/vllm",
    "https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline",
    "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k",
    "https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>",
    "https://arxiv.org/abs/2503.24290\"><b>Paper",
    "https://github.com/ray-project/ray",
    "https://projectnumina.ai/",
    "https://qwenlm.github.io/blog/qwen2.5-llm/",
    "https://huggingface.co/Qwen/Qwen2.5-7B",
    "https://huggingface.co/Qwen/Qwen2.5-1.5B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B",
    "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\"",
    "https://github.com/OpenRLHF/OpenRLHF",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json",
    "https://huggingface.co/Qwen/Qwen2.5-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py",
    "https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>",
    "https://allenai.org/blog/tulu-3-technical",
    "https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B",
    "https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline",
    "https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B"
  ]
}
```

</details>


---

## Repository 5: Open-Reasoner-Zero/Open-Reasoner-Zero

# GitHub Repository Data

**Repository:** [Open-Reasoner-Zero/Open-Reasoner-Zero](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero)

## Basic Information

- **Description:** Official Repo for Open-Reasoner-Zero
- **Created:** 2025-02-19T17:28:25+00:00
- **Last Updated:** 2025-06-20T07:02:06+00:00
- **Last Pushed:** 2025-06-02T16:36:00+00:00
- **Default Branch:** main
- **Size:** 16502 KB

## Statistics

- **Stars:** 1,968
- **Forks:** 104
- **Watchers:** 1,968
- **Open Issues:** 19
- **Total Issues:** 0
- **Pull Requests:** 5

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE)

## Languages

- **Python:** 337,249 bytes
- **Dockerfile:** 1,739 bytes

## Top Contributors

1. **REIGN12** - 21 contributions
2. **vwxyzjn** - 5 contributions
3. **YinminZhang** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 71

- `.flake8` (blob)
- `.gitignore` (blob)
- `LICENSE` (blob)
- `ORZ_paper.pdf` (blob)
- `README.md` (blob)
- `data` (tree)
- `data/eval_data` (tree)
- `data/eval_data/aime2024.json` (blob)
- `data/eval_data/gpqa_diamond.json` (blob)
- `data/eval_data/math500.json` (blob)

## Recent Issues

- 🟢 **#73** DeepSpeed does not detect CUDA for some reason: (open)
- 🟢 **#72** In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map="meta"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device("meta")` (open)
- 🟢 **#71** In `orz/ppo/actors.py`: `with torch.device("meta"): AutoModel.from_pretrained(...)` throws (open)
- 🟢 **#70** `llm_engine.model_executor` seems not available on modern vllm (0.8.5) (open)
- 🟢 **#69** On newer vllm should use `llm_engine.vllm_config` and similar (open)

## Recent Pull Requests

- 🟢 **#66** 重构 PPOExpConfig 类，继承自 CommonPPOConfig，简化配置管理 (open)
- 🔴 **#62** Major Updates for Open-Reasoner-Zero (closed)
- 🟢 **#57** Support FSDP and vllm colocate with tp size > 1 (open)
- 🔴 **#32** Local ppo mini (closed)
- 🔴 **#31** Add a mini example (closed)

## Recent Commits

- **3fdd9a07** doc: update ORZ-R1-Distill-Qwen-14B results - REIGN12 (2025-06-02T16:35:57+00:00)
- **a2351927** chores: update qr code - REIGN12 (2025-06-02T16:35:25+00:00)
- **20e5bc83** README: update readme - REIGN12 (2025-04-08T12:58:51+00:00)
- **9c73e259** README: update readme - REIGN12 (2025-04-02T03:41:38+00:00)
- **a27ce5c1** README: update readme - REIGN12 (2025-04-02T03:38:35+00:00)
- **a288753d** README: update readme - REIGN12 (2025-03-31T15:53:39+00:00)
- **a7193f2c** Major Updates for Open-Reasoner-Zero (#62) - Jingcheng Hu (2025-03-31T15:45:50+00:00)
- **e008f6d9** Fix: bug fix for weight sync logic when colocate=False - REIGN12 (2025-03-05T08:08:42+00:00)
- **effe6fb8** Fix: fix typo in report - REIGN12 (2025-03-01T09:03:59+00:00)
- **9464093c** Fix: minor bugfix for dockerfile - REIGN12 (2025-03-01T08:54:56+00:00)

## External Links Found in README

- https://github.com/deepspeedai/DeepSpeed
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json
- https://huggingface.co/Qwen/Qwen2.5-0.5B
- https://arxiv.org/abs/2503.24290
- https://huggingface.co/Open-Reasoner-Zero"
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B
- https://www.stepfun.com/
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
- https://arxiv.org/abs/2503.24290},
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py
- https://github.com/vllm-project/vllm
- https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline
- https://huggingface.co/datasets/open-r1/OpenR1-Math-220k
- https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor"/></a>
- https://arxiv.org/abs/2503.24290"><b>Paper
- https://github.com/ray-project/ray
- https://projectnumina.ai/

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 935587127,
  "name": "Open-Reasoner-Zero",
  "full_name": "Open-Reasoner-Zero/Open-Reasoner-Zero",
  "description": "Official Repo for Open-Reasoner-Zero",
  "html_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero",
  "clone_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "ssh_url": "git@github.com:Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "homepage": "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903",
  "topics": [],
  "default_branch": "main",
  "created_at": "2025-02-19T17:28:25+00:00",
  "updated_at": "2025-06-20T07:02:06+00:00",
  "pushed_at": "2025-06-02T16:36:00+00:00",
  "size_kb": 16502,
  "watchers_count": 1968,
  "stargazers_count": 1968,
  "forks_count": 104,
  "open_issues_count": 19,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE"
  },
  "languages": {
    "Python": 337249,
    "Dockerfile": 1739
  },
  "top_contributors": [
    {
      "login": "REIGN12",
      "contributions": 21
    },
    {
      "login": "vwxyzjn",
      "contributions": 5
    },
    {
      "login": "YinminZhang",
      "contributions": 3
    }
  ],
  "file_tree_count": 71,
  "file_tree_sample": [
    {
      "path": ".flake8",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "ORZ_paper.pdf",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "data",
      "type": "tree"
    },
    {
      "path": "data/eval_data",
      "type": "tree"
    },
    {
      "path": "data/eval_data/aime2024.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/gpqa_diamond.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/math500.json",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 5,
  "recent_issues": [
    {
      "number": 73,
      "title": "DeepSpeed does not detect CUDA for some reason:",
      "state": "open"
    },
    {
      "number": 72,
      "title": "In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map=\"meta\"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device(\"meta\")`",
      "state": "open"
    },
    {
      "number": 71,
      "title": "In `orz/ppo/actors.py`: `with torch.device(\"meta\"): AutoModel.from_pretrained(...)` throws",
      "state": "open"
    },
    {
      "number": 70,
      "title": "`llm_engine.model_executor` seems not available on modern vllm (0.8.5)",
      "state": "open"
    },
    {
      "number": 69,
      "title": "On newer vllm should use `llm_engine.vllm_config` and similar",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 66,
      "title": "\u91cd\u6784 PPOExpConfig \u7c7b\uff0c\u7ee7\u627f\u81ea CommonPPOConfig\uff0c\u7b80\u5316\u914d\u7f6e\u7ba1\u7406",
      "state": "open"
    },
    {
      "number": 62,
      "title": "Major Updates for Open-Reasoner-Zero",
      "state": "closed"
    },
    {
      "number": 57,
      "title": "Support FSDP and vllm colocate with tp size > 1",
      "state": "open"
    },
    {
      "number": 32,
      "title": "Local ppo mini",
      "state": "closed"
    },
    {
      "number": 31,
      "title": "Add a mini example",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "3fdd9a07b4fb01e06005e6e74fc56690cde8a341",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:57+00:00",
      "message": "doc: update ORZ-R1-Distill-Qwen-14B results"
    },
    {
      "sha": "a23519277efdd71a063e1f2913d5c496a5f96180",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:25+00:00",
      "message": "chores: update qr code"
    },
    {
      "sha": "20e5bc83ae75695fded4e1a59a21e487063e24d9",
      "author": "REIGN12",
      "date": "2025-04-08T12:58:51+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "9c73e2590d23a7245a8d6341337952828110c631",
      "author": "REIGN12",
      "date": "2025-04-02T03:41:38+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a27ce5c1c5204d267c821542af4bdb47c70aa40a",
      "author": "REIGN12",
      "date": "2025-04-02T03:38:35+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a288753de916c0f3ba932fb0d1c218a49b8a403b",
      "author": "REIGN12",
      "date": "2025-03-31T15:53:39+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a7193f2c14aa90d055f9c161bf4566ec41ced30c",
      "author": "Jingcheng Hu",
      "date": "2025-03-31T15:45:50+00:00",
      "message": "Major Updates for Open-Reasoner-Zero (#62)"
    },
    {
      "sha": "e008f6d95f0b9a0e992f6b8bac912515b50a4634",
      "author": "REIGN12",
      "date": "2025-03-05T08:08:42+00:00",
      "message": "Fix: bug fix for weight sync logic when colocate=False"
    },
    {
      "sha": "effe6fb81570ccfde7d28d8d335c83127e95263f",
      "author": "REIGN12",
      "date": "2025-03-01T09:03:59+00:00",
      "message": "Fix: fix typo in report"
    },
    {
      "sha": "9464093c67c32a5fca69556b603f915e55fc00a5",
      "author": "REIGN12",
      "date": "2025-03-01T08:54:56+00:00",
      "message": "Fix: minor bugfix for dockerfile"
    },
    {
      "sha": "f594903dc0e953a8d0429ae0ecf339efcbe456f7",
      "author": "Jingcheng Hu",
      "date": "2025-02-25T15:03:35+00:00",
      "message": "Merge pull request #31 from vwxyzjn/ppo-mini"
    },
    {
      "sha": "ce912e108ca03d5ded42e151e3c857d44fc3f277",
      "author": "Costa Huang",
      "date": "2025-02-24T20:33:48+00:00",
      "message": "quick change"
    },
    {
      "sha": "34d8abc52ecba9c1c620350eb70110d05ea6b676",
      "author": "Costa Huang",
      "date": "2025-02-24T20:32:51+00:00",
      "message": "set eval to false"
    },
    {
      "sha": "55904eae7c35932e8ee2efb717c788ed3cd9b75e",
      "author": "Costa Huang",
      "date": "2025-02-24T20:31:44+00:00",
      "message": "quick change"
    },
    {
      "sha": "dbc40d8042c4981138b58773ca4936570e2339be",
      "author": "Costa Huang",
      "date": "2025-02-24T20:28:34+00:00",
      "message": "Add single GPU exampel"
    },
    {
      "sha": "ec2ce7e528f660f3b05c19369cdcff9d89cbb6b0",
      "author": "Costa Huang",
      "date": "2025-02-24T20:06:00+00:00",
      "message": "Add a mini example"
    },
    {
      "sha": "7b5dec17a8dda164e8a4aaba33393afc3e0e95d9",
      "author": "Jingcheng Hu",
      "date": "2025-02-24T09:18:55+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "3ccec9eec8717a5de0575918237bc81b6ba3e3a2",
      "author": "REIGN12",
      "date": "2025-02-24T07:24:11+00:00",
      "message": "Merge branch 'main' of https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
    },
    {
      "sha": "df77747d28c8580db768732f8da34f913e0411ca",
      "author": "REIGN12",
      "date": "2025-02-24T07:23:34+00:00",
      "message": "Fix: bug fix for readme and 32b exp"
    },
    {
      "sha": "ec6b01ef61f6b66a30bff0ea763b37fe4e9d00d1",
      "author": "hanqer",
      "date": "2025-02-24T05:52:43+00:00",
      "message": "fix: fix the package dependencies of pip and apt."
    }
  ],
  "readme_text": "<div align=\"center\">\n\n# Open Reasoner Zero\n\n<img src=\"figure/logo.jpg\" width=\"300\"/>\n\n<div>\n\nAn Open Source Approach to Scaling Up Reinforcement Learning on the Base Model\n</div>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n    <a href=\"https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\" style=\"margin: 2px;\"><img alt=\"Code\" src=\"https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/></a>\n  \n  <a href=\"https://huggingface.co/Open-Reasoner-Zero\" target=\"_blank\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>\n\n  <a href=\"https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\" target=\"_blank\">\n  <img alt=\"Notion Page\"\n    src=\"https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>\n\n  <br>\n  <a href=\"https://arxiv.org/abs/2503.24290\"><b>Paper Arxiv Link </b>\ud83d\udc41\ufe0f</a>\n</div>\n\n<div>\n<br>\n\n</div>\n\n## Overview \ud83c\udf0a\nWe introduce **Open-Reasoner-Zero**, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility.\nUsing the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency\u2014requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline.\n\nTo enable broader participation in this pivotal moment we witnessed and accelerate research towards artificial general intelligence (AGI), \nwe release our source code, parameter settings, training data, and model weights.\nPlease refer to our [paper](https://arxiv.org/abs/2503.24290) for more insights across various model sizes. \n\n**Let the Reasoner-Zero tide rise!**\n\n\n## Main Results \ud83c\udfc6\n\n![](figure/teaser.png)\n\n*Figure 1 | Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\}. Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\} on benchmarks (averaged on 16 responses) during training. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, Open-Reasoner-Zero-32B achieves superior performance on AIME2024, MATH500, and GPQA Diamond benchmark-requiring only a tenth of the training steps.*\n\n![](figure/train_curve.png)\n*Figure 2 | Train-time Scale up on Train Reward and Response Length of Open-Reasoner-Zero (ORZ) - \\{0.5B, 1.5B, 7B, 32B\\}. Train Reward and Response Length increase steadily, demonstrating consistent scalability across model sizes. Interestingly, the ORZ-32B Response Length exhibits fluctuations without negatively impacting training stability, highlighting the robustness of our minimalist recipe.*\n\n## Releases \ud83d\udce6\n\n<strong>[2025/06/03]</strong>\nWe release [ORZ-R1-Distill-Qwen-14B](https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B), obtained by applying ORZ recipe to reasoning-enhanced models like DeepSeek-R1-Distill-Qwen-14B. This ORZ-R1-Distill-Qwen-14B achieves strong results on reasoning benchmarks, even surpassing the larger DeepSeek-R1-Distill-Qwen-32B model.\n\n| Model                        | AIME 2024 | AIME 2025 | MATH500 | GPQA Dia. |\n| ---------------------------- | --------- | --------- | ------- | --------- |\n| DeepSeek-R1-Distill-Qwen-14B | 69.7      | 49.1      | 93.9    | 59.1      |\n| DeepSeek-R1-Distill-Qwen-32B | 72.6      | 60.0      | 94.3    | **62.1**     |\n| **ORZ-R1-Distill-Qwen-14B**  | **75.2**  | **60.0**  | **95.6** | 60.4  |\n\n\n<strong>[2025/03/31]</strong>\nWe announce a major milestone for `Open-Reasoner-Zero`:\n\n- \ud83c\udf0a [Updated Paper](https://arxiv.org/abs/2503.24290) with new results.\n- \ud83d\udd2d [Easy-to-use Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground):\n  - [ORZ-1.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py) and [ORZ-0.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py) (main results in Figure 2). \n  - [Minimal resource training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py): ORZ-0.5B can be run on a single A800/H800 gpu!\n- \ud83e\udd29 [Updated Curated Datasets](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data): \n  - 129k data in total:\n    - [original 57k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json).\n    - [extended 72k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json).\n  - [13k hard data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json) mined from the above 129k data. \n    - used in the \"annealing\" stage of ORZ-32B training: **AIME2024 from ~41% to ~48%**!\n- \ud83e\udd17 More HF Models: \n  - Updated HF Models: [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B).\n  - Released HF Models: [`Open-Reasoner-Zero-1.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B) and [`Open-Reasoner-Zero-0.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B).\n- \ud83d\ude80 Full Suite of Critic Models for in-depth research: `Open-Reasoner-Zero-Critic-`{[0.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B), [1.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B), [7B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B),  [32B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B)}.\n\n<strong>[2025/02/18]</strong>\nWe release `Open-Reasoner-Zero`. \n\nAs part of this release, we open-source:\n- \ud83c\udf0a [Paper(WIP)](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf) on our comprehensive analysis and insights in Reasoner-Zero training\n- \ud83e\udd17 HF Model [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B)\n- \ud83c\udf81 [`Our curated 57k training data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data)\n- \ud83d\udcc4 [Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground) to enjoy your own Reasoner-Zero journey!\n\n## Key Features in Codebase \ud83d\udd11\n\n- Adopt single controller trainer design, flexible and researcher-friendly.\n- Colocate training and generation in the same GPUs to maximize GPU utilization.\n\n## Getting Started \ud83d\ude80\n### Data\n\nWe release all of curated high-quality training data in the [`data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data) folder:\n* curated 129k data:\n  * [original 57k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json), collected from various sources, including AIME (up to 2023), MATH, Numina-Math collection and Tulu3 MATH.\n  * [extended 72k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json), mainly cleaned from OpenR1-Math-220k.\n* [hard 13k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json), mined from the first stage of ORZ-32B training.\n\nThe details for how to collect data are described in our [paper](https://arxiv.org/abs/2503.24290).\n\n### Installation & Training Scripts\nWe release our [Dockerfile](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile) in [docker](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker) folder to facilitate the reproducibility of our training.\n\nTo install the package, run:\n```bash\npip install -e .\n```\n\n#### Start ORZ-32B PPO Training\nHere are the starting commands in 16 nodes. \n\nFirst on master node, run:\n```bash\nray start --head\n# you will see logging like:\n# Next steps\n#  To add another node to this Ray cluster, run\n#    ray start --address='<master-node-ip>:<master-node-port>'\n```\n\nthen on all other nodes, run:\n```bash\nray start --address='<master-node-ip>:<master-node-port>' # <master-node-ip> and <master-node-port> are from above loggings!\n```\n\nfinally on master node, just run:\n```bash\npython -m playground.orz_32b_ppo\n```\nYour training log will be shown in the master node terminal.\n\n------\n\n#### Start ORZ-0.5B PPO Training\nYou can start the ORZ-0.5B PPO training in single A800/H800 node:\n```bash\npython -m playground.orz_0p5b_ppo\n```\n\nYou can even run in **a single A800/H800 gpu**: \n```bash\npython -m playground.orz_0p5b_ppo_1gpu\n```\n\nnote: since we are not in multi-node setting, no `ray start` like logics are needed.\n\n------\n\n#### Start ORZ-7B PPO Training\n\nMulti-node Training on 4 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n\n# then on master node, run:\npython -m playground.orz_7b_ppo\n```\n\nYour training log will be shown in the master node terminal.\n\n-----\n\n#### Start ORZ-1.5B PPO Training\n\nMulti-node Training on 2 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n# then on master node, run:\npython -m playground.orz_1p5b_ppo\n```\n\n----\n\n#### Debug Settings\nIn the code, we leave an environment variable `DEBUG_MODE` to run in debug setting for researcher to iterate. (Thought for now, we recommend using `python -m playground.orz_0p5b_ppo_1gpu` for debugging.)\n\nThe debug running command examples:\n```bash\n# NOTE: just for debug, not final setting!\n\n## Debug command in a single GPU with `EleutherAI/pythia-14m`\nDEBUG_MODE=True python -m playground.orz_14m_ppo_mini\n## Debug command in a single node (8 GPUs) with `Qwen/Qwen2.5-7B`\nDEBUG_MODE=True python -m playground.orz_7b_ppo\n```\n\n### How to Use the Model\n#### Policy Model\nPolicy models can be used in the same way as any chat model in transformers and vllm, since we have put the chat template jinja in the tokenizer.\n\n#### Critic Model\nCritic models can be loaded the same way like in the [training code](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738). \n\n\n## Acknowledgements \ud83d\udc96 \n\n- This work was supported by computing resources and valuable feedback provided by [StepFun](https://www.stepfun.com/) and Tsinghua University.\n- Our training framework is built on [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF), [vllm](https://github.com/vllm-project/vllm), [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) and [ray](https://github.com/ray-project/ray).\n- Our model is based on [Qwen2.5 Series](https://qwenlm.github.io/blog/qwen2.5-llm/) of **base models**, including [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B), [Qwen2.5-1.5B](https://huggingface.co/Qwen/Qwen2.5-1.5B), [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B) and [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B).\n- We thank [Project Numina](https://projectnumina.ai/), [Tulu3](https://allenai.org/blog/tulu-3-technical) and [OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k) for their collected open sourced data.\n\n## Advertisement Time \ud83d\udce3\n\nWe are hiring talented researchers and engineers to join our team. If you are interested in our project and would like to contribute to the reasoner scale-up all the way to AGI, please feel free to reach out to us at hanqer@stepfun.com\n\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline)](https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline)\n\n## Community Discussions \ud83c\udf7a\n\nWe have several wechat groups to help discussions and sharing, you can scan the QR code below to join the latest group.\n\n<img src=\"figure/WeChatGroup.png\" width=\"300\" style=\"display: block; margin: 0 auto;\"/>\n\n## Citation\n\n```bibtex\n@misc{hu2025openreasonerzeroopensourceapproach,\n      title={Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model}, \n      author={Jingcheng Hu and Yinmin Zhang and Qi Han and Daxin Jiang and Xiangyu Zhang and Heung-Yeung Shum},\n      year={2025},\n      eprint={2503.24290},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2503.24290}, \n}\n```\n",
  "external_links_in_readme": [
    "https://github.com/deepspeedai/DeepSpeed",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json",
    "https://huggingface.co/Qwen/Qwen2.5-0.5B",
    "https://arxiv.org/abs/2503.24290",
    "https://huggingface.co/Open-Reasoner-Zero\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B",
    "https://www.stepfun.com/",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\"",
    "https://arxiv.org/abs/2503.24290},",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py",
    "https://github.com/vllm-project/vllm",
    "https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline",
    "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k",
    "https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>",
    "https://arxiv.org/abs/2503.24290\"><b>Paper",
    "https://github.com/ray-project/ray",
    "https://projectnumina.ai/",
    "https://qwenlm.github.io/blog/qwen2.5-llm/",
    "https://huggingface.co/Qwen/Qwen2.5-7B",
    "https://huggingface.co/Qwen/Qwen2.5-1.5B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B",
    "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\"",
    "https://github.com/OpenRLHF/OpenRLHF",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json",
    "https://huggingface.co/Qwen/Qwen2.5-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py",
    "https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>",
    "https://allenai.org/blog/tulu-3-technical",
    "https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B",
    "https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline",
    "https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B"
  ]
}
```

</details>


---

## Repository 6: ray-project/ray

# GitHub Repository Data

**Repository:** [ray-project/ray](https://github.com/ray-project/ray)

## Basic Information

- **Description:** Ray is an AI compute engine. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.
- **Created:** 2016-10-25T19:38:30+00:00
- **Last Updated:** 2025-06-22T02:23:32+00:00
- **Last Pushed:** 2025-06-22T00:36:15+00:00
- **Default Branch:** master
- **Size:** 566699 KB

## Statistics

- **Stars:** 37,635
- **Forks:** 6,509
- **Watchers:** 37,635
- **Open Issues:** 4,187
- **Total Issues:** 0
- **Pull Requests:** 33,248

## License

- **Type:** Apache License 2.0
- **SPDX ID:** Apache-2.0
- **URL:** [License](https://github.com/ray-project/ray/blob/master/LICENSE)

## Languages

- **Python:** 30,425,225 bytes
- **C++:** 7,808,301 bytes
- **Java:** 1,117,468 bytes
- **TypeScript:** 632,259 bytes
- **Starlark:** 539,261 bytes
- **Cython:** 395,033 bytes
- **Shell:** 167,037 bytes
- **C:** 39,275 bytes
- **Dockerfile:** 27,936 bytes
- **Jinja:** 9,660 bytes
- **CSS:** 2,887 bytes
- **Linker Script:** 2,821 bytes
- **PowerShell:** 2,649 bytes
- **HTML:** 1,831 bytes
- **Roff:** 1,228 bytes
- **JavaScript:** 875 bytes

## Topics

- `ray`
- `distributed`
- `parallel`
- `machine-learning`
- `reinforcement-learning`
- `deep-learning`
- `python`
- `rllib`
- `hyperparameter-search`
- `optimization`
- `data-science`
- `hyperparameter-optimization`
- `serving`
- `deployment`
- `pytorch`
- `tensorflow`
- `llm-serving`
- `large-language-models`
- `llm`
- `llm-inference`

## Top Contributors

1. **ericl** - 1334 contributions
2. **sven1977** - 1153 contributions
3. **edoakes** - 984 contributions
4. **robertnishihara** - 919 contributions
5. **krfricke** - 865 contributions
6. **rkooo567** - 814 contributions
7. **can-anyscale** - 797 contributions
8. **pcmoritz** - 633 contributions
9. **bveeramani** - 628 contributions
10. **richardliaw** - 590 contributions

## File Structure (Sample of 10 files)

Total files: 10,127

- `.bazelrc` (blob)
- `.bazelversion` (blob)
- `.buildkite` (tree)
- `.buildkite/.sunset_civ1_linux` (blob)
- `.buildkite/BUILD.bazel` (blob)
- `.buildkite/README.md` (blob)
- `.buildkite/_forge.rayci.yml` (blob)
- `.buildkite/base.rayci.yml` (blob)
- `.buildkite/bisect` (tree)
- `.buildkite/bisect/_forge.rayci.yml` (blob)

## Recent Issues

- 🟢 **#53994** [Rllib] Bug in TorchMultiDistribution logp prevents policy mapping from being used (open)
- 🟢 **#53993** [ci] Upgrade nightly test to run against KubeRay 1.4 (open)
- 🟢 **#53992** Fix autoscaler recovery docker config to use node-specific settings (open)
- 🟢 **#53991** [Serve] Make replica scheduler backoff configurable #52871 (open)
- 🟢 **#53990** Release test many_nodes_actor_test_on_v2.aws failed (open)

## Recent Pull Requests

- 🟢 **#53993** [ci] Upgrade nightly test to run against KubeRay 1.4 (open)
- 🟢 **#53992** Fix autoscaler recovery docker config to use node-specific settings (open)
- 🟢 **#53991** [Serve] Make replica scheduler backoff configurable #52871 (open)
- 🟢 **#53988** [Doc] Update Istio service mesh graph (open)
- 🔴 **#53986** [core] Fix ActorClass.remote return typing and expose Actor class methods to static analysis (closed)

## Recent Commits

- **efbc9967** [core] Fix ActorClass.remote return typing and expose Actor class methods to static analysis (#53986) - William Lin (2025-06-21T23:15:10+00:00)
- **b1f38412** [core] Use core worker client pool in GCS (#53654) - Dhyey Shah (2025-06-21T05:34:19+00:00)
- **5c4f8ad4** [core] Revert container tests to medium size instance (#53966) - Dhyey Shah (2025-06-21T00:36:18+00:00)
- **d1c665ee** [Core] Fix ray import error when both ROCR_VISIBLE_DEVICES and HIP_VISIBLE_DEVICES are set (#53757) - niu_he (2025-06-20T09:15:45+00:00)
- **2fd66caa** [core] Making NodeManager use ILocalTaskManager instead  of TaskManager. (#53961) - Ibrahim Rabbani (2025-06-20T09:12:24+00:00)
- **dd1e3cb4** defer loading csat so gtag loads first (#53968) - Vignesh Hirudayakanth (2025-06-20T05:25:36+00:00)
- **c139c8e7** fix ga4 events (#53967) - Vignesh Hirudayakanth (2025-06-20T02:31:45+00:00)
- **22f619da** [train][template] Remove clock emoji which does not always render well (#53965) - Timothy Seah (2025-06-20T01:12:28+00:00)
- **addab254** [core][gpu-objects] Support `ray.get` on the driver process for GPU objects (#53902) - Kai-Hsun Chen (2025-06-19T23:54:29+00:00)
- **3f378ed3** [kuberay] Update helm install command in prometheus doc to set serviceMonitor `release=prometheus` (#53952) - Owen Lin (You-Cheng Lin) (2025-06-19T23:32:21+00:00)

## External Links Found in README

- https://docs.ray.io/en/latest/rllib/index.html
- https://www.meetup.com/Bay-Area-Ray-Meetup/
- https://img.shields.io/twitter/follow/raydistributed.svg?style=social&logo=twitter
- https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview
- https://www.ray.io/join-slack
- http://docs.ray.io/en/master/?badge=master
- https://img.shields.io/badge/Discuss-Ask%20Questions-blue
- https://docs.google.com/drawings/d/1Pl8aCYOsZCo61cmp57c7Sja6HhIygGCvSZLi_AuBuqo/edit
- https://docs.ray.io/en/latest/tune/index.html
- https://docs.ray.io/en/latest/ray-air/getting-started.html
- https://twitter.com/raydistributed
- http://docs.ray.io/en/latest/index.html
- https://docs.ray.io/en/latest/train/train.html
- https://docs.ray.io/en/latest/data/dataset.html
- https://arxiv.org/abs/1807.05118
- https://www.usenix.org/system/files/nsdi21-wang.pdf
- https://docs.ray.io/en/latest/ray-overview/ray-libraries.html
- https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview
- https://arxiv.org/abs/1712.09381
- https://docs.ray.io/en/latest/ray-observability/ray-distributed-debugger.html>`__.

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 71932349,
  "name": "ray",
  "full_name": "ray-project/ray",
  "description": "Ray is an AI compute engine. Ray consists of a core distributed runtime and a set of AI Libraries for accelerating ML workloads.",
  "html_url": "https://github.com/ray-project/ray",
  "clone_url": "https://github.com/ray-project/ray.git",
  "ssh_url": "git@github.com:ray-project/ray.git",
  "homepage": "https://ray.io",
  "topics": [
    "ray",
    "distributed",
    "parallel",
    "machine-learning",
    "reinforcement-learning",
    "deep-learning",
    "python",
    "rllib",
    "hyperparameter-search",
    "optimization",
    "data-science",
    "hyperparameter-optimization",
    "serving",
    "deployment",
    "pytorch",
    "tensorflow",
    "llm-serving",
    "large-language-models",
    "llm",
    "llm-inference"
  ],
  "default_branch": "master",
  "created_at": "2016-10-25T19:38:30+00:00",
  "updated_at": "2025-06-22T02:23:32+00:00",
  "pushed_at": "2025-06-22T00:36:15+00:00",
  "size_kb": 566699,
  "watchers_count": 37635,
  "stargazers_count": 37635,
  "forks_count": 6509,
  "open_issues_count": 4187,
  "license": {
    "key": "apache-2.0",
    "name": "Apache License 2.0",
    "spdx_id": "Apache-2.0",
    "url": "https://github.com/ray-project/ray/blob/master/LICENSE"
  },
  "languages": {
    "Python": 30425225,
    "C++": 7808301,
    "Java": 1117468,
    "TypeScript": 632259,
    "Starlark": 539261,
    "Cython": 395033,
    "Shell": 167037,
    "C": 39275,
    "Dockerfile": 27936,
    "Jinja": 9660,
    "CSS": 2887,
    "Linker Script": 2821,
    "PowerShell": 2649,
    "HTML": 1831,
    "Roff": 1228,
    "JavaScript": 875
  },
  "top_contributors": [
    {
      "login": "ericl",
      "contributions": 1334
    },
    {
      "login": "sven1977",
      "contributions": 1153
    },
    {
      "login": "edoakes",
      "contributions": 984
    },
    {
      "login": "robertnishihara",
      "contributions": 919
    },
    {
      "login": "krfricke",
      "contributions": 865
    },
    {
      "login": "rkooo567",
      "contributions": 814
    },
    {
      "login": "can-anyscale",
      "contributions": 797
    },
    {
      "login": "pcmoritz",
      "contributions": 633
    },
    {
      "login": "bveeramani",
      "contributions": 628
    },
    {
      "login": "richardliaw",
      "contributions": 590
    },
    {
      "login": "jjyao",
      "contributions": 547
    },
    {
      "login": "aslonnie",
      "contributions": 539
    },
    {
      "login": "simon-mo",
      "contributions": 509
    },
    {
      "login": "stephanie-wang",
      "contributions": 465
    },
    {
      "login": "amogkam",
      "contributions": 450
    },
    {
      "login": "fishbone",
      "contributions": 422
    },
    {
      "login": "architkulkarni",
      "contributions": 422
    },
    {
      "login": "justinvyu",
      "contributions": 317
    },
    {
      "login": "ijrsvt",
      "contributions": 307
    },
    {
      "login": "rickyyx",
      "contributions": 304
    }
  ],
  "file_tree_count": 10127,
  "file_tree_sample": [
    {
      "path": ".bazelrc",
      "type": "blob"
    },
    {
      "path": ".bazelversion",
      "type": "blob"
    },
    {
      "path": ".buildkite",
      "type": "tree"
    },
    {
      "path": ".buildkite/.sunset_civ1_linux",
      "type": "blob"
    },
    {
      "path": ".buildkite/BUILD.bazel",
      "type": "blob"
    },
    {
      "path": ".buildkite/README.md",
      "type": "blob"
    },
    {
      "path": ".buildkite/_forge.rayci.yml",
      "type": "blob"
    },
    {
      "path": ".buildkite/base.rayci.yml",
      "type": "blob"
    },
    {
      "path": ".buildkite/bisect",
      "type": "tree"
    },
    {
      "path": ".buildkite/bisect/_forge.rayci.yml",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 33248,
  "recent_issues": [
    {
      "number": 53994,
      "title": "[Rllib] Bug in TorchMultiDistribution logp prevents policy mapping from being used",
      "state": "open"
    },
    {
      "number": 53993,
      "title": "[ci] Upgrade nightly test to run against KubeRay 1.4",
      "state": "open"
    },
    {
      "number": 53992,
      "title": "Fix autoscaler recovery docker config to use node-specific settings",
      "state": "open"
    },
    {
      "number": 53991,
      "title": "[Serve] Make replica scheduler backoff configurable #52871",
      "state": "open"
    },
    {
      "number": 53990,
      "title": "Release test many_nodes_actor_test_on_v2.aws failed",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 53993,
      "title": "[ci] Upgrade nightly test to run against KubeRay 1.4",
      "state": "open"
    },
    {
      "number": 53992,
      "title": "Fix autoscaler recovery docker config to use node-specific settings",
      "state": "open"
    },
    {
      "number": 53991,
      "title": "[Serve] Make replica scheduler backoff configurable #52871",
      "state": "open"
    },
    {
      "number": 53988,
      "title": "[Doc] Update Istio service mesh graph",
      "state": "open"
    },
    {
      "number": 53986,
      "title": "[core] Fix ActorClass.remote return typing and expose Actor class methods to static analysis",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "efbc9967be7963e80d1ded14e49ee6daeac04208",
      "author": "William Lin",
      "date": "2025-06-21T23:15:10+00:00",
      "message": "[core] Fix ActorClass.remote return typing and expose Actor class methods to static analysis (#53986)"
    },
    {
      "sha": "b1f3841272c228a03b2b0a478a76b9a247d491eb",
      "author": "Dhyey Shah",
      "date": "2025-06-21T05:34:19+00:00",
      "message": "[core] Use core worker client pool in GCS (#53654)"
    },
    {
      "sha": "5c4f8ad4c51034b9cb10423ec941571cbf31d882",
      "author": "Dhyey Shah",
      "date": "2025-06-21T00:36:18+00:00",
      "message": "[core] Revert container tests to medium size instance (#53966)"
    },
    {
      "sha": "d1c665ee6bdfbbff334053df281f80f37181b9e3",
      "author": "niu_he",
      "date": "2025-06-20T09:15:45+00:00",
      "message": "[Core] Fix ray import error when both ROCR_VISIBLE_DEVICES and HIP_VISIBLE_DEVICES are set (#53757)"
    },
    {
      "sha": "2fd66caa6a37c23c7524f06fd149b3a2f53e5eda",
      "author": "Ibrahim Rabbani",
      "date": "2025-06-20T09:12:24+00:00",
      "message": "[core] Making NodeManager use ILocalTaskManager instead  of TaskManager. (#53961)"
    },
    {
      "sha": "dd1e3cb42b4607954f8d7e52f2df660f972482e0",
      "author": "Vignesh Hirudayakanth",
      "date": "2025-06-20T05:25:36+00:00",
      "message": "defer loading csat so gtag loads first (#53968)"
    },
    {
      "sha": "c139c8e750a5c9cb0ab9ece5a4b8f1ac3efc325a",
      "author": "Vignesh Hirudayakanth",
      "date": "2025-06-20T02:31:45+00:00",
      "message": "fix ga4 events (#53967)"
    },
    {
      "sha": "22f619da9e616028bb4b4d17d681b5454e0f612e",
      "author": "Timothy Seah",
      "date": "2025-06-20T01:12:28+00:00",
      "message": "[train][template] Remove clock emoji which does not always render well (#53965)"
    },
    {
      "sha": "addab254beed955006310b3c1595236a27928bac",
      "author": "Kai-Hsun Chen",
      "date": "2025-06-19T23:54:29+00:00",
      "message": "[core][gpu-objects] Support `ray.get` on the driver process for GPU objects (#53902)"
    },
    {
      "sha": "3f378ed34f62c7302bc65be731ba6452f14c197d",
      "author": "Owen Lin (You-Cheng Lin)",
      "date": "2025-06-19T23:32:21+00:00",
      "message": "[kuberay] Update helm install command in prometheus doc to set serviceMonitor `release=prometheus` (#53952)"
    },
    {
      "sha": "2be023b52a2c8445f9c24bcab83581c1a8741a37",
      "author": "Ricardo Decal",
      "date": "2025-06-19T22:38:40+00:00",
      "message": "[Docs] Fix async code in serving notebook (#53864)"
    },
    {
      "sha": "5b1f823bd0200f494b5825c9bd8aeaaa096525d1",
      "author": "vickytsang",
      "date": "2025-06-19T22:29:41+00:00",
      "message": "[core][rocm] Allow CUDA_VISIBLE_DEVICS and HIP_VISIBLE_DEVICES (#53531)"
    },
    {
      "sha": "75ae5b1c1a0a6b054e4f23d83706f9cefa2bb93a",
      "author": "Timothy Seah",
      "date": "2025-06-19T20:26:21+00:00",
      "message": "[train][template] Pip install with python block instead (#53928)"
    },
    {
      "sha": "1d54fc7d9ced7b36ad727007513b3de8a0bc08a7",
      "author": "Balaji Veeramani",
      "date": "2025-06-19T19:52:57+00:00",
      "message": "[Data] Refactor `Planner` to avoid storing plan-specific state (#53955)"
    },
    {
      "sha": "023d497b38146a57a0cf6ddd919dcabf4c19a649",
      "author": "Qiaolin Yu",
      "date": "2025-06-19T19:33:04+00:00",
      "message": "[core] Avoid unnecessary deserialization/serialization of CallerWorkerId (#53939)"
    },
    {
      "sha": "50d860a9092a67d610d056c99a7adefb3c4bd16f",
      "author": "Cindy Zhang",
      "date": "2025-06-19T18:51:55+00:00",
      "message": "[serve] add ability to track child requests (#53941)"
    },
    {
      "sha": "840fb8aae94e233c938226ef22e7b529bb5db0f8",
      "author": "Kai-Hsun Chen",
      "date": "2025-06-19T17:55:47+00:00",
      "message": "[Doc][KubeRay] Add a doc for scheduler plugins (#53846)"
    },
    {
      "sha": "1b8302b40c4d8161586ccb200bebed13b91e3842",
      "author": "Cuong Nguyen",
      "date": "2025-06-19T17:55:19+00:00",
      "message": "[core][telemetry/08] record counter metric e2e (#53449)"
    },
    {
      "sha": "9e377904330de7b74377ce2d00cb8993ef4cca33",
      "author": "goutamvenkat-anyscale",
      "date": "2025-06-19T17:40:07+00:00",
      "message": "[HashShuffle] - Add warnings for when there are insufficient resources for Aggregators (#53705)"
    },
    {
      "sha": "218cb016e0adc1fc033311525e4c7bc3d56551e9",
      "author": "goutamvenkat-anyscale",
      "date": "2025-06-19T17:39:09+00:00",
      "message": "[Data] Join release tests (#53903)"
    }
  ],
  "readme_text": ".. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/ray_header_logo.png\n\n.. image:: https://readthedocs.org/projects/ray/badge/?version=master\n    :target: http://docs.ray.io/en/master/?badge=master\n\n.. image:: https://img.shields.io/badge/Ray-Join%20Slack-blue\n    :target: https://www.ray.io/join-slack\n\n.. image:: https://img.shields.io/badge/Discuss-Ask%20Questions-blue\n    :target: https://discuss.ray.io/\n\n.. image:: https://img.shields.io/twitter/follow/raydistributed.svg?style=social&logo=twitter\n    :target: https://twitter.com/raydistributed\n\n.. image:: https://img.shields.io/badge/Get_started_for_free-3C8AE9?logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8%2F9hAAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAEKADAAQAAAABAAAAEAAAAAA0VXHyAAABKElEQVQ4Ea2TvWoCQRRGnWCVWChIIlikC9hpJdikSbGgaONbpAoY8gKBdAGfwkfwKQypLQ1sEGyMYhN1Pd%2B6A8PqwBZeOHt%2FvsvMnd3ZXBRFPQjBZ9K6OY8ZxF%2B0IYw9PW3qz8aY6lk92bZ%2BVqSI3oC9T7%2FyCVnrF1ngj93us%2B540sf5BrCDfw9b6jJ5lx%2FyjtGKBBXc3cnqx0INN4ImbI%2Bl%2BPnI8zWfFEr4chLLrWHCp9OO9j19Kbc91HX0zzzBO8EbLK2Iv4ZvNO3is3h6jb%2BCwO0iL8AaWqB7ILPTxq3kDypqvBuYuwswqo6wgYJbT8XxBPZ8KS1TepkFdC79TAHHce%2F7LbVioi3wEfTpmeKtPRGEeoldSP%2FOeoEftpP4BRbgXrYZefsAI%2BP9JU7ImyEAAAAASUVORK5CYII%3D\n   :target: https://www.anyscale.com/ray-on-anyscale?utm_source=github&utm_medium=ray_readme&utm_campaign=get_started_badge\n\nRay is a unified framework for scaling AI and Python applications. Ray consists of a core distributed runtime and a set of AI libraries for simplifying ML compute:\n\n.. image:: https://github.com/ray-project/ray/raw/master/doc/source/images/what-is-ray-padded.svg\n\n..\n  https://docs.google.com/drawings/d/1Pl8aCYOsZCo61cmp57c7Sja6HhIygGCvSZLi_AuBuqo/edit\n\nLearn more about `Ray AI Libraries`_:\n\n- `Data`_: Scalable Datasets for ML\n- `Train`_: Distributed Training\n- `Tune`_: Scalable Hyperparameter Tuning\n- `RLlib`_: Scalable Reinforcement Learning\n- `Serve`_: Scalable and Programmable Serving\n\nOr more about `Ray Core`_ and its key abstractions:\n\n- `Tasks`_: Stateless functions executed in the cluster.\n- `Actors`_: Stateful worker processes created in the cluster.\n- `Objects`_: Immutable values accessible across the cluster.\n\nLearn more about Monitoring and Debugging:\n\n- Monitor Ray apps and clusters with the `Ray Dashboard <https://docs.ray.io/en/latest/ray-core/ray-dashboard.html>`__.\n- Debug Ray apps with the `Ray Distributed Debugger <https://docs.ray.io/en/latest/ray-observability/ray-distributed-debugger.html>`__.\n\nRay runs on any machine, cluster, cloud provider, and Kubernetes, and features a growing\n`ecosystem of community integrations`_.\n\nInstall Ray with: ``pip install ray``. For nightly wheels, see the\n`Installation page <https://docs.ray.io/en/latest/ray-overview/installation.html>`__.\n\n.. _`Serve`: https://docs.ray.io/en/latest/serve/index.html\n.. _`Data`: https://docs.ray.io/en/latest/data/dataset.html\n.. _`Workflow`: https://docs.ray.io/en/latest/workflows/concepts.html\n.. _`Train`: https://docs.ray.io/en/latest/train/train.html\n.. _`Tune`: https://docs.ray.io/en/latest/tune/index.html\n.. _`RLlib`: https://docs.ray.io/en/latest/rllib/index.html\n.. _`ecosystem of community integrations`: https://docs.ray.io/en/latest/ray-overview/ray-libraries.html\n\n\nWhy Ray?\n--------\n\nToday's ML workloads are increasingly compute-intensive. As convenient as they are, single-node development environments such as your laptop cannot scale to meet these demands.\n\nRay is a unified way to scale Python and AI applications from a laptop to a cluster.\n\nWith Ray, you can seamlessly scale the same code from a laptop to a cluster. Ray is designed to be general-purpose, meaning that it can performantly run any kind of workload. If your application is written in Python, you can scale it with Ray, no other infrastructure required.\n\nMore Information\n----------------\n\n- `Documentation`_\n- `Ray Architecture whitepaper`_\n- `Exoshuffle: large-scale data shuffle in Ray`_\n- `Ownership: a distributed futures system for fine-grained tasks`_\n- `RLlib paper`_\n- `Tune paper`_\n\n*Older documents:*\n\n- `Ray paper`_\n- `Ray HotOS paper`_\n- `Ray Architecture v1 whitepaper`_\n\n.. _`Ray AI Libraries`: https://docs.ray.io/en/latest/ray-air/getting-started.html\n.. _`Ray Core`: https://docs.ray.io/en/latest/ray-core/walkthrough.html\n.. _`Tasks`: https://docs.ray.io/en/latest/ray-core/tasks.html\n.. _`Actors`: https://docs.ray.io/en/latest/ray-core/actors.html\n.. _`Objects`: https://docs.ray.io/en/latest/ray-core/objects.html\n.. _`Documentation`: http://docs.ray.io/en/latest/index.html\n.. _`Ray Architecture v1 whitepaper`: https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview\n.. _`Ray Architecture whitepaper`: https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview\n.. _`Exoshuffle: large-scale data shuffle in Ray`: https://arxiv.org/abs/2203.05072\n.. _`Ownership: a distributed futures system for fine-grained tasks`: https://www.usenix.org/system/files/nsdi21-wang.pdf\n.. _`Ray paper`: https://arxiv.org/abs/1712.05889\n.. _`Ray HotOS paper`: https://arxiv.org/abs/1703.03924\n.. _`RLlib paper`: https://arxiv.org/abs/1712.09381\n.. _`Tune paper`: https://arxiv.org/abs/1807.05118\n\nGetting Involved\n----------------\n\n.. list-table::\n   :widths: 25 50 25 25\n   :header-rows: 1\n\n   * - Platform\n     - Purpose\n     - Estimated Response Time\n     - Support Level\n   * - `Discourse Forum`_\n     - For discussions about development and questions about usage.\n     - < 1 day\n     - Community\n   * - `GitHub Issues`_\n     - For reporting bugs and filing feature requests.\n     - < 2 days\n     - Ray OSS Team\n   * - `Slack`_\n     - For collaborating with other Ray users.\n     - < 2 days\n     - Community\n   * - `StackOverflow`_\n     - For asking questions about how to use Ray.\n     - 3-5 days\n     - Community\n   * - `Meetup Group`_\n     - For learning about Ray projects and best practices.\n     - Monthly\n     - Ray DevRel\n   * - `Twitter`_\n     - For staying up-to-date on new features.\n     - Daily\n     - Ray DevRel\n\n.. _`Discourse Forum`: https://discuss.ray.io/\n.. _`GitHub Issues`: https://github.com/ray-project/ray/issues\n.. _`StackOverflow`: https://stackoverflow.com/questions/tagged/ray\n.. _`Meetup Group`: https://www.meetup.com/Bay-Area-Ray-Meetup/\n.. _`Twitter`: https://twitter.com/raydistributed\n.. _`Slack`: https://www.ray.io/join-slack?utm_source=github&utm_medium=ray_readme&utm_campaign=getting_involved\n",
  "external_links_in_readme": [
    "https://docs.ray.io/en/latest/rllib/index.html",
    "https://www.meetup.com/Bay-Area-Ray-Meetup/",
    "https://img.shields.io/twitter/follow/raydistributed.svg?style=social&logo=twitter",
    "https://docs.google.com/document/d/1tBw9A4j62ruI5omIJbMxly-la5w4q_TjyJgJL_jN2fI/preview",
    "https://www.ray.io/join-slack",
    "http://docs.ray.io/en/master/?badge=master",
    "https://img.shields.io/badge/Discuss-Ask%20Questions-blue",
    "https://docs.google.com/drawings/d/1Pl8aCYOsZCo61cmp57c7Sja6HhIygGCvSZLi_AuBuqo/edit",
    "https://docs.ray.io/en/latest/tune/index.html",
    "https://docs.ray.io/en/latest/ray-air/getting-started.html",
    "https://twitter.com/raydistributed",
    "http://docs.ray.io/en/latest/index.html",
    "https://docs.ray.io/en/latest/train/train.html",
    "https://docs.ray.io/en/latest/data/dataset.html",
    "https://arxiv.org/abs/1807.05118",
    "https://www.usenix.org/system/files/nsdi21-wang.pdf",
    "https://docs.ray.io/en/latest/ray-overview/ray-libraries.html",
    "https://docs.google.com/document/d/1lAy0Owi-vPz2jEqBSaHNQcy2IBSDEHyXNOQZlGuj93c/preview",
    "https://arxiv.org/abs/1712.09381",
    "https://docs.ray.io/en/latest/ray-observability/ray-distributed-debugger.html>`__.",
    "https://arxiv.org/abs/1703.03924",
    "https://github.com/ray-project/ray/raw/master/doc/source/images/what-is-ray-padded.svg",
    "https://www.ray.io/join-slack?utm_source=github&utm_medium=ray_readme&utm_campaign=getting_involved",
    "https://docs.ray.io/en/latest/ray-core/ray-dashboard.html>`__.",
    "https://docs.ray.io/en/latest/serve/index.html",
    "https://img.shields.io/badge/Get_started_for_free-3C8AE9?logo=data%3Aimage%2Fpng%3Bbase64%2CiVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8%2F9hAAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAEKADAAQAAAABAAAAEAAAAAA0VXHyAAABKElEQVQ4Ea2TvWoCQRRGnWCVWChIIlikC9hpJdikSbGgaONbpAoY8gKBdAGfwkfwKQypLQ1sEGyMYhN1Pd%2B6A8PqwBZeOHt%2FvsvMnd3ZXBRFPQjBZ9K6OY8ZxF%2B0IYw9PW3qz8aY6lk92bZ%2BVqSI3oC9T7%2FyCVnrF1ngj93us%2B540sf5BrCDfw9b6jJ5lx%2FyjtGKBBXc3cnqx0INN4ImbI%2Bl%2BPnI8zWfFEr4chLLrWHCp9OO9j19Kbc91HX0zzzBO8EbLK2Iv4ZvNO3is3h6jb%2BCwO0iL8AaWqB7ILPTxq3kDypqvBuYuwswqo6wgYJbT8XxBPZ8KS1TepkFdC79TAHHce%2F7LbVioi3wEfTpmeKtPRGEeoldSP%2FOeoEftpP4BRbgXrYZefsAI%2BP9JU7ImyEAAAAASUVORK5CYII%3D",
    "https://docs.ray.io/en/latest/workflows/concepts.html",
    "https://docs.ray.io/en/latest/ray-core/objects.html",
    "https://arxiv.org/abs/2203.05072",
    "https://docs.ray.io/en/latest/ray-overview/installation.html>`__.",
    "https://img.shields.io/badge/Ray-Join%20Slack-blue",
    "https://www.anyscale.com/ray-on-anyscale?utm_source=github&utm_medium=ray_readme&utm_campaign=get_started_badge",
    "https://docs.ray.io/en/latest/ray-core/actors.html",
    "https://github.com/ray-project/ray/issues",
    "https://docs.ray.io/en/latest/ray-core/tasks.html",
    "https://discuss.ray.io/",
    "https://stackoverflow.com/questions/tagged/ray",
    "https://github.com/ray-project/ray/raw/master/doc/source/images/ray_header_logo.png",
    "https://arxiv.org/abs/1712.05889",
    "https://docs.ray.io/en/latest/ray-core/walkthrough.html",
    "https://readthedocs.org/projects/ray/badge/?version=master"
  ]
}
```

</details>


---

## Repository 7: Open-Reasoner-Zero/Open-Reasoner-Zero

# GitHub Repository Data

**Repository:** [Open-Reasoner-Zero/Open-Reasoner-Zero](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero)

## Basic Information

- **Description:** Official Repo for Open-Reasoner-Zero
- **Created:** 2025-02-19T17:28:25+00:00
- **Last Updated:** 2025-06-20T07:02:06+00:00
- **Last Pushed:** 2025-06-02T16:36:00+00:00
- **Default Branch:** main
- **Size:** 16502 KB

## Statistics

- **Stars:** 1,968
- **Forks:** 104
- **Watchers:** 1,968
- **Open Issues:** 19
- **Total Issues:** 0
- **Pull Requests:** 5

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE)

## Languages

- **Python:** 337,249 bytes
- **Dockerfile:** 1,739 bytes

## Top Contributors

1. **REIGN12** - 21 contributions
2. **vwxyzjn** - 5 contributions
3. **YinminZhang** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 71

- `.flake8` (blob)
- `.gitignore` (blob)
- `LICENSE` (blob)
- `ORZ_paper.pdf` (blob)
- `README.md` (blob)
- `data` (tree)
- `data/eval_data` (tree)
- `data/eval_data/aime2024.json` (blob)
- `data/eval_data/gpqa_diamond.json` (blob)
- `data/eval_data/math500.json` (blob)

## Recent Issues

- 🟢 **#73** DeepSpeed does not detect CUDA for some reason: (open)
- 🟢 **#72** In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map="meta"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device("meta")` (open)
- 🟢 **#71** In `orz/ppo/actors.py`: `with torch.device("meta"): AutoModel.from_pretrained(...)` throws (open)
- 🟢 **#70** `llm_engine.model_executor` seems not available on modern vllm (0.8.5) (open)
- 🟢 **#69** On newer vllm should use `llm_engine.vllm_config` and similar (open)

## Recent Pull Requests

- 🟢 **#66** 重构 PPOExpConfig 类，继承自 CommonPPOConfig，简化配置管理 (open)
- 🔴 **#62** Major Updates for Open-Reasoner-Zero (closed)
- 🟢 **#57** Support FSDP and vllm colocate with tp size > 1 (open)
- 🔴 **#32** Local ppo mini (closed)
- 🔴 **#31** Add a mini example (closed)

## Recent Commits

- **3fdd9a07** doc: update ORZ-R1-Distill-Qwen-14B results - REIGN12 (2025-06-02T16:35:57+00:00)
- **a2351927** chores: update qr code - REIGN12 (2025-06-02T16:35:25+00:00)
- **20e5bc83** README: update readme - REIGN12 (2025-04-08T12:58:51+00:00)
- **9c73e259** README: update readme - REIGN12 (2025-04-02T03:41:38+00:00)
- **a27ce5c1** README: update readme - REIGN12 (2025-04-02T03:38:35+00:00)
- **a288753d** README: update readme - REIGN12 (2025-03-31T15:53:39+00:00)
- **a7193f2c** Major Updates for Open-Reasoner-Zero (#62) - Jingcheng Hu (2025-03-31T15:45:50+00:00)
- **e008f6d9** Fix: bug fix for weight sync logic when colocate=False - REIGN12 (2025-03-05T08:08:42+00:00)
- **effe6fb8** Fix: fix typo in report - REIGN12 (2025-03-01T09:03:59+00:00)
- **9464093c** Fix: minor bugfix for dockerfile - REIGN12 (2025-03-01T08:54:56+00:00)

## External Links Found in README

- https://github.com/deepspeedai/DeepSpeed
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json
- https://huggingface.co/Qwen/Qwen2.5-0.5B
- https://arxiv.org/abs/2503.24290
- https://huggingface.co/Open-Reasoner-Zero"
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B
- https://www.stepfun.com/
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
- https://arxiv.org/abs/2503.24290},
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py
- https://github.com/vllm-project/vllm
- https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline
- https://huggingface.co/datasets/open-r1/OpenR1-Math-220k
- https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor"/></a>
- https://arxiv.org/abs/2503.24290"><b>Paper
- https://github.com/ray-project/ray
- https://projectnumina.ai/

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 935587127,
  "name": "Open-Reasoner-Zero",
  "full_name": "Open-Reasoner-Zero/Open-Reasoner-Zero",
  "description": "Official Repo for Open-Reasoner-Zero",
  "html_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero",
  "clone_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "ssh_url": "git@github.com:Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "homepage": "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903",
  "topics": [],
  "default_branch": "main",
  "created_at": "2025-02-19T17:28:25+00:00",
  "updated_at": "2025-06-20T07:02:06+00:00",
  "pushed_at": "2025-06-02T16:36:00+00:00",
  "size_kb": 16502,
  "watchers_count": 1968,
  "stargazers_count": 1968,
  "forks_count": 104,
  "open_issues_count": 19,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE"
  },
  "languages": {
    "Python": 337249,
    "Dockerfile": 1739
  },
  "top_contributors": [
    {
      "login": "REIGN12",
      "contributions": 21
    },
    {
      "login": "vwxyzjn",
      "contributions": 5
    },
    {
      "login": "YinminZhang",
      "contributions": 3
    }
  ],
  "file_tree_count": 71,
  "file_tree_sample": [
    {
      "path": ".flake8",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "ORZ_paper.pdf",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "data",
      "type": "tree"
    },
    {
      "path": "data/eval_data",
      "type": "tree"
    },
    {
      "path": "data/eval_data/aime2024.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/gpqa_diamond.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/math500.json",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 5,
  "recent_issues": [
    {
      "number": 73,
      "title": "DeepSpeed does not detect CUDA for some reason:",
      "state": "open"
    },
    {
      "number": 72,
      "title": "In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map=\"meta\"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device(\"meta\")`",
      "state": "open"
    },
    {
      "number": 71,
      "title": "In `orz/ppo/actors.py`: `with torch.device(\"meta\"): AutoModel.from_pretrained(...)` throws",
      "state": "open"
    },
    {
      "number": 70,
      "title": "`llm_engine.model_executor` seems not available on modern vllm (0.8.5)",
      "state": "open"
    },
    {
      "number": 69,
      "title": "On newer vllm should use `llm_engine.vllm_config` and similar",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 66,
      "title": "\u91cd\u6784 PPOExpConfig \u7c7b\uff0c\u7ee7\u627f\u81ea CommonPPOConfig\uff0c\u7b80\u5316\u914d\u7f6e\u7ba1\u7406",
      "state": "open"
    },
    {
      "number": 62,
      "title": "Major Updates for Open-Reasoner-Zero",
      "state": "closed"
    },
    {
      "number": 57,
      "title": "Support FSDP and vllm colocate with tp size > 1",
      "state": "open"
    },
    {
      "number": 32,
      "title": "Local ppo mini",
      "state": "closed"
    },
    {
      "number": 31,
      "title": "Add a mini example",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "3fdd9a07b4fb01e06005e6e74fc56690cde8a341",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:57+00:00",
      "message": "doc: update ORZ-R1-Distill-Qwen-14B results"
    },
    {
      "sha": "a23519277efdd71a063e1f2913d5c496a5f96180",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:25+00:00",
      "message": "chores: update qr code"
    },
    {
      "sha": "20e5bc83ae75695fded4e1a59a21e487063e24d9",
      "author": "REIGN12",
      "date": "2025-04-08T12:58:51+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "9c73e2590d23a7245a8d6341337952828110c631",
      "author": "REIGN12",
      "date": "2025-04-02T03:41:38+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a27ce5c1c5204d267c821542af4bdb47c70aa40a",
      "author": "REIGN12",
      "date": "2025-04-02T03:38:35+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a288753de916c0f3ba932fb0d1c218a49b8a403b",
      "author": "REIGN12",
      "date": "2025-03-31T15:53:39+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a7193f2c14aa90d055f9c161bf4566ec41ced30c",
      "author": "Jingcheng Hu",
      "date": "2025-03-31T15:45:50+00:00",
      "message": "Major Updates for Open-Reasoner-Zero (#62)"
    },
    {
      "sha": "e008f6d95f0b9a0e992f6b8bac912515b50a4634",
      "author": "REIGN12",
      "date": "2025-03-05T08:08:42+00:00",
      "message": "Fix: bug fix for weight sync logic when colocate=False"
    },
    {
      "sha": "effe6fb81570ccfde7d28d8d335c83127e95263f",
      "author": "REIGN12",
      "date": "2025-03-01T09:03:59+00:00",
      "message": "Fix: fix typo in report"
    },
    {
      "sha": "9464093c67c32a5fca69556b603f915e55fc00a5",
      "author": "REIGN12",
      "date": "2025-03-01T08:54:56+00:00",
      "message": "Fix: minor bugfix for dockerfile"
    },
    {
      "sha": "f594903dc0e953a8d0429ae0ecf339efcbe456f7",
      "author": "Jingcheng Hu",
      "date": "2025-02-25T15:03:35+00:00",
      "message": "Merge pull request #31 from vwxyzjn/ppo-mini"
    },
    {
      "sha": "ce912e108ca03d5ded42e151e3c857d44fc3f277",
      "author": "Costa Huang",
      "date": "2025-02-24T20:33:48+00:00",
      "message": "quick change"
    },
    {
      "sha": "34d8abc52ecba9c1c620350eb70110d05ea6b676",
      "author": "Costa Huang",
      "date": "2025-02-24T20:32:51+00:00",
      "message": "set eval to false"
    },
    {
      "sha": "55904eae7c35932e8ee2efb717c788ed3cd9b75e",
      "author": "Costa Huang",
      "date": "2025-02-24T20:31:44+00:00",
      "message": "quick change"
    },
    {
      "sha": "dbc40d8042c4981138b58773ca4936570e2339be",
      "author": "Costa Huang",
      "date": "2025-02-24T20:28:34+00:00",
      "message": "Add single GPU exampel"
    },
    {
      "sha": "ec2ce7e528f660f3b05c19369cdcff9d89cbb6b0",
      "author": "Costa Huang",
      "date": "2025-02-24T20:06:00+00:00",
      "message": "Add a mini example"
    },
    {
      "sha": "7b5dec17a8dda164e8a4aaba33393afc3e0e95d9",
      "author": "Jingcheng Hu",
      "date": "2025-02-24T09:18:55+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "3ccec9eec8717a5de0575918237bc81b6ba3e3a2",
      "author": "REIGN12",
      "date": "2025-02-24T07:24:11+00:00",
      "message": "Merge branch 'main' of https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
    },
    {
      "sha": "df77747d28c8580db768732f8da34f913e0411ca",
      "author": "REIGN12",
      "date": "2025-02-24T07:23:34+00:00",
      "message": "Fix: bug fix for readme and 32b exp"
    },
    {
      "sha": "ec6b01ef61f6b66a30bff0ea763b37fe4e9d00d1",
      "author": "hanqer",
      "date": "2025-02-24T05:52:43+00:00",
      "message": "fix: fix the package dependencies of pip and apt."
    }
  ],
  "readme_text": "<div align=\"center\">\n\n# Open Reasoner Zero\n\n<img src=\"figure/logo.jpg\" width=\"300\"/>\n\n<div>\n\nAn Open Source Approach to Scaling Up Reinforcement Learning on the Base Model\n</div>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n    <a href=\"https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\" style=\"margin: 2px;\"><img alt=\"Code\" src=\"https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/></a>\n  \n  <a href=\"https://huggingface.co/Open-Reasoner-Zero\" target=\"_blank\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>\n\n  <a href=\"https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\" target=\"_blank\">\n  <img alt=\"Notion Page\"\n    src=\"https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>\n\n  <br>\n  <a href=\"https://arxiv.org/abs/2503.24290\"><b>Paper Arxiv Link </b>\ud83d\udc41\ufe0f</a>\n</div>\n\n<div>\n<br>\n\n</div>\n\n## Overview \ud83c\udf0a\nWe introduce **Open-Reasoner-Zero**, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility.\nUsing the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency\u2014requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline.\n\nTo enable broader participation in this pivotal moment we witnessed and accelerate research towards artificial general intelligence (AGI), \nwe release our source code, parameter settings, training data, and model weights.\nPlease refer to our [paper](https://arxiv.org/abs/2503.24290) for more insights across various model sizes. \n\n**Let the Reasoner-Zero tide rise!**\n\n\n## Main Results \ud83c\udfc6\n\n![](figure/teaser.png)\n\n*Figure 1 | Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\}. Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\} on benchmarks (averaged on 16 responses) during training. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, Open-Reasoner-Zero-32B achieves superior performance on AIME2024, MATH500, and GPQA Diamond benchmark-requiring only a tenth of the training steps.*\n\n![](figure/train_curve.png)\n*Figure 2 | Train-time Scale up on Train Reward and Response Length of Open-Reasoner-Zero (ORZ) - \\{0.5B, 1.5B, 7B, 32B\\}. Train Reward and Response Length increase steadily, demonstrating consistent scalability across model sizes. Interestingly, the ORZ-32B Response Length exhibits fluctuations without negatively impacting training stability, highlighting the robustness of our minimalist recipe.*\n\n## Releases \ud83d\udce6\n\n<strong>[2025/06/03]</strong>\nWe release [ORZ-R1-Distill-Qwen-14B](https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B), obtained by applying ORZ recipe to reasoning-enhanced models like DeepSeek-R1-Distill-Qwen-14B. This ORZ-R1-Distill-Qwen-14B achieves strong results on reasoning benchmarks, even surpassing the larger DeepSeek-R1-Distill-Qwen-32B model.\n\n| Model                        | AIME 2024 | AIME 2025 | MATH500 | GPQA Dia. |\n| ---------------------------- | --------- | --------- | ------- | --------- |\n| DeepSeek-R1-Distill-Qwen-14B | 69.7      | 49.1      | 93.9    | 59.1      |\n| DeepSeek-R1-Distill-Qwen-32B | 72.6      | 60.0      | 94.3    | **62.1**     |\n| **ORZ-R1-Distill-Qwen-14B**  | **75.2**  | **60.0**  | **95.6** | 60.4  |\n\n\n<strong>[2025/03/31]</strong>\nWe announce a major milestone for `Open-Reasoner-Zero`:\n\n- \ud83c\udf0a [Updated Paper](https://arxiv.org/abs/2503.24290) with new results.\n- \ud83d\udd2d [Easy-to-use Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground):\n  - [ORZ-1.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py) and [ORZ-0.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py) (main results in Figure 2). \n  - [Minimal resource training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py): ORZ-0.5B can be run on a single A800/H800 gpu!\n- \ud83e\udd29 [Updated Curated Datasets](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data): \n  - 129k data in total:\n    - [original 57k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json).\n    - [extended 72k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json).\n  - [13k hard data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json) mined from the above 129k data. \n    - used in the \"annealing\" stage of ORZ-32B training: **AIME2024 from ~41% to ~48%**!\n- \ud83e\udd17 More HF Models: \n  - Updated HF Models: [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B).\n  - Released HF Models: [`Open-Reasoner-Zero-1.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B) and [`Open-Reasoner-Zero-0.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B).\n- \ud83d\ude80 Full Suite of Critic Models for in-depth research: `Open-Reasoner-Zero-Critic-`{[0.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B), [1.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B), [7B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B),  [32B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B)}.\n\n<strong>[2025/02/18]</strong>\nWe release `Open-Reasoner-Zero`. \n\nAs part of this release, we open-source:\n- \ud83c\udf0a [Paper(WIP)](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf) on our comprehensive analysis and insights in Reasoner-Zero training\n- \ud83e\udd17 HF Model [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B)\n- \ud83c\udf81 [`Our curated 57k training data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data)\n- \ud83d\udcc4 [Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground) to enjoy your own Reasoner-Zero journey!\n\n## Key Features in Codebase \ud83d\udd11\n\n- Adopt single controller trainer design, flexible and researcher-friendly.\n- Colocate training and generation in the same GPUs to maximize GPU utilization.\n\n## Getting Started \ud83d\ude80\n### Data\n\nWe release all of curated high-quality training data in the [`data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data) folder:\n* curated 129k data:\n  * [original 57k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json), collected from various sources, including AIME (up to 2023), MATH, Numina-Math collection and Tulu3 MATH.\n  * [extended 72k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json), mainly cleaned from OpenR1-Math-220k.\n* [hard 13k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json), mined from the first stage of ORZ-32B training.\n\nThe details for how to collect data are described in our [paper](https://arxiv.org/abs/2503.24290).\n\n### Installation & Training Scripts\nWe release our [Dockerfile](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile) in [docker](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker) folder to facilitate the reproducibility of our training.\n\nTo install the package, run:\n```bash\npip install -e .\n```\n\n#### Start ORZ-32B PPO Training\nHere are the starting commands in 16 nodes. \n\nFirst on master node, run:\n```bash\nray start --head\n# you will see logging like:\n# Next steps\n#  To add another node to this Ray cluster, run\n#    ray start --address='<master-node-ip>:<master-node-port>'\n```\n\nthen on all other nodes, run:\n```bash\nray start --address='<master-node-ip>:<master-node-port>' # <master-node-ip> and <master-node-port> are from above loggings!\n```\n\nfinally on master node, just run:\n```bash\npython -m playground.orz_32b_ppo\n```\nYour training log will be shown in the master node terminal.\n\n------\n\n#### Start ORZ-0.5B PPO Training\nYou can start the ORZ-0.5B PPO training in single A800/H800 node:\n```bash\npython -m playground.orz_0p5b_ppo\n```\n\nYou can even run in **a single A800/H800 gpu**: \n```bash\npython -m playground.orz_0p5b_ppo_1gpu\n```\n\nnote: since we are not in multi-node setting, no `ray start` like logics are needed.\n\n------\n\n#### Start ORZ-7B PPO Training\n\nMulti-node Training on 4 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n\n# then on master node, run:\npython -m playground.orz_7b_ppo\n```\n\nYour training log will be shown in the master node terminal.\n\n-----\n\n#### Start ORZ-1.5B PPO Training\n\nMulti-node Training on 2 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n# then on master node, run:\npython -m playground.orz_1p5b_ppo\n```\n\n----\n\n#### Debug Settings\nIn the code, we leave an environment variable `DEBUG_MODE` to run in debug setting for researcher to iterate. (Thought for now, we recommend using `python -m playground.orz_0p5b_ppo_1gpu` for debugging.)\n\nThe debug running command examples:\n```bash\n# NOTE: just for debug, not final setting!\n\n## Debug command in a single GPU with `EleutherAI/pythia-14m`\nDEBUG_MODE=True python -m playground.orz_14m_ppo_mini\n## Debug command in a single node (8 GPUs) with `Qwen/Qwen2.5-7B`\nDEBUG_MODE=True python -m playground.orz_7b_ppo\n```\n\n### How to Use the Model\n#### Policy Model\nPolicy models can be used in the same way as any chat model in transformers and vllm, since we have put the chat template jinja in the tokenizer.\n\n#### Critic Model\nCritic models can be loaded the same way like in the [training code](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738). \n\n\n## Acknowledgements \ud83d\udc96 \n\n- This work was supported by computing resources and valuable feedback provided by [StepFun](https://www.stepfun.com/) and Tsinghua University.\n- Our training framework is built on [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF), [vllm](https://github.com/vllm-project/vllm), [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) and [ray](https://github.com/ray-project/ray).\n- Our model is based on [Qwen2.5 Series](https://qwenlm.github.io/blog/qwen2.5-llm/) of **base models**, including [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B), [Qwen2.5-1.5B](https://huggingface.co/Qwen/Qwen2.5-1.5B), [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B) and [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B).\n- We thank [Project Numina](https://projectnumina.ai/), [Tulu3](https://allenai.org/blog/tulu-3-technical) and [OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k) for their collected open sourced data.\n\n## Advertisement Time \ud83d\udce3\n\nWe are hiring talented researchers and engineers to join our team. If you are interested in our project and would like to contribute to the reasoner scale-up all the way to AGI, please feel free to reach out to us at hanqer@stepfun.com\n\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline)](https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline)\n\n## Community Discussions \ud83c\udf7a\n\nWe have several wechat groups to help discussions and sharing, you can scan the QR code below to join the latest group.\n\n<img src=\"figure/WeChatGroup.png\" width=\"300\" style=\"display: block; margin: 0 auto;\"/>\n\n## Citation\n\n```bibtex\n@misc{hu2025openreasonerzeroopensourceapproach,\n      title={Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model}, \n      author={Jingcheng Hu and Yinmin Zhang and Qi Han and Daxin Jiang and Xiangyu Zhang and Heung-Yeung Shum},\n      year={2025},\n      eprint={2503.24290},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2503.24290}, \n}\n```\n",
  "external_links_in_readme": [
    "https://github.com/deepspeedai/DeepSpeed",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json",
    "https://huggingface.co/Qwen/Qwen2.5-0.5B",
    "https://arxiv.org/abs/2503.24290",
    "https://huggingface.co/Open-Reasoner-Zero\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B",
    "https://www.stepfun.com/",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\"",
    "https://arxiv.org/abs/2503.24290},",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py",
    "https://github.com/vllm-project/vllm",
    "https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline",
    "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k",
    "https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>",
    "https://arxiv.org/abs/2503.24290\"><b>Paper",
    "https://github.com/ray-project/ray",
    "https://projectnumina.ai/",
    "https://qwenlm.github.io/blog/qwen2.5-llm/",
    "https://huggingface.co/Qwen/Qwen2.5-7B",
    "https://huggingface.co/Qwen/Qwen2.5-1.5B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B",
    "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\"",
    "https://github.com/OpenRLHF/OpenRLHF",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json",
    "https://huggingface.co/Qwen/Qwen2.5-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py",
    "https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>",
    "https://allenai.org/blog/tulu-3-technical",
    "https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B",
    "https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline",
    "https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B"
  ]
}
```

</details>


---

## Repository 8: Open-Reasoner-Zero/Open-Reasoner-Zero

# GitHub Repository Data

**Repository:** [Open-Reasoner-Zero/Open-Reasoner-Zero](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero)

## Basic Information

- **Description:** Official Repo for Open-Reasoner-Zero
- **Created:** 2025-02-19T17:28:25+00:00
- **Last Updated:** 2025-06-20T07:02:06+00:00
- **Last Pushed:** 2025-06-02T16:36:00+00:00
- **Default Branch:** main
- **Size:** 16502 KB

## Statistics

- **Stars:** 1,968
- **Forks:** 104
- **Watchers:** 1,968
- **Open Issues:** 19
- **Total Issues:** 0
- **Pull Requests:** 5

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE)

## Languages

- **Python:** 337,249 bytes
- **Dockerfile:** 1,739 bytes

## Top Contributors

1. **REIGN12** - 21 contributions
2. **vwxyzjn** - 5 contributions
3. **YinminZhang** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 71

- `.flake8` (blob)
- `.gitignore` (blob)
- `LICENSE` (blob)
- `ORZ_paper.pdf` (blob)
- `README.md` (blob)
- `data` (tree)
- `data/eval_data` (tree)
- `data/eval_data/aime2024.json` (blob)
- `data/eval_data/gpqa_diamond.json` (blob)
- `data/eval_data/math500.json` (blob)

## Recent Issues

- 🟢 **#73** DeepSpeed does not detect CUDA for some reason: (open)
- 🟢 **#72** In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map="meta"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device("meta")` (open)
- 🟢 **#71** In `orz/ppo/actors.py`: `with torch.device("meta"): AutoModel.from_pretrained(...)` throws (open)
- 🟢 **#70** `llm_engine.model_executor` seems not available on modern vllm (0.8.5) (open)
- 🟢 **#69** On newer vllm should use `llm_engine.vllm_config` and similar (open)

## Recent Pull Requests

- 🟢 **#66** 重构 PPOExpConfig 类，继承自 CommonPPOConfig，简化配置管理 (open)
- 🔴 **#62** Major Updates for Open-Reasoner-Zero (closed)
- 🟢 **#57** Support FSDP and vllm colocate with tp size > 1 (open)
- 🔴 **#32** Local ppo mini (closed)
- 🔴 **#31** Add a mini example (closed)

## Recent Commits

- **3fdd9a07** doc: update ORZ-R1-Distill-Qwen-14B results - REIGN12 (2025-06-02T16:35:57+00:00)
- **a2351927** chores: update qr code - REIGN12 (2025-06-02T16:35:25+00:00)
- **20e5bc83** README: update readme - REIGN12 (2025-04-08T12:58:51+00:00)
- **9c73e259** README: update readme - REIGN12 (2025-04-02T03:41:38+00:00)
- **a27ce5c1** README: update readme - REIGN12 (2025-04-02T03:38:35+00:00)
- **a288753d** README: update readme - REIGN12 (2025-03-31T15:53:39+00:00)
- **a7193f2c** Major Updates for Open-Reasoner-Zero (#62) - Jingcheng Hu (2025-03-31T15:45:50+00:00)
- **e008f6d9** Fix: bug fix for weight sync logic when colocate=False - REIGN12 (2025-03-05T08:08:42+00:00)
- **effe6fb8** Fix: fix typo in report - REIGN12 (2025-03-01T09:03:59+00:00)
- **9464093c** Fix: minor bugfix for dockerfile - REIGN12 (2025-03-01T08:54:56+00:00)

## External Links Found in README

- https://github.com/deepspeedai/DeepSpeed
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json
- https://huggingface.co/Qwen/Qwen2.5-0.5B
- https://arxiv.org/abs/2503.24290
- https://huggingface.co/Open-Reasoner-Zero"
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B
- https://www.stepfun.com/
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
- https://arxiv.org/abs/2503.24290},
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py
- https://github.com/vllm-project/vllm
- https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline
- https://huggingface.co/datasets/open-r1/OpenR1-Math-220k
- https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor"/></a>
- https://arxiv.org/abs/2503.24290"><b>Paper
- https://github.com/ray-project/ray
- https://projectnumina.ai/

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 935587127,
  "name": "Open-Reasoner-Zero",
  "full_name": "Open-Reasoner-Zero/Open-Reasoner-Zero",
  "description": "Official Repo for Open-Reasoner-Zero",
  "html_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero",
  "clone_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "ssh_url": "git@github.com:Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "homepage": "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903",
  "topics": [],
  "default_branch": "main",
  "created_at": "2025-02-19T17:28:25+00:00",
  "updated_at": "2025-06-20T07:02:06+00:00",
  "pushed_at": "2025-06-02T16:36:00+00:00",
  "size_kb": 16502,
  "watchers_count": 1968,
  "stargazers_count": 1968,
  "forks_count": 104,
  "open_issues_count": 19,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE"
  },
  "languages": {
    "Python": 337249,
    "Dockerfile": 1739
  },
  "top_contributors": [
    {
      "login": "REIGN12",
      "contributions": 21
    },
    {
      "login": "vwxyzjn",
      "contributions": 5
    },
    {
      "login": "YinminZhang",
      "contributions": 3
    }
  ],
  "file_tree_count": 71,
  "file_tree_sample": [
    {
      "path": ".flake8",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "ORZ_paper.pdf",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "data",
      "type": "tree"
    },
    {
      "path": "data/eval_data",
      "type": "tree"
    },
    {
      "path": "data/eval_data/aime2024.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/gpqa_diamond.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/math500.json",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 5,
  "recent_issues": [
    {
      "number": 73,
      "title": "DeepSpeed does not detect CUDA for some reason:",
      "state": "open"
    },
    {
      "number": 72,
      "title": "In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map=\"meta\"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device(\"meta\")`",
      "state": "open"
    },
    {
      "number": 71,
      "title": "In `orz/ppo/actors.py`: `with torch.device(\"meta\"): AutoModel.from_pretrained(...)` throws",
      "state": "open"
    },
    {
      "number": 70,
      "title": "`llm_engine.model_executor` seems not available on modern vllm (0.8.5)",
      "state": "open"
    },
    {
      "number": 69,
      "title": "On newer vllm should use `llm_engine.vllm_config` and similar",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 66,
      "title": "\u91cd\u6784 PPOExpConfig \u7c7b\uff0c\u7ee7\u627f\u81ea CommonPPOConfig\uff0c\u7b80\u5316\u914d\u7f6e\u7ba1\u7406",
      "state": "open"
    },
    {
      "number": 62,
      "title": "Major Updates for Open-Reasoner-Zero",
      "state": "closed"
    },
    {
      "number": 57,
      "title": "Support FSDP and vllm colocate with tp size > 1",
      "state": "open"
    },
    {
      "number": 32,
      "title": "Local ppo mini",
      "state": "closed"
    },
    {
      "number": 31,
      "title": "Add a mini example",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "3fdd9a07b4fb01e06005e6e74fc56690cde8a341",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:57+00:00",
      "message": "doc: update ORZ-R1-Distill-Qwen-14B results"
    },
    {
      "sha": "a23519277efdd71a063e1f2913d5c496a5f96180",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:25+00:00",
      "message": "chores: update qr code"
    },
    {
      "sha": "20e5bc83ae75695fded4e1a59a21e487063e24d9",
      "author": "REIGN12",
      "date": "2025-04-08T12:58:51+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "9c73e2590d23a7245a8d6341337952828110c631",
      "author": "REIGN12",
      "date": "2025-04-02T03:41:38+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a27ce5c1c5204d267c821542af4bdb47c70aa40a",
      "author": "REIGN12",
      "date": "2025-04-02T03:38:35+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a288753de916c0f3ba932fb0d1c218a49b8a403b",
      "author": "REIGN12",
      "date": "2025-03-31T15:53:39+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a7193f2c14aa90d055f9c161bf4566ec41ced30c",
      "author": "Jingcheng Hu",
      "date": "2025-03-31T15:45:50+00:00",
      "message": "Major Updates for Open-Reasoner-Zero (#62)"
    },
    {
      "sha": "e008f6d95f0b9a0e992f6b8bac912515b50a4634",
      "author": "REIGN12",
      "date": "2025-03-05T08:08:42+00:00",
      "message": "Fix: bug fix for weight sync logic when colocate=False"
    },
    {
      "sha": "effe6fb81570ccfde7d28d8d335c83127e95263f",
      "author": "REIGN12",
      "date": "2025-03-01T09:03:59+00:00",
      "message": "Fix: fix typo in report"
    },
    {
      "sha": "9464093c67c32a5fca69556b603f915e55fc00a5",
      "author": "REIGN12",
      "date": "2025-03-01T08:54:56+00:00",
      "message": "Fix: minor bugfix for dockerfile"
    },
    {
      "sha": "f594903dc0e953a8d0429ae0ecf339efcbe456f7",
      "author": "Jingcheng Hu",
      "date": "2025-02-25T15:03:35+00:00",
      "message": "Merge pull request #31 from vwxyzjn/ppo-mini"
    },
    {
      "sha": "ce912e108ca03d5ded42e151e3c857d44fc3f277",
      "author": "Costa Huang",
      "date": "2025-02-24T20:33:48+00:00",
      "message": "quick change"
    },
    {
      "sha": "34d8abc52ecba9c1c620350eb70110d05ea6b676",
      "author": "Costa Huang",
      "date": "2025-02-24T20:32:51+00:00",
      "message": "set eval to false"
    },
    {
      "sha": "55904eae7c35932e8ee2efb717c788ed3cd9b75e",
      "author": "Costa Huang",
      "date": "2025-02-24T20:31:44+00:00",
      "message": "quick change"
    },
    {
      "sha": "dbc40d8042c4981138b58773ca4936570e2339be",
      "author": "Costa Huang",
      "date": "2025-02-24T20:28:34+00:00",
      "message": "Add single GPU exampel"
    },
    {
      "sha": "ec2ce7e528f660f3b05c19369cdcff9d89cbb6b0",
      "author": "Costa Huang",
      "date": "2025-02-24T20:06:00+00:00",
      "message": "Add a mini example"
    },
    {
      "sha": "7b5dec17a8dda164e8a4aaba33393afc3e0e95d9",
      "author": "Jingcheng Hu",
      "date": "2025-02-24T09:18:55+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "3ccec9eec8717a5de0575918237bc81b6ba3e3a2",
      "author": "REIGN12",
      "date": "2025-02-24T07:24:11+00:00",
      "message": "Merge branch 'main' of https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
    },
    {
      "sha": "df77747d28c8580db768732f8da34f913e0411ca",
      "author": "REIGN12",
      "date": "2025-02-24T07:23:34+00:00",
      "message": "Fix: bug fix for readme and 32b exp"
    },
    {
      "sha": "ec6b01ef61f6b66a30bff0ea763b37fe4e9d00d1",
      "author": "hanqer",
      "date": "2025-02-24T05:52:43+00:00",
      "message": "fix: fix the package dependencies of pip and apt."
    }
  ],
  "readme_text": "<div align=\"center\">\n\n# Open Reasoner Zero\n\n<img src=\"figure/logo.jpg\" width=\"300\"/>\n\n<div>\n\nAn Open Source Approach to Scaling Up Reinforcement Learning on the Base Model\n</div>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n    <a href=\"https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\" style=\"margin: 2px;\"><img alt=\"Code\" src=\"https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/></a>\n  \n  <a href=\"https://huggingface.co/Open-Reasoner-Zero\" target=\"_blank\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>\n\n  <a href=\"https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\" target=\"_blank\">\n  <img alt=\"Notion Page\"\n    src=\"https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>\n\n  <br>\n  <a href=\"https://arxiv.org/abs/2503.24290\"><b>Paper Arxiv Link </b>\ud83d\udc41\ufe0f</a>\n</div>\n\n<div>\n<br>\n\n</div>\n\n## Overview \ud83c\udf0a\nWe introduce **Open-Reasoner-Zero**, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility.\nUsing the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency\u2014requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline.\n\nTo enable broader participation in this pivotal moment we witnessed and accelerate research towards artificial general intelligence (AGI), \nwe release our source code, parameter settings, training data, and model weights.\nPlease refer to our [paper](https://arxiv.org/abs/2503.24290) for more insights across various model sizes. \n\n**Let the Reasoner-Zero tide rise!**\n\n\n## Main Results \ud83c\udfc6\n\n![](figure/teaser.png)\n\n*Figure 1 | Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\}. Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\} on benchmarks (averaged on 16 responses) during training. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, Open-Reasoner-Zero-32B achieves superior performance on AIME2024, MATH500, and GPQA Diamond benchmark-requiring only a tenth of the training steps.*\n\n![](figure/train_curve.png)\n*Figure 2 | Train-time Scale up on Train Reward and Response Length of Open-Reasoner-Zero (ORZ) - \\{0.5B, 1.5B, 7B, 32B\\}. Train Reward and Response Length increase steadily, demonstrating consistent scalability across model sizes. Interestingly, the ORZ-32B Response Length exhibits fluctuations without negatively impacting training stability, highlighting the robustness of our minimalist recipe.*\n\n## Releases \ud83d\udce6\n\n<strong>[2025/06/03]</strong>\nWe release [ORZ-R1-Distill-Qwen-14B](https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B), obtained by applying ORZ recipe to reasoning-enhanced models like DeepSeek-R1-Distill-Qwen-14B. This ORZ-R1-Distill-Qwen-14B achieves strong results on reasoning benchmarks, even surpassing the larger DeepSeek-R1-Distill-Qwen-32B model.\n\n| Model                        | AIME 2024 | AIME 2025 | MATH500 | GPQA Dia. |\n| ---------------------------- | --------- | --------- | ------- | --------- |\n| DeepSeek-R1-Distill-Qwen-14B | 69.7      | 49.1      | 93.9    | 59.1      |\n| DeepSeek-R1-Distill-Qwen-32B | 72.6      | 60.0      | 94.3    | **62.1**     |\n| **ORZ-R1-Distill-Qwen-14B**  | **75.2**  | **60.0**  | **95.6** | 60.4  |\n\n\n<strong>[2025/03/31]</strong>\nWe announce a major milestone for `Open-Reasoner-Zero`:\n\n- \ud83c\udf0a [Updated Paper](https://arxiv.org/abs/2503.24290) with new results.\n- \ud83d\udd2d [Easy-to-use Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground):\n  - [ORZ-1.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py) and [ORZ-0.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py) (main results in Figure 2). \n  - [Minimal resource training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py): ORZ-0.5B can be run on a single A800/H800 gpu!\n- \ud83e\udd29 [Updated Curated Datasets](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data): \n  - 129k data in total:\n    - [original 57k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json).\n    - [extended 72k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json).\n  - [13k hard data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json) mined from the above 129k data. \n    - used in the \"annealing\" stage of ORZ-32B training: **AIME2024 from ~41% to ~48%**!\n- \ud83e\udd17 More HF Models: \n  - Updated HF Models: [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B).\n  - Released HF Models: [`Open-Reasoner-Zero-1.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B) and [`Open-Reasoner-Zero-0.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B).\n- \ud83d\ude80 Full Suite of Critic Models for in-depth research: `Open-Reasoner-Zero-Critic-`{[0.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B), [1.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B), [7B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B),  [32B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B)}.\n\n<strong>[2025/02/18]</strong>\nWe release `Open-Reasoner-Zero`. \n\nAs part of this release, we open-source:\n- \ud83c\udf0a [Paper(WIP)](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf) on our comprehensive analysis and insights in Reasoner-Zero training\n- \ud83e\udd17 HF Model [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B)\n- \ud83c\udf81 [`Our curated 57k training data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data)\n- \ud83d\udcc4 [Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground) to enjoy your own Reasoner-Zero journey!\n\n## Key Features in Codebase \ud83d\udd11\n\n- Adopt single controller trainer design, flexible and researcher-friendly.\n- Colocate training and generation in the same GPUs to maximize GPU utilization.\n\n## Getting Started \ud83d\ude80\n### Data\n\nWe release all of curated high-quality training data in the [`data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data) folder:\n* curated 129k data:\n  * [original 57k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json), collected from various sources, including AIME (up to 2023), MATH, Numina-Math collection and Tulu3 MATH.\n  * [extended 72k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json), mainly cleaned from OpenR1-Math-220k.\n* [hard 13k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json), mined from the first stage of ORZ-32B training.\n\nThe details for how to collect data are described in our [paper](https://arxiv.org/abs/2503.24290).\n\n### Installation & Training Scripts\nWe release our [Dockerfile](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile) in [docker](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker) folder to facilitate the reproducibility of our training.\n\nTo install the package, run:\n```bash\npip install -e .\n```\n\n#### Start ORZ-32B PPO Training\nHere are the starting commands in 16 nodes. \n\nFirst on master node, run:\n```bash\nray start --head\n# you will see logging like:\n# Next steps\n#  To add another node to this Ray cluster, run\n#    ray start --address='<master-node-ip>:<master-node-port>'\n```\n\nthen on all other nodes, run:\n```bash\nray start --address='<master-node-ip>:<master-node-port>' # <master-node-ip> and <master-node-port> are from above loggings!\n```\n\nfinally on master node, just run:\n```bash\npython -m playground.orz_32b_ppo\n```\nYour training log will be shown in the master node terminal.\n\n------\n\n#### Start ORZ-0.5B PPO Training\nYou can start the ORZ-0.5B PPO training in single A800/H800 node:\n```bash\npython -m playground.orz_0p5b_ppo\n```\n\nYou can even run in **a single A800/H800 gpu**: \n```bash\npython -m playground.orz_0p5b_ppo_1gpu\n```\n\nnote: since we are not in multi-node setting, no `ray start` like logics are needed.\n\n------\n\n#### Start ORZ-7B PPO Training\n\nMulti-node Training on 4 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n\n# then on master node, run:\npython -m playground.orz_7b_ppo\n```\n\nYour training log will be shown in the master node terminal.\n\n-----\n\n#### Start ORZ-1.5B PPO Training\n\nMulti-node Training on 2 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n# then on master node, run:\npython -m playground.orz_1p5b_ppo\n```\n\n----\n\n#### Debug Settings\nIn the code, we leave an environment variable `DEBUG_MODE` to run in debug setting for researcher to iterate. (Thought for now, we recommend using `python -m playground.orz_0p5b_ppo_1gpu` for debugging.)\n\nThe debug running command examples:\n```bash\n# NOTE: just for debug, not final setting!\n\n## Debug command in a single GPU with `EleutherAI/pythia-14m`\nDEBUG_MODE=True python -m playground.orz_14m_ppo_mini\n## Debug command in a single node (8 GPUs) with `Qwen/Qwen2.5-7B`\nDEBUG_MODE=True python -m playground.orz_7b_ppo\n```\n\n### How to Use the Model\n#### Policy Model\nPolicy models can be used in the same way as any chat model in transformers and vllm, since we have put the chat template jinja in the tokenizer.\n\n#### Critic Model\nCritic models can be loaded the same way like in the [training code](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738). \n\n\n## Acknowledgements \ud83d\udc96 \n\n- This work was supported by computing resources and valuable feedback provided by [StepFun](https://www.stepfun.com/) and Tsinghua University.\n- Our training framework is built on [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF), [vllm](https://github.com/vllm-project/vllm), [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) and [ray](https://github.com/ray-project/ray).\n- Our model is based on [Qwen2.5 Series](https://qwenlm.github.io/blog/qwen2.5-llm/) of **base models**, including [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B), [Qwen2.5-1.5B](https://huggingface.co/Qwen/Qwen2.5-1.5B), [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B) and [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B).\n- We thank [Project Numina](https://projectnumina.ai/), [Tulu3](https://allenai.org/blog/tulu-3-technical) and [OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k) for their collected open sourced data.\n\n## Advertisement Time \ud83d\udce3\n\nWe are hiring talented researchers and engineers to join our team. If you are interested in our project and would like to contribute to the reasoner scale-up all the way to AGI, please feel free to reach out to us at hanqer@stepfun.com\n\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline)](https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline)\n\n## Community Discussions \ud83c\udf7a\n\nWe have several wechat groups to help discussions and sharing, you can scan the QR code below to join the latest group.\n\n<img src=\"figure/WeChatGroup.png\" width=\"300\" style=\"display: block; margin: 0 auto;\"/>\n\n## Citation\n\n```bibtex\n@misc{hu2025openreasonerzeroopensourceapproach,\n      title={Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model}, \n      author={Jingcheng Hu and Yinmin Zhang and Qi Han and Daxin Jiang and Xiangyu Zhang and Heung-Yeung Shum},\n      year={2025},\n      eprint={2503.24290},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2503.24290}, \n}\n```\n",
  "external_links_in_readme": [
    "https://github.com/deepspeedai/DeepSpeed",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json",
    "https://huggingface.co/Qwen/Qwen2.5-0.5B",
    "https://arxiv.org/abs/2503.24290",
    "https://huggingface.co/Open-Reasoner-Zero\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B",
    "https://www.stepfun.com/",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\"",
    "https://arxiv.org/abs/2503.24290},",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py",
    "https://github.com/vllm-project/vllm",
    "https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline",
    "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k",
    "https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>",
    "https://arxiv.org/abs/2503.24290\"><b>Paper",
    "https://github.com/ray-project/ray",
    "https://projectnumina.ai/",
    "https://qwenlm.github.io/blog/qwen2.5-llm/",
    "https://huggingface.co/Qwen/Qwen2.5-7B",
    "https://huggingface.co/Qwen/Qwen2.5-1.5B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B",
    "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\"",
    "https://github.com/OpenRLHF/OpenRLHF",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json",
    "https://huggingface.co/Qwen/Qwen2.5-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py",
    "https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>",
    "https://allenai.org/blog/tulu-3-technical",
    "https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B",
    "https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline",
    "https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B"
  ]
}
```

</details>


---

## Repository 9: Open-Reasoner-Zero/Open-Reasoner-Zero

# GitHub Repository Data

**Repository:** [Open-Reasoner-Zero/Open-Reasoner-Zero](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero)

## Basic Information

- **Description:** Official Repo for Open-Reasoner-Zero
- **Created:** 2025-02-19T17:28:25+00:00
- **Last Updated:** 2025-06-20T07:02:06+00:00
- **Last Pushed:** 2025-06-02T16:36:00+00:00
- **Default Branch:** main
- **Size:** 16502 KB

## Statistics

- **Stars:** 1,968
- **Forks:** 104
- **Watchers:** 1,968
- **Open Issues:** 19
- **Total Issues:** 0
- **Pull Requests:** 5

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE)

## Languages

- **Python:** 337,249 bytes
- **Dockerfile:** 1,739 bytes

## Top Contributors

1. **REIGN12** - 21 contributions
2. **vwxyzjn** - 5 contributions
3. **YinminZhang** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 71

- `.flake8` (blob)
- `.gitignore` (blob)
- `LICENSE` (blob)
- `ORZ_paper.pdf` (blob)
- `README.md` (blob)
- `data` (tree)
- `data/eval_data` (tree)
- `data/eval_data/aime2024.json` (blob)
- `data/eval_data/gpqa_diamond.json` (blob)
- `data/eval_data/math500.json` (blob)

## Recent Issues

- 🟢 **#73** DeepSpeed does not detect CUDA for some reason: (open)
- 🟢 **#72** In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map="meta"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device("meta")` (open)
- 🟢 **#71** In `orz/ppo/actors.py`: `with torch.device("meta"): AutoModel.from_pretrained(...)` throws (open)
- 🟢 **#70** `llm_engine.model_executor` seems not available on modern vllm (0.8.5) (open)
- 🟢 **#69** On newer vllm should use `llm_engine.vllm_config` and similar (open)

## Recent Pull Requests

- 🟢 **#66** 重构 PPOExpConfig 类，继承自 CommonPPOConfig，简化配置管理 (open)
- 🔴 **#62** Major Updates for Open-Reasoner-Zero (closed)
- 🟢 **#57** Support FSDP and vllm colocate with tp size > 1 (open)
- 🔴 **#32** Local ppo mini (closed)
- 🔴 **#31** Add a mini example (closed)

## Recent Commits

- **3fdd9a07** doc: update ORZ-R1-Distill-Qwen-14B results - REIGN12 (2025-06-02T16:35:57+00:00)
- **a2351927** chores: update qr code - REIGN12 (2025-06-02T16:35:25+00:00)
- **20e5bc83** README: update readme - REIGN12 (2025-04-08T12:58:51+00:00)
- **9c73e259** README: update readme - REIGN12 (2025-04-02T03:41:38+00:00)
- **a27ce5c1** README: update readme - REIGN12 (2025-04-02T03:38:35+00:00)
- **a288753d** README: update readme - REIGN12 (2025-03-31T15:53:39+00:00)
- **a7193f2c** Major Updates for Open-Reasoner-Zero (#62) - Jingcheng Hu (2025-03-31T15:45:50+00:00)
- **e008f6d9** Fix: bug fix for weight sync logic when colocate=False - REIGN12 (2025-03-05T08:08:42+00:00)
- **effe6fb8** Fix: fix typo in report - REIGN12 (2025-03-01T09:03:59+00:00)
- **9464093c** Fix: minor bugfix for dockerfile - REIGN12 (2025-03-01T08:54:56+00:00)

## External Links Found in README

- https://github.com/deepspeedai/DeepSpeed
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json
- https://huggingface.co/Qwen/Qwen2.5-0.5B
- https://arxiv.org/abs/2503.24290
- https://huggingface.co/Open-Reasoner-Zero"
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B
- https://www.stepfun.com/
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
- https://arxiv.org/abs/2503.24290},
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py
- https://github.com/vllm-project/vllm
- https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline
- https://huggingface.co/datasets/open-r1/OpenR1-Math-220k
- https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor"/></a>
- https://arxiv.org/abs/2503.24290"><b>Paper
- https://github.com/ray-project/ray
- https://projectnumina.ai/

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 935587127,
  "name": "Open-Reasoner-Zero",
  "full_name": "Open-Reasoner-Zero/Open-Reasoner-Zero",
  "description": "Official Repo for Open-Reasoner-Zero",
  "html_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero",
  "clone_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "ssh_url": "git@github.com:Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "homepage": "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903",
  "topics": [],
  "default_branch": "main",
  "created_at": "2025-02-19T17:28:25+00:00",
  "updated_at": "2025-06-20T07:02:06+00:00",
  "pushed_at": "2025-06-02T16:36:00+00:00",
  "size_kb": 16502,
  "watchers_count": 1968,
  "stargazers_count": 1968,
  "forks_count": 104,
  "open_issues_count": 19,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE"
  },
  "languages": {
    "Python": 337249,
    "Dockerfile": 1739
  },
  "top_contributors": [
    {
      "login": "REIGN12",
      "contributions": 21
    },
    {
      "login": "vwxyzjn",
      "contributions": 5
    },
    {
      "login": "YinminZhang",
      "contributions": 3
    }
  ],
  "file_tree_count": 71,
  "file_tree_sample": [
    {
      "path": ".flake8",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "ORZ_paper.pdf",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "data",
      "type": "tree"
    },
    {
      "path": "data/eval_data",
      "type": "tree"
    },
    {
      "path": "data/eval_data/aime2024.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/gpqa_diamond.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/math500.json",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 5,
  "recent_issues": [
    {
      "number": 73,
      "title": "DeepSpeed does not detect CUDA for some reason:",
      "state": "open"
    },
    {
      "number": 72,
      "title": "In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map=\"meta\"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device(\"meta\")`",
      "state": "open"
    },
    {
      "number": 71,
      "title": "In `orz/ppo/actors.py`: `with torch.device(\"meta\"): AutoModel.from_pretrained(...)` throws",
      "state": "open"
    },
    {
      "number": 70,
      "title": "`llm_engine.model_executor` seems not available on modern vllm (0.8.5)",
      "state": "open"
    },
    {
      "number": 69,
      "title": "On newer vllm should use `llm_engine.vllm_config` and similar",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 66,
      "title": "\u91cd\u6784 PPOExpConfig \u7c7b\uff0c\u7ee7\u627f\u81ea CommonPPOConfig\uff0c\u7b80\u5316\u914d\u7f6e\u7ba1\u7406",
      "state": "open"
    },
    {
      "number": 62,
      "title": "Major Updates for Open-Reasoner-Zero",
      "state": "closed"
    },
    {
      "number": 57,
      "title": "Support FSDP and vllm colocate with tp size > 1",
      "state": "open"
    },
    {
      "number": 32,
      "title": "Local ppo mini",
      "state": "closed"
    },
    {
      "number": 31,
      "title": "Add a mini example",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "3fdd9a07b4fb01e06005e6e74fc56690cde8a341",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:57+00:00",
      "message": "doc: update ORZ-R1-Distill-Qwen-14B results"
    },
    {
      "sha": "a23519277efdd71a063e1f2913d5c496a5f96180",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:25+00:00",
      "message": "chores: update qr code"
    },
    {
      "sha": "20e5bc83ae75695fded4e1a59a21e487063e24d9",
      "author": "REIGN12",
      "date": "2025-04-08T12:58:51+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "9c73e2590d23a7245a8d6341337952828110c631",
      "author": "REIGN12",
      "date": "2025-04-02T03:41:38+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a27ce5c1c5204d267c821542af4bdb47c70aa40a",
      "author": "REIGN12",
      "date": "2025-04-02T03:38:35+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a288753de916c0f3ba932fb0d1c218a49b8a403b",
      "author": "REIGN12",
      "date": "2025-03-31T15:53:39+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a7193f2c14aa90d055f9c161bf4566ec41ced30c",
      "author": "Jingcheng Hu",
      "date": "2025-03-31T15:45:50+00:00",
      "message": "Major Updates for Open-Reasoner-Zero (#62)"
    },
    {
      "sha": "e008f6d95f0b9a0e992f6b8bac912515b50a4634",
      "author": "REIGN12",
      "date": "2025-03-05T08:08:42+00:00",
      "message": "Fix: bug fix for weight sync logic when colocate=False"
    },
    {
      "sha": "effe6fb81570ccfde7d28d8d335c83127e95263f",
      "author": "REIGN12",
      "date": "2025-03-01T09:03:59+00:00",
      "message": "Fix: fix typo in report"
    },
    {
      "sha": "9464093c67c32a5fca69556b603f915e55fc00a5",
      "author": "REIGN12",
      "date": "2025-03-01T08:54:56+00:00",
      "message": "Fix: minor bugfix for dockerfile"
    },
    {
      "sha": "f594903dc0e953a8d0429ae0ecf339efcbe456f7",
      "author": "Jingcheng Hu",
      "date": "2025-02-25T15:03:35+00:00",
      "message": "Merge pull request #31 from vwxyzjn/ppo-mini"
    },
    {
      "sha": "ce912e108ca03d5ded42e151e3c857d44fc3f277",
      "author": "Costa Huang",
      "date": "2025-02-24T20:33:48+00:00",
      "message": "quick change"
    },
    {
      "sha": "34d8abc52ecba9c1c620350eb70110d05ea6b676",
      "author": "Costa Huang",
      "date": "2025-02-24T20:32:51+00:00",
      "message": "set eval to false"
    },
    {
      "sha": "55904eae7c35932e8ee2efb717c788ed3cd9b75e",
      "author": "Costa Huang",
      "date": "2025-02-24T20:31:44+00:00",
      "message": "quick change"
    },
    {
      "sha": "dbc40d8042c4981138b58773ca4936570e2339be",
      "author": "Costa Huang",
      "date": "2025-02-24T20:28:34+00:00",
      "message": "Add single GPU exampel"
    },
    {
      "sha": "ec2ce7e528f660f3b05c19369cdcff9d89cbb6b0",
      "author": "Costa Huang",
      "date": "2025-02-24T20:06:00+00:00",
      "message": "Add a mini example"
    },
    {
      "sha": "7b5dec17a8dda164e8a4aaba33393afc3e0e95d9",
      "author": "Jingcheng Hu",
      "date": "2025-02-24T09:18:55+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "3ccec9eec8717a5de0575918237bc81b6ba3e3a2",
      "author": "REIGN12",
      "date": "2025-02-24T07:24:11+00:00",
      "message": "Merge branch 'main' of https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
    },
    {
      "sha": "df77747d28c8580db768732f8da34f913e0411ca",
      "author": "REIGN12",
      "date": "2025-02-24T07:23:34+00:00",
      "message": "Fix: bug fix for readme and 32b exp"
    },
    {
      "sha": "ec6b01ef61f6b66a30bff0ea763b37fe4e9d00d1",
      "author": "hanqer",
      "date": "2025-02-24T05:52:43+00:00",
      "message": "fix: fix the package dependencies of pip and apt."
    }
  ],
  "readme_text": "<div align=\"center\">\n\n# Open Reasoner Zero\n\n<img src=\"figure/logo.jpg\" width=\"300\"/>\n\n<div>\n\nAn Open Source Approach to Scaling Up Reinforcement Learning on the Base Model\n</div>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n    <a href=\"https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\" style=\"margin: 2px;\"><img alt=\"Code\" src=\"https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/></a>\n  \n  <a href=\"https://huggingface.co/Open-Reasoner-Zero\" target=\"_blank\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>\n\n  <a href=\"https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\" target=\"_blank\">\n  <img alt=\"Notion Page\"\n    src=\"https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>\n\n  <br>\n  <a href=\"https://arxiv.org/abs/2503.24290\"><b>Paper Arxiv Link </b>\ud83d\udc41\ufe0f</a>\n</div>\n\n<div>\n<br>\n\n</div>\n\n## Overview \ud83c\udf0a\nWe introduce **Open-Reasoner-Zero**, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility.\nUsing the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency\u2014requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline.\n\nTo enable broader participation in this pivotal moment we witnessed and accelerate research towards artificial general intelligence (AGI), \nwe release our source code, parameter settings, training data, and model weights.\nPlease refer to our [paper](https://arxiv.org/abs/2503.24290) for more insights across various model sizes. \n\n**Let the Reasoner-Zero tide rise!**\n\n\n## Main Results \ud83c\udfc6\n\n![](figure/teaser.png)\n\n*Figure 1 | Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\}. Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\} on benchmarks (averaged on 16 responses) during training. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, Open-Reasoner-Zero-32B achieves superior performance on AIME2024, MATH500, and GPQA Diamond benchmark-requiring only a tenth of the training steps.*\n\n![](figure/train_curve.png)\n*Figure 2 | Train-time Scale up on Train Reward and Response Length of Open-Reasoner-Zero (ORZ) - \\{0.5B, 1.5B, 7B, 32B\\}. Train Reward and Response Length increase steadily, demonstrating consistent scalability across model sizes. Interestingly, the ORZ-32B Response Length exhibits fluctuations without negatively impacting training stability, highlighting the robustness of our minimalist recipe.*\n\n## Releases \ud83d\udce6\n\n<strong>[2025/06/03]</strong>\nWe release [ORZ-R1-Distill-Qwen-14B](https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B), obtained by applying ORZ recipe to reasoning-enhanced models like DeepSeek-R1-Distill-Qwen-14B. This ORZ-R1-Distill-Qwen-14B achieves strong results on reasoning benchmarks, even surpassing the larger DeepSeek-R1-Distill-Qwen-32B model.\n\n| Model                        | AIME 2024 | AIME 2025 | MATH500 | GPQA Dia. |\n| ---------------------------- | --------- | --------- | ------- | --------- |\n| DeepSeek-R1-Distill-Qwen-14B | 69.7      | 49.1      | 93.9    | 59.1      |\n| DeepSeek-R1-Distill-Qwen-32B | 72.6      | 60.0      | 94.3    | **62.1**     |\n| **ORZ-R1-Distill-Qwen-14B**  | **75.2**  | **60.0**  | **95.6** | 60.4  |\n\n\n<strong>[2025/03/31]</strong>\nWe announce a major milestone for `Open-Reasoner-Zero`:\n\n- \ud83c\udf0a [Updated Paper](https://arxiv.org/abs/2503.24290) with new results.\n- \ud83d\udd2d [Easy-to-use Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground):\n  - [ORZ-1.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py) and [ORZ-0.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py) (main results in Figure 2). \n  - [Minimal resource training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py): ORZ-0.5B can be run on a single A800/H800 gpu!\n- \ud83e\udd29 [Updated Curated Datasets](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data): \n  - 129k data in total:\n    - [original 57k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json).\n    - [extended 72k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json).\n  - [13k hard data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json) mined from the above 129k data. \n    - used in the \"annealing\" stage of ORZ-32B training: **AIME2024 from ~41% to ~48%**!\n- \ud83e\udd17 More HF Models: \n  - Updated HF Models: [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B).\n  - Released HF Models: [`Open-Reasoner-Zero-1.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B) and [`Open-Reasoner-Zero-0.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B).\n- \ud83d\ude80 Full Suite of Critic Models for in-depth research: `Open-Reasoner-Zero-Critic-`{[0.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B), [1.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B), [7B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B),  [32B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B)}.\n\n<strong>[2025/02/18]</strong>\nWe release `Open-Reasoner-Zero`. \n\nAs part of this release, we open-source:\n- \ud83c\udf0a [Paper(WIP)](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf) on our comprehensive analysis and insights in Reasoner-Zero training\n- \ud83e\udd17 HF Model [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B)\n- \ud83c\udf81 [`Our curated 57k training data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data)\n- \ud83d\udcc4 [Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground) to enjoy your own Reasoner-Zero journey!\n\n## Key Features in Codebase \ud83d\udd11\n\n- Adopt single controller trainer design, flexible and researcher-friendly.\n- Colocate training and generation in the same GPUs to maximize GPU utilization.\n\n## Getting Started \ud83d\ude80\n### Data\n\nWe release all of curated high-quality training data in the [`data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data) folder:\n* curated 129k data:\n  * [original 57k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json), collected from various sources, including AIME (up to 2023), MATH, Numina-Math collection and Tulu3 MATH.\n  * [extended 72k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json), mainly cleaned from OpenR1-Math-220k.\n* [hard 13k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json), mined from the first stage of ORZ-32B training.\n\nThe details for how to collect data are described in our [paper](https://arxiv.org/abs/2503.24290).\n\n### Installation & Training Scripts\nWe release our [Dockerfile](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile) in [docker](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker) folder to facilitate the reproducibility of our training.\n\nTo install the package, run:\n```bash\npip install -e .\n```\n\n#### Start ORZ-32B PPO Training\nHere are the starting commands in 16 nodes. \n\nFirst on master node, run:\n```bash\nray start --head\n# you will see logging like:\n# Next steps\n#  To add another node to this Ray cluster, run\n#    ray start --address='<master-node-ip>:<master-node-port>'\n```\n\nthen on all other nodes, run:\n```bash\nray start --address='<master-node-ip>:<master-node-port>' # <master-node-ip> and <master-node-port> are from above loggings!\n```\n\nfinally on master node, just run:\n```bash\npython -m playground.orz_32b_ppo\n```\nYour training log will be shown in the master node terminal.\n\n------\n\n#### Start ORZ-0.5B PPO Training\nYou can start the ORZ-0.5B PPO training in single A800/H800 node:\n```bash\npython -m playground.orz_0p5b_ppo\n```\n\nYou can even run in **a single A800/H800 gpu**: \n```bash\npython -m playground.orz_0p5b_ppo_1gpu\n```\n\nnote: since we are not in multi-node setting, no `ray start` like logics are needed.\n\n------\n\n#### Start ORZ-7B PPO Training\n\nMulti-node Training on 4 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n\n# then on master node, run:\npython -m playground.orz_7b_ppo\n```\n\nYour training log will be shown in the master node terminal.\n\n-----\n\n#### Start ORZ-1.5B PPO Training\n\nMulti-node Training on 2 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n# then on master node, run:\npython -m playground.orz_1p5b_ppo\n```\n\n----\n\n#### Debug Settings\nIn the code, we leave an environment variable `DEBUG_MODE` to run in debug setting for researcher to iterate. (Thought for now, we recommend using `python -m playground.orz_0p5b_ppo_1gpu` for debugging.)\n\nThe debug running command examples:\n```bash\n# NOTE: just for debug, not final setting!\n\n## Debug command in a single GPU with `EleutherAI/pythia-14m`\nDEBUG_MODE=True python -m playground.orz_14m_ppo_mini\n## Debug command in a single node (8 GPUs) with `Qwen/Qwen2.5-7B`\nDEBUG_MODE=True python -m playground.orz_7b_ppo\n```\n\n### How to Use the Model\n#### Policy Model\nPolicy models can be used in the same way as any chat model in transformers and vllm, since we have put the chat template jinja in the tokenizer.\n\n#### Critic Model\nCritic models can be loaded the same way like in the [training code](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738). \n\n\n## Acknowledgements \ud83d\udc96 \n\n- This work was supported by computing resources and valuable feedback provided by [StepFun](https://www.stepfun.com/) and Tsinghua University.\n- Our training framework is built on [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF), [vllm](https://github.com/vllm-project/vllm), [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) and [ray](https://github.com/ray-project/ray).\n- Our model is based on [Qwen2.5 Series](https://qwenlm.github.io/blog/qwen2.5-llm/) of **base models**, including [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B), [Qwen2.5-1.5B](https://huggingface.co/Qwen/Qwen2.5-1.5B), [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B) and [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B).\n- We thank [Project Numina](https://projectnumina.ai/), [Tulu3](https://allenai.org/blog/tulu-3-technical) and [OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k) for their collected open sourced data.\n\n## Advertisement Time \ud83d\udce3\n\nWe are hiring talented researchers and engineers to join our team. If you are interested in our project and would like to contribute to the reasoner scale-up all the way to AGI, please feel free to reach out to us at hanqer@stepfun.com\n\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline)](https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline)\n\n## Community Discussions \ud83c\udf7a\n\nWe have several wechat groups to help discussions and sharing, you can scan the QR code below to join the latest group.\n\n<img src=\"figure/WeChatGroup.png\" width=\"300\" style=\"display: block; margin: 0 auto;\"/>\n\n## Citation\n\n```bibtex\n@misc{hu2025openreasonerzeroopensourceapproach,\n      title={Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model}, \n      author={Jingcheng Hu and Yinmin Zhang and Qi Han and Daxin Jiang and Xiangyu Zhang and Heung-Yeung Shum},\n      year={2025},\n      eprint={2503.24290},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2503.24290}, \n}\n```\n",
  "external_links_in_readme": [
    "https://github.com/deepspeedai/DeepSpeed",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json",
    "https://huggingface.co/Qwen/Qwen2.5-0.5B",
    "https://arxiv.org/abs/2503.24290",
    "https://huggingface.co/Open-Reasoner-Zero\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B",
    "https://www.stepfun.com/",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\"",
    "https://arxiv.org/abs/2503.24290},",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py",
    "https://github.com/vllm-project/vllm",
    "https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline",
    "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k",
    "https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>",
    "https://arxiv.org/abs/2503.24290\"><b>Paper",
    "https://github.com/ray-project/ray",
    "https://projectnumina.ai/",
    "https://qwenlm.github.io/blog/qwen2.5-llm/",
    "https://huggingface.co/Qwen/Qwen2.5-7B",
    "https://huggingface.co/Qwen/Qwen2.5-1.5B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B",
    "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\"",
    "https://github.com/OpenRLHF/OpenRLHF",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json",
    "https://huggingface.co/Qwen/Qwen2.5-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py",
    "https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>",
    "https://allenai.org/blog/tulu-3-technical",
    "https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B",
    "https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline",
    "https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B"
  ]
}
```

</details>


---

## Repository 10: Open-Reasoner-Zero/Open-Reasoner-Zero

# GitHub Repository Data

**Repository:** [Open-Reasoner-Zero/Open-Reasoner-Zero](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero)

## Basic Information

- **Description:** Official Repo for Open-Reasoner-Zero
- **Created:** 2025-02-19T17:28:25+00:00
- **Last Updated:** 2025-06-20T07:02:06+00:00
- **Last Pushed:** 2025-06-02T16:36:00+00:00
- **Default Branch:** main
- **Size:** 16502 KB

## Statistics

- **Stars:** 1,968
- **Forks:** 104
- **Watchers:** 1,968
- **Open Issues:** 19
- **Total Issues:** 0
- **Pull Requests:** 5

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE)

## Languages

- **Python:** 337,249 bytes
- **Dockerfile:** 1,739 bytes

## Top Contributors

1. **REIGN12** - 21 contributions
2. **vwxyzjn** - 5 contributions
3. **YinminZhang** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 71

- `.flake8` (blob)
- `.gitignore` (blob)
- `LICENSE` (blob)
- `ORZ_paper.pdf` (blob)
- `README.md` (blob)
- `data` (tree)
- `data/eval_data` (tree)
- `data/eval_data/aime2024.json` (blob)
- `data/eval_data/gpqa_diamond.json` (blob)
- `data/eval_data/math500.json` (blob)

## Recent Issues

- 🟢 **#73** DeepSpeed does not detect CUDA for some reason: (open)
- 🟢 **#72** In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map="meta"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device("meta")` (open)
- 🟢 **#71** In `orz/ppo/actors.py`: `with torch.device("meta"): AutoModel.from_pretrained(...)` throws (open)
- 🟢 **#70** `llm_engine.model_executor` seems not available on modern vllm (0.8.5) (open)
- 🟢 **#69** On newer vllm should use `llm_engine.vllm_config` and similar (open)

## Recent Pull Requests

- 🟢 **#66** 重构 PPOExpConfig 类，继承自 CommonPPOConfig，简化配置管理 (open)
- 🔴 **#62** Major Updates for Open-Reasoner-Zero (closed)
- 🟢 **#57** Support FSDP and vllm colocate with tp size > 1 (open)
- 🔴 **#32** Local ppo mini (closed)
- 🔴 **#31** Add a mini example (closed)

## Recent Commits

- **3fdd9a07** doc: update ORZ-R1-Distill-Qwen-14B results - REIGN12 (2025-06-02T16:35:57+00:00)
- **a2351927** chores: update qr code - REIGN12 (2025-06-02T16:35:25+00:00)
- **20e5bc83** README: update readme - REIGN12 (2025-04-08T12:58:51+00:00)
- **9c73e259** README: update readme - REIGN12 (2025-04-02T03:41:38+00:00)
- **a27ce5c1** README: update readme - REIGN12 (2025-04-02T03:38:35+00:00)
- **a288753d** README: update readme - REIGN12 (2025-03-31T15:53:39+00:00)
- **a7193f2c** Major Updates for Open-Reasoner-Zero (#62) - Jingcheng Hu (2025-03-31T15:45:50+00:00)
- **e008f6d9** Fix: bug fix for weight sync logic when colocate=False - REIGN12 (2025-03-05T08:08:42+00:00)
- **effe6fb8** Fix: fix typo in report - REIGN12 (2025-03-01T09:03:59+00:00)
- **9464093c** Fix: minor bugfix for dockerfile - REIGN12 (2025-03-01T08:54:56+00:00)

## External Links Found in README

- https://github.com/deepspeedai/DeepSpeed
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json
- https://huggingface.co/Qwen/Qwen2.5-0.5B
- https://arxiv.org/abs/2503.24290
- https://huggingface.co/Open-Reasoner-Zero"
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B
- https://www.stepfun.com/
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
- https://arxiv.org/abs/2503.24290},
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py
- https://github.com/vllm-project/vllm
- https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline
- https://huggingface.co/datasets/open-r1/OpenR1-Math-220k
- https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor"/></a>
- https://arxiv.org/abs/2503.24290"><b>Paper
- https://github.com/ray-project/ray
- https://projectnumina.ai/

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 935587127,
  "name": "Open-Reasoner-Zero",
  "full_name": "Open-Reasoner-Zero/Open-Reasoner-Zero",
  "description": "Official Repo for Open-Reasoner-Zero",
  "html_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero",
  "clone_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "ssh_url": "git@github.com:Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "homepage": "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903",
  "topics": [],
  "default_branch": "main",
  "created_at": "2025-02-19T17:28:25+00:00",
  "updated_at": "2025-06-20T07:02:06+00:00",
  "pushed_at": "2025-06-02T16:36:00+00:00",
  "size_kb": 16502,
  "watchers_count": 1968,
  "stargazers_count": 1968,
  "forks_count": 104,
  "open_issues_count": 19,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE"
  },
  "languages": {
    "Python": 337249,
    "Dockerfile": 1739
  },
  "top_contributors": [
    {
      "login": "REIGN12",
      "contributions": 21
    },
    {
      "login": "vwxyzjn",
      "contributions": 5
    },
    {
      "login": "YinminZhang",
      "contributions": 3
    }
  ],
  "file_tree_count": 71,
  "file_tree_sample": [
    {
      "path": ".flake8",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "ORZ_paper.pdf",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "data",
      "type": "tree"
    },
    {
      "path": "data/eval_data",
      "type": "tree"
    },
    {
      "path": "data/eval_data/aime2024.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/gpqa_diamond.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/math500.json",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 5,
  "recent_issues": [
    {
      "number": 73,
      "title": "DeepSpeed does not detect CUDA for some reason:",
      "state": "open"
    },
    {
      "number": 72,
      "title": "In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map=\"meta\"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device(\"meta\")`",
      "state": "open"
    },
    {
      "number": 71,
      "title": "In `orz/ppo/actors.py`: `with torch.device(\"meta\"): AutoModel.from_pretrained(...)` throws",
      "state": "open"
    },
    {
      "number": 70,
      "title": "`llm_engine.model_executor` seems not available on modern vllm (0.8.5)",
      "state": "open"
    },
    {
      "number": 69,
      "title": "On newer vllm should use `llm_engine.vllm_config` and similar",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 66,
      "title": "\u91cd\u6784 PPOExpConfig \u7c7b\uff0c\u7ee7\u627f\u81ea CommonPPOConfig\uff0c\u7b80\u5316\u914d\u7f6e\u7ba1\u7406",
      "state": "open"
    },
    {
      "number": 62,
      "title": "Major Updates for Open-Reasoner-Zero",
      "state": "closed"
    },
    {
      "number": 57,
      "title": "Support FSDP and vllm colocate with tp size > 1",
      "state": "open"
    },
    {
      "number": 32,
      "title": "Local ppo mini",
      "state": "closed"
    },
    {
      "number": 31,
      "title": "Add a mini example",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "3fdd9a07b4fb01e06005e6e74fc56690cde8a341",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:57+00:00",
      "message": "doc: update ORZ-R1-Distill-Qwen-14B results"
    },
    {
      "sha": "a23519277efdd71a063e1f2913d5c496a5f96180",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:25+00:00",
      "message": "chores: update qr code"
    },
    {
      "sha": "20e5bc83ae75695fded4e1a59a21e487063e24d9",
      "author": "REIGN12",
      "date": "2025-04-08T12:58:51+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "9c73e2590d23a7245a8d6341337952828110c631",
      "author": "REIGN12",
      "date": "2025-04-02T03:41:38+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a27ce5c1c5204d267c821542af4bdb47c70aa40a",
      "author": "REIGN12",
      "date": "2025-04-02T03:38:35+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a288753de916c0f3ba932fb0d1c218a49b8a403b",
      "author": "REIGN12",
      "date": "2025-03-31T15:53:39+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a7193f2c14aa90d055f9c161bf4566ec41ced30c",
      "author": "Jingcheng Hu",
      "date": "2025-03-31T15:45:50+00:00",
      "message": "Major Updates for Open-Reasoner-Zero (#62)"
    },
    {
      "sha": "e008f6d95f0b9a0e992f6b8bac912515b50a4634",
      "author": "REIGN12",
      "date": "2025-03-05T08:08:42+00:00",
      "message": "Fix: bug fix for weight sync logic when colocate=False"
    },
    {
      "sha": "effe6fb81570ccfde7d28d8d335c83127e95263f",
      "author": "REIGN12",
      "date": "2025-03-01T09:03:59+00:00",
      "message": "Fix: fix typo in report"
    },
    {
      "sha": "9464093c67c32a5fca69556b603f915e55fc00a5",
      "author": "REIGN12",
      "date": "2025-03-01T08:54:56+00:00",
      "message": "Fix: minor bugfix for dockerfile"
    },
    {
      "sha": "f594903dc0e953a8d0429ae0ecf339efcbe456f7",
      "author": "Jingcheng Hu",
      "date": "2025-02-25T15:03:35+00:00",
      "message": "Merge pull request #31 from vwxyzjn/ppo-mini"
    },
    {
      "sha": "ce912e108ca03d5ded42e151e3c857d44fc3f277",
      "author": "Costa Huang",
      "date": "2025-02-24T20:33:48+00:00",
      "message": "quick change"
    },
    {
      "sha": "34d8abc52ecba9c1c620350eb70110d05ea6b676",
      "author": "Costa Huang",
      "date": "2025-02-24T20:32:51+00:00",
      "message": "set eval to false"
    },
    {
      "sha": "55904eae7c35932e8ee2efb717c788ed3cd9b75e",
      "author": "Costa Huang",
      "date": "2025-02-24T20:31:44+00:00",
      "message": "quick change"
    },
    {
      "sha": "dbc40d8042c4981138b58773ca4936570e2339be",
      "author": "Costa Huang",
      "date": "2025-02-24T20:28:34+00:00",
      "message": "Add single GPU exampel"
    },
    {
      "sha": "ec2ce7e528f660f3b05c19369cdcff9d89cbb6b0",
      "author": "Costa Huang",
      "date": "2025-02-24T20:06:00+00:00",
      "message": "Add a mini example"
    },
    {
      "sha": "7b5dec17a8dda164e8a4aaba33393afc3e0e95d9",
      "author": "Jingcheng Hu",
      "date": "2025-02-24T09:18:55+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "3ccec9eec8717a5de0575918237bc81b6ba3e3a2",
      "author": "REIGN12",
      "date": "2025-02-24T07:24:11+00:00",
      "message": "Merge branch 'main' of https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
    },
    {
      "sha": "df77747d28c8580db768732f8da34f913e0411ca",
      "author": "REIGN12",
      "date": "2025-02-24T07:23:34+00:00",
      "message": "Fix: bug fix for readme and 32b exp"
    },
    {
      "sha": "ec6b01ef61f6b66a30bff0ea763b37fe4e9d00d1",
      "author": "hanqer",
      "date": "2025-02-24T05:52:43+00:00",
      "message": "fix: fix the package dependencies of pip and apt."
    }
  ],
  "readme_text": "<div align=\"center\">\n\n# Open Reasoner Zero\n\n<img src=\"figure/logo.jpg\" width=\"300\"/>\n\n<div>\n\nAn Open Source Approach to Scaling Up Reinforcement Learning on the Base Model\n</div>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n    <a href=\"https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\" style=\"margin: 2px;\"><img alt=\"Code\" src=\"https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/></a>\n  \n  <a href=\"https://huggingface.co/Open-Reasoner-Zero\" target=\"_blank\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>\n\n  <a href=\"https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\" target=\"_blank\">\n  <img alt=\"Notion Page\"\n    src=\"https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>\n\n  <br>\n  <a href=\"https://arxiv.org/abs/2503.24290\"><b>Paper Arxiv Link </b>\ud83d\udc41\ufe0f</a>\n</div>\n\n<div>\n<br>\n\n</div>\n\n## Overview \ud83c\udf0a\nWe introduce **Open-Reasoner-Zero**, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility.\nUsing the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency\u2014requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline.\n\nTo enable broader participation in this pivotal moment we witnessed and accelerate research towards artificial general intelligence (AGI), \nwe release our source code, parameter settings, training data, and model weights.\nPlease refer to our [paper](https://arxiv.org/abs/2503.24290) for more insights across various model sizes. \n\n**Let the Reasoner-Zero tide rise!**\n\n\n## Main Results \ud83c\udfc6\n\n![](figure/teaser.png)\n\n*Figure 1 | Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\}. Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\} on benchmarks (averaged on 16 responses) during training. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, Open-Reasoner-Zero-32B achieves superior performance on AIME2024, MATH500, and GPQA Diamond benchmark-requiring only a tenth of the training steps.*\n\n![](figure/train_curve.png)\n*Figure 2 | Train-time Scale up on Train Reward and Response Length of Open-Reasoner-Zero (ORZ) - \\{0.5B, 1.5B, 7B, 32B\\}. Train Reward and Response Length increase steadily, demonstrating consistent scalability across model sizes. Interestingly, the ORZ-32B Response Length exhibits fluctuations without negatively impacting training stability, highlighting the robustness of our minimalist recipe.*\n\n## Releases \ud83d\udce6\n\n<strong>[2025/06/03]</strong>\nWe release [ORZ-R1-Distill-Qwen-14B](https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B), obtained by applying ORZ recipe to reasoning-enhanced models like DeepSeek-R1-Distill-Qwen-14B. This ORZ-R1-Distill-Qwen-14B achieves strong results on reasoning benchmarks, even surpassing the larger DeepSeek-R1-Distill-Qwen-32B model.\n\n| Model                        | AIME 2024 | AIME 2025 | MATH500 | GPQA Dia. |\n| ---------------------------- | --------- | --------- | ------- | --------- |\n| DeepSeek-R1-Distill-Qwen-14B | 69.7      | 49.1      | 93.9    | 59.1      |\n| DeepSeek-R1-Distill-Qwen-32B | 72.6      | 60.0      | 94.3    | **62.1**     |\n| **ORZ-R1-Distill-Qwen-14B**  | **75.2**  | **60.0**  | **95.6** | 60.4  |\n\n\n<strong>[2025/03/31]</strong>\nWe announce a major milestone for `Open-Reasoner-Zero`:\n\n- \ud83c\udf0a [Updated Paper](https://arxiv.org/abs/2503.24290) with new results.\n- \ud83d\udd2d [Easy-to-use Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground):\n  - [ORZ-1.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py) and [ORZ-0.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py) (main results in Figure 2). \n  - [Minimal resource training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py): ORZ-0.5B can be run on a single A800/H800 gpu!\n- \ud83e\udd29 [Updated Curated Datasets](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data): \n  - 129k data in total:\n    - [original 57k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json).\n    - [extended 72k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json).\n  - [13k hard data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json) mined from the above 129k data. \n    - used in the \"annealing\" stage of ORZ-32B training: **AIME2024 from ~41% to ~48%**!\n- \ud83e\udd17 More HF Models: \n  - Updated HF Models: [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B).\n  - Released HF Models: [`Open-Reasoner-Zero-1.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B) and [`Open-Reasoner-Zero-0.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B).\n- \ud83d\ude80 Full Suite of Critic Models for in-depth research: `Open-Reasoner-Zero-Critic-`{[0.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B), [1.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B), [7B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B),  [32B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B)}.\n\n<strong>[2025/02/18]</strong>\nWe release `Open-Reasoner-Zero`. \n\nAs part of this release, we open-source:\n- \ud83c\udf0a [Paper(WIP)](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf) on our comprehensive analysis and insights in Reasoner-Zero training\n- \ud83e\udd17 HF Model [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B)\n- \ud83c\udf81 [`Our curated 57k training data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data)\n- \ud83d\udcc4 [Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground) to enjoy your own Reasoner-Zero journey!\n\n## Key Features in Codebase \ud83d\udd11\n\n- Adopt single controller trainer design, flexible and researcher-friendly.\n- Colocate training and generation in the same GPUs to maximize GPU utilization.\n\n## Getting Started \ud83d\ude80\n### Data\n\nWe release all of curated high-quality training data in the [`data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data) folder:\n* curated 129k data:\n  * [original 57k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json), collected from various sources, including AIME (up to 2023), MATH, Numina-Math collection and Tulu3 MATH.\n  * [extended 72k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json), mainly cleaned from OpenR1-Math-220k.\n* [hard 13k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json), mined from the first stage of ORZ-32B training.\n\nThe details for how to collect data are described in our [paper](https://arxiv.org/abs/2503.24290).\n\n### Installation & Training Scripts\nWe release our [Dockerfile](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile) in [docker](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker) folder to facilitate the reproducibility of our training.\n\nTo install the package, run:\n```bash\npip install -e .\n```\n\n#### Start ORZ-32B PPO Training\nHere are the starting commands in 16 nodes. \n\nFirst on master node, run:\n```bash\nray start --head\n# you will see logging like:\n# Next steps\n#  To add another node to this Ray cluster, run\n#    ray start --address='<master-node-ip>:<master-node-port>'\n```\n\nthen on all other nodes, run:\n```bash\nray start --address='<master-node-ip>:<master-node-port>' # <master-node-ip> and <master-node-port> are from above loggings!\n```\n\nfinally on master node, just run:\n```bash\npython -m playground.orz_32b_ppo\n```\nYour training log will be shown in the master node terminal.\n\n------\n\n#### Start ORZ-0.5B PPO Training\nYou can start the ORZ-0.5B PPO training in single A800/H800 node:\n```bash\npython -m playground.orz_0p5b_ppo\n```\n\nYou can even run in **a single A800/H800 gpu**: \n```bash\npython -m playground.orz_0p5b_ppo_1gpu\n```\n\nnote: since we are not in multi-node setting, no `ray start` like logics are needed.\n\n------\n\n#### Start ORZ-7B PPO Training\n\nMulti-node Training on 4 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n\n# then on master node, run:\npython -m playground.orz_7b_ppo\n```\n\nYour training log will be shown in the master node terminal.\n\n-----\n\n#### Start ORZ-1.5B PPO Training\n\nMulti-node Training on 2 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n# then on master node, run:\npython -m playground.orz_1p5b_ppo\n```\n\n----\n\n#### Debug Settings\nIn the code, we leave an environment variable `DEBUG_MODE` to run in debug setting for researcher to iterate. (Thought for now, we recommend using `python -m playground.orz_0p5b_ppo_1gpu` for debugging.)\n\nThe debug running command examples:\n```bash\n# NOTE: just for debug, not final setting!\n\n## Debug command in a single GPU with `EleutherAI/pythia-14m`\nDEBUG_MODE=True python -m playground.orz_14m_ppo_mini\n## Debug command in a single node (8 GPUs) with `Qwen/Qwen2.5-7B`\nDEBUG_MODE=True python -m playground.orz_7b_ppo\n```\n\n### How to Use the Model\n#### Policy Model\nPolicy models can be used in the same way as any chat model in transformers and vllm, since we have put the chat template jinja in the tokenizer.\n\n#### Critic Model\nCritic models can be loaded the same way like in the [training code](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738). \n\n\n## Acknowledgements \ud83d\udc96 \n\n- This work was supported by computing resources and valuable feedback provided by [StepFun](https://www.stepfun.com/) and Tsinghua University.\n- Our training framework is built on [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF), [vllm](https://github.com/vllm-project/vllm), [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) and [ray](https://github.com/ray-project/ray).\n- Our model is based on [Qwen2.5 Series](https://qwenlm.github.io/blog/qwen2.5-llm/) of **base models**, including [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B), [Qwen2.5-1.5B](https://huggingface.co/Qwen/Qwen2.5-1.5B), [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B) and [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B).\n- We thank [Project Numina](https://projectnumina.ai/), [Tulu3](https://allenai.org/blog/tulu-3-technical) and [OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k) for their collected open sourced data.\n\n## Advertisement Time \ud83d\udce3\n\nWe are hiring talented researchers and engineers to join our team. If you are interested in our project and would like to contribute to the reasoner scale-up all the way to AGI, please feel free to reach out to us at hanqer@stepfun.com\n\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline)](https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline)\n\n## Community Discussions \ud83c\udf7a\n\nWe have several wechat groups to help discussions and sharing, you can scan the QR code below to join the latest group.\n\n<img src=\"figure/WeChatGroup.png\" width=\"300\" style=\"display: block; margin: 0 auto;\"/>\n\n## Citation\n\n```bibtex\n@misc{hu2025openreasonerzeroopensourceapproach,\n      title={Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model}, \n      author={Jingcheng Hu and Yinmin Zhang and Qi Han and Daxin Jiang and Xiangyu Zhang and Heung-Yeung Shum},\n      year={2025},\n      eprint={2503.24290},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2503.24290}, \n}\n```\n",
  "external_links_in_readme": [
    "https://github.com/deepspeedai/DeepSpeed",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json",
    "https://huggingface.co/Qwen/Qwen2.5-0.5B",
    "https://arxiv.org/abs/2503.24290",
    "https://huggingface.co/Open-Reasoner-Zero\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B",
    "https://www.stepfun.com/",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\"",
    "https://arxiv.org/abs/2503.24290},",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py",
    "https://github.com/vllm-project/vllm",
    "https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline",
    "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k",
    "https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>",
    "https://arxiv.org/abs/2503.24290\"><b>Paper",
    "https://github.com/ray-project/ray",
    "https://projectnumina.ai/",
    "https://qwenlm.github.io/blog/qwen2.5-llm/",
    "https://huggingface.co/Qwen/Qwen2.5-7B",
    "https://huggingface.co/Qwen/Qwen2.5-1.5B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B",
    "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\"",
    "https://github.com/OpenRLHF/OpenRLHF",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json",
    "https://huggingface.co/Qwen/Qwen2.5-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py",
    "https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>",
    "https://allenai.org/blog/tulu-3-technical",
    "https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B",
    "https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline",
    "https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B"
  ]
}
```

</details>


---

## Repository 11: Open-Reasoner-Zero/Open-Reasoner-Zero

# GitHub Repository Data

**Repository:** [Open-Reasoner-Zero/Open-Reasoner-Zero](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero)

## Basic Information

- **Description:** Official Repo for Open-Reasoner-Zero
- **Created:** 2025-02-19T17:28:25+00:00
- **Last Updated:** 2025-06-20T07:02:06+00:00
- **Last Pushed:** 2025-06-02T16:36:00+00:00
- **Default Branch:** main
- **Size:** 16502 KB

## Statistics

- **Stars:** 1,968
- **Forks:** 104
- **Watchers:** 1,968
- **Open Issues:** 19
- **Total Issues:** 0
- **Pull Requests:** 5

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE)

## Languages

- **Python:** 337,249 bytes
- **Dockerfile:** 1,739 bytes

## Top Contributors

1. **REIGN12** - 21 contributions
2. **vwxyzjn** - 5 contributions
3. **YinminZhang** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 71

- `.flake8` (blob)
- `.gitignore` (blob)
- `LICENSE` (blob)
- `ORZ_paper.pdf` (blob)
- `README.md` (blob)
- `data` (tree)
- `data/eval_data` (tree)
- `data/eval_data/aime2024.json` (blob)
- `data/eval_data/gpqa_diamond.json` (blob)
- `data/eval_data/math500.json` (blob)

## Recent Issues

- 🟢 **#73** DeepSpeed does not detect CUDA for some reason: (open)
- 🟢 **#72** In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map="meta"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device("meta")` (open)
- 🟢 **#71** In `orz/ppo/actors.py`: `with torch.device("meta"): AutoModel.from_pretrained(...)` throws (open)
- 🟢 **#70** `llm_engine.model_executor` seems not available on modern vllm (0.8.5) (open)
- 🟢 **#69** On newer vllm should use `llm_engine.vllm_config` and similar (open)

## Recent Pull Requests

- 🟢 **#66** 重构 PPOExpConfig 类，继承自 CommonPPOConfig，简化配置管理 (open)
- 🔴 **#62** Major Updates for Open-Reasoner-Zero (closed)
- 🟢 **#57** Support FSDP and vllm colocate with tp size > 1 (open)
- 🔴 **#32** Local ppo mini (closed)
- 🔴 **#31** Add a mini example (closed)

## Recent Commits

- **3fdd9a07** doc: update ORZ-R1-Distill-Qwen-14B results - REIGN12 (2025-06-02T16:35:57+00:00)
- **a2351927** chores: update qr code - REIGN12 (2025-06-02T16:35:25+00:00)
- **20e5bc83** README: update readme - REIGN12 (2025-04-08T12:58:51+00:00)
- **9c73e259** README: update readme - REIGN12 (2025-04-02T03:41:38+00:00)
- **a27ce5c1** README: update readme - REIGN12 (2025-04-02T03:38:35+00:00)
- **a288753d** README: update readme - REIGN12 (2025-03-31T15:53:39+00:00)
- **a7193f2c** Major Updates for Open-Reasoner-Zero (#62) - Jingcheng Hu (2025-03-31T15:45:50+00:00)
- **e008f6d9** Fix: bug fix for weight sync logic when colocate=False - REIGN12 (2025-03-05T08:08:42+00:00)
- **effe6fb8** Fix: fix typo in report - REIGN12 (2025-03-01T09:03:59+00:00)
- **9464093c** Fix: minor bugfix for dockerfile - REIGN12 (2025-03-01T08:54:56+00:00)

## External Links Found in README

- https://github.com/deepspeedai/DeepSpeed
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json
- https://huggingface.co/Qwen/Qwen2.5-0.5B
- https://arxiv.org/abs/2503.24290
- https://huggingface.co/Open-Reasoner-Zero"
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B
- https://www.stepfun.com/
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
- https://arxiv.org/abs/2503.24290},
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py
- https://github.com/vllm-project/vllm
- https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline
- https://huggingface.co/datasets/open-r1/OpenR1-Math-220k
- https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor"/></a>
- https://arxiv.org/abs/2503.24290"><b>Paper
- https://github.com/ray-project/ray
- https://projectnumina.ai/

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 935587127,
  "name": "Open-Reasoner-Zero",
  "full_name": "Open-Reasoner-Zero/Open-Reasoner-Zero",
  "description": "Official Repo for Open-Reasoner-Zero",
  "html_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero",
  "clone_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "ssh_url": "git@github.com:Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "homepage": "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903",
  "topics": [],
  "default_branch": "main",
  "created_at": "2025-02-19T17:28:25+00:00",
  "updated_at": "2025-06-20T07:02:06+00:00",
  "pushed_at": "2025-06-02T16:36:00+00:00",
  "size_kb": 16502,
  "watchers_count": 1968,
  "stargazers_count": 1968,
  "forks_count": 104,
  "open_issues_count": 19,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE"
  },
  "languages": {
    "Python": 337249,
    "Dockerfile": 1739
  },
  "top_contributors": [
    {
      "login": "REIGN12",
      "contributions": 21
    },
    {
      "login": "vwxyzjn",
      "contributions": 5
    },
    {
      "login": "YinminZhang",
      "contributions": 3
    }
  ],
  "file_tree_count": 71,
  "file_tree_sample": [
    {
      "path": ".flake8",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "ORZ_paper.pdf",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "data",
      "type": "tree"
    },
    {
      "path": "data/eval_data",
      "type": "tree"
    },
    {
      "path": "data/eval_data/aime2024.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/gpqa_diamond.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/math500.json",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 5,
  "recent_issues": [
    {
      "number": 73,
      "title": "DeepSpeed does not detect CUDA for some reason:",
      "state": "open"
    },
    {
      "number": 72,
      "title": "In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map=\"meta\"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device(\"meta\")`",
      "state": "open"
    },
    {
      "number": 71,
      "title": "In `orz/ppo/actors.py`: `with torch.device(\"meta\"): AutoModel.from_pretrained(...)` throws",
      "state": "open"
    },
    {
      "number": 70,
      "title": "`llm_engine.model_executor` seems not available on modern vllm (0.8.5)",
      "state": "open"
    },
    {
      "number": 69,
      "title": "On newer vllm should use `llm_engine.vllm_config` and similar",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 66,
      "title": "\u91cd\u6784 PPOExpConfig \u7c7b\uff0c\u7ee7\u627f\u81ea CommonPPOConfig\uff0c\u7b80\u5316\u914d\u7f6e\u7ba1\u7406",
      "state": "open"
    },
    {
      "number": 62,
      "title": "Major Updates for Open-Reasoner-Zero",
      "state": "closed"
    },
    {
      "number": 57,
      "title": "Support FSDP and vllm colocate with tp size > 1",
      "state": "open"
    },
    {
      "number": 32,
      "title": "Local ppo mini",
      "state": "closed"
    },
    {
      "number": 31,
      "title": "Add a mini example",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "3fdd9a07b4fb01e06005e6e74fc56690cde8a341",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:57+00:00",
      "message": "doc: update ORZ-R1-Distill-Qwen-14B results"
    },
    {
      "sha": "a23519277efdd71a063e1f2913d5c496a5f96180",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:25+00:00",
      "message": "chores: update qr code"
    },
    {
      "sha": "20e5bc83ae75695fded4e1a59a21e487063e24d9",
      "author": "REIGN12",
      "date": "2025-04-08T12:58:51+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "9c73e2590d23a7245a8d6341337952828110c631",
      "author": "REIGN12",
      "date": "2025-04-02T03:41:38+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a27ce5c1c5204d267c821542af4bdb47c70aa40a",
      "author": "REIGN12",
      "date": "2025-04-02T03:38:35+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a288753de916c0f3ba932fb0d1c218a49b8a403b",
      "author": "REIGN12",
      "date": "2025-03-31T15:53:39+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a7193f2c14aa90d055f9c161bf4566ec41ced30c",
      "author": "Jingcheng Hu",
      "date": "2025-03-31T15:45:50+00:00",
      "message": "Major Updates for Open-Reasoner-Zero (#62)"
    },
    {
      "sha": "e008f6d95f0b9a0e992f6b8bac912515b50a4634",
      "author": "REIGN12",
      "date": "2025-03-05T08:08:42+00:00",
      "message": "Fix: bug fix for weight sync logic when colocate=False"
    },
    {
      "sha": "effe6fb81570ccfde7d28d8d335c83127e95263f",
      "author": "REIGN12",
      "date": "2025-03-01T09:03:59+00:00",
      "message": "Fix: fix typo in report"
    },
    {
      "sha": "9464093c67c32a5fca69556b603f915e55fc00a5",
      "author": "REIGN12",
      "date": "2025-03-01T08:54:56+00:00",
      "message": "Fix: minor bugfix for dockerfile"
    },
    {
      "sha": "f594903dc0e953a8d0429ae0ecf339efcbe456f7",
      "author": "Jingcheng Hu",
      "date": "2025-02-25T15:03:35+00:00",
      "message": "Merge pull request #31 from vwxyzjn/ppo-mini"
    },
    {
      "sha": "ce912e108ca03d5ded42e151e3c857d44fc3f277",
      "author": "Costa Huang",
      "date": "2025-02-24T20:33:48+00:00",
      "message": "quick change"
    },
    {
      "sha": "34d8abc52ecba9c1c620350eb70110d05ea6b676",
      "author": "Costa Huang",
      "date": "2025-02-24T20:32:51+00:00",
      "message": "set eval to false"
    },
    {
      "sha": "55904eae7c35932e8ee2efb717c788ed3cd9b75e",
      "author": "Costa Huang",
      "date": "2025-02-24T20:31:44+00:00",
      "message": "quick change"
    },
    {
      "sha": "dbc40d8042c4981138b58773ca4936570e2339be",
      "author": "Costa Huang",
      "date": "2025-02-24T20:28:34+00:00",
      "message": "Add single GPU exampel"
    },
    {
      "sha": "ec2ce7e528f660f3b05c19369cdcff9d89cbb6b0",
      "author": "Costa Huang",
      "date": "2025-02-24T20:06:00+00:00",
      "message": "Add a mini example"
    },
    {
      "sha": "7b5dec17a8dda164e8a4aaba33393afc3e0e95d9",
      "author": "Jingcheng Hu",
      "date": "2025-02-24T09:18:55+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "3ccec9eec8717a5de0575918237bc81b6ba3e3a2",
      "author": "REIGN12",
      "date": "2025-02-24T07:24:11+00:00",
      "message": "Merge branch 'main' of https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
    },
    {
      "sha": "df77747d28c8580db768732f8da34f913e0411ca",
      "author": "REIGN12",
      "date": "2025-02-24T07:23:34+00:00",
      "message": "Fix: bug fix for readme and 32b exp"
    },
    {
      "sha": "ec6b01ef61f6b66a30bff0ea763b37fe4e9d00d1",
      "author": "hanqer",
      "date": "2025-02-24T05:52:43+00:00",
      "message": "fix: fix the package dependencies of pip and apt."
    }
  ],
  "readme_text": "<div align=\"center\">\n\n# Open Reasoner Zero\n\n<img src=\"figure/logo.jpg\" width=\"300\"/>\n\n<div>\n\nAn Open Source Approach to Scaling Up Reinforcement Learning on the Base Model\n</div>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n    <a href=\"https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\" style=\"margin: 2px;\"><img alt=\"Code\" src=\"https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/></a>\n  \n  <a href=\"https://huggingface.co/Open-Reasoner-Zero\" target=\"_blank\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>\n\n  <a href=\"https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\" target=\"_blank\">\n  <img alt=\"Notion Page\"\n    src=\"https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>\n\n  <br>\n  <a href=\"https://arxiv.org/abs/2503.24290\"><b>Paper Arxiv Link </b>\ud83d\udc41\ufe0f</a>\n</div>\n\n<div>\n<br>\n\n</div>\n\n## Overview \ud83c\udf0a\nWe introduce **Open-Reasoner-Zero**, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility.\nUsing the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency\u2014requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline.\n\nTo enable broader participation in this pivotal moment we witnessed and accelerate research towards artificial general intelligence (AGI), \nwe release our source code, parameter settings, training data, and model weights.\nPlease refer to our [paper](https://arxiv.org/abs/2503.24290) for more insights across various model sizes. \n\n**Let the Reasoner-Zero tide rise!**\n\n\n## Main Results \ud83c\udfc6\n\n![](figure/teaser.png)\n\n*Figure 1 | Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\}. Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\} on benchmarks (averaged on 16 responses) during training. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, Open-Reasoner-Zero-32B achieves superior performance on AIME2024, MATH500, and GPQA Diamond benchmark-requiring only a tenth of the training steps.*\n\n![](figure/train_curve.png)\n*Figure 2 | Train-time Scale up on Train Reward and Response Length of Open-Reasoner-Zero (ORZ) - \\{0.5B, 1.5B, 7B, 32B\\}. Train Reward and Response Length increase steadily, demonstrating consistent scalability across model sizes. Interestingly, the ORZ-32B Response Length exhibits fluctuations without negatively impacting training stability, highlighting the robustness of our minimalist recipe.*\n\n## Releases \ud83d\udce6\n\n<strong>[2025/06/03]</strong>\nWe release [ORZ-R1-Distill-Qwen-14B](https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B), obtained by applying ORZ recipe to reasoning-enhanced models like DeepSeek-R1-Distill-Qwen-14B. This ORZ-R1-Distill-Qwen-14B achieves strong results on reasoning benchmarks, even surpassing the larger DeepSeek-R1-Distill-Qwen-32B model.\n\n| Model                        | AIME 2024 | AIME 2025 | MATH500 | GPQA Dia. |\n| ---------------------------- | --------- | --------- | ------- | --------- |\n| DeepSeek-R1-Distill-Qwen-14B | 69.7      | 49.1      | 93.9    | 59.1      |\n| DeepSeek-R1-Distill-Qwen-32B | 72.6      | 60.0      | 94.3    | **62.1**     |\n| **ORZ-R1-Distill-Qwen-14B**  | **75.2**  | **60.0**  | **95.6** | 60.4  |\n\n\n<strong>[2025/03/31]</strong>\nWe announce a major milestone for `Open-Reasoner-Zero`:\n\n- \ud83c\udf0a [Updated Paper](https://arxiv.org/abs/2503.24290) with new results.\n- \ud83d\udd2d [Easy-to-use Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground):\n  - [ORZ-1.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py) and [ORZ-0.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py) (main results in Figure 2). \n  - [Minimal resource training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py): ORZ-0.5B can be run on a single A800/H800 gpu!\n- \ud83e\udd29 [Updated Curated Datasets](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data): \n  - 129k data in total:\n    - [original 57k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json).\n    - [extended 72k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json).\n  - [13k hard data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json) mined from the above 129k data. \n    - used in the \"annealing\" stage of ORZ-32B training: **AIME2024 from ~41% to ~48%**!\n- \ud83e\udd17 More HF Models: \n  - Updated HF Models: [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B).\n  - Released HF Models: [`Open-Reasoner-Zero-1.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B) and [`Open-Reasoner-Zero-0.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B).\n- \ud83d\ude80 Full Suite of Critic Models for in-depth research: `Open-Reasoner-Zero-Critic-`{[0.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B), [1.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B), [7B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B),  [32B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B)}.\n\n<strong>[2025/02/18]</strong>\nWe release `Open-Reasoner-Zero`. \n\nAs part of this release, we open-source:\n- \ud83c\udf0a [Paper(WIP)](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf) on our comprehensive analysis and insights in Reasoner-Zero training\n- \ud83e\udd17 HF Model [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B)\n- \ud83c\udf81 [`Our curated 57k training data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data)\n- \ud83d\udcc4 [Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground) to enjoy your own Reasoner-Zero journey!\n\n## Key Features in Codebase \ud83d\udd11\n\n- Adopt single controller trainer design, flexible and researcher-friendly.\n- Colocate training and generation in the same GPUs to maximize GPU utilization.\n\n## Getting Started \ud83d\ude80\n### Data\n\nWe release all of curated high-quality training data in the [`data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data) folder:\n* curated 129k data:\n  * [original 57k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json), collected from various sources, including AIME (up to 2023), MATH, Numina-Math collection and Tulu3 MATH.\n  * [extended 72k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json), mainly cleaned from OpenR1-Math-220k.\n* [hard 13k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json), mined from the first stage of ORZ-32B training.\n\nThe details for how to collect data are described in our [paper](https://arxiv.org/abs/2503.24290).\n\n### Installation & Training Scripts\nWe release our [Dockerfile](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile) in [docker](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker) folder to facilitate the reproducibility of our training.\n\nTo install the package, run:\n```bash\npip install -e .\n```\n\n#### Start ORZ-32B PPO Training\nHere are the starting commands in 16 nodes. \n\nFirst on master node, run:\n```bash\nray start --head\n# you will see logging like:\n# Next steps\n#  To add another node to this Ray cluster, run\n#    ray start --address='<master-node-ip>:<master-node-port>'\n```\n\nthen on all other nodes, run:\n```bash\nray start --address='<master-node-ip>:<master-node-port>' # <master-node-ip> and <master-node-port> are from above loggings!\n```\n\nfinally on master node, just run:\n```bash\npython -m playground.orz_32b_ppo\n```\nYour training log will be shown in the master node terminal.\n\n------\n\n#### Start ORZ-0.5B PPO Training\nYou can start the ORZ-0.5B PPO training in single A800/H800 node:\n```bash\npython -m playground.orz_0p5b_ppo\n```\n\nYou can even run in **a single A800/H800 gpu**: \n```bash\npython -m playground.orz_0p5b_ppo_1gpu\n```\n\nnote: since we are not in multi-node setting, no `ray start` like logics are needed.\n\n------\n\n#### Start ORZ-7B PPO Training\n\nMulti-node Training on 4 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n\n# then on master node, run:\npython -m playground.orz_7b_ppo\n```\n\nYour training log will be shown in the master node terminal.\n\n-----\n\n#### Start ORZ-1.5B PPO Training\n\nMulti-node Training on 2 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n# then on master node, run:\npython -m playground.orz_1p5b_ppo\n```\n\n----\n\n#### Debug Settings\nIn the code, we leave an environment variable `DEBUG_MODE` to run in debug setting for researcher to iterate. (Thought for now, we recommend using `python -m playground.orz_0p5b_ppo_1gpu` for debugging.)\n\nThe debug running command examples:\n```bash\n# NOTE: just for debug, not final setting!\n\n## Debug command in a single GPU with `EleutherAI/pythia-14m`\nDEBUG_MODE=True python -m playground.orz_14m_ppo_mini\n## Debug command in a single node (8 GPUs) with `Qwen/Qwen2.5-7B`\nDEBUG_MODE=True python -m playground.orz_7b_ppo\n```\n\n### How to Use the Model\n#### Policy Model\nPolicy models can be used in the same way as any chat model in transformers and vllm, since we have put the chat template jinja in the tokenizer.\n\n#### Critic Model\nCritic models can be loaded the same way like in the [training code](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738). \n\n\n## Acknowledgements \ud83d\udc96 \n\n- This work was supported by computing resources and valuable feedback provided by [StepFun](https://www.stepfun.com/) and Tsinghua University.\n- Our training framework is built on [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF), [vllm](https://github.com/vllm-project/vllm), [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) and [ray](https://github.com/ray-project/ray).\n- Our model is based on [Qwen2.5 Series](https://qwenlm.github.io/blog/qwen2.5-llm/) of **base models**, including [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B), [Qwen2.5-1.5B](https://huggingface.co/Qwen/Qwen2.5-1.5B), [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B) and [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B).\n- We thank [Project Numina](https://projectnumina.ai/), [Tulu3](https://allenai.org/blog/tulu-3-technical) and [OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k) for their collected open sourced data.\n\n## Advertisement Time \ud83d\udce3\n\nWe are hiring talented researchers and engineers to join our team. If you are interested in our project and would like to contribute to the reasoner scale-up all the way to AGI, please feel free to reach out to us at hanqer@stepfun.com\n\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline)](https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline)\n\n## Community Discussions \ud83c\udf7a\n\nWe have several wechat groups to help discussions and sharing, you can scan the QR code below to join the latest group.\n\n<img src=\"figure/WeChatGroup.png\" width=\"300\" style=\"display: block; margin: 0 auto;\"/>\n\n## Citation\n\n```bibtex\n@misc{hu2025openreasonerzeroopensourceapproach,\n      title={Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model}, \n      author={Jingcheng Hu and Yinmin Zhang and Qi Han and Daxin Jiang and Xiangyu Zhang and Heung-Yeung Shum},\n      year={2025},\n      eprint={2503.24290},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2503.24290}, \n}\n```\n",
  "external_links_in_readme": [
    "https://github.com/deepspeedai/DeepSpeed",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json",
    "https://huggingface.co/Qwen/Qwen2.5-0.5B",
    "https://arxiv.org/abs/2503.24290",
    "https://huggingface.co/Open-Reasoner-Zero\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B",
    "https://www.stepfun.com/",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\"",
    "https://arxiv.org/abs/2503.24290},",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py",
    "https://github.com/vllm-project/vllm",
    "https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline",
    "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k",
    "https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>",
    "https://arxiv.org/abs/2503.24290\"><b>Paper",
    "https://github.com/ray-project/ray",
    "https://projectnumina.ai/",
    "https://qwenlm.github.io/blog/qwen2.5-llm/",
    "https://huggingface.co/Qwen/Qwen2.5-7B",
    "https://huggingface.co/Qwen/Qwen2.5-1.5B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B",
    "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\"",
    "https://github.com/OpenRLHF/OpenRLHF",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json",
    "https://huggingface.co/Qwen/Qwen2.5-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py",
    "https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>",
    "https://allenai.org/blog/tulu-3-technical",
    "https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B",
    "https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline",
    "https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B"
  ]
}
```

</details>


---

## Repository 12: Open-Reasoner-Zero/Open-Reasoner-Zero"

# GitHub Data Extraction Error

Error: 404 {"message": "Not Found", "documentation_url": "https://docs.github.com/rest/repos/repos#get-a-repository", "status": "404"}

Repository: https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"

---

## Repository 13: vllm-project/vllm

# GitHub Repository Data

**Repository:** [vllm-project/vllm](https://github.com/vllm-project/vllm)

## Basic Information

- **Description:** A high-throughput and memory-efficient inference and serving engine for LLMs
- **Created:** 2023-02-09T11:23:20+00:00
- **Last Updated:** 2025-06-22T02:31:54+00:00
- **Last Pushed:** 2025-06-21T20:57:19+00:00
- **Default Branch:** main
- **Size:** 59796 KB

## Statistics

- **Stars:** 50,400
- **Forks:** 8,235
- **Watchers:** 50,400
- **Open Issues:** 2,653
- **Total Issues:** 0
- **Pull Requests:** 10,104

## License

- **Type:** Apache License 2.0
- **SPDX ID:** Apache-2.0
- **URL:** [License](https://github.com/vllm-project/vllm/blob/main/LICENSE)

## Languages

- **Python:** 16,001,090 bytes
- **Cuda:** 1,729,659 bytes
- **C++:** 686,819 bytes
- **Shell:** 128,926 bytes
- **C:** 93,474 bytes
- **CMake:** 62,225 bytes
- **Dockerfile:** 16,413 bytes
- **Jinja:** 1,650 bytes

## Topics

- `gpt`
- `llm`
- `pytorch`
- `llmops`
- `mlops`
- `model-serving`
- `transformer`
- `llm-serving`
- `inference`
- `llama`
- `amd`
- `rocm`
- `cuda`
- `inferentia`
- `trainium`
- `tpu`
- `xpu`
- `hpu`
- `deepseek`
- `qwen`

## Top Contributors

1. **WoosukKwon** - 569 contributions
2. **DarkLight1337** - 444 contributions
3. **youkaichao** - 443 contributions
4. **mgoin** - 254 contributions
5. **Isotr0py** - 195 contributions
6. **hmellor** - 172 contributions
7. **simon-mo** - 156 contributions
8. **njhill** - 151 contributions
9. **ywang96** - 143 contributions
10. **jeejeelee** - 143 contributions

## File Structure (Sample of 10 files)

Total files: 2,941

- `.buildkite` (tree)
- `.buildkite/check-wheel-size.py` (blob)
- `.buildkite/generate_index.py` (blob)
- `.buildkite/lm-eval-harness` (tree)
- `.buildkite/lm-eval-harness/configs` (tree)
- `.buildkite/lm-eval-harness/configs/DeepSeek-V2-Lite-Chat.yaml` (blob)
- `.buildkite/lm-eval-harness/configs/Meta-Llama-3-70B-Instruct-FBGEMM-nonuniform.yaml` (blob)
- `.buildkite/lm-eval-harness/configs/Meta-Llama-3-70B-Instruct.yaml` (blob)
- `.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-Channelwise-compressed-tensors.yaml` (blob)
- `.buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-FBGEMM-nonuniform.yaml` (blob)

## Recent Issues

- 🟢 **#19940** [Bugfix] fix sampling seeding being off when sequences are prempted (open)
- 🟢 **#19939** [PERF] Speedup of MRoPE prepare inputs (open)
- 🟢 **#19938** [Bug]: openapi doesnt generate for Java cleanly AnyOf for Stop has a default value (open)
- 🟢 **#19937** [BugFix] Fix multi-node offline data parallel (open)
- 🟢 **#19936** [Usage]: mac run vllm failed by docker  (open)

## Recent Pull Requests

- 🟢 **#19940** [Bugfix] fix sampling seeding being off when sequences are prempted (open)
- 🟢 **#19939** [PERF] Speedup of MRoPE prepare inputs (open)
- 🟢 **#19937** [BugFix] Fix multi-node offline data parallel (open)
- 🔴 **#19932** [Docs] Add GPT2ForSequenceClassification to supported models in docs (closed)
- 🔴 **#19931** [Docs] Add GPT2ForSequenceClassification to supported models in docs (closed)

## Recent Commits

- **3b1e4c6a** [Docs] Add GPT2ForSequenceClassification to supported models in docs (#19932) - Adrian (2025-06-21T20:57:19+00:00)
- **2c5302fa** [Multimodal] Optimize Qwen2/2.5-VL startup time (#19756) - Woosuk Kwon (2025-06-21T20:01:07+00:00)
- **caa680fd** [doc] add contact us in community (#19922) - Reid (2025-06-21T17:29:06+00:00)
- **c3bf9bad** [New model support]Support Tarsier2 (#19887) - 汪志鹏 (2025-06-21T04:01:51+00:00)
- **6f170f11** [Bugfix] Fix bnb 8bit model weights loading (#19917) - Isotr0py (2025-06-21T03:29:09+00:00)
- **8ca81bb0** Fix: Check the type of params to be a Sequence not list. (#19910) - Rabin Adhikari (2025-06-20T23:03:17+00:00)
- **e773a9e1** [Misc] Clean up useless code (#19889) - wangxiyuan (2025-06-20T21:09:09+00:00)
- **71baf85a** [Kernel] mark TorchSDPABackend swap_blocks NotImplementedError (#19749) - Ning Xie (2025-06-20T18:18:11+00:00)
- **79f2f1c2** [CPU][CI] Fallback sliding window to v0 and fix CPU pooling model tests (#19901) - Li, Jiang (2025-06-20T15:30:36+00:00)
- **2e3e3c86** Export NaNs in logits to scheduler_stats if output is corrupted (#18777) - Vlad Tiberiu Mihailescu (2025-06-20T14:47:16+00:00)

## External Links Found in README

- https://arxiv.org/abs/2210.17323
- https://github.com/vllm-project/vllm/security/advisories
- https://lu.ma/vllm-ollama
- https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing
- https://slack.vllm.ai
- https://pytorch.org/blog/vllm-joins-pytorch
- https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing
- https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing
- https://chat.lmsys.org
- https://blog.vllm.ai/"><b>Blog</b></a>
- https://lu.ma/zep56hui
- https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg
- https://lu.ma/first-vllm-meetup
- https://blog.vllm.ai/2024/09/05/perf-update.html
- https://discuss.vllm.ai"><b>User
- https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing
- https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing
- https://docs.vllm.ai/en/latest/models/supported_models.html
- https://lu.ma/ygxbpzhl
- https://slack.vllm.ai"><b>Developer

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 599547518,
  "name": "vllm",
  "full_name": "vllm-project/vllm",
  "description": "A high-throughput and memory-efficient inference and serving engine for LLMs",
  "html_url": "https://github.com/vllm-project/vllm",
  "clone_url": "https://github.com/vllm-project/vllm.git",
  "ssh_url": "git@github.com:vllm-project/vllm.git",
  "homepage": "https://docs.vllm.ai",
  "topics": [
    "gpt",
    "llm",
    "pytorch",
    "llmops",
    "mlops",
    "model-serving",
    "transformer",
    "llm-serving",
    "inference",
    "llama",
    "amd",
    "rocm",
    "cuda",
    "inferentia",
    "trainium",
    "tpu",
    "xpu",
    "hpu",
    "deepseek",
    "qwen"
  ],
  "default_branch": "main",
  "created_at": "2023-02-09T11:23:20+00:00",
  "updated_at": "2025-06-22T02:31:54+00:00",
  "pushed_at": "2025-06-21T20:57:19+00:00",
  "size_kb": 59796,
  "watchers_count": 50400,
  "stargazers_count": 50400,
  "forks_count": 8235,
  "open_issues_count": 2653,
  "license": {
    "key": "apache-2.0",
    "name": "Apache License 2.0",
    "spdx_id": "Apache-2.0",
    "url": "https://github.com/vllm-project/vllm/blob/main/LICENSE"
  },
  "languages": {
    "Python": 16001090,
    "Cuda": 1729659,
    "C++": 686819,
    "Shell": 128926,
    "C": 93474,
    "CMake": 62225,
    "Dockerfile": 16413,
    "Jinja": 1650
  },
  "top_contributors": [
    {
      "login": "WoosukKwon",
      "contributions": 569
    },
    {
      "login": "DarkLight1337",
      "contributions": 444
    },
    {
      "login": "youkaichao",
      "contributions": 443
    },
    {
      "login": "mgoin",
      "contributions": 254
    },
    {
      "login": "Isotr0py",
      "contributions": 195
    },
    {
      "login": "hmellor",
      "contributions": 172
    },
    {
      "login": "simon-mo",
      "contributions": 156
    },
    {
      "login": "njhill",
      "contributions": 151
    },
    {
      "login": "ywang96",
      "contributions": 143
    },
    {
      "login": "jeejeelee",
      "contributions": 143
    },
    {
      "login": "russellb",
      "contributions": 142
    },
    {
      "login": "zhuohan123",
      "contributions": 111
    },
    {
      "login": "robertgshaw2-redhat",
      "contributions": 109
    },
    {
      "login": "reidliu41",
      "contributions": 104
    },
    {
      "login": "tlrmchlsmth",
      "contributions": 99
    },
    {
      "login": "khluu",
      "contributions": 80
    },
    {
      "login": "LucasWilkinson",
      "contributions": 70
    },
    {
      "login": "comaniac",
      "contributions": 67
    },
    {
      "login": "Yard1",
      "contributions": 65
    },
    {
      "login": "heheda12345",
      "contributions": 53
    }
  ],
  "file_tree_count": 2941,
  "file_tree_sample": [
    {
      "path": ".buildkite",
      "type": "tree"
    },
    {
      "path": ".buildkite/check-wheel-size.py",
      "type": "blob"
    },
    {
      "path": ".buildkite/generate_index.py",
      "type": "blob"
    },
    {
      "path": ".buildkite/lm-eval-harness",
      "type": "tree"
    },
    {
      "path": ".buildkite/lm-eval-harness/configs",
      "type": "tree"
    },
    {
      "path": ".buildkite/lm-eval-harness/configs/DeepSeek-V2-Lite-Chat.yaml",
      "type": "blob"
    },
    {
      "path": ".buildkite/lm-eval-harness/configs/Meta-Llama-3-70B-Instruct-FBGEMM-nonuniform.yaml",
      "type": "blob"
    },
    {
      "path": ".buildkite/lm-eval-harness/configs/Meta-Llama-3-70B-Instruct.yaml",
      "type": "blob"
    },
    {
      "path": ".buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-Channelwise-compressed-tensors.yaml",
      "type": "blob"
    },
    {
      "path": ".buildkite/lm-eval-harness/configs/Meta-Llama-3-8B-Instruct-FBGEMM-nonuniform.yaml",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 10104,
  "recent_issues": [
    {
      "number": 19940,
      "title": "[Bugfix] fix sampling seeding being off when sequences are prempted",
      "state": "open"
    },
    {
      "number": 19939,
      "title": "[PERF] Speedup of MRoPE prepare inputs",
      "state": "open"
    },
    {
      "number": 19938,
      "title": "[Bug]: openapi doesnt generate for Java cleanly AnyOf for Stop has a default value",
      "state": "open"
    },
    {
      "number": 19937,
      "title": "[BugFix] Fix multi-node offline data parallel",
      "state": "open"
    },
    {
      "number": 19936,
      "title": "[Usage]: mac run vllm failed by docker ",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 19940,
      "title": "[Bugfix] fix sampling seeding being off when sequences are prempted",
      "state": "open"
    },
    {
      "number": 19939,
      "title": "[PERF] Speedup of MRoPE prepare inputs",
      "state": "open"
    },
    {
      "number": 19937,
      "title": "[BugFix] Fix multi-node offline data parallel",
      "state": "open"
    },
    {
      "number": 19932,
      "title": "[Docs] Add GPT2ForSequenceClassification to supported models in docs",
      "state": "closed"
    },
    {
      "number": 19931,
      "title": "[Docs] Add GPT2ForSequenceClassification to supported models in docs",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "3b1e4c6a23af94b76f7e1290ec48d671f1420d8d",
      "author": "Adrian",
      "date": "2025-06-21T20:57:19+00:00",
      "message": "[Docs] Add GPT2ForSequenceClassification to supported models in docs (#19932)"
    },
    {
      "sha": "2c5302fadd81c06f61e5a3973ed4c0e6a4a2be40",
      "author": "Woosuk Kwon",
      "date": "2025-06-21T20:01:07+00:00",
      "message": "[Multimodal] Optimize Qwen2/2.5-VL startup time (#19756)"
    },
    {
      "sha": "caa680fd2e70cc947911f4185750aaa3bcbdd122",
      "author": "Reid",
      "date": "2025-06-21T17:29:06+00:00",
      "message": "[doc] add contact us in community (#19922)"
    },
    {
      "sha": "c3bf9bad11193ee684ed6083b6692d0b5bf2bac7",
      "author": "\u6c6a\u5fd7\u9e4f",
      "date": "2025-06-21T04:01:51+00:00",
      "message": "[New model support]Support Tarsier2 (#19887)"
    },
    {
      "sha": "6f170f11dddcfafa061785d4fb4993f7bcb16107",
      "author": "Isotr0py",
      "date": "2025-06-21T03:29:09+00:00",
      "message": "[Bugfix] Fix bnb 8bit model weights loading (#19917)"
    },
    {
      "sha": "8ca81bb0691bf8909ecb6eb4dd43f4af6dcaaa66",
      "author": "Rabin Adhikari",
      "date": "2025-06-20T23:03:17+00:00",
      "message": "Fix: Check the type of params to be a Sequence not list. (#19910)"
    },
    {
      "sha": "e773a9e1c2c175e193b383ed497ad6fcb73cdfe5",
      "author": "wangxiyuan",
      "date": "2025-06-20T21:09:09+00:00",
      "message": "[Misc] Clean up useless code (#19889)"
    },
    {
      "sha": "71baf85ae11be24d4ea32d30cb5b8dfb0912a6cc",
      "author": "Ning Xie",
      "date": "2025-06-20T18:18:11+00:00",
      "message": "[Kernel] mark TorchSDPABackend swap_blocks NotImplementedError (#19749)"
    },
    {
      "sha": "79f2f1c2a1999d1e7a5202062bad4e115fd9d775",
      "author": "Li, Jiang",
      "date": "2025-06-20T15:30:36+00:00",
      "message": "[CPU][CI] Fallback sliding window to v0 and fix CPU pooling model tests (#19901)"
    },
    {
      "sha": "2e3e3c86dc5d14d0ee8f782f5caedc9b999a63c7",
      "author": "Vlad Tiberiu Mihailescu",
      "date": "2025-06-20T14:47:16+00:00",
      "message": "Export NaNs in logits to scheduler_stats if output is corrupted (#18777)"
    },
    {
      "sha": "7e8977fcd4e9c3bf6b114c7dc715b28a61b5cdb0",
      "author": "Chendi.Xue",
      "date": "2025-06-20T14:44:56+00:00",
      "message": "[custom_op][vllm-plugin] update custom_op class to use op_registry (#19164)"
    },
    {
      "sha": "f1e840e8429614d5bb2f928bcbec0d0469c70415",
      "author": "Adrian",
      "date": "2025-06-20T12:07:41+00:00",
      "message": "[Model] GPT2ForSequenceClassification model (#19663)"
    },
    {
      "sha": "7771d1de882f53863f04d609723b8c29646ee5da",
      "author": "Thomas Parnell",
      "date": "2025-06-20T11:16:48+00:00",
      "message": "[Fix] import regex instead of re (#19875)"
    },
    {
      "sha": "71d1219545b5139bf8f00fc72bdd3682cce62775",
      "author": "Ning Xie",
      "date": "2025-06-20T10:50:13+00:00",
      "message": "[Kernel] correct cpu worker function parameter type (#19745)"
    },
    {
      "sha": "e384f2f10824df7789c6da35256cf957788c0208",
      "author": "Reid",
      "date": "2025-06-20T08:02:21+00:00",
      "message": "[Misc] refactor example - openai_transcription_client (#19851)"
    },
    {
      "sha": "089a306f197dcc1152f2802ba1c56fbdeb86ac27",
      "author": "Reid",
      "date": "2025-06-20T07:25:15+00:00",
      "message": "[Misc] update cuda version (#19526)"
    },
    {
      "sha": "5e666f72cdb1aa7dade649d92a45e93983937fd2",
      "author": "kourosh hakhamaneshi",
      "date": "2025-06-20T05:01:16+00:00",
      "message": "[Bugfix][Ray] Set the cuda context eagerly in the ray worker  (#19583)"
    },
    {
      "sha": "e3a3e4db463d5fc45def4d39d256ccf42fb70044",
      "author": "qli88",
      "date": "2025-06-20T04:43:20+00:00",
      "message": "[Bugfix] Enable PP with AITER+V1 (#19822)"
    },
    {
      "sha": "e41bf15cd04e6681249ab7d382cef6450a2115f5",
      "author": "Xerxes",
      "date": "2025-06-20T04:43:07+00:00",
      "message": "[Chore]: qwen3-moe-type-hints-mistake (#19860)"
    },
    {
      "sha": "5aa4a015ce4c85ad292a2f7d61df60a57ffc75b2",
      "author": "Brayden Zhong",
      "date": "2025-06-20T04:28:55+00:00",
      "message": "[Benchmark] Fix `Value of type \"SampleRequest\" is not indexable` (#18032)"
    }
  ],
  "readme_text": "<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-dark.png\">\n    <img alt=\"vLLM\" src=\"https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-light.png\" width=55%>\n  </picture>\n</p>\n\n<h3 align=\"center\">\nEasy, fast, and cheap LLM serving for everyone\n</h3>\n\n<p align=\"center\">\n| <a href=\"https://docs.vllm.ai\"><b>Documentation</b></a> | <a href=\"https://blog.vllm.ai/\"><b>Blog</b></a> | <a href=\"https://arxiv.org/abs/2309.06180\"><b>Paper</b></a> | <a href=\"https://x.com/vllm_project\"><b>Twitter/X</b></a> | <a href=\"https://discuss.vllm.ai\"><b>User Forum</b></a> | <a href=\"https://slack.vllm.ai\"><b>Developer Slack</b></a> |\n</p>\n\n---\n\n*Latest News* \ud83d\udd25\n- [2025/05] We hosted [NYC vLLM Meetup](https://lu.ma/c1rqyf1f)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?usp=sharing).\n- [2025/05] vLLM is now a hosted project under PyTorch Foundation! Please find the announcement [here](https://pytorch.org/blog/pytorch-foundation-welcomes-vllm/).\n- [2025/04] We hosted [Asia Developer Day](https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing).\n- [2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post [here](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html).\n\n<details>\n<summary>Previous News</summary>\n\n- [2025/03] We hosted [vLLM x Ollama Inference Night](https://lu.ma/vllm-ollama)! Please find the meetup slides from the vLLM team [here](https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/edit?usp=sharing).\n- [2025/03] We hosted [the first vLLM China Meetup](https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg)! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing).\n- [2025/03] We hosted [the East Coast vLLM Meetup](https://lu.ma/7mu4k4xx)! Please find the meetup slides [here](https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0).\n- [2025/02] We hosted [the ninth vLLM meetup](https://lu.ma/h7g3kuj9) with Meta! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing) and AMD [here](https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing). The slides from Meta will not be posted.\n- [2025/01] We hosted [the eighth vLLM meetup](https://lu.ma/zep56hui) with Google Cloud! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing), and Google Cloud team [here](https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing).\n- [2024/12] vLLM joins [pytorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!\n- [2024/11] We hosted [the seventh vLLM meetup](https://lu.ma/h0qvrajz) with Snowflake! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing), and Snowflake team [here](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing).\n- [2024/10] We have just created a developer slack ([slack.vllm.ai](https://slack.vllm.ai)) focusing on coordinating contributions and discussing features. Please feel free to join us there!\n- [2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team [here](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing). Learn more from the [talks](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR) from other vLLM contributors and users!\n- [2024/09] We hosted [the sixth vLLM meetup](https://lu.ma/87q3nvnh) with NVIDIA! Please find the meetup slides [here](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing).\n- [2024/07] We hosted [the fifth vLLM meetup](https://lu.ma/lp0gyjqr) with AWS! Please find the meetup slides [here](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing).\n- [2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post [here](https://blog.vllm.ai/2024/07/23/llama31.html).\n- [2024/06] We hosted [the fourth vLLM meetup](https://lu.ma/agivllm) with Cloudflare and BentoML! Please find the meetup slides [here](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing).\n- [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).\n- [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) with IBM! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).\n- [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) with a16z! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).\n- [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.\n- [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).\n\n</details>\n\n---\n## About\n\nvLLM is a fast and easy-to-use library for LLM inference and serving.\n\nOriginally developed in the [Sky Computing Lab](https://sky.cs.berkeley.edu) at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.\n\nvLLM is fast with:\n\n- State-of-the-art serving throughput\n- Efficient management of attention key and value memory with [**PagedAttention**](https://blog.vllm.ai/2023/06/20/vllm.html)\n- Continuous batching of incoming requests\n- Fast model execution with CUDA/HIP graph\n- Quantizations: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), [AutoRound](https://arxiv.org/abs/2309.05516), INT4, INT8, and FP8\n- Optimized CUDA kernels, including integration with FlashAttention and FlashInfer\n- Speculative decoding\n- Chunked prefill\n\n**Performance benchmark**: We include a performance benchmark at the end of [our blog post](https://blog.vllm.ai/2024/09/05/perf-update.html). It compares the performance of vLLM against other LLM serving engines ([TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [SGLang](https://github.com/sgl-project/sglang) and [LMDeploy](https://github.com/InternLM/lmdeploy)). The implementation is under [nightly-benchmarks folder](.buildkite/nightly-benchmarks/) and you can [reproduce](https://github.com/vllm-project/vllm/issues/8176) this benchmark using our one-click runnable script.\n\nvLLM is flexible and easy to use with:\n\n- Seamless integration with popular Hugging Face models\n- High-throughput serving with various decoding algorithms, including *parallel sampling*, *beam search*, and more\n- Tensor parallelism and pipeline parallelism support for distributed inference\n- Streaming outputs\n- OpenAI-compatible API server\n- Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron\n- Prefix caching support\n- Multi-LoRA support\n\nvLLM seamlessly supports most popular open-source models on HuggingFace, including:\n- Transformer-like LLMs (e.g., Llama)\n- Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)\n- Embedding Models (e.g., E5-Mistral)\n- Multi-modal LLMs (e.g., LLaVA)\n\nFind the full list of supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).\n\n## Getting Started\n\nInstall vLLM with `pip` or [from source](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source):\n\n```bash\npip install vllm\n```\n\nVisit our [documentation](https://docs.vllm.ai/en/latest/) to learn more.\n- [Installation](https://docs.vllm.ai/en/latest/getting_started/installation.html)\n- [Quickstart](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)\n- [List of Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)\n\n## Contributing\n\nWe welcome and value any contributions and collaborations.\nPlease check out [Contributing to vLLM](https://docs.vllm.ai/en/latest/contributing/index.html) for how to get involved.\n\n## Sponsors\n\nvLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!\n\n<!-- Note: Please sort them in alphabetical order. -->\n<!-- Note: Please keep these consistent with docs/community/sponsors.md -->\nCash Donations:\n- a16z\n- Dropbox\n- Sequoia Capital\n- Skywork AI\n- ZhenFund\n\nCompute Resources:\n- AMD\n- Anyscale\n- AWS\n- Crusoe Cloud\n- Databricks\n- DeepInfra\n- Google Cloud\n- Intel\n- Lambda Lab\n- Nebius\n- Novita AI\n- NVIDIA\n- Replicate\n- Roblox\n- RunPod\n- Trainy\n- UC Berkeley\n- UC San Diego\n\nSlack Sponsor: Anyscale\n\nWe also have an official fundraising venue through [OpenCollective](https://opencollective.com/vllm). We plan to use the fund to support the development, maintenance, and adoption of vLLM.\n\n## Citation\n\nIf you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):\n\n```bibtex\n@inproceedings{kwon2023efficient,\n  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},\n  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},\n  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},\n  year={2023}\n}\n```\n\n## Contact Us\n\n- For technical questions and feature requests, please use GitHub [Issues](https://github.com/vllm-project/vllm/issues) or [Discussions](https://github.com/vllm-project/vllm/discussions)\n- For discussing with fellow users, please use the [vLLM Forum](https://discuss.vllm.ai)\n- For coordinating contributions and development, please use [Slack](https://slack.vllm.ai)\n- For security disclosures, please use GitHub's [Security Advisories](https://github.com/vllm-project/vllm/security/advisories) feature\n- For collaborations and partnerships, please contact us at [vllm-questions@lists.berkeley.edu](mailto:vllm-questions@lists.berkeley.edu)\n<!-- Please keep this section up to date with vllm/docs/community/contact_us.md. -->\n\n## Media Kit\n\n- If you wish to use vLLM's logo, please refer to [our media kit repo](https://github.com/vllm-project/media-kit)\n",
  "external_links_in_readme": [
    "https://arxiv.org/abs/2210.17323",
    "https://github.com/vllm-project/vllm/security/advisories",
    "https://lu.ma/vllm-ollama",
    "https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing",
    "https://slack.vllm.ai",
    "https://pytorch.org/blog/vllm-joins-pytorch",
    "https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing",
    "https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing",
    "https://chat.lmsys.org",
    "https://blog.vllm.ai/\"><b>Blog</b></a>",
    "https://lu.ma/zep56hui",
    "https://mp.weixin.qq.com/s/n77GibL2corAtQHtVEAzfg",
    "https://lu.ma/first-vllm-meetup",
    "https://blog.vllm.ai/2024/09/05/perf-update.html",
    "https://discuss.vllm.ai\"><b>User",
    "https://docs.google.com/presentation/d/1REHvfQMKGnvz6p3Fd23HhSO4c8j5WPGZV0bKYLwnHyQ/edit?usp=sharing",
    "https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing",
    "https://docs.vllm.ai/en/latest/models/supported_models.html",
    "https://lu.ma/ygxbpzhl",
    "https://slack.vllm.ai\"><b>Developer",
    "https://lu.ma/agivllm",
    "https://x.com/vllm_project\"><b>Twitter/X</b></a>",
    "https://lu.ma/h0qvrajz",
    "https://github.com/InternLM/lmdeploy",
    "https://docs.google.com/presentation/d/1NHiv8EUFF1NLd3fEYODm56nDmL26lEeXCaDgyDlTsRs/edit#slide=id.g31441846c39_0_0",
    "https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-dark.png\">",
    "https://lu.ma/87q3nvnh",
    "https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR",
    "https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing",
    "https://discuss.vllm.ai",
    "https://lu.ma/7mu4k4xx",
    "https://docs.vllm.ai/en/latest/contributing/index.html",
    "https://docs.vllm.ai\"><b>Documentation</b></a>",
    "https://docs.google.com/presentation/d/16T2PDD1YwRnZ4Tu8Q5r6n53c5Lr5c73UV9Vd2_eBo4U/edit?usp=sharing",
    "https://lu.ma/h7g3kuj9",
    "https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing",
    "https://docs.google.com/presentation/d/19cp6Qu8u48ihB91A064XfaXruNYiBOUKrBxAmDOllOo/edit?usp=sharing",
    "https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing",
    "https://blog.vllm.ai/2023/06/20/vllm.html",
    "https://vllm.ai",
    "https://lu.ma/c1rqyf1f",
    "https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing",
    "https://github.com/vllm-project/vllm/discussions",
    "https://docs.vllm.ai/en/latest/getting_started/quickstart.html",
    "https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/",
    "https://lu.ma/lp0gyjqr",
    "https://github.com/NVIDIA/TensorRT-LLM",
    "https://arxiv.org/abs/2306.00978",
    "https://github.com/vllm-project/vllm/issues/8176",
    "https://arxiv.org/abs/2309.05516",
    "https://opencollective.com/vllm",
    "https://arxiv.org/abs/2309.06180",
    "https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing",
    "https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing",
    "https://pytorch.org/blog/pytorch-foundation-welcomes-vllm/",
    "https://blog.vllm.ai/2024/07/23/llama31.html",
    "https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source",
    "https://docs.google.com/presentation/d/1_q_aW_ioMJWUImf1s1YM-ZhjXz8cUeL0IJvaquOYBeA/edit?usp=sharing",
    "https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing",
    "https://github.com/vllm-project/vllm/issues",
    "https://sky.cs.berkeley.edu",
    "https://docs.vllm.ai/en/latest/getting_started/installation.html",
    "https://github.com/vllm-project/media-kit",
    "https://robloxandvllmmeetup2024.splashthat.com/",
    "https://github.com/sgl-project/sglang",
    "https://docs.google.com/presentation/d/1jzC_PZVXrVNSFVCW-V4cFXb6pn7zZ2CyP_Flwo05aqg/edit?usp=sharing",
    "https://docs.vllm.ai/en/latest/",
    "https://raw.githubusercontent.com/vllm-project/vllm/main/docs/assets/logos/vllm-logo-text-light.png\"",
    "https://drive.google.com/file/d/1Zk5qEJIkTmlQ2eQcXQZlljAx3m9s7nwn/view?usp=sharing",
    "https://www.sginnovate.com/event/limited-availability-morning-evening-slots-remaining-inaugural-vllm-asia-developer-day",
    "https://blog.vllm.ai/2025/01/27/v1-alpha-release.html",
    "https://arxiv.org/abs/2309.06180\"><b>Paper</b></a>"
  ]
}
```

</details>


---

## Repository 14: Open-Reasoner-Zero/Open-Reasoner-Zero

# GitHub Repository Data

**Repository:** [Open-Reasoner-Zero/Open-Reasoner-Zero](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero)

## Basic Information

- **Description:** Official Repo for Open-Reasoner-Zero
- **Created:** 2025-02-19T17:28:25+00:00
- **Last Updated:** 2025-06-20T07:02:06+00:00
- **Last Pushed:** 2025-06-02T16:36:00+00:00
- **Default Branch:** main
- **Size:** 16502 KB

## Statistics

- **Stars:** 1,968
- **Forks:** 104
- **Watchers:** 1,968
- **Open Issues:** 19
- **Total Issues:** 0
- **Pull Requests:** 5

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE)

## Languages

- **Python:** 337,249 bytes
- **Dockerfile:** 1,739 bytes

## Top Contributors

1. **REIGN12** - 21 contributions
2. **vwxyzjn** - 5 contributions
3. **YinminZhang** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 71

- `.flake8` (blob)
- `.gitignore` (blob)
- `LICENSE` (blob)
- `ORZ_paper.pdf` (blob)
- `README.md` (blob)
- `data` (tree)
- `data/eval_data` (tree)
- `data/eval_data/aime2024.json` (blob)
- `data/eval_data/gpqa_diamond.json` (blob)
- `data/eval_data/math500.json` (blob)

## Recent Issues

- 🟢 **#73** DeepSpeed does not detect CUDA for some reason: (open)
- 🟢 **#72** In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map="meta"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device("meta")` (open)
- 🟢 **#71** In `orz/ppo/actors.py`: `with torch.device("meta"): AutoModel.from_pretrained(...)` throws (open)
- 🟢 **#70** `llm_engine.model_executor` seems not available on modern vllm (0.8.5) (open)
- 🟢 **#69** On newer vllm should use `llm_engine.vllm_config` and similar (open)

## Recent Pull Requests

- 🟢 **#66** 重构 PPOExpConfig 类，继承自 CommonPPOConfig，简化配置管理 (open)
- 🔴 **#62** Major Updates for Open-Reasoner-Zero (closed)
- 🟢 **#57** Support FSDP and vllm colocate with tp size > 1 (open)
- 🔴 **#32** Local ppo mini (closed)
- 🔴 **#31** Add a mini example (closed)

## Recent Commits

- **3fdd9a07** doc: update ORZ-R1-Distill-Qwen-14B results - REIGN12 (2025-06-02T16:35:57+00:00)
- **a2351927** chores: update qr code - REIGN12 (2025-06-02T16:35:25+00:00)
- **20e5bc83** README: update readme - REIGN12 (2025-04-08T12:58:51+00:00)
- **9c73e259** README: update readme - REIGN12 (2025-04-02T03:41:38+00:00)
- **a27ce5c1** README: update readme - REIGN12 (2025-04-02T03:38:35+00:00)
- **a288753d** README: update readme - REIGN12 (2025-03-31T15:53:39+00:00)
- **a7193f2c** Major Updates for Open-Reasoner-Zero (#62) - Jingcheng Hu (2025-03-31T15:45:50+00:00)
- **e008f6d9** Fix: bug fix for weight sync logic when colocate=False - REIGN12 (2025-03-05T08:08:42+00:00)
- **effe6fb8** Fix: fix typo in report - REIGN12 (2025-03-01T09:03:59+00:00)
- **9464093c** Fix: minor bugfix for dockerfile - REIGN12 (2025-03-01T08:54:56+00:00)

## External Links Found in README

- https://github.com/deepspeedai/DeepSpeed
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json
- https://huggingface.co/Qwen/Qwen2.5-0.5B
- https://arxiv.org/abs/2503.24290
- https://huggingface.co/Open-Reasoner-Zero"
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B
- https://www.stepfun.com/
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
- https://arxiv.org/abs/2503.24290},
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py
- https://github.com/vllm-project/vllm
- https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline
- https://huggingface.co/datasets/open-r1/OpenR1-Math-220k
- https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor"/></a>
- https://arxiv.org/abs/2503.24290"><b>Paper
- https://github.com/ray-project/ray
- https://projectnumina.ai/

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 935587127,
  "name": "Open-Reasoner-Zero",
  "full_name": "Open-Reasoner-Zero/Open-Reasoner-Zero",
  "description": "Official Repo for Open-Reasoner-Zero",
  "html_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero",
  "clone_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "ssh_url": "git@github.com:Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "homepage": "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903",
  "topics": [],
  "default_branch": "main",
  "created_at": "2025-02-19T17:28:25+00:00",
  "updated_at": "2025-06-20T07:02:06+00:00",
  "pushed_at": "2025-06-02T16:36:00+00:00",
  "size_kb": 16502,
  "watchers_count": 1968,
  "stargazers_count": 1968,
  "forks_count": 104,
  "open_issues_count": 19,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE"
  },
  "languages": {
    "Python": 337249,
    "Dockerfile": 1739
  },
  "top_contributors": [
    {
      "login": "REIGN12",
      "contributions": 21
    },
    {
      "login": "vwxyzjn",
      "contributions": 5
    },
    {
      "login": "YinminZhang",
      "contributions": 3
    }
  ],
  "file_tree_count": 71,
  "file_tree_sample": [
    {
      "path": ".flake8",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "ORZ_paper.pdf",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "data",
      "type": "tree"
    },
    {
      "path": "data/eval_data",
      "type": "tree"
    },
    {
      "path": "data/eval_data/aime2024.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/gpqa_diamond.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/math500.json",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 5,
  "recent_issues": [
    {
      "number": 73,
      "title": "DeepSpeed does not detect CUDA for some reason:",
      "state": "open"
    },
    {
      "number": 72,
      "title": "In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map=\"meta\"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device(\"meta\")`",
      "state": "open"
    },
    {
      "number": 71,
      "title": "In `orz/ppo/actors.py`: `with torch.device(\"meta\"): AutoModel.from_pretrained(...)` throws",
      "state": "open"
    },
    {
      "number": 70,
      "title": "`llm_engine.model_executor` seems not available on modern vllm (0.8.5)",
      "state": "open"
    },
    {
      "number": 69,
      "title": "On newer vllm should use `llm_engine.vllm_config` and similar",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 66,
      "title": "\u91cd\u6784 PPOExpConfig \u7c7b\uff0c\u7ee7\u627f\u81ea CommonPPOConfig\uff0c\u7b80\u5316\u914d\u7f6e\u7ba1\u7406",
      "state": "open"
    },
    {
      "number": 62,
      "title": "Major Updates for Open-Reasoner-Zero",
      "state": "closed"
    },
    {
      "number": 57,
      "title": "Support FSDP and vllm colocate with tp size > 1",
      "state": "open"
    },
    {
      "number": 32,
      "title": "Local ppo mini",
      "state": "closed"
    },
    {
      "number": 31,
      "title": "Add a mini example",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "3fdd9a07b4fb01e06005e6e74fc56690cde8a341",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:57+00:00",
      "message": "doc: update ORZ-R1-Distill-Qwen-14B results"
    },
    {
      "sha": "a23519277efdd71a063e1f2913d5c496a5f96180",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:25+00:00",
      "message": "chores: update qr code"
    },
    {
      "sha": "20e5bc83ae75695fded4e1a59a21e487063e24d9",
      "author": "REIGN12",
      "date": "2025-04-08T12:58:51+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "9c73e2590d23a7245a8d6341337952828110c631",
      "author": "REIGN12",
      "date": "2025-04-02T03:41:38+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a27ce5c1c5204d267c821542af4bdb47c70aa40a",
      "author": "REIGN12",
      "date": "2025-04-02T03:38:35+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a288753de916c0f3ba932fb0d1c218a49b8a403b",
      "author": "REIGN12",
      "date": "2025-03-31T15:53:39+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a7193f2c14aa90d055f9c161bf4566ec41ced30c",
      "author": "Jingcheng Hu",
      "date": "2025-03-31T15:45:50+00:00",
      "message": "Major Updates for Open-Reasoner-Zero (#62)"
    },
    {
      "sha": "e008f6d95f0b9a0e992f6b8bac912515b50a4634",
      "author": "REIGN12",
      "date": "2025-03-05T08:08:42+00:00",
      "message": "Fix: bug fix for weight sync logic when colocate=False"
    },
    {
      "sha": "effe6fb81570ccfde7d28d8d335c83127e95263f",
      "author": "REIGN12",
      "date": "2025-03-01T09:03:59+00:00",
      "message": "Fix: fix typo in report"
    },
    {
      "sha": "9464093c67c32a5fca69556b603f915e55fc00a5",
      "author": "REIGN12",
      "date": "2025-03-01T08:54:56+00:00",
      "message": "Fix: minor bugfix for dockerfile"
    },
    {
      "sha": "f594903dc0e953a8d0429ae0ecf339efcbe456f7",
      "author": "Jingcheng Hu",
      "date": "2025-02-25T15:03:35+00:00",
      "message": "Merge pull request #31 from vwxyzjn/ppo-mini"
    },
    {
      "sha": "ce912e108ca03d5ded42e151e3c857d44fc3f277",
      "author": "Costa Huang",
      "date": "2025-02-24T20:33:48+00:00",
      "message": "quick change"
    },
    {
      "sha": "34d8abc52ecba9c1c620350eb70110d05ea6b676",
      "author": "Costa Huang",
      "date": "2025-02-24T20:32:51+00:00",
      "message": "set eval to false"
    },
    {
      "sha": "55904eae7c35932e8ee2efb717c788ed3cd9b75e",
      "author": "Costa Huang",
      "date": "2025-02-24T20:31:44+00:00",
      "message": "quick change"
    },
    {
      "sha": "dbc40d8042c4981138b58773ca4936570e2339be",
      "author": "Costa Huang",
      "date": "2025-02-24T20:28:34+00:00",
      "message": "Add single GPU exampel"
    },
    {
      "sha": "ec2ce7e528f660f3b05c19369cdcff9d89cbb6b0",
      "author": "Costa Huang",
      "date": "2025-02-24T20:06:00+00:00",
      "message": "Add a mini example"
    },
    {
      "sha": "7b5dec17a8dda164e8a4aaba33393afc3e0e95d9",
      "author": "Jingcheng Hu",
      "date": "2025-02-24T09:18:55+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "3ccec9eec8717a5de0575918237bc81b6ba3e3a2",
      "author": "REIGN12",
      "date": "2025-02-24T07:24:11+00:00",
      "message": "Merge branch 'main' of https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
    },
    {
      "sha": "df77747d28c8580db768732f8da34f913e0411ca",
      "author": "REIGN12",
      "date": "2025-02-24T07:23:34+00:00",
      "message": "Fix: bug fix for readme and 32b exp"
    },
    {
      "sha": "ec6b01ef61f6b66a30bff0ea763b37fe4e9d00d1",
      "author": "hanqer",
      "date": "2025-02-24T05:52:43+00:00",
      "message": "fix: fix the package dependencies of pip and apt."
    }
  ],
  "readme_text": "<div align=\"center\">\n\n# Open Reasoner Zero\n\n<img src=\"figure/logo.jpg\" width=\"300\"/>\n\n<div>\n\nAn Open Source Approach to Scaling Up Reinforcement Learning on the Base Model\n</div>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n    <a href=\"https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\" style=\"margin: 2px;\"><img alt=\"Code\" src=\"https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/></a>\n  \n  <a href=\"https://huggingface.co/Open-Reasoner-Zero\" target=\"_blank\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>\n\n  <a href=\"https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\" target=\"_blank\">\n  <img alt=\"Notion Page\"\n    src=\"https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>\n\n  <br>\n  <a href=\"https://arxiv.org/abs/2503.24290\"><b>Paper Arxiv Link </b>\ud83d\udc41\ufe0f</a>\n</div>\n\n<div>\n<br>\n\n</div>\n\n## Overview \ud83c\udf0a\nWe introduce **Open-Reasoner-Zero**, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility.\nUsing the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency\u2014requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline.\n\nTo enable broader participation in this pivotal moment we witnessed and accelerate research towards artificial general intelligence (AGI), \nwe release our source code, parameter settings, training data, and model weights.\nPlease refer to our [paper](https://arxiv.org/abs/2503.24290) for more insights across various model sizes. \n\n**Let the Reasoner-Zero tide rise!**\n\n\n## Main Results \ud83c\udfc6\n\n![](figure/teaser.png)\n\n*Figure 1 | Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\}. Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\} on benchmarks (averaged on 16 responses) during training. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, Open-Reasoner-Zero-32B achieves superior performance on AIME2024, MATH500, and GPQA Diamond benchmark-requiring only a tenth of the training steps.*\n\n![](figure/train_curve.png)\n*Figure 2 | Train-time Scale up on Train Reward and Response Length of Open-Reasoner-Zero (ORZ) - \\{0.5B, 1.5B, 7B, 32B\\}. Train Reward and Response Length increase steadily, demonstrating consistent scalability across model sizes. Interestingly, the ORZ-32B Response Length exhibits fluctuations without negatively impacting training stability, highlighting the robustness of our minimalist recipe.*\n\n## Releases \ud83d\udce6\n\n<strong>[2025/06/03]</strong>\nWe release [ORZ-R1-Distill-Qwen-14B](https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B), obtained by applying ORZ recipe to reasoning-enhanced models like DeepSeek-R1-Distill-Qwen-14B. This ORZ-R1-Distill-Qwen-14B achieves strong results on reasoning benchmarks, even surpassing the larger DeepSeek-R1-Distill-Qwen-32B model.\n\n| Model                        | AIME 2024 | AIME 2025 | MATH500 | GPQA Dia. |\n| ---------------------------- | --------- | --------- | ------- | --------- |\n| DeepSeek-R1-Distill-Qwen-14B | 69.7      | 49.1      | 93.9    | 59.1      |\n| DeepSeek-R1-Distill-Qwen-32B | 72.6      | 60.0      | 94.3    | **62.1**     |\n| **ORZ-R1-Distill-Qwen-14B**  | **75.2**  | **60.0**  | **95.6** | 60.4  |\n\n\n<strong>[2025/03/31]</strong>\nWe announce a major milestone for `Open-Reasoner-Zero`:\n\n- \ud83c\udf0a [Updated Paper](https://arxiv.org/abs/2503.24290) with new results.\n- \ud83d\udd2d [Easy-to-use Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground):\n  - [ORZ-1.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py) and [ORZ-0.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py) (main results in Figure 2). \n  - [Minimal resource training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py): ORZ-0.5B can be run on a single A800/H800 gpu!\n- \ud83e\udd29 [Updated Curated Datasets](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data): \n  - 129k data in total:\n    - [original 57k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json).\n    - [extended 72k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json).\n  - [13k hard data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json) mined from the above 129k data. \n    - used in the \"annealing\" stage of ORZ-32B training: **AIME2024 from ~41% to ~48%**!\n- \ud83e\udd17 More HF Models: \n  - Updated HF Models: [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B).\n  - Released HF Models: [`Open-Reasoner-Zero-1.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B) and [`Open-Reasoner-Zero-0.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B).\n- \ud83d\ude80 Full Suite of Critic Models for in-depth research: `Open-Reasoner-Zero-Critic-`{[0.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B), [1.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B), [7B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B),  [32B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B)}.\n\n<strong>[2025/02/18]</strong>\nWe release `Open-Reasoner-Zero`. \n\nAs part of this release, we open-source:\n- \ud83c\udf0a [Paper(WIP)](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf) on our comprehensive analysis and insights in Reasoner-Zero training\n- \ud83e\udd17 HF Model [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B)\n- \ud83c\udf81 [`Our curated 57k training data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data)\n- \ud83d\udcc4 [Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground) to enjoy your own Reasoner-Zero journey!\n\n## Key Features in Codebase \ud83d\udd11\n\n- Adopt single controller trainer design, flexible and researcher-friendly.\n- Colocate training and generation in the same GPUs to maximize GPU utilization.\n\n## Getting Started \ud83d\ude80\n### Data\n\nWe release all of curated high-quality training data in the [`data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data) folder:\n* curated 129k data:\n  * [original 57k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json), collected from various sources, including AIME (up to 2023), MATH, Numina-Math collection and Tulu3 MATH.\n  * [extended 72k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json), mainly cleaned from OpenR1-Math-220k.\n* [hard 13k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json), mined from the first stage of ORZ-32B training.\n\nThe details for how to collect data are described in our [paper](https://arxiv.org/abs/2503.24290).\n\n### Installation & Training Scripts\nWe release our [Dockerfile](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile) in [docker](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker) folder to facilitate the reproducibility of our training.\n\nTo install the package, run:\n```bash\npip install -e .\n```\n\n#### Start ORZ-32B PPO Training\nHere are the starting commands in 16 nodes. \n\nFirst on master node, run:\n```bash\nray start --head\n# you will see logging like:\n# Next steps\n#  To add another node to this Ray cluster, run\n#    ray start --address='<master-node-ip>:<master-node-port>'\n```\n\nthen on all other nodes, run:\n```bash\nray start --address='<master-node-ip>:<master-node-port>' # <master-node-ip> and <master-node-port> are from above loggings!\n```\n\nfinally on master node, just run:\n```bash\npython -m playground.orz_32b_ppo\n```\nYour training log will be shown in the master node terminal.\n\n------\n\n#### Start ORZ-0.5B PPO Training\nYou can start the ORZ-0.5B PPO training in single A800/H800 node:\n```bash\npython -m playground.orz_0p5b_ppo\n```\n\nYou can even run in **a single A800/H800 gpu**: \n```bash\npython -m playground.orz_0p5b_ppo_1gpu\n```\n\nnote: since we are not in multi-node setting, no `ray start` like logics are needed.\n\n------\n\n#### Start ORZ-7B PPO Training\n\nMulti-node Training on 4 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n\n# then on master node, run:\npython -m playground.orz_7b_ppo\n```\n\nYour training log will be shown in the master node terminal.\n\n-----\n\n#### Start ORZ-1.5B PPO Training\n\nMulti-node Training on 2 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n# then on master node, run:\npython -m playground.orz_1p5b_ppo\n```\n\n----\n\n#### Debug Settings\nIn the code, we leave an environment variable `DEBUG_MODE` to run in debug setting for researcher to iterate. (Thought for now, we recommend using `python -m playground.orz_0p5b_ppo_1gpu` for debugging.)\n\nThe debug running command examples:\n```bash\n# NOTE: just for debug, not final setting!\n\n## Debug command in a single GPU with `EleutherAI/pythia-14m`\nDEBUG_MODE=True python -m playground.orz_14m_ppo_mini\n## Debug command in a single node (8 GPUs) with `Qwen/Qwen2.5-7B`\nDEBUG_MODE=True python -m playground.orz_7b_ppo\n```\n\n### How to Use the Model\n#### Policy Model\nPolicy models can be used in the same way as any chat model in transformers and vllm, since we have put the chat template jinja in the tokenizer.\n\n#### Critic Model\nCritic models can be loaded the same way like in the [training code](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738). \n\n\n## Acknowledgements \ud83d\udc96 \n\n- This work was supported by computing resources and valuable feedback provided by [StepFun](https://www.stepfun.com/) and Tsinghua University.\n- Our training framework is built on [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF), [vllm](https://github.com/vllm-project/vllm), [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) and [ray](https://github.com/ray-project/ray).\n- Our model is based on [Qwen2.5 Series](https://qwenlm.github.io/blog/qwen2.5-llm/) of **base models**, including [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B), [Qwen2.5-1.5B](https://huggingface.co/Qwen/Qwen2.5-1.5B), [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B) and [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B).\n- We thank [Project Numina](https://projectnumina.ai/), [Tulu3](https://allenai.org/blog/tulu-3-technical) and [OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k) for their collected open sourced data.\n\n## Advertisement Time \ud83d\udce3\n\nWe are hiring talented researchers and engineers to join our team. If you are interested in our project and would like to contribute to the reasoner scale-up all the way to AGI, please feel free to reach out to us at hanqer@stepfun.com\n\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline)](https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline)\n\n## Community Discussions \ud83c\udf7a\n\nWe have several wechat groups to help discussions and sharing, you can scan the QR code below to join the latest group.\n\n<img src=\"figure/WeChatGroup.png\" width=\"300\" style=\"display: block; margin: 0 auto;\"/>\n\n## Citation\n\n```bibtex\n@misc{hu2025openreasonerzeroopensourceapproach,\n      title={Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model}, \n      author={Jingcheng Hu and Yinmin Zhang and Qi Han and Daxin Jiang and Xiangyu Zhang and Heung-Yeung Shum},\n      year={2025},\n      eprint={2503.24290},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2503.24290}, \n}\n```\n",
  "external_links_in_readme": [
    "https://github.com/deepspeedai/DeepSpeed",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json",
    "https://huggingface.co/Qwen/Qwen2.5-0.5B",
    "https://arxiv.org/abs/2503.24290",
    "https://huggingface.co/Open-Reasoner-Zero\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B",
    "https://www.stepfun.com/",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\"",
    "https://arxiv.org/abs/2503.24290},",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py",
    "https://github.com/vllm-project/vllm",
    "https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline",
    "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k",
    "https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>",
    "https://arxiv.org/abs/2503.24290\"><b>Paper",
    "https://github.com/ray-project/ray",
    "https://projectnumina.ai/",
    "https://qwenlm.github.io/blog/qwen2.5-llm/",
    "https://huggingface.co/Qwen/Qwen2.5-7B",
    "https://huggingface.co/Qwen/Qwen2.5-1.5B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B",
    "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\"",
    "https://github.com/OpenRLHF/OpenRLHF",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json",
    "https://huggingface.co/Qwen/Qwen2.5-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py",
    "https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>",
    "https://allenai.org/blog/tulu-3-technical",
    "https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B",
    "https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline",
    "https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B"
  ]
}
```

</details>


---

## Repository 15: Open-Reasoner-Zero/Open-Reasoner-Zero

# GitHub Repository Data

**Repository:** [Open-Reasoner-Zero/Open-Reasoner-Zero](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero)

## Basic Information

- **Description:** Official Repo for Open-Reasoner-Zero
- **Created:** 2025-02-19T17:28:25+00:00
- **Last Updated:** 2025-06-20T07:02:06+00:00
- **Last Pushed:** 2025-06-02T16:36:00+00:00
- **Default Branch:** main
- **Size:** 16502 KB

## Statistics

- **Stars:** 1,968
- **Forks:** 104
- **Watchers:** 1,968
- **Open Issues:** 19
- **Total Issues:** 0
- **Pull Requests:** 5

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE)

## Languages

- **Python:** 337,249 bytes
- **Dockerfile:** 1,739 bytes

## Top Contributors

1. **REIGN12** - 21 contributions
2. **vwxyzjn** - 5 contributions
3. **YinminZhang** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 71

- `.flake8` (blob)
- `.gitignore` (blob)
- `LICENSE` (blob)
- `ORZ_paper.pdf` (blob)
- `README.md` (blob)
- `data` (tree)
- `data/eval_data` (tree)
- `data/eval_data/aime2024.json` (blob)
- `data/eval_data/gpqa_diamond.json` (blob)
- `data/eval_data/math500.json` (blob)

## Recent Issues

- 🟢 **#73** DeepSpeed does not detect CUDA for some reason: (open)
- 🟢 **#72** In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map="meta"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device("meta")` (open)
- 🟢 **#71** In `orz/ppo/actors.py`: `with torch.device("meta"): AutoModel.from_pretrained(...)` throws (open)
- 🟢 **#70** `llm_engine.model_executor` seems not available on modern vllm (0.8.5) (open)
- 🟢 **#69** On newer vllm should use `llm_engine.vllm_config` and similar (open)

## Recent Pull Requests

- 🟢 **#66** 重构 PPOExpConfig 类，继承自 CommonPPOConfig，简化配置管理 (open)
- 🔴 **#62** Major Updates for Open-Reasoner-Zero (closed)
- 🟢 **#57** Support FSDP and vllm colocate with tp size > 1 (open)
- 🔴 **#32** Local ppo mini (closed)
- 🔴 **#31** Add a mini example (closed)

## Recent Commits

- **3fdd9a07** doc: update ORZ-R1-Distill-Qwen-14B results - REIGN12 (2025-06-02T16:35:57+00:00)
- **a2351927** chores: update qr code - REIGN12 (2025-06-02T16:35:25+00:00)
- **20e5bc83** README: update readme - REIGN12 (2025-04-08T12:58:51+00:00)
- **9c73e259** README: update readme - REIGN12 (2025-04-02T03:41:38+00:00)
- **a27ce5c1** README: update readme - REIGN12 (2025-04-02T03:38:35+00:00)
- **a288753d** README: update readme - REIGN12 (2025-03-31T15:53:39+00:00)
- **a7193f2c** Major Updates for Open-Reasoner-Zero (#62) - Jingcheng Hu (2025-03-31T15:45:50+00:00)
- **e008f6d9** Fix: bug fix for weight sync logic when colocate=False - REIGN12 (2025-03-05T08:08:42+00:00)
- **effe6fb8** Fix: fix typo in report - REIGN12 (2025-03-01T09:03:59+00:00)
- **9464093c** Fix: minor bugfix for dockerfile - REIGN12 (2025-03-01T08:54:56+00:00)

## External Links Found in README

- https://github.com/deepspeedai/DeepSpeed
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json
- https://huggingface.co/Qwen/Qwen2.5-0.5B
- https://arxiv.org/abs/2503.24290
- https://huggingface.co/Open-Reasoner-Zero"
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B
- https://www.stepfun.com/
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
- https://arxiv.org/abs/2503.24290},
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py
- https://github.com/vllm-project/vllm
- https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline
- https://huggingface.co/datasets/open-r1/OpenR1-Math-220k
- https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor"/></a>
- https://arxiv.org/abs/2503.24290"><b>Paper
- https://github.com/ray-project/ray
- https://projectnumina.ai/

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 935587127,
  "name": "Open-Reasoner-Zero",
  "full_name": "Open-Reasoner-Zero/Open-Reasoner-Zero",
  "description": "Official Repo for Open-Reasoner-Zero",
  "html_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero",
  "clone_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "ssh_url": "git@github.com:Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "homepage": "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903",
  "topics": [],
  "default_branch": "main",
  "created_at": "2025-02-19T17:28:25+00:00",
  "updated_at": "2025-06-20T07:02:06+00:00",
  "pushed_at": "2025-06-02T16:36:00+00:00",
  "size_kb": 16502,
  "watchers_count": 1968,
  "stargazers_count": 1968,
  "forks_count": 104,
  "open_issues_count": 19,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE"
  },
  "languages": {
    "Python": 337249,
    "Dockerfile": 1739
  },
  "top_contributors": [
    {
      "login": "REIGN12",
      "contributions": 21
    },
    {
      "login": "vwxyzjn",
      "contributions": 5
    },
    {
      "login": "YinminZhang",
      "contributions": 3
    }
  ],
  "file_tree_count": 71,
  "file_tree_sample": [
    {
      "path": ".flake8",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "ORZ_paper.pdf",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "data",
      "type": "tree"
    },
    {
      "path": "data/eval_data",
      "type": "tree"
    },
    {
      "path": "data/eval_data/aime2024.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/gpqa_diamond.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/math500.json",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 5,
  "recent_issues": [
    {
      "number": 73,
      "title": "DeepSpeed does not detect CUDA for some reason:",
      "state": "open"
    },
    {
      "number": 72,
      "title": "In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map=\"meta\"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device(\"meta\")`",
      "state": "open"
    },
    {
      "number": 71,
      "title": "In `orz/ppo/actors.py`: `with torch.device(\"meta\"): AutoModel.from_pretrained(...)` throws",
      "state": "open"
    },
    {
      "number": 70,
      "title": "`llm_engine.model_executor` seems not available on modern vllm (0.8.5)",
      "state": "open"
    },
    {
      "number": 69,
      "title": "On newer vllm should use `llm_engine.vllm_config` and similar",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 66,
      "title": "\u91cd\u6784 PPOExpConfig \u7c7b\uff0c\u7ee7\u627f\u81ea CommonPPOConfig\uff0c\u7b80\u5316\u914d\u7f6e\u7ba1\u7406",
      "state": "open"
    },
    {
      "number": 62,
      "title": "Major Updates for Open-Reasoner-Zero",
      "state": "closed"
    },
    {
      "number": 57,
      "title": "Support FSDP and vllm colocate with tp size > 1",
      "state": "open"
    },
    {
      "number": 32,
      "title": "Local ppo mini",
      "state": "closed"
    },
    {
      "number": 31,
      "title": "Add a mini example",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "3fdd9a07b4fb01e06005e6e74fc56690cde8a341",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:57+00:00",
      "message": "doc: update ORZ-R1-Distill-Qwen-14B results"
    },
    {
      "sha": "a23519277efdd71a063e1f2913d5c496a5f96180",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:25+00:00",
      "message": "chores: update qr code"
    },
    {
      "sha": "20e5bc83ae75695fded4e1a59a21e487063e24d9",
      "author": "REIGN12",
      "date": "2025-04-08T12:58:51+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "9c73e2590d23a7245a8d6341337952828110c631",
      "author": "REIGN12",
      "date": "2025-04-02T03:41:38+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a27ce5c1c5204d267c821542af4bdb47c70aa40a",
      "author": "REIGN12",
      "date": "2025-04-02T03:38:35+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a288753de916c0f3ba932fb0d1c218a49b8a403b",
      "author": "REIGN12",
      "date": "2025-03-31T15:53:39+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a7193f2c14aa90d055f9c161bf4566ec41ced30c",
      "author": "Jingcheng Hu",
      "date": "2025-03-31T15:45:50+00:00",
      "message": "Major Updates for Open-Reasoner-Zero (#62)"
    },
    {
      "sha": "e008f6d95f0b9a0e992f6b8bac912515b50a4634",
      "author": "REIGN12",
      "date": "2025-03-05T08:08:42+00:00",
      "message": "Fix: bug fix for weight sync logic when colocate=False"
    },
    {
      "sha": "effe6fb81570ccfde7d28d8d335c83127e95263f",
      "author": "REIGN12",
      "date": "2025-03-01T09:03:59+00:00",
      "message": "Fix: fix typo in report"
    },
    {
      "sha": "9464093c67c32a5fca69556b603f915e55fc00a5",
      "author": "REIGN12",
      "date": "2025-03-01T08:54:56+00:00",
      "message": "Fix: minor bugfix for dockerfile"
    },
    {
      "sha": "f594903dc0e953a8d0429ae0ecf339efcbe456f7",
      "author": "Jingcheng Hu",
      "date": "2025-02-25T15:03:35+00:00",
      "message": "Merge pull request #31 from vwxyzjn/ppo-mini"
    },
    {
      "sha": "ce912e108ca03d5ded42e151e3c857d44fc3f277",
      "author": "Costa Huang",
      "date": "2025-02-24T20:33:48+00:00",
      "message": "quick change"
    },
    {
      "sha": "34d8abc52ecba9c1c620350eb70110d05ea6b676",
      "author": "Costa Huang",
      "date": "2025-02-24T20:32:51+00:00",
      "message": "set eval to false"
    },
    {
      "sha": "55904eae7c35932e8ee2efb717c788ed3cd9b75e",
      "author": "Costa Huang",
      "date": "2025-02-24T20:31:44+00:00",
      "message": "quick change"
    },
    {
      "sha": "dbc40d8042c4981138b58773ca4936570e2339be",
      "author": "Costa Huang",
      "date": "2025-02-24T20:28:34+00:00",
      "message": "Add single GPU exampel"
    },
    {
      "sha": "ec2ce7e528f660f3b05c19369cdcff9d89cbb6b0",
      "author": "Costa Huang",
      "date": "2025-02-24T20:06:00+00:00",
      "message": "Add a mini example"
    },
    {
      "sha": "7b5dec17a8dda164e8a4aaba33393afc3e0e95d9",
      "author": "Jingcheng Hu",
      "date": "2025-02-24T09:18:55+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "3ccec9eec8717a5de0575918237bc81b6ba3e3a2",
      "author": "REIGN12",
      "date": "2025-02-24T07:24:11+00:00",
      "message": "Merge branch 'main' of https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
    },
    {
      "sha": "df77747d28c8580db768732f8da34f913e0411ca",
      "author": "REIGN12",
      "date": "2025-02-24T07:23:34+00:00",
      "message": "Fix: bug fix for readme and 32b exp"
    },
    {
      "sha": "ec6b01ef61f6b66a30bff0ea763b37fe4e9d00d1",
      "author": "hanqer",
      "date": "2025-02-24T05:52:43+00:00",
      "message": "fix: fix the package dependencies of pip and apt."
    }
  ],
  "readme_text": "<div align=\"center\">\n\n# Open Reasoner Zero\n\n<img src=\"figure/logo.jpg\" width=\"300\"/>\n\n<div>\n\nAn Open Source Approach to Scaling Up Reinforcement Learning on the Base Model\n</div>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n    <a href=\"https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\" style=\"margin: 2px;\"><img alt=\"Code\" src=\"https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/></a>\n  \n  <a href=\"https://huggingface.co/Open-Reasoner-Zero\" target=\"_blank\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>\n\n  <a href=\"https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\" target=\"_blank\">\n  <img alt=\"Notion Page\"\n    src=\"https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>\n\n  <br>\n  <a href=\"https://arxiv.org/abs/2503.24290\"><b>Paper Arxiv Link </b>\ud83d\udc41\ufe0f</a>\n</div>\n\n<div>\n<br>\n\n</div>\n\n## Overview \ud83c\udf0a\nWe introduce **Open-Reasoner-Zero**, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility.\nUsing the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency\u2014requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline.\n\nTo enable broader participation in this pivotal moment we witnessed and accelerate research towards artificial general intelligence (AGI), \nwe release our source code, parameter settings, training data, and model weights.\nPlease refer to our [paper](https://arxiv.org/abs/2503.24290) for more insights across various model sizes. \n\n**Let the Reasoner-Zero tide rise!**\n\n\n## Main Results \ud83c\udfc6\n\n![](figure/teaser.png)\n\n*Figure 1 | Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\}. Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\} on benchmarks (averaged on 16 responses) during training. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, Open-Reasoner-Zero-32B achieves superior performance on AIME2024, MATH500, and GPQA Diamond benchmark-requiring only a tenth of the training steps.*\n\n![](figure/train_curve.png)\n*Figure 2 | Train-time Scale up on Train Reward and Response Length of Open-Reasoner-Zero (ORZ) - \\{0.5B, 1.5B, 7B, 32B\\}. Train Reward and Response Length increase steadily, demonstrating consistent scalability across model sizes. Interestingly, the ORZ-32B Response Length exhibits fluctuations without negatively impacting training stability, highlighting the robustness of our minimalist recipe.*\n\n## Releases \ud83d\udce6\n\n<strong>[2025/06/03]</strong>\nWe release [ORZ-R1-Distill-Qwen-14B](https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B), obtained by applying ORZ recipe to reasoning-enhanced models like DeepSeek-R1-Distill-Qwen-14B. This ORZ-R1-Distill-Qwen-14B achieves strong results on reasoning benchmarks, even surpassing the larger DeepSeek-R1-Distill-Qwen-32B model.\n\n| Model                        | AIME 2024 | AIME 2025 | MATH500 | GPQA Dia. |\n| ---------------------------- | --------- | --------- | ------- | --------- |\n| DeepSeek-R1-Distill-Qwen-14B | 69.7      | 49.1      | 93.9    | 59.1      |\n| DeepSeek-R1-Distill-Qwen-32B | 72.6      | 60.0      | 94.3    | **62.1**     |\n| **ORZ-R1-Distill-Qwen-14B**  | **75.2**  | **60.0**  | **95.6** | 60.4  |\n\n\n<strong>[2025/03/31]</strong>\nWe announce a major milestone for `Open-Reasoner-Zero`:\n\n- \ud83c\udf0a [Updated Paper](https://arxiv.org/abs/2503.24290) with new results.\n- \ud83d\udd2d [Easy-to-use Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground):\n  - [ORZ-1.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py) and [ORZ-0.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py) (main results in Figure 2). \n  - [Minimal resource training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py): ORZ-0.5B can be run on a single A800/H800 gpu!\n- \ud83e\udd29 [Updated Curated Datasets](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data): \n  - 129k data in total:\n    - [original 57k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json).\n    - [extended 72k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json).\n  - [13k hard data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json) mined from the above 129k data. \n    - used in the \"annealing\" stage of ORZ-32B training: **AIME2024 from ~41% to ~48%**!\n- \ud83e\udd17 More HF Models: \n  - Updated HF Models: [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B).\n  - Released HF Models: [`Open-Reasoner-Zero-1.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B) and [`Open-Reasoner-Zero-0.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B).\n- \ud83d\ude80 Full Suite of Critic Models for in-depth research: `Open-Reasoner-Zero-Critic-`{[0.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B), [1.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B), [7B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B),  [32B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B)}.\n\n<strong>[2025/02/18]</strong>\nWe release `Open-Reasoner-Zero`. \n\nAs part of this release, we open-source:\n- \ud83c\udf0a [Paper(WIP)](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf) on our comprehensive analysis and insights in Reasoner-Zero training\n- \ud83e\udd17 HF Model [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B)\n- \ud83c\udf81 [`Our curated 57k training data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data)\n- \ud83d\udcc4 [Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground) to enjoy your own Reasoner-Zero journey!\n\n## Key Features in Codebase \ud83d\udd11\n\n- Adopt single controller trainer design, flexible and researcher-friendly.\n- Colocate training and generation in the same GPUs to maximize GPU utilization.\n\n## Getting Started \ud83d\ude80\n### Data\n\nWe release all of curated high-quality training data in the [`data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data) folder:\n* curated 129k data:\n  * [original 57k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json), collected from various sources, including AIME (up to 2023), MATH, Numina-Math collection and Tulu3 MATH.\n  * [extended 72k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json), mainly cleaned from OpenR1-Math-220k.\n* [hard 13k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json), mined from the first stage of ORZ-32B training.\n\nThe details for how to collect data are described in our [paper](https://arxiv.org/abs/2503.24290).\n\n### Installation & Training Scripts\nWe release our [Dockerfile](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile) in [docker](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker) folder to facilitate the reproducibility of our training.\n\nTo install the package, run:\n```bash\npip install -e .\n```\n\n#### Start ORZ-32B PPO Training\nHere are the starting commands in 16 nodes. \n\nFirst on master node, run:\n```bash\nray start --head\n# you will see logging like:\n# Next steps\n#  To add another node to this Ray cluster, run\n#    ray start --address='<master-node-ip>:<master-node-port>'\n```\n\nthen on all other nodes, run:\n```bash\nray start --address='<master-node-ip>:<master-node-port>' # <master-node-ip> and <master-node-port> are from above loggings!\n```\n\nfinally on master node, just run:\n```bash\npython -m playground.orz_32b_ppo\n```\nYour training log will be shown in the master node terminal.\n\n------\n\n#### Start ORZ-0.5B PPO Training\nYou can start the ORZ-0.5B PPO training in single A800/H800 node:\n```bash\npython -m playground.orz_0p5b_ppo\n```\n\nYou can even run in **a single A800/H800 gpu**: \n```bash\npython -m playground.orz_0p5b_ppo_1gpu\n```\n\nnote: since we are not in multi-node setting, no `ray start` like logics are needed.\n\n------\n\n#### Start ORZ-7B PPO Training\n\nMulti-node Training on 4 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n\n# then on master node, run:\npython -m playground.orz_7b_ppo\n```\n\nYour training log will be shown in the master node terminal.\n\n-----\n\n#### Start ORZ-1.5B PPO Training\n\nMulti-node Training on 2 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n# then on master node, run:\npython -m playground.orz_1p5b_ppo\n```\n\n----\n\n#### Debug Settings\nIn the code, we leave an environment variable `DEBUG_MODE` to run in debug setting for researcher to iterate. (Thought for now, we recommend using `python -m playground.orz_0p5b_ppo_1gpu` for debugging.)\n\nThe debug running command examples:\n```bash\n# NOTE: just for debug, not final setting!\n\n## Debug command in a single GPU with `EleutherAI/pythia-14m`\nDEBUG_MODE=True python -m playground.orz_14m_ppo_mini\n## Debug command in a single node (8 GPUs) with `Qwen/Qwen2.5-7B`\nDEBUG_MODE=True python -m playground.orz_7b_ppo\n```\n\n### How to Use the Model\n#### Policy Model\nPolicy models can be used in the same way as any chat model in transformers and vllm, since we have put the chat template jinja in the tokenizer.\n\n#### Critic Model\nCritic models can be loaded the same way like in the [training code](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738). \n\n\n## Acknowledgements \ud83d\udc96 \n\n- This work was supported by computing resources and valuable feedback provided by [StepFun](https://www.stepfun.com/) and Tsinghua University.\n- Our training framework is built on [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF), [vllm](https://github.com/vllm-project/vllm), [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) and [ray](https://github.com/ray-project/ray).\n- Our model is based on [Qwen2.5 Series](https://qwenlm.github.io/blog/qwen2.5-llm/) of **base models**, including [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B), [Qwen2.5-1.5B](https://huggingface.co/Qwen/Qwen2.5-1.5B), [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B) and [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B).\n- We thank [Project Numina](https://projectnumina.ai/), [Tulu3](https://allenai.org/blog/tulu-3-technical) and [OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k) for their collected open sourced data.\n\n## Advertisement Time \ud83d\udce3\n\nWe are hiring talented researchers and engineers to join our team. If you are interested in our project and would like to contribute to the reasoner scale-up all the way to AGI, please feel free to reach out to us at hanqer@stepfun.com\n\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline)](https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline)\n\n## Community Discussions \ud83c\udf7a\n\nWe have several wechat groups to help discussions and sharing, you can scan the QR code below to join the latest group.\n\n<img src=\"figure/WeChatGroup.png\" width=\"300\" style=\"display: block; margin: 0 auto;\"/>\n\n## Citation\n\n```bibtex\n@misc{hu2025openreasonerzeroopensourceapproach,\n      title={Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model}, \n      author={Jingcheng Hu and Yinmin Zhang and Qi Han and Daxin Jiang and Xiangyu Zhang and Heung-Yeung Shum},\n      year={2025},\n      eprint={2503.24290},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2503.24290}, \n}\n```\n",
  "external_links_in_readme": [
    "https://github.com/deepspeedai/DeepSpeed",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json",
    "https://huggingface.co/Qwen/Qwen2.5-0.5B",
    "https://arxiv.org/abs/2503.24290",
    "https://huggingface.co/Open-Reasoner-Zero\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B",
    "https://www.stepfun.com/",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\"",
    "https://arxiv.org/abs/2503.24290},",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py",
    "https://github.com/vllm-project/vllm",
    "https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline",
    "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k",
    "https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>",
    "https://arxiv.org/abs/2503.24290\"><b>Paper",
    "https://github.com/ray-project/ray",
    "https://projectnumina.ai/",
    "https://qwenlm.github.io/blog/qwen2.5-llm/",
    "https://huggingface.co/Qwen/Qwen2.5-7B",
    "https://huggingface.co/Qwen/Qwen2.5-1.5B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B",
    "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\"",
    "https://github.com/OpenRLHF/OpenRLHF",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json",
    "https://huggingface.co/Qwen/Qwen2.5-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py",
    "https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>",
    "https://allenai.org/blog/tulu-3-technical",
    "https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B",
    "https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline",
    "https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B"
  ]
}
```

</details>


---

## Repository 16: Open-Reasoner-Zero/Open-Reasoner-Zero

# GitHub Repository Data

**Repository:** [Open-Reasoner-Zero/Open-Reasoner-Zero](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero)

## Basic Information

- **Description:** Official Repo for Open-Reasoner-Zero
- **Created:** 2025-02-19T17:28:25+00:00
- **Last Updated:** 2025-06-20T07:02:06+00:00
- **Last Pushed:** 2025-06-02T16:36:00+00:00
- **Default Branch:** main
- **Size:** 16502 KB

## Statistics

- **Stars:** 1,968
- **Forks:** 104
- **Watchers:** 1,968
- **Open Issues:** 19
- **Total Issues:** 0
- **Pull Requests:** 5

## License

- **Type:** MIT License
- **SPDX ID:** MIT
- **URL:** [License](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE)

## Languages

- **Python:** 337,249 bytes
- **Dockerfile:** 1,739 bytes

## Top Contributors

1. **REIGN12** - 21 contributions
2. **vwxyzjn** - 5 contributions
3. **YinminZhang** - 3 contributions

## File Structure (Sample of 10 files)

Total files: 71

- `.flake8` (blob)
- `.gitignore` (blob)
- `LICENSE` (blob)
- `ORZ_paper.pdf` (blob)
- `README.md` (blob)
- `data` (tree)
- `data/eval_data` (tree)
- `data/eval_data/aime2024.json` (blob)
- `data/eval_data/gpqa_diamond.json` (blob)
- `data/eval_data/math500.json` (blob)

## Recent Issues

- 🟢 **#73** DeepSpeed does not detect CUDA for some reason: (open)
- 🟢 **#72** In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map="meta"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device("meta")` (open)
- 🟢 **#71** In `orz/ppo/actors.py`: `with torch.device("meta"): AutoModel.from_pretrained(...)` throws (open)
- 🟢 **#70** `llm_engine.model_executor` seems not available on modern vllm (0.8.5) (open)
- 🟢 **#69** On newer vllm should use `llm_engine.vllm_config` and similar (open)

## Recent Pull Requests

- 🟢 **#66** 重构 PPOExpConfig 类，继承自 CommonPPOConfig，简化配置管理 (open)
- 🔴 **#62** Major Updates for Open-Reasoner-Zero (closed)
- 🟢 **#57** Support FSDP and vllm colocate with tp size > 1 (open)
- 🔴 **#32** Local ppo mini (closed)
- 🔴 **#31** Add a mini example (closed)

## Recent Commits

- **3fdd9a07** doc: update ORZ-R1-Distill-Qwen-14B results - REIGN12 (2025-06-02T16:35:57+00:00)
- **a2351927** chores: update qr code - REIGN12 (2025-06-02T16:35:25+00:00)
- **20e5bc83** README: update readme - REIGN12 (2025-04-08T12:58:51+00:00)
- **9c73e259** README: update readme - REIGN12 (2025-04-02T03:41:38+00:00)
- **a27ce5c1** README: update readme - REIGN12 (2025-04-02T03:38:35+00:00)
- **a288753d** README: update readme - REIGN12 (2025-03-31T15:53:39+00:00)
- **a7193f2c** Major Updates for Open-Reasoner-Zero (#62) - Jingcheng Hu (2025-03-31T15:45:50+00:00)
- **e008f6d9** Fix: bug fix for weight sync logic when colocate=False - REIGN12 (2025-03-05T08:08:42+00:00)
- **effe6fb8** Fix: fix typo in report - REIGN12 (2025-03-01T09:03:59+00:00)
- **9464093c** Fix: minor bugfix for dockerfile - REIGN12 (2025-03-01T08:54:56+00:00)

## External Links Found in README

- https://github.com/deepspeedai/DeepSpeed
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json
- https://huggingface.co/Qwen/Qwen2.5-0.5B
- https://arxiv.org/abs/2503.24290
- https://huggingface.co/Open-Reasoner-Zero"
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B
- https://www.stepfun.com/
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
- https://arxiv.org/abs/2503.24290},
- https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B
- https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py
- https://github.com/vllm-project/vllm
- https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline
- https://huggingface.co/datasets/open-r1/OpenR1-Math-220k
- https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor"/></a>
- https://arxiv.org/abs/2503.24290"><b>Paper
- https://github.com/ray-project/ray
- https://projectnumina.ai/

## Raw Data

<details>
<summary>Click to expand raw JSON data</summary>

```json
{
  "id": 935587127,
  "name": "Open-Reasoner-Zero",
  "full_name": "Open-Reasoner-Zero/Open-Reasoner-Zero",
  "description": "Official Repo for Open-Reasoner-Zero",
  "html_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero",
  "clone_url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "ssh_url": "git@github.com:Open-Reasoner-Zero/Open-Reasoner-Zero.git",
  "homepage": "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903",
  "topics": [],
  "default_branch": "main",
  "created_at": "2025-02-19T17:28:25+00:00",
  "updated_at": "2025-06-20T07:02:06+00:00",
  "pushed_at": "2025-06-02T16:36:00+00:00",
  "size_kb": 16502,
  "watchers_count": 1968,
  "stargazers_count": 1968,
  "forks_count": 104,
  "open_issues_count": 19,
  "license": {
    "key": "mit",
    "name": "MIT License",
    "spdx_id": "MIT",
    "url": "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/LICENSE"
  },
  "languages": {
    "Python": 337249,
    "Dockerfile": 1739
  },
  "top_contributors": [
    {
      "login": "REIGN12",
      "contributions": 21
    },
    {
      "login": "vwxyzjn",
      "contributions": 5
    },
    {
      "login": "YinminZhang",
      "contributions": 3
    }
  ],
  "file_tree_count": 71,
  "file_tree_sample": [
    {
      "path": ".flake8",
      "type": "blob"
    },
    {
      "path": ".gitignore",
      "type": "blob"
    },
    {
      "path": "LICENSE",
      "type": "blob"
    },
    {
      "path": "ORZ_paper.pdf",
      "type": "blob"
    },
    {
      "path": "README.md",
      "type": "blob"
    },
    {
      "path": "data",
      "type": "tree"
    },
    {
      "path": "data/eval_data",
      "type": "tree"
    },
    {
      "path": "data/eval_data/aime2024.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/gpqa_diamond.json",
      "type": "blob"
    },
    {
      "path": "data/eval_data/math500.json",
      "type": "blob"
    }
  ],
  "issues_count": 0,
  "pulls_count": 5,
  "recent_issues": [
    {
      "number": 73,
      "title": "DeepSpeed does not detect CUDA for some reason:",
      "state": "open"
    },
    {
      "number": 72,
      "title": "In `orz/ppo/actors.py` include on top of file `from torch.nn.attention.flex_attention import BlockMask, flex_attention` and pass in `device_map=\"meta\"` explicitly to `AutoModel.from_pretrained(...)` under the `with torch.device(\"meta\")`",
      "state": "open"
    },
    {
      "number": 71,
      "title": "In `orz/ppo/actors.py`: `with torch.device(\"meta\"): AutoModel.from_pretrained(...)` throws",
      "state": "open"
    },
    {
      "number": 70,
      "title": "`llm_engine.model_executor` seems not available on modern vllm (0.8.5)",
      "state": "open"
    },
    {
      "number": 69,
      "title": "On newer vllm should use `llm_engine.vllm_config` and similar",
      "state": "open"
    }
  ],
  "recent_pulls": [
    {
      "number": 66,
      "title": "\u91cd\u6784 PPOExpConfig \u7c7b\uff0c\u7ee7\u627f\u81ea CommonPPOConfig\uff0c\u7b80\u5316\u914d\u7f6e\u7ba1\u7406",
      "state": "open"
    },
    {
      "number": 62,
      "title": "Major Updates for Open-Reasoner-Zero",
      "state": "closed"
    },
    {
      "number": 57,
      "title": "Support FSDP and vllm colocate with tp size > 1",
      "state": "open"
    },
    {
      "number": 32,
      "title": "Local ppo mini",
      "state": "closed"
    },
    {
      "number": 31,
      "title": "Add a mini example",
      "state": "closed"
    }
  ],
  "recent_commits": [
    {
      "sha": "3fdd9a07b4fb01e06005e6e74fc56690cde8a341",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:57+00:00",
      "message": "doc: update ORZ-R1-Distill-Qwen-14B results"
    },
    {
      "sha": "a23519277efdd71a063e1f2913d5c496a5f96180",
      "author": "REIGN12",
      "date": "2025-06-02T16:35:25+00:00",
      "message": "chores: update qr code"
    },
    {
      "sha": "20e5bc83ae75695fded4e1a59a21e487063e24d9",
      "author": "REIGN12",
      "date": "2025-04-08T12:58:51+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "9c73e2590d23a7245a8d6341337952828110c631",
      "author": "REIGN12",
      "date": "2025-04-02T03:41:38+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a27ce5c1c5204d267c821542af4bdb47c70aa40a",
      "author": "REIGN12",
      "date": "2025-04-02T03:38:35+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a288753de916c0f3ba932fb0d1c218a49b8a403b",
      "author": "REIGN12",
      "date": "2025-03-31T15:53:39+00:00",
      "message": "README: update readme"
    },
    {
      "sha": "a7193f2c14aa90d055f9c161bf4566ec41ced30c",
      "author": "Jingcheng Hu",
      "date": "2025-03-31T15:45:50+00:00",
      "message": "Major Updates for Open-Reasoner-Zero (#62)"
    },
    {
      "sha": "e008f6d95f0b9a0e992f6b8bac912515b50a4634",
      "author": "REIGN12",
      "date": "2025-03-05T08:08:42+00:00",
      "message": "Fix: bug fix for weight sync logic when colocate=False"
    },
    {
      "sha": "effe6fb81570ccfde7d28d8d335c83127e95263f",
      "author": "REIGN12",
      "date": "2025-03-01T09:03:59+00:00",
      "message": "Fix: fix typo in report"
    },
    {
      "sha": "9464093c67c32a5fca69556b603f915e55fc00a5",
      "author": "REIGN12",
      "date": "2025-03-01T08:54:56+00:00",
      "message": "Fix: minor bugfix for dockerfile"
    },
    {
      "sha": "f594903dc0e953a8d0429ae0ecf339efcbe456f7",
      "author": "Jingcheng Hu",
      "date": "2025-02-25T15:03:35+00:00",
      "message": "Merge pull request #31 from vwxyzjn/ppo-mini"
    },
    {
      "sha": "ce912e108ca03d5ded42e151e3c857d44fc3f277",
      "author": "Costa Huang",
      "date": "2025-02-24T20:33:48+00:00",
      "message": "quick change"
    },
    {
      "sha": "34d8abc52ecba9c1c620350eb70110d05ea6b676",
      "author": "Costa Huang",
      "date": "2025-02-24T20:32:51+00:00",
      "message": "set eval to false"
    },
    {
      "sha": "55904eae7c35932e8ee2efb717c788ed3cd9b75e",
      "author": "Costa Huang",
      "date": "2025-02-24T20:31:44+00:00",
      "message": "quick change"
    },
    {
      "sha": "dbc40d8042c4981138b58773ca4936570e2339be",
      "author": "Costa Huang",
      "date": "2025-02-24T20:28:34+00:00",
      "message": "Add single GPU exampel"
    },
    {
      "sha": "ec2ce7e528f660f3b05c19369cdcff9d89cbb6b0",
      "author": "Costa Huang",
      "date": "2025-02-24T20:06:00+00:00",
      "message": "Add a mini example"
    },
    {
      "sha": "7b5dec17a8dda164e8a4aaba33393afc3e0e95d9",
      "author": "Jingcheng Hu",
      "date": "2025-02-24T09:18:55+00:00",
      "message": "Update README.md"
    },
    {
      "sha": "3ccec9eec8717a5de0575918237bc81b6ba3e3a2",
      "author": "REIGN12",
      "date": "2025-02-24T07:24:11+00:00",
      "message": "Merge branch 'main' of https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero"
    },
    {
      "sha": "df77747d28c8580db768732f8da34f913e0411ca",
      "author": "REIGN12",
      "date": "2025-02-24T07:23:34+00:00",
      "message": "Fix: bug fix for readme and 32b exp"
    },
    {
      "sha": "ec6b01ef61f6b66a30bff0ea763b37fe4e9d00d1",
      "author": "hanqer",
      "date": "2025-02-24T05:52:43+00:00",
      "message": "fix: fix the package dependencies of pip and apt."
    }
  ],
  "readme_text": "<div align=\"center\">\n\n# Open Reasoner Zero\n\n<img src=\"figure/logo.jpg\" width=\"300\"/>\n\n<div>\n\nAn Open Source Approach to Scaling Up Reinforcement Learning on the Base Model\n</div>\n</div>\n\n<div align=\"center\" style=\"line-height: 1;\">\n    <a href=\"https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\" style=\"margin: 2px;\"><img alt=\"Code\" src=\"https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\" style=\"display: inline-block; vertical-align: middle;\"/></a>\n  \n  <a href=\"https://huggingface.co/Open-Reasoner-Zero\" target=\"_blank\"><img alt=\"Hugging Face\"\n    src=\"https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>\n\n  <a href=\"https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\" target=\"_blank\">\n  <img alt=\"Notion Page\"\n    src=\"https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>\n\n  <br>\n  <a href=\"https://arxiv.org/abs/2503.24290\"><b>Paper Arxiv Link </b>\ud83d\udc41\ufe0f</a>\n</div>\n\n<div>\n<br>\n\n</div>\n\n## Overview \ud83c\udf0a\nWe introduce **Open-Reasoner-Zero**, the first open source implementation of large-scale reasoning-oriented RL training focusing on scalability, simplicity and accessibility.\nUsing the same base model as DeepSeek-R1-Zero-Qwen-32B, our implementation achieves superior performance on AIME2024, MATH500, and the GPQA Diamond benchmark while demonstrating remarkable efficiency\u2014requiring only a tenth of the training steps, compared to DeepSeek-R1-Zero pipeline.\n\nTo enable broader participation in this pivotal moment we witnessed and accelerate research towards artificial general intelligence (AGI), \nwe release our source code, parameter settings, training data, and model weights.\nPlease refer to our [paper](https://arxiv.org/abs/2503.24290) for more insights across various model sizes. \n\n**Let the Reasoner-Zero tide rise!**\n\n\n## Main Results \ud83c\udfc6\n\n![](figure/teaser.png)\n\n*Figure 1 | Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\}. Evaluation performance of Open-Reasoner-Zero-\\{7B, 32B\\} on benchmarks (averaged on 16 responses) during training. Using the same base model as DeepSeek-R1-Zero-Qwen-32B, Open-Reasoner-Zero-32B achieves superior performance on AIME2024, MATH500, and GPQA Diamond benchmark-requiring only a tenth of the training steps.*\n\n![](figure/train_curve.png)\n*Figure 2 | Train-time Scale up on Train Reward and Response Length of Open-Reasoner-Zero (ORZ) - \\{0.5B, 1.5B, 7B, 32B\\}. Train Reward and Response Length increase steadily, demonstrating consistent scalability across model sizes. Interestingly, the ORZ-32B Response Length exhibits fluctuations without negatively impacting training stability, highlighting the robustness of our minimalist recipe.*\n\n## Releases \ud83d\udce6\n\n<strong>[2025/06/03]</strong>\nWe release [ORZ-R1-Distill-Qwen-14B](https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B), obtained by applying ORZ recipe to reasoning-enhanced models like DeepSeek-R1-Distill-Qwen-14B. This ORZ-R1-Distill-Qwen-14B achieves strong results on reasoning benchmarks, even surpassing the larger DeepSeek-R1-Distill-Qwen-32B model.\n\n| Model                        | AIME 2024 | AIME 2025 | MATH500 | GPQA Dia. |\n| ---------------------------- | --------- | --------- | ------- | --------- |\n| DeepSeek-R1-Distill-Qwen-14B | 69.7      | 49.1      | 93.9    | 59.1      |\n| DeepSeek-R1-Distill-Qwen-32B | 72.6      | 60.0      | 94.3    | **62.1**     |\n| **ORZ-R1-Distill-Qwen-14B**  | **75.2**  | **60.0**  | **95.6** | 60.4  |\n\n\n<strong>[2025/03/31]</strong>\nWe announce a major milestone for `Open-Reasoner-Zero`:\n\n- \ud83c\udf0a [Updated Paper](https://arxiv.org/abs/2503.24290) with new results.\n- \ud83d\udd2d [Easy-to-use Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground):\n  - [ORZ-1.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py) and [ORZ-0.5B training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py) (main results in Figure 2). \n  - [Minimal resource training scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py): ORZ-0.5B can be run on a single A800/H800 gpu!\n- \ud83e\udd29 [Updated Curated Datasets](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data): \n  - 129k data in total:\n    - [original 57k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json).\n    - [extended 72k data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json).\n  - [13k hard data](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json) mined from the above 129k data. \n    - used in the \"annealing\" stage of ORZ-32B training: **AIME2024 from ~41% to ~48%**!\n- \ud83e\udd17 More HF Models: \n  - Updated HF Models: [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B).\n  - Released HF Models: [`Open-Reasoner-Zero-1.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B) and [`Open-Reasoner-Zero-0.5B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B).\n- \ud83d\ude80 Full Suite of Critic Models for in-depth research: `Open-Reasoner-Zero-Critic-`{[0.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B), [1.5B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B), [7B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B),  [32B](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B)}.\n\n<strong>[2025/02/18]</strong>\nWe release `Open-Reasoner-Zero`. \n\nAs part of this release, we open-source:\n- \ud83c\udf0a [Paper(WIP)](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf) on our comprehensive analysis and insights in Reasoner-Zero training\n- \ud83e\udd17 HF Model [`Open-Reasoner-Zero-7B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B) and [`Open-Reasoner-Zero-32B`](https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B)\n- \ud83c\udf81 [`Our curated 57k training data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data)\n- \ud83d\udcc4 [Training Scripts](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground) to enjoy your own Reasoner-Zero journey!\n\n## Key Features in Codebase \ud83d\udd11\n\n- Adopt single controller trainer design, flexible and researcher-friendly.\n- Colocate training and generation in the same GPUs to maximize GPU utilization.\n\n## Getting Started \ud83d\ude80\n### Data\n\nWe release all of curated high-quality training data in the [`data`](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data) folder:\n* curated 129k data:\n  * [original 57k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json), collected from various sources, including AIME (up to 2023), MATH, Numina-Math collection and Tulu3 MATH.\n  * [extended 72k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json), mainly cleaned from OpenR1-Math-220k.\n* [hard 13k](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json), mined from the first stage of ORZ-32B training.\n\nThe details for how to collect data are described in our [paper](https://arxiv.org/abs/2503.24290).\n\n### Installation & Training Scripts\nWe release our [Dockerfile](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile) in [docker](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker) folder to facilitate the reproducibility of our training.\n\nTo install the package, run:\n```bash\npip install -e .\n```\n\n#### Start ORZ-32B PPO Training\nHere are the starting commands in 16 nodes. \n\nFirst on master node, run:\n```bash\nray start --head\n# you will see logging like:\n# Next steps\n#  To add another node to this Ray cluster, run\n#    ray start --address='<master-node-ip>:<master-node-port>'\n```\n\nthen on all other nodes, run:\n```bash\nray start --address='<master-node-ip>:<master-node-port>' # <master-node-ip> and <master-node-port> are from above loggings!\n```\n\nfinally on master node, just run:\n```bash\npython -m playground.orz_32b_ppo\n```\nYour training log will be shown in the master node terminal.\n\n------\n\n#### Start ORZ-0.5B PPO Training\nYou can start the ORZ-0.5B PPO training in single A800/H800 node:\n```bash\npython -m playground.orz_0p5b_ppo\n```\n\nYou can even run in **a single A800/H800 gpu**: \n```bash\npython -m playground.orz_0p5b_ppo_1gpu\n```\n\nnote: since we are not in multi-node setting, no `ray start` like logics are needed.\n\n------\n\n#### Start ORZ-7B PPO Training\n\nMulti-node Training on 4 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n\n# then on master node, run:\npython -m playground.orz_7b_ppo\n```\n\nYour training log will be shown in the master node terminal.\n\n-----\n\n#### Start ORZ-1.5B PPO Training\n\nMulti-node Training on 2 nodes:\n```bash\n# set up for multi-node training\nray start --head # on master node\nray start --address='<master-node-ip>:<master-node-port>' # then on other nodes\n# then on master node, run:\npython -m playground.orz_1p5b_ppo\n```\n\n----\n\n#### Debug Settings\nIn the code, we leave an environment variable `DEBUG_MODE` to run in debug setting for researcher to iterate. (Thought for now, we recommend using `python -m playground.orz_0p5b_ppo_1gpu` for debugging.)\n\nThe debug running command examples:\n```bash\n# NOTE: just for debug, not final setting!\n\n## Debug command in a single GPU with `EleutherAI/pythia-14m`\nDEBUG_MODE=True python -m playground.orz_14m_ppo_mini\n## Debug command in a single node (8 GPUs) with `Qwen/Qwen2.5-7B`\nDEBUG_MODE=True python -m playground.orz_7b_ppo\n```\n\n### How to Use the Model\n#### Policy Model\nPolicy models can be used in the same way as any chat model in transformers and vllm, since we have put the chat template jinja in the tokenizer.\n\n#### Critic Model\nCritic models can be loaded the same way like in the [training code](https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738). \n\n\n## Acknowledgements \ud83d\udc96 \n\n- This work was supported by computing resources and valuable feedback provided by [StepFun](https://www.stepfun.com/) and Tsinghua University.\n- Our training framework is built on [OpenRLHF](https://github.com/OpenRLHF/OpenRLHF), [vllm](https://github.com/vllm-project/vllm), [DeepSpeed](https://github.com/deepspeedai/DeepSpeed) and [ray](https://github.com/ray-project/ray).\n- Our model is based on [Qwen2.5 Series](https://qwenlm.github.io/blog/qwen2.5-llm/) of **base models**, including [Qwen2.5-0.5B](https://huggingface.co/Qwen/Qwen2.5-0.5B), [Qwen2.5-1.5B](https://huggingface.co/Qwen/Qwen2.5-1.5B), [Qwen2.5-7B](https://huggingface.co/Qwen/Qwen2.5-7B) and [Qwen2.5-32B](https://huggingface.co/Qwen/Qwen2.5-32B).\n- We thank [Project Numina](https://projectnumina.ai/), [Tulu3](https://allenai.org/blog/tulu-3-technical) and [OpenR1-Math-220k](https://huggingface.co/datasets/open-r1/OpenR1-Math-220k) for their collected open sourced data.\n\n## Advertisement Time \ud83d\udce3\n\nWe are hiring talented researchers and engineers to join our team. If you are interested in our project and would like to contribute to the reasoner scale-up all the way to AGI, please feel free to reach out to us at hanqer@stepfun.com\n\n\n[![Star History Chart](https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline)](https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline)\n\n## Community Discussions \ud83c\udf7a\n\nWe have several wechat groups to help discussions and sharing, you can scan the QR code below to join the latest group.\n\n<img src=\"figure/WeChatGroup.png\" width=\"300\" style=\"display: block; margin: 0 auto;\"/>\n\n## Citation\n\n```bibtex\n@misc{hu2025openreasonerzeroopensourceapproach,\n      title={Open-Reasoner-Zero: An Open Source Approach to Scaling Up Reinforcement Learning on the Base Model}, \n      author={Jingcheng Hu and Yinmin Zhang and Qi Han and Daxin Jiang and Xiangyu Zhang and Heung-Yeung Shum},\n      year={2025},\n      eprint={2503.24290},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2503.24290}, \n}\n```\n",
  "external_links_in_readme": [
    "https://github.com/deepspeedai/DeepSpeed",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_13k_collection_hard.json",
    "https://huggingface.co/Qwen/Qwen2.5-0.5B",
    "https://arxiv.org/abs/2503.24290",
    "https://huggingface.co/Open-Reasoner-Zero\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/data",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-1.5B",
    "https://www.stepfun.com/",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/playground",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero\"",
    "https://arxiv.org/abs/2503.24290},",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-0.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo.py",
    "https://github.com/vllm-project/vllm",
    "https://star-history.com/#Open-Reasoner-Zero/Open-Reasoner-Zero&Timeline",
    "https://huggingface.co/datasets/open-r1/OpenR1-Math-220k",
    "https://img.shields.io/badge/HuggingFace-fcd022?style=for-the-badge&logo=huggingface&logoColor=000&labelColor\"/></a>",
    "https://arxiv.org/abs/2503.24290\"><b>Paper",
    "https://github.com/ray-project/ray",
    "https://projectnumina.ai/",
    "https://qwenlm.github.io/blog/qwen2.5-llm/",
    "https://huggingface.co/Qwen/Qwen2.5-7B",
    "https://huggingface.co/Qwen/Qwen2.5-1.5B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-7B",
    "https://yasminezhang.notion.site/Open-Reasoner-Zero-19e12cf72d418007b9cdebf44b0e7903\"",
    "https://github.com/OpenRLHF/OpenRLHF",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/tree/main/docker",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_72k_collection_extended.json",
    "https://huggingface.co/Qwen/Qwen2.5-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-32B",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-1.5B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_1p5b_ppo.py",
    "https://img.shields.io/badge/Notion-%23000000.svg?style=for-the-badge&logo=notion&logoColor=white\"/></a>",
    "https://allenai.org/blog/tulu-3-technical",
    "https://img.shields.io/badge/Open%20Reasoner%20Zero-000000?style=for-the-badge&logo=github&logoColor=000&logoColor=white\"",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/orz/ppo/actors.py#L738",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-Critic-0.5B",
    "https://api.star-history.com/svg?repos=Open-Reasoner-Zero/Open-Reasoner-Zero&type=Timeline",
    "https://huggingface.co/Open-Reasoner-Zero/ORZ-R1-Distill-Qwen-14B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/data/orz_math_57k_collected.json",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-32B",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/ORZ_paper.pdf",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/docker/Dockerfile",
    "https://github.com/Open-Reasoner-Zero/Open-Reasoner-Zero/blob/main/playground/orz_0p5b_ppo_1gpu.py",
    "https://huggingface.co/Open-Reasoner-Zero/Open-Reasoner-Zero-7B"
  ]
}
```

</details>


---

